[
    {
        "repo": "numpy/numpy",
        "pr_number": 23740,
        "body": "Mostly synced from SciPy, and also addresses some warnings that show up with Meson 1.1.0",
        "changed_files": [
            {
                "filename": "building_with_meson.md",
                "patch": "@@ -9,11 +9,11 @@ into a problem._\n \n **Install build tools:** Use one of:\n \n-- `mamba env create -f environment.yml && mamba activate numpy-dev\n+- `mamba env create -f environment.yml && mamba activate numpy-dev`\n \n-- `python -m pip install -r build_requirements.txt\n-  # also make sure you have pkg-config and the usual system dependencies for\n-  # NumPy`\n+- `python -m pip install -r build_requirements.txt`\n+  *Note: also make sure you have `pkg-config` and the usual system dependencies\n+  for NumPy*\n \n Then install spin:\n - `python -m pip install spin`\n@@ -42,10 +42,10 @@ Note that `pip` will use the default build system, which is (as of now) still\n `build-backend = \"mesonpy\"` line at the top of `pyproject.toml`.\n \n After that is done, `pip install .` or `pip install --no-build-isolation .`\n-will work as expected. As does building an sdist or wheel with `python -m build`.\n-Note, however, that `pip install -e .` (in-place developer install) does not!\n-Use `spin` instead (see above).\n-\n+will work as expected. As does building an sdist or wheel with `python -m build`,\n+or `pip install -e . --no-build-isolation` for an editable install.\n+For a more complete developer experience than editable installs, consider using\n+`spin` instead though (see above).\n \n \n ### Workaround for a hiccup on Fedora"
            },
            {
                "filename": "meson.build",
                "patch": "@@ -6,7 +6,7 @@ project(\n   # See `numpy/__init__.py`\n   version: '1.24.0.dev0',\n   license: 'BSD-3',\n-  meson_version: '>= 0.64.0',\n+  meson_version: '>= 1.1.0',\n   default_options: [\n     'buildtype=debugoptimized',\n     'b_ndebug=if-release',\n@@ -22,6 +22,7 @@ fs = import('fs')\n \n cc = meson.get_compiler('c')\n cpp = meson.get_compiler('cpp')\n+cy = meson.get_compiler('cython')\n \n # Check compiler is recent enough (see the SciPy Toolchain Roadmap for details)\n if cc.get_id() == 'gcc'\n@@ -34,16 +35,24 @@ elif cc.get_id() == 'msvc'\n           'when building with MSVC')\n   endif\n endif\n+if not cy.version().version_compare('>=0.29.34')\n+  error('NumPy requires Cython >= 0.29.34')\n+endif\n \n-# https://mesonbuild.com/Python-module.html\n-py_mod = import('python')\n-py = py_mod.find_installation(pure: false)\n+py = import('python').find_installation(pure: false)\n py_dep = py.dependency()\n \n if not cc.has_header('Python.h', dependencies: py_dep)\n   error('Cannot compile `Python.h`. Perhaps you need to install python-dev|python-devel')\n endif\n \n+# Add default compile flags for any compiler that supports them.\n+# Note that MSVC does not support strict aliasing at all, and neither do the\n+# Intel compilers on Windows, so the `-fno` flavor of the flag should be fine.\n+add_project_arguments(\n+  cc.get_supported_arguments( '-fno-strict-aliasing'), language : 'c'\n+)\n+\n # Generate version number. Note that this will not (yet) update the version\n # number seen by pip or reflected in wheel filenames. See\n # https://github.com/mesonbuild/meson-python/issues/159 for that."
            },
            {
                "filename": "meson_options.txt",
                "patch": "@@ -2,12 +2,12 @@ option('blas', type: 'string', value: 'openblas',\n         description: 'option for BLAS library switching')\n option('lapack', type: 'string', value: 'openblas',\n         description: 'option for LAPACK library switching')\n-option('disable-svml', type: 'boolean', value: 'false',\n+option('disable-svml', type: 'boolean', value: false,\n         description: 'Disable building against SVML')\n-option('disable-threading', type: 'boolean', value: 'false',\n+option('disable-threading', type: 'boolean', value: false,\n         description: 'Disable threading support (see `NPY_ALLOW_THREADS` docs)')\n # TODO: flip value to 'false' once we have `npy_cpu_dispatch_config.h` & co.\n-option('disable-simd-optimizations', type: 'boolean', value: 'true',\n+option('disable-simd-optimizations', type: 'boolean', value: true,\n         description: 'Disable SIMD features beyond the baseline ones')\n-option('relaxed-strides-debug', type: 'boolean', value: 'false',\n+option('relaxed-strides-debug', type: 'boolean', value: false,\n         description: 'Enable relaxed strides debug mode (see `NPY_RELAXED_STRIDES_DEBUG` docs)')"
            },
            {
                "filename": "numpy/core/src/umath/override.c",
                "patch": "@@ -315,10 +315,9 @@ PyUFunc_CheckOverride(PyUFuncObject *ufunc, char *method,\n \n     /* Call __array_ufunc__ functions in correct order */\n     while (1) {\n-        PyObject *override_obj;\n-        PyObject *override_array_ufunc;\n+        PyObject *override_obj = NULL;\n+        PyObject *override_array_ufunc = NULL;\n \n-        override_obj = NULL;\n         *result = NULL;\n \n         /* Choose an overriding argument */"
            },
            {
                "filename": "numpy/meson.build",
                "patch": "@@ -11,23 +11,15 @@ endif\n is_windows = host_machine.system() == 'windows'\n is_mingw = is_windows and cc.get_id() == 'gcc'\n \n-if is_windows\n+if is_mingw\n   # For mingw-w64, link statically against the UCRT.\n   gcc_link_args = ['-lucrt', '-static']\n-  if is_mingw\n-    add_project_link_arguments(gcc_link_args, language: ['c', 'cpp'])\n-    # Force gcc to float64 long doubles for compatibility with MSVC\n-    # builds, for C only.\n-    add_project_arguments('-mlong-double-64', language: 'c')\n-    # Make fprintf(\"%zd\") work (see https://github.com/rgommers/scipy/issues/118)\n-    add_project_arguments('-D__USE_MINGW_ANSI_STDIO=1', language: ['c', 'cpp'])\n-    # Manual add of MS_WIN64 macro when not using MSVC.\n-    # https://bugs.python.org/issue28267\n-    bitness = run_command('_build_utils/gcc_build_bitness.py').stdout().strip()\n-    if bitness == '64'\n-      add_project_arguments('-DMS_WIN64', language: ['c', 'cpp'])\n-    endif\n-  endif\n+  add_project_link_arguments(gcc_link_args, language: ['c', 'cpp'])\n+  # Force gcc to float64 long doubles for compatibility with MSVC\n+  # builds, for C only.\n+  add_project_arguments('-mlong-double-64', language: 'c')\n+  # Make fprintf(\"%zd\") work (see https://github.com/rgommers/scipy/issues/118)\n+  add_project_arguments('-D__USE_MINGW_ANSI_STDIO=1', language: ['c', 'cpp'])\n endif\n \n # Enable UNIX large file support on 32-bit systems (64 bit off_t,\n@@ -58,13 +50,27 @@ lapack_name = get_option('lapack')\n # pkg-config uses a lower-case name while CMake uses a capitalized name, so try\n # that too to make the fallback detection with CMake work\n if blas_name == 'openblas'\n-  blas_name = ['openblas', 'OpenBLAS']\n+  blas = dependency(['openblas', 'OpenBLAS'], required: false)\n+else\n+  blas = dependency(blas_name, required: false)\n+endif\n+have_blas = blas.found()\n+if have_blas and blas_name == 'blas'\n+  # Netlib BLAS has a separate `libcblas.so` which we use directly in the g77\n+  # ABI wrappers, so detect it and error out if we cannot find it.\n+  # In the future, this should be done automatically for:\n+  #   `dependency('blas', modules: cblas)`\n+  # see https://github.com/mesonbuild/meson/pull/10921.\n+  cblas = dependency('cblas')\n+else\n+  cblas = []\n endif\n+\n if lapack_name == 'openblas'\n   lapack_name = ['openblas', 'OpenBLAS']\n endif\n-blas = dependency(blas_name, required: false)\n lapack = dependency(lapack_name, required: false)\n+have_lapack = lapack.found()\n \n dependency_map = {\n   'BLAS': blas,\n@@ -74,8 +80,6 @@ dependency_map = {\n # BLAS and LAPACK are optional dependencies for NumPy. We can only use a BLAS\n # which provides a CBLAS interface.\n # TODO: add ILP64 support\n-have_blas = blas.found() # TODO: and blas.has_cblas()\n-have_lapack = lapack.found()\n if have_blas\n   # TODO: this is a shortcut - it needs to be checked rather than used\n   # unconditionally, and then added to the extension modules that need the"
            },
            {
                "filename": "tools/ci/cirrus_macosx_arm64.yml",
                "patch": "@@ -37,9 +37,11 @@ macos_arm64_test_task:\n     python --version\n \n     RUNNER_OS=\"macOS\"\n-    CFLAGS=\"-std=c99 -fno-strict-aliasing\"\n     SDKROOT=/Applications/Xcode-14.0.0.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk\n     \n+    # NOTE: OpenBLAS is not used in this job; if that's done in the future, ensure\n+    # PKG_CONFIG_PATH points to the directory containing the openblas.pc file\n+    # that's installed with the cibw_before_build.sh command.\n     # used for installing OpenBLAS/gfortran\n     bash tools/wheels/cibw_before_build.sh $PWD\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23435,
        "body": "Leverages AVX512 FP16 simd sort from x86-simd-sort to speed up sorting float16 arrays by nearly 3x. \r\n\r\n```\r\n       before           after         ratio\r\n     [094416f7]       [9d1ef15e]\r\n     <main>           <spr-simd-sort>\r\n+     7.67\u00b10.02\u03bcs      12.7\u00b10.01\u03bcs     1.66  bench_function_base.Sort.time_sort('merge', 'int16', ('uniform',))\r\n+     7.67\u00b10.02\u03bcs      12.7\u00b10.02\u03bcs     1.66  bench_function_base.Sort.time_sort('merge', 'int16', ('ordered',))\r\n+      42.5\u00b10.1\u03bcs       69.2\u00b10.3\u03bcs     1.63  bench_function_base.Sort.time_sort('merge', 'uint32', ('sorted_block', 1000))\r\n+     42.5\u00b10.07\u03bcs       69.1\u00b10.7\u03bcs     1.63  bench_function_base.Sort.time_sort('merge', 'int32', ('sorted_block', 1000))\r\n+      78.5\u00b10.3\u03bcs          128\u00b11\u03bcs     1.62  bench_function_base.Sort.time_sort('merge', 'int32', ('sorted_block', 100))\r\n+      78.6\u00b10.2\u03bcs        125\u00b10.5\u03bcs     1.60  bench_function_base.Sort.time_sort('merge', 'uint32', ('sorted_block', 100))\r\n+       102\u00b10.5\u03bcs          150\u00b12\u03bcs     1.47  bench_function_base.Sort.time_sort('merge', 'float16', ('sorted_block', 1000))\r\n+     8.53\u00b10.02\u03bcs       12.1\u00b10.1\u03bcs     1.42  bench_function_base.Sort.time_sort('merge', 'uint32', ('uniform',))\r\n+     8.54\u00b10.02\u03bcs       12.0\u00b10.3\u03bcs     1.41  bench_function_base.Sort.time_sort('merge', 'uint32', ('ordered',))\r\n+     18.1\u00b10.03\u03bcs      23.7\u00b10.06\u03bcs     1.31  bench_function_base.Sort.time_sort('merge', 'float64', ('uniform',))\r\n+     18.1\u00b10.01\u03bcs      23.6\u00b10.05\u03bcs     1.30  bench_function_base.Sort.time_sort('merge', 'float64', ('ordered',))\r\n+       142\u00b10.1\u03bcs        173\u00b10.3\u03bcs     1.22  bench_function_base.Sort.time_sort('merge', 'uint32', ('sorted_block', 10))\r\n+       141\u00b10.3\u03bcs        172\u00b10.5\u03bcs     1.21  bench_function_base.Sort.time_sort('merge', 'int32', ('sorted_block', 10))\r\n+     8.54\u00b10.03\u03bcs       10.3\u00b10.6\u03bcs     1.21  bench_function_base.Sort.time_sort('merge', 'int32', ('ordered',))\r\n+     8.53\u00b10.05\u03bcs       10.3\u00b10.5\u03bcs     1.20  bench_function_base.Sort.time_sort('merge', 'int32', ('uniform',))\r\n+      30.5\u00b10.1\u03bcs       35.9\u00b10.5\u03bcs     1.18  bench_function_base.Sort.time_sort('heap', 'int32', ('uniform',))\r\n+         174\u00b11\u03bcs          199\u00b12\u03bcs     1.14  bench_function_base.Sort.time_sort('merge', 'float16', ('sorted_block', 100))\r\n+       125\u00b10.9\u03bcs        142\u00b10.4\u03bcs     1.13  bench_function_base.Sort.time_sort('merge', 'float64', ('sorted_block', 100))\r\n+       124\u00b10.1\u03bcs        140\u00b10.3\u03bcs     1.13  bench_function_base.Sort.time_sort('merge', 'float32', ('sorted_block', 100))\r\n+       186\u00b10.2\u03bcs        209\u00b10.7\u03bcs     1.12  bench_function_base.Sort.time_sort('merge', 'float32', ('sorted_block', 10))\r\n+      71.0\u00b10.1\u03bcs       79.5\u00b10.1\u03bcs     1.12  bench_function_base.Sort.time_sort('merge', 'float32', ('sorted_block', 1000))\r\n+     73.0\u00b10.07\u03bcs       79.5\u00b10.4\u03bcs     1.09  bench_function_base.Sort.time_sort('merge', 'float64', ('sorted_block', 1000))\r\n+     29.8\u00b10.05\u03bcs       31.6\u00b10.3\u03bcs     1.06  bench_function_base.Sort.time_sort('heap', 'uint32', ('uniform',))\r\n+       720\u00b10.9\u03bcs          765\u00b16\u03bcs     1.06  bench_function_base.Sort.time_sort('heap', 'int32', ('reversed',))\r\n+         869\u00b13\u03bcs          921\u00b16\u03bcs     1.06  bench_function_base.Sort.time_sort('merge', 'float64', ('random',))\r\n+         314\u00b13\u03bcs          332\u00b12\u03bcs     1.06  bench_function_base.Sort.time_sort('merge', 'float16', ('sorted_block', 10))\r\n+      19.4\u00b10.1\u03bcs       20.5\u00b10.2\u03bcs     1.05  bench_function_base.Sort.time_sort('merge', 'float64', ('reversed',))\r\n+         971\u00b14\u03bcs         1.02\u00b10ms     1.05  bench_function_base.Sort.time_sort('heap', 'int16', ('sorted_block', 100))\r\n+        1.09\u00b10ms      1.15\u00b10.01ms     1.05  bench_function_base.Sort.time_sort('heap', 'float64', ('sorted_block', 1000))\r\n-         860\u00b12\u03bcs          819\u00b12\u03bcs     0.95  bench_function_base.Sort.time_sort('heap', 'float64', ('ordered',))\r\n-         977\u00b14\u03bcs          924\u00b12\u03bcs     0.95  bench_function_base.Sort.time_sort('heap', 'int64', ('sorted_block', 1000))\r\n-      18.0\u00b10.7\u03bcs      16.9\u00b10.05\u03bcs     0.94  bench_function_base.Sort.time_sort('merge', 'float32', ('reversed',))\r\n-        70.3\u00b12\u03bcs       65.3\u00b10.2\u03bcs     0.93  bench_function_base.Sort.time_sort('heap', 'float16', ('uniform',))\r\n-      18.1\u00b10.3\u03bcs      16.8\u00b10.07\u03bcs     0.93  bench_function_base.Sort.time_sort('merge', 'float32', ('uniform',))\r\n-      18.1\u00b10.1\u03bcs       16.7\u00b10.1\u03bcs     0.92  bench_function_base.Sort.time_sort('merge', 'float32', ('ordered',))\r\n-         943\u00b14\u03bcs          869\u00b15\u03bcs     0.92  bench_function_base.Sort.time_sort('merge', 'float32', ('random',))\r\n-      36.4\u00b10.5\u03bcs       32.4\u00b10.2\u03bcs     0.89  bench_function_base.Sort.time_sort('heap', 'int16', ('uniform',))\r\n-      55.4\u00b10.4\u03bcs      48.2\u00b10.02\u03bcs     0.87  bench_function_base.Sort.time_sort('merge', 'int64', ('sorted_block', 1000))\r\n-      99.3\u00b10.1\u03bcs       84.7\u00b10.2\u03bcs     0.85  bench_function_base.Sort.time_sort('merge', 'int64', ('sorted_block', 100))\r\n-      38.1\u00b10.1\u03bcs       30.7\u00b10.1\u03bcs     0.81  bench_function_base.Sort.time_sort('heap', 'int64', ('uniform',))\r\n-         187\u00b17\u03bcs        148\u00b10.2\u03bcs     0.79  bench_function_base.Sort.time_sort('merge', 'int64', ('sorted_block', 10))\r\n-     9.09\u00b10.05\u03bcs      6.37\u00b10.07\u03bcs     0.70  bench_function_base.Sort.time_sort('quick', 'float16', ('uniform',))\r\n-     9.20\u00b10.05\u03bcs      6.32\u00b10.05\u03bcs     0.69  bench_function_base.Sort.time_sort('quick', 'float16', ('reversed',))\r\n-       164\u00b10.3\u03bcs      61.9\u00b10.07\u03bcs     0.38  bench_function_base.Sort.time_sort('quick', 'float16', ('sorted_block', 100))\r\n-       172\u00b10.2\u03bcs      63.9\u00b10.05\u03bcs     0.37  bench_function_base.Sort.time_sort('quick', 'float16', ('sorted_block', 1000))\r\n-        170\u00b10.2\u03bcs      63.2\u00b10.02\u03bcs     0.37  bench_function_base.Sort.time_sort('quick', 'float16', ('sorted_block', 10))\r\n-       169\u00b10.4\u03bcs      62.7\u00b10.05\u03bcs     0.37  bench_function_base.Sort.time_sort('quick', 'float16', ('random',))\r\n-       180\u00b10.2\u03bcs      65.8\u00b10.08\u03bcs     0.37  bench_function_base.Sort.time_sort('quick', 'float16', ('ordered',))\r\n```\r\n\r\n\r\n\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/npysort/simd_qsort_16bit.dispatch.cpp",
                "patch": "@@ -1,22 +1,29 @@\n /*@targets\n- * $maxopt $keep_baseline avx512_icl\n+ * $maxopt $keep_baseline avx512_icl avx512_spr\n  */\n // policy $keep_baseline is used to avoid skip building avx512_skx\n // when its part of baseline features (--cpu-baseline), since\n // 'baseline' option isn't specified within targets.\n \n #include \"simd_qsort.hpp\"\n \n-#if defined(NPY_HAVE_AVX512_ICL) && !defined(_MSC_VER)\n+#if defined(NPY_HAVE_AVX512_SPR) && !defined(_MSC_VER)\n+    #include \"x86-simd-sort/src/avx512fp16-16bit-qsort.hpp\"\n+#elif defined(NPY_HAVE_AVX512_ICL) && !defined(_MSC_VER)\n     #include \"x86-simd-sort/src/avx512-16bit-qsort.hpp\"\n #endif\n \n namespace np { namespace qsort_simd {\n \n-#if defined(NPY_HAVE_AVX512_ICL) && !defined(_MSC_VER)\n+#if !defined(_MSC_VER)\n+#if defined(NPY_HAVE_AVX512_ICL) || defined(NPY_HAVE_AVX512_SPR)\n template<> void NPY_CPU_DISPATCH_CURFX(QSort)(Half *arr, intptr_t size)\n {\n+#if defined(NPY_HAVE_AVX512_SPR)\n+    avx512_qsort(reinterpret_cast<_Float16*>(arr), size);\n+#else\n     avx512_qsort_fp16(reinterpret_cast<uint16_t*>(arr), size);\n+#endif\n }\n template<> void NPY_CPU_DISPATCH_CURFX(QSort)(uint16_t *arr, intptr_t size)\n {\n@@ -26,6 +33,7 @@ template<> void NPY_CPU_DISPATCH_CURFX(QSort)(int16_t *arr, intptr_t size)\n {\n     avx512_qsort(arr, size);\n }\n-#endif // NPY_HAVE_AVX512_ICL\n+#endif // NPY_HAVE_AVX512_ICL || SPR\n+#endif // _MSC_VER\n \n }} // namespace np::qsort_simd"
            },
            {
                "filename": "numpy/core/src/npysort/x86-simd-sort",
                "patch": "@@ -1 +1 @@\n-Subproject commit 58501d026a390895f7fd7ebbe0fb7aea55055ad7\n+Subproject commit 1735e86cda95a469357a19ab8984ad8530372e75"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21487,
        "body": "Instead of using a type alias, make npy_half a struct.\r\nAs a consequence, cleanup npy_half usage to always reference function declared in\r\nnumpy/halffloat.h. This avoids vodoo incantation in files that should now\r\nnothing of npy_half internal.\r\n\r\nSome of numpy_half manipulating functions are promoted to inline to avoid\r\nperformance regression.\r\n",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/21487.c_api.rst",
                "patch": "@@ -0,0 +1,19 @@\n+``halffloat.h`` has received several changes\n+--------------------------------------------\n+\n+The ``npy_half`` type is no longer a type alias, but a strong type. This make\n+typing safer as ``npy_half x = 1`` or ``x += 1`` now raise an error.\n+\n+The ``halffloat.h`` header has been updated so that any code interacting with\n+``npy_half`` through ``npy_half_*`` functions should recompile without error.\n+Code that directly manipulate the bit representation of ``npy_half`` should do\n+it explicitly by accessing the ``bits`` field of ``npy_half``. A few functions\n+like ``npy_half_neg`` or ``npy_half_abs`` have been added to avoid this\n+situation.\n+\n+For ABI compatibility, the legacy symbols that interacted with ``npy_half`` are\n+still compiled in, but they are not exposed through the ``halffloat.h`` header.\n+\n+Downstream user that still need the legacy ABI can define the\n+``NPY_USE_LEGACY_HALF`` macro before including ``halffloat.h``. In that case the\n+legacy API is exposed."
            },
            {
                "filename": "numpy/core/include/numpy/halffloat.h",
                "patch": "@@ -8,51 +8,164 @@\n extern \"C\" {\n #endif\n \n+/* since NumPy 1.24, 2022-05, npy_half are represented as a struct instead of a\n+ * type alias.\n+ * To keep ABI compatibility with older version of numpy, npy routines that\n+ * manipulates are renamed using the convention defined in macro NPY_HALF_API.\n+ * Each symbol is then redefined based on that naming scheme to keep client code\n+ * mostly unchanged at API level.\n+ *\n+ * If for any reason, the legacy API must be used, define NPY_USE_LEGACY_HALF\n+ * before including this file. This behavior is not expected for internal Numpy\n+ * code.\n+ */\n+\n+#define NPY_HALF_LEGACY_API(name) npy_##name\n+\n+#ifdef NPY_USE_LEGACY_HALF\n+\n+#define NPY_HALF_API(name) NPY_HALF_LEGACY_API(name)\n+#define NPY_HALF_BITS(v) v\n+#define NPY_HALF_BUILD(v) v\n+#define NPY_HALF_INIT(v) v\n+\n+#else\n+\n+#define NPY_HALF_API(name) npy_strongly_typed_##name\n+#define NPY_HALF_BITS(v) ((v).bits)\n+#define NPY_HALF_BUILD(v) ((npy_half){v})\n+#define NPY_HALF_INIT(v) {v}\n+\n+#endif\n+\n /*\n  * Half-precision routines\n  */\n \n /* Conversions */\n+#define npy_half_to_float NPY_HALF_API(half_to_float)\n float npy_half_to_float(npy_half h);\n+\n+#define npy_half_to_double NPY_HALF_API(half_to_double)\n double npy_half_to_double(npy_half h);\n+\n+#define npy_float_to_half NPY_HALF_API(float_to_half)\n npy_half npy_float_to_half(float f);\n+\n+#define npy_double_to_half NPY_HALF_API(double_to_half)\n npy_half npy_double_to_half(double d);\n+\n /* Comparisons */\n+#define npy_half_eq NPY_HALF_API(half_eq)\n int npy_half_eq(npy_half h1, npy_half h2);\n+\n+#define npy_half_ne NPY_HALF_API(half_ne)\n int npy_half_ne(npy_half h1, npy_half h2);\n+\n+#define npy_half_le NPY_HALF_API(half_le)\n int npy_half_le(npy_half h1, npy_half h2);\n+\n+#define npy_half_lt NPY_HALF_API(half_lt)\n int npy_half_lt(npy_half h1, npy_half h2);\n+\n+#define npy_half_ge NPY_HALF_API(half_ge)\n int npy_half_ge(npy_half h1, npy_half h2);\n+\n+#define npy_half_gt NPY_HALF_API(half_gt)\n int npy_half_gt(npy_half h1, npy_half h2);\n+\n /* faster *_nonan variants for when you know h1 and h2 are not NaN */\n+#define npy_half_eq_nonan NPY_HALF_API(half_eq_nonan)\n int npy_half_eq_nonan(npy_half h1, npy_half h2);\n+\n+#define npy_half_lt_nonan NPY_HALF_API(half_lt_nonan)\n int npy_half_lt_nonan(npy_half h1, npy_half h2);\n+\n+#define npy_half_le_nonan NPY_HALF_API(half_le_nonan)\n int npy_half_le_nonan(npy_half h1, npy_half h2);\n+\n /* Miscellaneous functions */\n-int npy_half_iszero(npy_half h);\n-int npy_half_isnan(npy_half h);\n-int npy_half_isinf(npy_half h);\n-int npy_half_isfinite(npy_half h);\n-int npy_half_signbit(npy_half h);\n+#define npy_half_copysign NPY_HALF_API(half_copysign)\n npy_half npy_half_copysign(npy_half x, npy_half y);\n+\n+#define npy_half_spacing NPY_HALF_API(half_spacing)\n npy_half npy_half_spacing(npy_half h);\n+\n+#define npy_half_nextafter NPY_HALF_API(half_nextafter)\n npy_half npy_half_nextafter(npy_half x, npy_half y);\n+\n+#define npy_half_divmod NPY_HALF_API(half_divmod)\n npy_half npy_half_divmod(npy_half x, npy_half y, npy_half *modulus);\n \n+#ifdef NPY_USE_LEGACY_HALF\n+\n+int NPY_HALF_API(half_iszero)(npy_half_bits_t h);\n+int NPY_HALF_API(half_isnan)(npy_half_bits_t h);\n+int NPY_HALF_API(half_isinf)(npy_half_bits_t h);\n+int NPY_HALF_API(half_isfinite)(npy_half_bits_t h);\n+int NPY_HALF_API(half_signbit)(npy_half_bits_t h);\n+\n+#else\n+\n+#define npy_half_iszero NPY_HALF_API(half_iszero)\n+NPY_INLINE int npy_half_iszero(npy_half h) {\n+  return (NPY_HALF_BITS(h)&0x7fff) == 0;\n+}\n+\n+#define npy_half_isnan NPY_HALF_API(half_isnan)\n+NPY_INLINE int npy_half_isnan(npy_half h) {\n+    return ((NPY_HALF_BITS(h)&0x7c00u) == 0x7c00u) && ((NPY_HALF_BITS(h)&0x03ffu) != 0x0000u);\n+}\n+\n+#define npy_half_isinf NPY_HALF_API(half_isinf)\n+NPY_INLINE int npy_half_isinf(npy_half h) {\n+  return ((NPY_HALF_BITS(h)&0x7fffu) == 0x7c00u);\n+}\n+\n+#define npy_half_isfinite NPY_HALF_API(half_isfinite)\n+NPY_INLINE int npy_half_isfinite(npy_half h) {\n+  return (NPY_HALF_BITS(h)&0x7c00u) != 0x7c00u;\n+}\n+\n+#define npy_half_signbit NPY_HALF_API(half_signbit)\n+NPY_INLINE int npy_half_signbit(npy_half h) {\n+  return (NPY_HALF_BITS(h)&0x8000u) != 0;\n+}\n+\n+#define npy_half_neg NPY_HALF_API(half_neg)\n+NPY_INLINE npy_half npy_half_neg(npy_half h) {\n+  npy_half res = NPY_HALF_INIT((npy_uint16)(NPY_HALF_BITS(h)^0x8000u));\n+  return res;\n+}\n+\n+#define npy_half_abs NPY_HALF_API(half_abs)\n+NPY_INLINE npy_half npy_half_abs(npy_half h) {\n+  npy_half res = NPY_HALF_INIT((npy_uint16)(NPY_HALF_BITS(h)&0x7fffu));\n+  return res;\n+}\n+\n+#define npy_half_pos NPY_HALF_API(half_pos)\n+NPY_INLINE npy_half npy_half_pos(npy_half h) {\n+  npy_half res = NPY_HALF_INIT((npy_uint16)(+NPY_HALF_BITS(h)));\n+  return res;\n+}\n+\n+#endif\n+\n /*\n  * Half-precision constants\n  */\n \n-#define NPY_HALF_ZERO   (0x0000u)\n-#define NPY_HALF_PZERO  (0x0000u)\n-#define NPY_HALF_NZERO  (0x8000u)\n-#define NPY_HALF_ONE    (0x3c00u)\n-#define NPY_HALF_NEGONE (0xbc00u)\n-#define NPY_HALF_PINF   (0x7c00u)\n-#define NPY_HALF_NINF   (0xfc00u)\n-#define NPY_HALF_NAN    (0x7e00u)\n+#define NPY_HALF_ZERO   NPY_HALF_BUILD(0x0000u)\n+#define NPY_HALF_PZERO  NPY_HALF_BUILD(0x0000u)\n+#define NPY_HALF_NZERO  NPY_HALF_BUILD(0x8000u)\n+#define NPY_HALF_ONE    NPY_HALF_BUILD(0x3c00u)\n+#define NPY_HALF_NEGONE NPY_HALF_BUILD(0xbc00u)\n+#define NPY_HALF_PINF   NPY_HALF_BUILD(0x7c00u)\n+#define NPY_HALF_NINF   NPY_HALF_BUILD(0xfc00u)\n+#define NPY_HALF_NAN    NPY_HALF_BUILD(0x7e00u)\n \n-#define NPY_MAX_HALF    (0x7bffu)\n+#define NPY_MAX_HALF    NPY_HALF_BUILD(0x7bffu)\n \n /*\n  * Bit-level conversions\n@@ -67,4 +180,5 @@ npy_uint64 npy_halfbits_to_doublebits(npy_uint16 h);\n }\n #endif\n \n+\n #endif  /* NUMPY_CORE_INCLUDE_NUMPY_HALFFLOAT_H_ */"
            },
            {
                "filename": "numpy/core/include/numpy/npy_common.h",
                "patch": "@@ -1030,7 +1030,12 @@ typedef struct { npy_longdouble real, imag; } npy_clongdouble;\n \n /* half/float16 isn't a floating-point type in C */\n #define NPY_FLOAT16 NPY_HALF\n-typedef npy_uint16 npy_half;\n+typedef npy_uint16 npy_half_bits_t;\n+#ifdef NPY_USE_LEGACY_HALF\n+typedef npy_half_bits_t npy_half;\n+#else\n+typedef struct npy_half { npy_half_bits_t bits;} npy_half;\n+#endif\n typedef npy_half npy_float16;\n \n #if NPY_BITSOF_LONGDOUBLE == 32"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1159,6 +1159,14 @@ def generate_umath_doc_header(ext, build_dir):\n     config.add_extension('_struct_ufunc_tests',\n                     sources=[join('src', 'umath', '_struct_ufunc_tests.c')])\n \n+    #######################################################################\n+    #                        half_legacy_test module                      #\n+    #######################################################################\n+\n+    config.add_extension('_half_legacy_tests',\n+                    sources=[join('src', 'npymath', '_half_legacy_tests.c')],\n+                    libraries=['npymath'])\n+\n \n     #######################################################################\n     #                        operand_flag_tests module                    #"
            },
            {
                "filename": "numpy/core/src/multiarray/arraytypes.c.src",
                "patch": "@@ -219,11 +219,11 @@ static PyObject *\n \n     if ((ap == NULL) || PyArray_ISBEHAVED_RO(ap)) {\n         t1 = *((@type@ *)ip);\n-        return @func1@((@type1@)t1);\n+        return @func1@(t1);\n     }\n     else {\n         PyArray_DESCR(ap)->f->copyswap(&t1, ip, PyArray_ISBYTESWAPPED(ap), ap);\n-        return @func1@((@type1@)t1);\n+        return @func1@(t1);\n     }\n }\n \n@@ -237,7 +237,7 @@ static int\n         temp = PyArrayScalar_VAL(op, @kind@);\n     }\n     else {\n-        temp = (@type@)@func2@(op);\n+        temp = @func2@(op);\n     }\n     if (PyErr_Occurred()) {\n         PyObject *type, *value, *traceback;\n@@ -1252,7 +1252,7 @@ static void\n     npy_half *op = output;\n \n     while (n--) {\n-        *op++ = npy_@name@bits_to_halfbits(*ip);\n+        *op++ = (npy_half){npy_@name@bits_to_halfbits(*ip)};\n #if @iscomplex@\n         ip += 2;\n #else\n@@ -1269,7 +1269,7 @@ HALF_to_@TYPE@(void *input, void *output, npy_intp n,\n     @itype@ *op = output;\n \n     while (n--) {\n-        *op++ = npy_halfbits_to_@name@bits(*ip++);\n+        *op++ = npy_halfbits_to_@name@bits(NPY_HALF_BITS(*ip++));\n #if @iscomplex@\n         *op++ = 0;\n #endif\n@@ -1383,7 +1383,7 @@ BOOL_to_@TOTYPE@(void *input, void *output, npy_intp n,\n     @totype@ *op = output;\n \n     while (n--) {\n-        *op++ = (@totype@)((*ip++ != NPY_FALSE) ? @one@ : @zero@);\n+        *op++ = (*ip++ != NPY_FALSE) ? @one@ : @zero@;\n     }\n }\n /**end repeat**/"
            },
            {
                "filename": "numpy/core/src/multiarray/dragon4.c",
                "patch": "@@ -2216,7 +2216,7 @@ Dragon4_PrintFloat_IEEE_binary16(\n     const npy_uint32 bufferSize = sizeof(scratch->repr);\n     BigInt *bigints = scratch->bigints;\n \n-    npy_uint16 val = *value;\n+    npy_uint16 bits = value->bits;\n     npy_uint32 floatExponent, floatMantissa, floatSign;\n \n     npy_uint32 mantissa;\n@@ -2226,9 +2226,9 @@ Dragon4_PrintFloat_IEEE_binary16(\n     char signbit = '\\0';\n \n     /* deconstruct the floating point value */\n-    floatMantissa = val & bitmask_u32(10);\n-    floatExponent = (val >> 10) & bitmask_u32(5);\n-    floatSign = val >> 15;\n+    floatMantissa = bits & bitmask_u32(10);\n+    floatExponent = (bits >> 10) & bitmask_u32(5);\n+    floatSign = bits >> 15;\n \n     /* output the sign */\n     if (floatSign != 0) {"
            },
            {
                "filename": "numpy/core/src/multiarray/lowlevel_strided_loops.c.src",
                "patch": "@@ -802,9 +802,9 @@ NPY_NO_EXPORT PyArrayMethod_StridedLoop *\n #if @is_half1@\n \n #  if @is_float2@\n-#    define _CONVERT_FN(x) npy_halfbits_to_floatbits(x)\n+#    define _CONVERT_FN(x) npy_halfbits_to_floatbits(NPY_HALF_BITS(x))\n #  elif @is_double2@\n-#    define _CONVERT_FN(x) npy_halfbits_to_doublebits(x)\n+#    define _CONVERT_FN(x) npy_halfbits_to_doublebits(NPY_HALF_BITS(x))\n #  elif @is_half2@\n #    define _CONVERT_FN(x) (x)\n #  elif @is_bool2@\n@@ -816,9 +816,9 @@ NPY_NO_EXPORT PyArrayMethod_StridedLoop *\n #elif @is_half2@\n \n #  if @is_float1@\n-#    define _CONVERT_FN(x) npy_floatbits_to_halfbits(x)\n+#    define _CONVERT_FN(x) (npy_half){npy_floatbits_to_halfbits(x)}\n #  elif @is_double1@\n-#    define _CONVERT_FN(x) npy_doublebits_to_halfbits(x)\n+#    define _CONVERT_FN(x) (npy_half){npy_doublebits_to_halfbits(x)}\n #  elif @is_half1@\n #    define _CONVERT_FN(x) (x)\n #  elif @is_bool1@"
            },
            {
                "filename": "numpy/core/src/npymath/_half_legacy_tests.c",
                "patch": "@@ -0,0 +1,82 @@\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#define NPY_USE_LEGACY_HALF\n+#include \"numpy/halffloat.h\"\n+\n+\n+static PyMethodDef TestMethods[] = {\n+        {NULL, NULL, 0, NULL}\n+};\n+\n+\n+static void\n+check_legacy_symbols(void)\n+{\n+  void *funcs[] = {(void *)&npy_half_to_float,\n+                   (void *)&npy_half_to_double,\n+                   (void *)&npy_float_to_half,\n+                   (void *)&npy_double_to_half,\n+                   (void *)&npy_half_eq,\n+                   (void *)&npy_half_ne,\n+                   (void *)&npy_half_le,\n+                   (void *)&npy_half_lt,\n+                   (void *)&npy_half_ge,\n+                   (void *)&npy_half_gt,\n+                   (void *)&npy_half_eq_nonan,\n+                   (void *)&npy_half_lt_nonan,\n+                   (void *)&npy_half_le_nonan,\n+                   (void *)&npy_half_iszero,\n+                   (void *)&npy_half_isnan,\n+                   (void *)&npy_half_isinf,\n+                   (void *)&npy_half_isfinite,\n+                   (void *)&npy_half_signbit,\n+                   (void *)&npy_half_copysign,\n+                   (void *)&npy_half_spacing,\n+                   (void *)&npy_half_nextafter,\n+                   (void *)&npy_half_divmod,\n+                   NULL};\n+  // Flagged as a volatile pointer to make sure the pointer is not dismissed,\n+  // preventing the compiler to *not* link with the above symbols.\n+  void **volatile checker = funcs;\n+  while (*checker)\n+    ++checker;\n+}\n+\n+\n+static struct PyModuleDef moduledef = {\n+    PyModuleDef_HEAD_INIT,\n+    \"_half_legacy_tests\",\n+    NULL,\n+    -1,\n+    TestMethods,\n+    NULL,\n+    NULL,\n+    NULL,\n+    NULL\n+};\n+\n+PyMODINIT_FUNC PyInit__half_legacy_tests(void)\n+{\n+    PyObject *m = PyModule_Create(&moduledef);\n+    if (!m) {\n+        goto fail;\n+    }\n+\n+    check_legacy_symbols();\n+\n+    return m;\n+\n+fail:\n+    if (!PyErr_Occurred()) {\n+        PyErr_SetString(PyExc_RuntimeError,\n+                        \"cannot load _half_legacy_tests module.\");\n+    }\n+    if (m) {\n+        Py_DECREF(m);\n+        m = NULL;\n+    }\n+    return m;\n+}"
            },
            {
                "filename": "numpy/core/src/npymath/halffloat.c",
                "patch": "@@ -2,6 +2,10 @@\n \n #include \"numpy/halffloat.h\"\n \n+#ifdef NPY_USE_LEGACY_HALF\n+#error halffloat.c should not be compiled with legacy API set\n+#endif\n+\n /*\n  * This chooses between 'ties to even' and 'ties away from zero'.\n  */\n@@ -14,6 +18,18 @@\n #define NPY_HALF_GENERATE_UNDERFLOW 1\n #define NPY_HALF_GENERATE_INVALID 1\n \n+/* Force export of inline symbols. Those symbols are set inline for performance\n+ * reason, but we still want them as part of the ABI.\n+ */\n+extern int npy_half_iszero(npy_half h);\n+extern int npy_half_isnan(npy_half h);\n+extern int npy_half_isinf(npy_half h);\n+extern int npy_half_isfinite(npy_half h);\n+extern int npy_half_signbit(npy_half h);\n+extern npy_half npy_half_neg(npy_half h);\n+extern npy_half npy_half_abs(npy_half h);\n+extern npy_half npy_half_pos(npy_half h);\n+\n /*\n  ********************************************************************\n  *                   HALF-PRECISION ROUTINES                        *\n@@ -23,93 +39,68 @@\n float npy_half_to_float(npy_half h)\n {\n     union { float ret; npy_uint32 retbits; } conv;\n-    conv.retbits = npy_halfbits_to_floatbits(h);\n+    conv.retbits = npy_halfbits_to_floatbits(h.bits);\n     return conv.ret;\n }\n \n double npy_half_to_double(npy_half h)\n {\n     union { double ret; npy_uint64 retbits; } conv;\n-    conv.retbits = npy_halfbits_to_doublebits(h);\n+    conv.retbits = npy_halfbits_to_doublebits(h.bits);\n     return conv.ret;\n }\n \n npy_half npy_float_to_half(float f)\n {\n     union { float f; npy_uint32 fbits; } conv;\n     conv.f = f;\n-    return npy_floatbits_to_halfbits(conv.fbits);\n+    return (npy_half){npy_floatbits_to_halfbits(conv.fbits)};\n }\n \n npy_half npy_double_to_half(double d)\n {\n     union { double d; npy_uint64 dbits; } conv;\n     conv.d = d;\n-    return npy_doublebits_to_halfbits(conv.dbits);\n-}\n-\n-int npy_half_iszero(npy_half h)\n-{\n-    return (h&0x7fff) == 0;\n-}\n-\n-int npy_half_isnan(npy_half h)\n-{\n-    return ((h&0x7c00u) == 0x7c00u) && ((h&0x03ffu) != 0x0000u);\n-}\n-\n-int npy_half_isinf(npy_half h)\n-{\n-    return ((h&0x7fffu) == 0x7c00u);\n-}\n-\n-int npy_half_isfinite(npy_half h)\n-{\n-    return ((h&0x7c00u) != 0x7c00u);\n-}\n-\n-int npy_half_signbit(npy_half h)\n-{\n-    return (h&0x8000u) != 0;\n+    return (npy_half){npy_doublebits_to_halfbits(conv.dbits)};\n }\n \n npy_half npy_half_spacing(npy_half h)\n {\n     npy_half ret;\n-    npy_uint16 h_exp = h&0x7c00u;\n-    npy_uint16 h_sig = h&0x03ffu;\n+    npy_uint16 h_exp = h.bits&0x7c00u;\n+    npy_uint16 h_sig = h.bits&0x03ffu;\n     if (h_exp == 0x7c00u) {\n #if NPY_HALF_GENERATE_INVALID\n         npy_set_floatstatus_invalid();\n #endif\n         ret = NPY_HALF_NAN;\n-    } else if (h == 0x7bffu) {\n+    } else if (h.bits == 0x7bffu) {\n #if NPY_HALF_GENERATE_OVERFLOW\n         npy_set_floatstatus_overflow();\n #endif\n         ret = NPY_HALF_PINF;\n-    } else if ((h&0x8000u) && h_sig == 0) { /* Negative boundary case */\n+    } else if ((h.bits&0x8000u) && h_sig == 0) { /* Negative boundary case */\n         if (h_exp > 0x2c00u) { /* If result is normalized */\n-            ret = h_exp - 0x2c00u;\n+            ret = (npy_half){h_exp - 0x2c00u};\n         } else if(h_exp > 0x0400u) { /* The result is a subnormal, but not the smallest */\n-            ret = 1 << ((h_exp >> 10) - 2);\n+            ret = (npy_half){1 << ((h_exp >> 10) - 2)};\n         } else {\n-            ret = 0x0001u; /* Smallest subnormal half */\n+            ret = (npy_half){0x0001u}; /* Smallest subnormal half */\n         }\n     } else if (h_exp > 0x2800u) { /* If result is still normalized */\n-        ret = h_exp - 0x2800u;\n+        ret = (npy_half){h_exp - 0x2800u};\n     } else if (h_exp > 0x0400u) { /* The result is a subnormal, but not the smallest */\n-        ret = 1 << ((h_exp >> 10) - 1);\n+        ret = (npy_half){1 << ((h_exp >> 10) - 1)};\n     } else {\n-        ret = 0x0001u;\n+        ret = (npy_half){0x0001u};\n     }\n \n     return ret;\n }\n \n npy_half npy_half_copysign(npy_half x, npy_half y)\n {\n-    return (x&0x7fffu) | (y&0x8000u);\n+    return (npy_half){(x.bits&0x7fffu) | (y.bits&0x8000u)};\n }\n \n npy_half npy_half_nextafter(npy_half x, npy_half y)\n@@ -121,18 +112,18 @@ npy_half npy_half_nextafter(npy_half x, npy_half y)\n     } else if (npy_half_eq_nonan(x, y)) {\n         ret = x;\n     } else if (npy_half_iszero(x)) {\n-        ret = (y&0x8000u) + 1; /* Smallest subnormal half */\n-    } else if (!(x&0x8000u)) { /* x > 0 */\n-        if ((npy_int16)x > (npy_int16)y) { /* x > y */\n-            ret = x-1;\n+        ret = (npy_half){(y.bits&0x8000u) + 1}; /* Smallest subnormal half */\n+    } else if (!(x.bits&0x8000u)) { /* x > 0 */\n+        if ((npy_int16)x.bits > (npy_int16)y.bits) { /* x > y */\n+            ret = (npy_half){x.bits-1};\n         } else {\n-            ret = x+1;\n+            ret = (npy_half){x.bits+1};\n         }\n     } else {\n-        if (!(y&0x8000u) || (x&0x7fffu) > (y&0x7fffu)) { /* x < y */\n-            ret = x-1;\n+        if (!(y.bits&0x8000u) || (x.bits&0x7fffu) > (y.bits&0x7fffu)) { /* x < y */\n+            ret = (npy_half){x.bits-1};\n         } else {\n-            ret = x+1;\n+            ret = (npy_half){x.bits+1};\n         }\n     }\n #if NPY_HALF_GENERATE_OVERFLOW\n@@ -146,7 +137,7 @@ npy_half npy_half_nextafter(npy_half x, npy_half y)\n \n int npy_half_eq_nonan(npy_half h1, npy_half h2)\n {\n-    return (h1 == h2 || ((h1 | h2) & 0x7fff) == 0);\n+    return (h1.bits == h2.bits || ((h1.bits | h2.bits) & 0x7fff) == 0);\n }\n \n int npy_half_eq(npy_half h1, npy_half h2)\n@@ -158,7 +149,7 @@ int npy_half_eq(npy_half h1, npy_half h2)\n      *   - If the values are both signed zeros, equal.\n      */\n     return (!npy_half_isnan(h1) && !npy_half_isnan(h2)) &&\n-           (h1 == h2 || ((h1 | h2) & 0x7fff) == 0);\n+           (h1.bits == h2.bits || ((h1.bits | h2.bits) & 0x7fff) == 0);\n }\n \n int npy_half_ne(npy_half h1, npy_half h2)\n@@ -168,18 +159,18 @@ int npy_half_ne(npy_half h1, npy_half h2)\n \n int npy_half_lt_nonan(npy_half h1, npy_half h2)\n {\n-    if (h1&0x8000u) {\n-        if (h2&0x8000u) {\n-            return (h1&0x7fffu) > (h2&0x7fffu);\n+    if (h1.bits&0x8000u) {\n+        if (h2.bits&0x8000u) {\n+            return (h1.bits&0x7fffu) > (h2.bits&0x7fffu);\n         } else {\n             /* Signed zeros are equal, have to check for it */\n-            return (h1 != 0x8000u) || (h2 != 0x0000u);\n+            return (h1.bits != 0x8000u) || (h2.bits != 0x0000u);\n         }\n     } else {\n-        if (h2&0x8000u) {\n+        if (h2.bits&0x8000u) {\n             return 0;\n         } else {\n-            return (h1&0x7fffu) < (h2&0x7fffu);\n+            return (h1.bits&0x7fffu) < (h2.bits&0x7fffu);\n         }\n     }\n }\n@@ -196,18 +187,18 @@ int npy_half_gt(npy_half h1, npy_half h2)\n \n int npy_half_le_nonan(npy_half h1, npy_half h2)\n {\n-    if (h1&0x8000u) {\n-        if (h2&0x8000u) {\n-            return (h1&0x7fffu) >= (h2&0x7fffu);\n+    if (h1.bits&0x8000u) {\n+        if (h2.bits&0x8000u) {\n+            return (h1.bits&0x7fffu) >= (h2.bits&0x7fffu);\n         } else {\n             return 1;\n         }\n     } else {\n-        if (h2&0x8000u) {\n+        if (h2.bits&0x8000u) {\n             /* Signed zeros are equal, have to check for it */\n-            return (h1 == 0x0000u) && (h2 == 0x8000u);\n+            return (h1.bits == 0x0000u) && (h2.bits == 0x8000u);\n         } else {\n-            return (h1&0x7fffu) <= (h2&0x7fffu);\n+            return (h1.bits&0x7fffu) <= (h2.bits&0x7fffu);\n         }\n     }\n }\n@@ -553,3 +544,125 @@ npy_uint64 npy_halfbits_to_doublebits(npy_uint16 h)\n             return d_sgn + (((npy_uint64)(h&0x7fffu) + 0xfc000u) << 42);\n     }\n }\n+\n+/*\n+ * Legacy API, kept for ABI Compatibility since NumPy 1.24, 2022-05.\n+ * npy_half used ito be represented by a type alias to npy_uint16 before it got\n+ * encapsulated in a struct.\n+ */\n+\n+#undef npy_half_to_float\n+#undef npy_half_to_double\n+#undef npy_float_to_half\n+#undef npy_double_to_half\n+#undef npy_half_eq\n+#undef npy_half_ne\n+#undef npy_half_le\n+#undef npy_half_lt\n+#undef npy_half_ge\n+#undef npy_half_gt\n+#undef npy_half_eq_nonan\n+#undef npy_half_lt_nonan\n+#undef npy_half_le_nonan\n+#undef npy_half_copysign\n+#undef npy_half_spacing\n+#undef npy_half_nextafter\n+#undef npy_half_divmod\n+#undef npy_half_iszero\n+#undef npy_half_isnan\n+#undef npy_half_isinf\n+#undef npy_half_isfinite\n+#undef npy_half_signbit\n+#undef npy_half_neg\n+#undef npy_half_abs\n+#undef npy_half_pos\n+\n+float NPY_HALF_LEGACY_API(half_to_float)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_to_float)((npy_half){h});\n+}\n+\n+double NPY_HALF_LEGACY_API(half_to_double)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_to_double)((npy_half){h});\n+}\n+\n+npy_half_bits_t NPY_HALF_LEGACY_API(float_to_half)(float f) {\n+  return NPY_HALF_API(float_to_half)(f).bits;\n+}\n+\n+npy_half_bits_t NPY_HALF_LEGACY_API(double_to_half)(double d) {\n+  return NPY_HALF_API(double_to_half)(d).bits;\n+}\n+\n+int NPY_HALF_LEGACY_API(half_eq)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_eq)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_ne)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_ne)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_le)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_le)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_lt)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_lt)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_ge)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_ge)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_gt)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_gt)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_eq_nonan)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_eq_nonan)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_lt_nonan)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_lt_nonan)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_le_nonan)(npy_half_bits_t h1, npy_half_bits_t h2) {\n+  return NPY_HALF_API(half_le_nonan)((npy_half){h1}, (npy_half){h2});\n+}\n+\n+npy_half_bits_t NPY_HALF_LEGACY_API(half_copysign)(npy_half_bits_t x, npy_half_bits_t y) {\n+  return NPY_HALF_API(half_copysign)((npy_half){x}, (npy_half){y}).bits;\n+}\n+\n+npy_half_bits_t NPY_HALF_LEGACY_API(half_spacing)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_spacing)((npy_half){h}).bits;\n+}\n+npy_half_bits_t NPY_HALF_LEGACY_API(half_nextafter)(npy_half_bits_t x, npy_half_bits_t y) {\n+  return NPY_HALF_API(half_nextafter)((npy_half){x}, (npy_half){y}).bits;\n+}\n+\n+npy_half_bits_t NPY_HALF_LEGACY_API(half_divmod)(npy_half_bits_t x, npy_half_bits_t y, npy_half_bits_t *modulus) {\n+  npy_half mod;\n+  npy_half res = NPY_HALF_API(half_divmod)((npy_half){x}, (npy_half){y}, &mod);\n+  *modulus = mod.bits;\n+  return res.bits;\n+}\n+\n+int NPY_HALF_LEGACY_API(half_iszero)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_iszero)((npy_half){h});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_isnan)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_isnan)((npy_half){h});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_isinf)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_isinf)((npy_half){h});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_isfinite)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_isfinite)((npy_half){h});\n+}\n+\n+int NPY_HALF_LEGACY_API(half_signbit)(npy_half_bits_t h) {\n+  return NPY_HALF_API(half_signbit)((npy_half){h});\n+}"
            },
            {
                "filename": "numpy/core/src/npysort/binsearch.cpp",
                "patch": "@@ -13,12 +13,12 @@\n #include <functional>  // for std::less and std::less_equal\n \n // Enumerators for the variant of binsearch\n-enum arg_t\n+enum class arg_t\n {\n     noarg,\n     arg\n };\n-enum side_t\n+enum class side_t\n {\n     left,\n     right\n@@ -29,25 +29,25 @@ template <class Tag, side_t side>\n struct side_to_cmp;\n \n template <class Tag>\n-struct side_to_cmp<Tag, left> {\n+struct side_to_cmp<Tag, side_t::left> {\n     static constexpr auto value = Tag::less;\n };\n \n template <class Tag>\n-struct side_to_cmp<Tag, right> {\n+struct side_to_cmp<Tag, side_t::right> {\n     static constexpr auto value = Tag::less_equal;\n };\n \n template <side_t side>\n struct side_to_generic_cmp;\n \n template <>\n-struct side_to_generic_cmp<left> {\n+struct side_to_generic_cmp<side_t::left> {\n     using type = std::less<int>;\n };\n \n template <>\n-struct side_to_generic_cmp<right> {\n+struct side_to_generic_cmp<side_t::right> {\n     using type = std::less_equal<int>;\n };\n \n@@ -273,7 +273,7 @@ template <arg_t arg>\n struct binsearch_base;\n \n template <>\n-struct binsearch_base<arg> {\n+struct binsearch_base<arg_t::arg> {\n     using function_type = PyArray_ArgBinSearchFunc *;\n     struct value_type {\n         int typenum;\n@@ -285,18 +285,18 @@ struct binsearch_base<arg> {\n     {\n         return std::array<value_type, sizeof...(Tags)>{\n                 value_type{Tags::type_value,\n-                           {(function_type)&argbinsearch<Tags, left>,\n-                            (function_type)argbinsearch<Tags, right>}}...};\n+                           {(function_type)&argbinsearch<Tags, side_t::left>,\n+                            (function_type)argbinsearch<Tags, side_t::right>}}...};\n     }\n     static constexpr std::array<function_type, 2> npy_map = {\n-            (function_type)&npy_argbinsearch<left>,\n-            (function_type)&npy_argbinsearch<right>};\n+            (function_type)&npy_argbinsearch<side_t::left>,\n+            (function_type)&npy_argbinsearch<side_t::right>};\n };\n-constexpr std::array<binsearch_base<arg>::function_type, 2>\n-        binsearch_base<arg>::npy_map;\n+constexpr std::array<binsearch_base<arg_t::arg>::function_type, 2>\n+        binsearch_base<arg_t::arg>::npy_map;\n \n template <>\n-struct binsearch_base<noarg> {\n+struct binsearch_base<arg_t::noarg> {\n     using function_type = PyArray_BinSearchFunc *;\n     struct value_type {\n         int typenum;\n@@ -308,15 +308,15 @@ struct binsearch_base<noarg> {\n     {\n         return std::array<value_type, sizeof...(Tags)>{\n                 value_type{Tags::type_value,\n-                           {(function_type)&binsearch<Tags, left>,\n-                            (function_type)binsearch<Tags, right>}}...};\n+                           {(function_type)&binsearch<Tags, side_t::left>,\n+                            (function_type)binsearch<Tags, side_t::right>}}...};\n     }\n     static constexpr std::array<function_type, 2> npy_map = {\n-            (function_type)&npy_binsearch<left>,\n-            (function_type)&npy_binsearch<right>};\n+            (function_type)&npy_binsearch<side_t::left>,\n+            (function_type)&npy_binsearch<side_t::right>};\n };\n-constexpr std::array<binsearch_base<noarg>::function_type, 2>\n-        binsearch_base<noarg>::npy_map;\n+constexpr std::array<binsearch_base<arg_t::noarg>::function_type, 2>\n+        binsearch_base<arg_t::noarg>::npy_map;\n \n // Handle generation of all binsearch variants\n template <arg_t arg>\n@@ -392,12 +392,12 @@ extern \"C\" {\n NPY_NO_EXPORT PyArray_BinSearchFunc *\n get_binsearch_func(PyArray_Descr *dtype, NPY_SEARCHSIDE side)\n {\n-    return _get_binsearch_func<noarg>(dtype, side);\n+    return _get_binsearch_func<arg_t::noarg>(dtype, side);\n }\n \n NPY_NO_EXPORT PyArray_ArgBinSearchFunc *\n get_argbinsearch_func(PyArray_Descr *dtype, NPY_SEARCHSIDE side)\n {\n-    return _get_binsearch_func<arg>(dtype, side);\n+    return _get_binsearch_func<arg_t::arg>(dtype, side);\n }\n }"
            },
            {
                "filename": "numpy/core/src/npysort/npysort_common.h",
                "patch": "@@ -3,6 +3,7 @@\n \n #include <stdlib.h>\n #include <numpy/ndarraytypes.h>\n+#include <numpy/halffloat.h>\n \n #ifdef __cplusplus\n extern \"C\" {\n@@ -141,47 +142,16 @@ LONGDOUBLE_LT(npy_longdouble a, npy_longdouble b)\n     return a < b || (b != b && a == a);\n }\n \n-\n-NPY_INLINE static int\n-_npy_half_isnan(npy_half h)\n-{\n-    return ((h&0x7c00u) == 0x7c00u) && ((h&0x03ffu) != 0x0000u);\n-}\n-\n-\n-NPY_INLINE static int\n-_npy_half_lt_nonan(npy_half h1, npy_half h2)\n-{\n-    if (h1&0x8000u) {\n-        if (h2&0x8000u) {\n-            return (h1&0x7fffu) > (h2&0x7fffu);\n-        }\n-        else {\n-            /* Signed zeros are equal, have to check for it */\n-            return (h1 != 0x8000u) || (h2 != 0x0000u);\n-        }\n-    }\n-    else {\n-        if (h2&0x8000u) {\n-            return 0;\n-        }\n-        else {\n-            return (h1&0x7fffu) < (h2&0x7fffu);\n-        }\n-    }\n-}\n-\n-\n NPY_INLINE static int\n HALF_LT(npy_half a, npy_half b)\n {\n     int ret;\n \n-    if (_npy_half_isnan(b)) {\n-        ret = !_npy_half_isnan(a);\n+    if (npy_half_isnan(b)) {\n+        ret = !npy_half_isnan(a);\n     }\n     else {\n-        ret = !_npy_half_isnan(a) && _npy_half_lt_nonan(a, b);\n+        ret = !npy_half_isnan(a) && npy_half_lt_nonan(a, b);\n     }\n \n     return ret;"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1939,15 +1939,15 @@ HALF_conjugate(char **args, npy_intp const *dimensions, npy_intp const *steps, v\n NPY_NO_EXPORT NPY_GCC_OPT_3 void\n HALF_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    UNARY_LOOP_FAST(npy_half, npy_half, *out = in&0x7fffu);\n+    UNARY_LOOP_FAST(npy_half, npy_half, *out = npy_half_abs(in));\n }\n \n NPY_NO_EXPORT void\n HALF_negative(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n     UNARY_LOOP {\n         const npy_half in1 = *(npy_half *)ip1;\n-        *((npy_half *)op1) = in1^0x8000u;\n+        *((npy_half *)op1) = npy_half_neg(in1);\n     }\n }\n \n@@ -1956,7 +1956,7 @@ HALF_positive(char **args, npy_intp const *dimensions, npy_intp const *steps, vo\n {\n     UNARY_LOOP {\n         const npy_half in1 = *(npy_half *)ip1;\n-        *((npy_half *)op1) = +in1;\n+        *((npy_half *)op1) = npy_half_pos(in1);\n     }\n }\n \n@@ -1967,8 +1967,8 @@ HALF_sign(char **args, npy_intp const *dimensions, npy_intp const *steps, void *\n     UNARY_LOOP {\n         const npy_half in1 = *(npy_half *)ip1;\n         *((npy_half *)op1) = npy_half_isnan(in1) ? in1 :\n-                    (((in1&0x7fffu) == 0) ? 0 :\n-                      (((in1&0x8000u) == 0) ? NPY_HALF_ONE : NPY_HALF_NEGONE));\n+                    (npy_half_iszero(in1) ? NPY_HALF_ZERO :\n+                     (npy_half_signbit(in1) ? NPY_HALF_NEGONE : NPY_HALF_ONE));\n     }\n }\n "
            },
            {
                "filename": "numpy/core/src/umath/scalarmath.c.src",
                "patch": "@@ -370,7 +370,7 @@ half_ctype_floor_divide(npy_half a, npy_half b, npy_half *out)\n {\n     npy_half mod;\n \n-    if (!b) {\n+    if (!NPY_HALF_BITS(b)) {\n         float res = npy_half_to_float(a) / npy_half_to_float(b);\n         *out = npy_float_to_half(res);\n     }\n@@ -514,7 +514,7 @@ static NPY_INLINE int\n static NPY_INLINE int\n half_ctype_negative(npy_half a, npy_half *out)\n {\n-    *out = a^0x8000u;\n+    *out = npy_half_neg(a);\n     return 0;\n }\n \n@@ -607,7 +607,7 @@ static NPY_INLINE int\n static NPY_INLINE int\n half_ctype_absolute(npy_half a, npy_half *out)\n {\n-    *out = a&0x7fffu;\n+    *out = npy_half_abs(a);\n     return 0;\n }\n "
            },
            {
                "filename": "numpy/core/tests/test_api.py",
                "patch": "@@ -2,6 +2,7 @@\n \n import numpy as np\n from numpy.core._rational_tests import rational\n+import numpy.core._half_legacy_tests\n import pytest\n from numpy.testing import (\n      assert_, assert_equal, assert_array_equal, assert_raises, assert_warns,"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23460,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nAddress ReDOS vulnerability flagged by CodeQL first mentioned in #23338.\r\n\r\nThere was an capture group in `crackfortran.nameargspattern` that used an unchecked `.*` when it could have instead used `[^@\\s]*` to achieve the same result.",
        "changed_files": [
            {
                "filename": "numpy/f2py/crackfortran.py",
                "patch": "@@ -935,7 +935,7 @@ def appenddecl(decl, decl2, force=1):\n     r'(?:,(?P<attributes>[\\w(),]+))?(::)?(?P<name>\\b[a-z$_][\\w$]*\\b)'\n     r'(?:\\((?P<params>[\\w,]*)\\))?\\Z', re.I)\n nameargspattern = re.compile(\n-    r'\\s*(?P<name>\\b[\\w$]+\\b)\\s*(@\\(@\\s*(?P<args>[\\w\\s,]*)\\s*@\\)@|)\\s*((result(\\s*@\\(@\\s*(?P<result>\\b[\\w$]+\\b)\\s*@\\)@|))|(bind\\s*@\\(@\\s*(?P<bind>.*)\\s*@\\)@))*\\s*\\Z', re.I)\n+    r'\\s*(?P<name>\\b[\\w$]+\\b)\\s*(@\\(@\\s*(?P<args>[\\w\\s,]*)\\s*@\\)@|)\\s*((result(\\s*@\\(@\\s*(?P<result>\\b[\\w$]+\\b)\\s*@\\)@|))|(bind\\s*@\\(@\\s*(?P<bind>(?:(?!@\\)@).)*)\\s*@\\)@))*\\s*\\Z', re.I)\n operatorpattern = re.compile(\n     r'\\s*(?P<scheme>(operator|assignment))'\n     r'@\\(@\\s*(?P<name>[^)]+)\\s*@\\)@\\s*\\Z', re.I)"
            },
            {
                "filename": "numpy/f2py/tests/test_crackfortran.py",
                "patch": "@@ -1,9 +1,10 @@\n import importlib\n import codecs\n+import time\n import unicodedata\n import pytest\n import numpy as np\n-from numpy.f2py.crackfortran import markinnerspaces\n+from numpy.f2py.crackfortran import markinnerspaces, nameargspattern\n from . import util\n from numpy.f2py import crackfortran\n import textwrap\n@@ -276,3 +277,49 @@ class TestUnicodeComment(util.F2PyTest):\n     )\n     def test_encoding_comment(self):\n         self.module.foo(3)\n+\n+class TestNameArgsPatternBacktracking:\n+    @pytest.mark.parametrize(\n+        ['adversary'],\n+        [\n+            ('@)@bind@(@',),\n+            ('@)@bind                         @(@',),\n+            ('@)@bind foo bar baz@(@',)\n+        ]\n+    )\n+    def test_nameargspattern_backtracking(self, adversary):\n+        '''address ReDOS vulnerability:\n+        https://github.com/numpy/numpy/issues/23338'''\n+        last_median = 0.\n+        trials_per_count = 128\n+        start_reps, end_reps = 15, 25\n+        times_median_doubled = 0\n+        for ii in range(start_reps, end_reps):\n+            repeated_adversary = adversary * ii\n+            times = []\n+            for _ in range(trials_per_count):\n+                t0 = time.perf_counter()\n+                mtch = nameargspattern.search(repeated_adversary)\n+                times.append(time.perf_counter() - t0)\n+            # We should use a measure of time that's resilient to outliers.\n+            # Times jump around a lot due to the CPU's scheduler.\n+            median = np.median(times)\n+            assert not mtch\n+            # if the adversary is capped with @)@, it becomes acceptable\n+            # according to the old version of the regex.\n+            # that should still be true.\n+            good_version_of_adversary = repeated_adversary + '@)@'\n+            assert nameargspattern.search(good_version_of_adversary)\n+            if ii > start_reps:\n+                # the hallmark of exponentially catastrophic backtracking\n+                # is that runtime doubles for every added instance of\n+                # the problematic pattern.\n+                times_median_doubled += median > 2 * last_median\n+                # also try to rule out non-exponential but still bad cases\n+                # arbitrarily, we should set a hard limit of 10ms as too slow\n+                assert median < trials_per_count * 0.01\n+            last_median = median\n+        # we accept that maybe the median might double once, due to\n+        # the CPU scheduler acting weird or whatever. More than that\n+        # seems suspicious.\n+        assert times_median_doubled < 2\n\\ No newline at end of file"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23171,
        "body": "Adds the initial support for using the Optimized Routines library to improve performance on AArch64.\r\n\r\n`cos` and `sin` are implemented to demonstrate the flow through to the library calls, more will be added in a follow up patch to align with the existing SVML integration.\r\n\r\nI've updated both setup.py and meson.build, but I'm unsure which gets triggered when \ud83e\udd14 \r\n\r\n```\r\n16:47:00  -         729\u00b12\u03bcs        397\u00b10.4\u03bcs     0.54  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 1, 'd')\r\n16:47:00  -        732\u00b110\u03bcs          414\u00b16\u03bcs     0.56  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 2, 'd')\r\n16:47:00  -        776\u00b120\u03bcs         436\u00b110\u03bcs     0.56  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 4, 'd')\r\n16:47:00  -         731\u00b12\u03bcs        419\u00b10.3\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 1, 'd')\r\n16:47:00  -         732\u00b13\u03bcs        439\u00b10.5\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 2, 'd')\r\n16:47:00  -        735\u00b120\u03bcs         440\u00b110\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 4, 'd')\r\n16:47:00  -         736\u00b11\u03bcs          420\u00b11\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 1, 'd')\r\n16:47:00  -       736\u00b10.7\u03bcs          441\u00b11\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 2, 'd')\r\n16:47:00  -         736\u00b12\u03bcs          442\u00b12\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 4, 'd')\r\n16:47:00  -         871\u00b15\u03bcs        395\u00b10.3\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 1, 'd')\r\n16:47:00  -        876\u00b110\u03bcs          413\u00b16\u03bcs     0.47  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 2, 'd')\r\n16:47:00  -        928\u00b130\u03bcs         436\u00b110\u03bcs     0.47  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 4, 'd')\r\n16:47:00  -         871\u00b15\u03bcs        417\u00b10.2\u03bcs     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 1, 'd')\r\n16:47:00  -         874\u00b12\u03bcs        430\u00b10.2\u03bcs     0.49  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 2, 'd')\r\n16:47:00  -        876\u00b120\u03bcs         430\u00b110\u03bcs     0.49  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 4, 'd')\r\n16:47:00  -         876\u00b14\u03bcs          419\u00b11\u03bcs     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 1, 'd')\r\n16:47:00  -         872\u00b12\u03bcs          431\u00b12\u03bcs     0.49  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 2, 'd')\r\n16:47:00  -         875\u00b16\u03bcs          434\u00b12\u03bcs     0.50  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 4, 'd')\r\n```\r\n\r\n\r\nSee: https://mail.python.org/archives/list/numpy-discussion@python.org/message/GTHX4TFRUCGQI2VPHEWMEC4GBOAOOH4C/",
        "changed_files": [
            {
                "filename": ".gitmodules",
                "patch": "@@ -4,3 +4,6 @@\n [submodule \"numpy/core/src/umath/svml\"]\n \tpath = numpy/core/src/umath/svml\n \turl = https://github.com/numpy/SVML.git\n+[submodule \"numpy/core/src/umath/optimized-routines\"]\n+\tpath = numpy/core/src/umath/optimized-routines\n+\turl = https://github.com/ARM-software/optimized-routines.git"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -1,3 +1,5 @@\n+# Copyright 2023 Arm Limited and/or its affiliates <open-source-office@arm.com>\n+#\n # This file should contain what setup.py + setup_common.py do (WIP)\n #\n # Potential issues to address or keep track of:\n@@ -72,6 +74,17 @@ cdata = configuration_data()\n cdata.set('NPY_ABI_VERSION', C_ABI_VERSION)\n cdata.set('NPY_API_VERSION', C_API_VERSION)\n \n+use_optimized_routines = (\n+  host_machine.cpu_family() == 'aarch64' and\n+  not get_option('disable-optimized-routines')\n+)\n+if use_optimized_routines\n+  cdata.set10('NPY_CAN_LINK_AOR', true)\n+  if not fs.exists('src/umath/optimized-routines')\n+    error('Missing the `Optimized Routines` git submodule! Run `git submodule update --init` to fix this.')\n+  endif\n+endif\n+\n use_svml = (\n   host_machine.system() == 'linux' and\n   host_machine.cpu_family() == 'x86_64' and\n@@ -779,6 +792,22 @@ src_umath = [\n   'src/umath/_scaled_float_dtype.c',\n ]\n \n+# Optimized Routines source files. If functionality is migrated to universal intrinsics and\n+# the object files are no longer needed, comment out the relevant object files\n+# here. Note that this migration is desirable; we then get the performance\n+# benefits for all platforms rather than only for AArch64.\n+src_optimized_routines = []\n+optimized_routines_include = []\n+if use_optimized_routines\n+  optimized_routines_include += [\n+    'src/umath/optimized-routines/math/include/'\n+  ]\n+  src_optimized_routines += [\n+    'src/umath/optimized-routines/math/v_cos.c',\n+    'src/umath/optimized-routines/math/v_sin.c',\n+  ]\n+endif\n+\n # SVML object files. If functionality is migrated to universal intrinsics and\n # the object files are no longer needed, comment out the relevant object files\n # here. Note that this migration is desirable; we then get the performance\n@@ -845,6 +874,7 @@ py.extension_module('_multiarray_umath',\n     src_numpy_api[1],  # __multiarray_api.h\n     src_umath_doc_h,\n     npy_math_internal_h,\n+    src_optimized_routines,\n   ],\n   objects: svml_objects,\n   c_args: c_args_common,\n@@ -855,6 +885,7 @@ py.extension_module('_multiarray_umath',\n     'src/multiarray',\n     'src/npymath',\n     'src/umath',\n+    optimized_routines_include\n   ],\n   dependencies: blas,\n   link_with: npymath_lib,"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1,3 +1,5 @@\n+# Copyright 2023 Arm Limited and/or its affiliates <open-source-office@arm.com>\n+\n import os\n import sys\n import sysconfig\n@@ -34,6 +36,12 @@\n # useful to avoid improperly requiring SVML when cross compiling.\n NPY_DISABLE_SVML = (os.environ.get('NPY_DISABLE_SVML', \"0\") == \"1\")\n \n+# Set NPY_DISABLE_AOR=1 in the environment to disable the vendored\n+# Optimized Routines library. This option only has significance on\n+# aarch64 host and is most useful to avoid improperly requiring\n+# Optimized Routines when cross compiling.\n+NPY_DISABLE_AOR = (os.environ.get('NPY_DISABLE_AOR', \"0\") == \"1\")\n+\n # XXX: ugly, we use a class to avoid calling twice some expensive functions in\n # config.h/numpyconfig.h. I don't see a better way because distutils force\n # config.h generation inside an Extension class, and as such sharing\n@@ -68,6 +76,20 @@ def check_complex(self, *a, **kw):\n             out = copy.deepcopy(pickle.loads(self._check_complex))\n         return out\n \n+def can_link_optimized_routines():\n+    \"\"\"Optimized Routines library is supported only on aarch64 architecture\n+    \"\"\"\n+    if NPY_DISABLE_AOR:\n+        return False\n+    platform = sysconfig.get_platform()\n+    return any([\"aarch64\" in platform, \"arm64\" in platform])\n+\n+def check_optimized_routines_submodule(optimized_routines_path):\n+    if not os.path.exists(optimized_routines_path + \"/README\"):\n+        raise RuntimeError(\"Missing `Optimized Routines` submodule! Run \"\n+                           \"`git submodule update --init` to fix this.\")\n+    return True\n+\n def can_link_svml():\n     \"\"\"SVML library is supported only on x86_64 architecture and currently\n     only on linux\n@@ -481,6 +503,8 @@ def generate_config_h(ext, build_dir):\n             # Inline check\n             inline = config_cmd.check_inline()\n \n+            if can_link_optimized_routines():\n+                moredefs.append(('NPY_CAN_LINK_AOR', 1))\n             if can_link_svml():\n                 moredefs.append(('NPY_CAN_LINK_SVML', 1))\n \n@@ -1047,6 +1071,28 @@ def generate_umath_doc_header(ext, build_dir):\n             join(codegen_dir, 'ufunc_docstrings.py'),\n             ]\n \n+    optimized_routines_path = join(\n+        'numpy', 'core', 'src', 'umath', 'optimized-routines'\n+    )\n+    optimized_routines_sources = []\n+    if (\n+        can_link_optimized_routines() and\n+        check_optimized_routines_submodule(optimized_routines_path)\n+    ):\n+        # These are listed as explicitly enabled for now, over time this list\n+        # should transition to universal intrinsics\n+        config.add_include_dirs(join(\n+            optimized_routines_path, 'math', 'include'\n+        ))\n+\n+        optimized_routines = [\n+            'cos', 'sin'\n+        ]\n+        optimized_routines_sources = [\n+            join(optimized_routines_path, 'math', f'v_{routine}.c')\n+            for routine in optimized_routines\n+        ]\n+\n     svml_path = join('numpy', 'core', 'src', 'umath', 'svml')\n     svml_objs = []\n     # we have converted the following into universal intrinsics\n@@ -1070,7 +1116,9 @@ def generate_umath_doc_header(ext, build_dir):\n                          language = 'c',\n                          sources=multiarray_src + umath_src +\n                                  common_src +\n-                                 [generate_config_h,\n+                                 optimized_routines_sources +\n+                                 [\n+                                  generate_config_h,\n                                   generate_numpyconfig_h,\n                                   generate_numpy_api,\n                                   join(codegen_dir, 'generate_numpy_api.py'),"
            },
            {
                "filename": "numpy/core/src/umath/loops_umath_fp.dispatch.c.src",
                "patch": "@@ -1,14 +1,30 @@\n /*@targets\n- ** $maxopt baseline avx512_skx\n+ ** $maxopt baseline\n+ ** avx512_skx\n+ ** asimd\n  */\n+/*\n+ * Copyright 2023 Arm Limited and/or its affiliates <open-source-office@arm.com>\n+ */\n+\n #include \"numpy/npy_math.h\"\n #include \"simd/simd.h\"\n #include \"loops_utils.h\"\n #include \"loops.h\"\n #include \"npy_svml.h\"\n #include \"fast_loop_macros.h\"\n \n-#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+#if NPY_SIMD\n+#if defined(NPY_HAVE_ASIMD) && defined(NPY_CAN_LINK_AOR)\n+#include <mathlib.h>\n+#define _NPY_OPTIMIZED_ROUTINES\n+#endif\n+#if defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+#define _NPY_SVML\n+#endif\n+#endif\n+\n+#if defined(_NPY_SVML)\n /**begin repeat\n  * #sfx = f32, f64#\n  * #func_suffix = f16, 8#\n@@ -58,7 +74,9 @@ simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src, npy_intp ssrc,\n }\n /**end repeat1**/\n /**end repeat**/\n+#endif\n \n+#if defined(_NPY_OPTIMIZED_ROUTINES) || defined(_NPY_SVML)\n /**begin repeat\n  * #func = sin, cos#\n  */\n@@ -74,7 +92,11 @@ simd_@func@_f64(const double *src, npy_intp ssrc,\n         } else {\n             x = npyv_loadn_tillz_f64(src, ssrc, len);\n         }\n+        #if defined(_NPY_OPTIMIZED_ROUTINES)\n+        npyv_f64 out = __v_@func@(x);\n+        #else\n         npyv_f64 out = __svml_@func@8(x);\n+        #endif\n         if (sdst == 1) {\n             npyv_store_till_f64(dst, len, out);\n         } else {\n@@ -84,15 +106,16 @@ simd_@func@_f64(const double *src, npy_intp ssrc,\n     npyv_cleanup();\n }\n /**end repeat**/\n+#endif\n \n+#if defined(_NPY_SVML)\n /**begin repeat\n  * #sfx = f32, f64#\n  * #func_suffix = f16, 8#\n  */\n /**begin repeat1\n  * #func = pow, atan2#\n  */\n-\n static void\n simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src1, npy_intp ssrc1,\n                   const npyv_lanetype_@sfx@ *src2, npy_intp ssrc2,\n@@ -182,7 +205,7 @@ avx512_@func@_f16(const npy_half *src, npy_half *dst, npy_intp len)\n NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(HALF_@func@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {\n-#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+#if defined(_NPY_SVML)\n     const npy_half *src = (npy_half*)args[0];\n           npy_half *dst = (npy_half*)args[1];\n     const int lsize = sizeof(src[0]);\n@@ -216,7 +239,7 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(HALF_@func@)\n NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {\n-#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+#if defined(_NPY_SVML)\n     const @type@ *src = (@type@*)args[0];\n           @type@ *dst = (@type@*)args[1];\n     const int lsize = sizeof(src[0]);\n@@ -252,7 +275,7 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {\n-#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+#if defined(_NPY_SVML)\n     const @type@ *src1 = (@type@*)args[0];\n     const @type@ *src2 = (@type@*)args[1];\n           @type@ *dst  = (@type@*)args[2];\n@@ -284,7 +307,7 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(DOUBLE_@func@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {\n-#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+#if defined(_NPY_SVML) || defined(_NPY_OPTIMIZED_ROUTINES)\n     const double *src = (double*)args[0];\n           double *dst = (double*)args[1];\n     const int lsize = sizeof(src[0]);"
            },
            {
                "filename": "numpy/core/src/umath/optimized-routines",
                "patch": "@@ -0,0 +1 @@\n+Subproject commit 1d697da3e242f8e84145a9a6fc689bc0be394137"
            },
            {
                "filename": "tools/ci/cirrus_general.yml",
                "patch": "@@ -1,3 +1,5 @@\n+# Copyright 2023 Arm Limited and/or its affiliates <open-source-office@arm.com>\n+\n build_and_store_wheels: &BUILD_AND_STORE_WHEELS\n   install_cibuildwheel_script:\n     - python -m pip install cibuildwheel==2.12.0\n@@ -34,6 +36,7 @@ linux_aarch64_task:\n   build_script: |\n     apt install -y python3-venv python-is-python3 gfortran libatlas-base-dev libgfortran5 eatmydata\n     git fetch origin\n+    git submodule update --init\n     ./tools/travis-before-install.sh\n     which python\n     echo $CIRRUS_CHANGE_MESSAGE"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 9349,
        "body": "This makes\r\n```\r\n    class C(object):\r\n        @np.vectorize\r\n        def meth(self, obj): return self, obj\r\n    c = C()\r\n    c.meth([1, 2])\r\n```\r\nreturn\r\n```\r\n    [(c, c), (1, 2)]\r\n```\r\ninstead of erroring because one extra argument is expected.\r\n\r\nNote that the docstring of `vectorize` said that `pyfunc` was `A python\r\nfunction or method`.",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/9349.compatibility.rst",
                "patch": "@@ -0,0 +1,17 @@\n+``np.vectorize`` follows the descriptor protocol\n+------------------------------------------------\n+\n+Before numpy 1.25, a function wrapped in ``np.vectorize`` would *not* follow\n+the descriptor protocol; in particular, attaching such an object to a class\n+would implicitly create a static method::\n+\n+   >>> def func(x):\n+   ...     return x ** 2\n+   >>> class C(object):\n+   ...     static = np.vectorize(func)\n+   >>> C().static(1)\n+   1\n+\n+Since numpy 1.25, the vectorized function now binds ``self`` as the first\n+argument, and the earlier example will fail.  To recover the earlier behavior,\n+use `static = staticmethod(np.vectorize(func))`."
            },
            {
                "filename": "numpy/lib/function_base.py",
                "patch": "@@ -2189,6 +2189,24 @@ class vectorize:\n     The new keyword argument interface and `excluded` argument support\n     further degrades performance.\n \n+    .. versionchanged:: 1.25.0\n+\n+    Before numpy 1.25, a vectorized function would *not* follow the descriptor\n+    protocol; in particular, attaching such an object to a class would\n+    implicitly create a static method:\n+\n+    >>> def func(x):\n+    ...     return x ** 2\n+    >>> class C(object):\n+    ...     static = np.vectorize(func)\n+    >>> C().static(1)\n+    1\n+\n+    Since numpy 1.25, the vectorized function now binds `self` as the first\n+    argument, and the earlier example will fail.  To recover the earlier\n+    behavior, explicitly wrap `func` using `staticmethod` before passing it to\n+    `vectorize`.\n+\n     References\n     ----------\n     .. [1] :doc:`/reference/c-api/generalized-ufuncs`\n@@ -2298,6 +2316,14 @@ def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n         else:\n             self._in_and_out_core_dims = None\n \n+    def __get__(self, instance, owner):\n+        return type(self)(self.pyfunc.__get__(instance, owner),\n+                          otypes=self.otypes,\n+                          doc=self.__doc__,\n+                          excluded=self.excluded,\n+                          cache=self.cache,\n+                          signature=self.signature)\n+\n     def __call__(self, *args, **kwargs):\n         \"\"\"\n         Return arrays with the results of `pyfunc` broadcast (vectorized) over"
            },
            {
                "filename": "numpy/lib/tests/test_function_base.py",
                "patch": "@@ -1767,6 +1767,26 @@ def test_size_zero_output(self):\n         with assert_raises_regex(ValueError, 'new output dimensions'):\n             f(x)\n \n+    def test_method(self):\n+        class C(object):\n+            v = vectorize(lambda self, x: (self, x))\n+            vc = vectorize(classmethod(lambda cls, x: (cls, x)))\n+            cv = classmethod(vectorize(lambda cls, x: (cls, x)))\n+            vs = vectorize(staticmethod(lambda x: x))\n+            sv = staticmethod(vectorize(lambda x: x))\n+\n+        c = C()\n+        assert_array_equal(c.v([1, 2]), ([c, c], [1, 2]))\n+        assert_array_equal(C.v(c, [1, 2]), ([c, c], [1, 2]))\n+        assert_array_equal(c.vc([1, 2]), ([C, C], [1, 2]))\n+        assert_array_equal(C.vc([1, 2]), ([C, C], [1, 2]))\n+        assert_array_equal(c.cv([1, 2]), ([C, C], [1, 2]))\n+        assert_array_equal(C.cv([1, 2]), ([C, C], [1, 2]))\n+        assert_array_equal(c.vs([1, 2]), [1, 2])\n+        assert_array_equal(C.vs([1, 2]), [1, 2])\n+        assert_array_equal(c.sv([1, 2]), [1, 2])\n+        assert_array_equal(C.sv([1, 2]), [1, 2])\n+\n     def test_subclasses(self):\n         class subclass(np.ndarray):\n             pass"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23010,
        "body": "Closes #23009.\r\n\r\n## Methods\r\n- [x] `__abs__`\r\n- [x] `__add__`\r\n- [x] `__and__`\r\n- [x] `__bool__`\r\n- [x] `__complex__`\r\n- [x] `__dlpack__`\r\n- [x] `__dlpack_device__`\r\n- [x] `__eq__`\r\n- [x] `__float__`\r\n- [x] `__floordiv__`\r\n- [x] `__ge__`\r\n- [x] `__getitem__`\r\n- [x] `__gt__`\r\n- [x] `__int__`\r\n- [x] `__invert__`\r\n- [x] `__le__`\r\n- [x] `__lshift__`\r\n- [x] `__lt__`\r\n- [x] `__matmul__`\r\n- [x] `__mod__`\r\n- [x] `__mul__`\r\n- [x] `__ne__`\r\n- [x] `__neg__`\r\n- [x] `__or__`\r\n- [x] `__pos__`\r\n- [x] `__pow__`\r\n- [x] `__rshift__`\r\n- [x] `__setitem__`\r\n- [x] `__sub__`\r\n- [x] `__truediv__`\r\n- [x] `__xor__`\r\n## Creation Functions\r\n- [x] `arange`\r\n- [x] `asarray`\r\n- [x] `empty`\r\n- [x] `empty_like`\r\n- [x] `eye`\r\n- [x] `from_dlpack`\r\n- [x] `full`\r\n- [x] `full_like`\r\n- [x] `linspace`\r\n- [x] `meshgrid`\r\n- [x] `ones`\r\n- [x] `ones_like`\r\n- [x] `tril`\r\n- [x] `triu`\r\n- [x] `zeros`\r\n- [x] `zeros_like`\r\n## Data Type Functions\r\n- [x] `astype`\r\n## Element-wise Functions (`ufuncs`)\r\nMost of these are covered in `bench_ufunc` but not for very large arrays.\r\n\r\n- [X] `abs`\r\n- [X] `acos`\r\n- [X] `acosh`\r\n- [X] `add`\r\n- [X] `asin`\r\n- [X] `asinh`\r\n- [X] `atan`\r\n- [X] `atan2`\r\n- [X] `atanh`\r\n- [X] `bitwise_and`\r\n- [X] `bitwise_left_shift` == `left_shift`\r\n- [X] `bitwise_invert` == `invert`\r\n- [X] `bitwise_or`\r\n- [X] `bitwise_right_shift` == `right_shift`\r\n- [X] `bitwise_xor`\r\n- [X] `ceil`\r\n- [X] `conj`\r\n- [X] `cos`\r\n- [X] `cosh`\r\n- [X] `divide`\r\n- [X] `equal`\r\n- [X] `exp`\r\n- [X] `expm1`\r\n- [X] `floor`\r\n- [X] `floor_divide`\r\n- [X] `greater`\r\n- [X] `greater_equal`\r\n- [X] `isfinite`\r\n- [X] `isinf`\r\n- [X] `isnan`\r\n- [X] `less`\r\n- [X] `less_equal`\r\n- [X] `log`\r\n- [X] `log1p`\r\n- [X] `log2`\r\n- [X] `log10`\r\n- [X] `logaddexp`\r\n- [X] `logical_and`\r\n- [X] `logical_not`\r\n- [X] `logical_or`\r\n- [X] `logical_xor`\r\n- [X] `multiply`\r\n- [X] `negative`\r\n- [X] `not_equal`\r\n- [X] `positive`\r\n- [X] `pow`\r\n- [X] `real`\r\n- [X] `remainder`\r\n- [X] `round`\r\n- [X] `sign`\r\n- [X] `sin`\r\n- [X] `sinh`\r\n- [X] `square`\r\n- [X] `sqrt`\r\n- [X] `subtract`\r\n- [X] `tan`\r\n- [X] `tanh`\r\n- [X] `trunc`\r\n## Linear Algebra Functions\r\n- [X] `matmul`\r\n- [x] `matrix_transpose` == `transpose()`\r\n- [x] `tensordot`\r\n- [x] `vecdot` == `vdot`\r\n## Manipulation Functions\r\n- [x] `broadcast_arrays`\r\n- [x] `broadcast_to`\r\n- [x] `concat`\r\n- [x] `expand_dims`\r\n- [x] `flip`\r\n- [x] `permute_dims`\r\n- [x] `reshape`\r\n- [x] `roll`\r\n- [x] `squeeze`\r\n- [x] `stack`\r\n## Searching Functions\r\n- [x] `argmax`\r\n- [x] `argmin`\r\n- [x] `nonzero`\r\n- [x] `where`\r\n## Set Functions\r\n- [x] `unique_all`\r\n- [x] `unique_counts`\r\n- [x] `unique_inverse`\r\n- [x] `unique_values`\r\n## Sorting Functions\r\n- [x] `argsort`\r\n- [x] `sort`\r\n## Statistical Functions\r\n- [x] `max`\r\n- [x] `mean`\r\n- [x] `min`\r\n- [x] `prod`\r\n- [x] `std`\r\n- [x] `sum`\r\n- [x] `var`\r\n## Utility Functions\r\n- [x] `all`\r\n- [x] `any`",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_core.py",
                "patch": "@@ -215,13 +215,41 @@ class Indices(Benchmark):\n     def time_indices(self):\n         np.indices((1000, 500))\n \n-class VarComplex(Benchmark):\n-    params = [10**n for n in range(0, 9)]\n-    def setup(self, n):\n-        self.arr = np.random.randn(n) + 1j * np.random.randn(n)\n \n-    def teardown(self, n):\n-        del self.arr\n+class StatsMethods(Benchmark):\n+    # Not testing, but in array_api (redundant)\n+    # 8, 16, 32 bit variants, and 128 complexes\n+    params = [['int64', 'uint64', 'float64', 'intp',\n+               'complex64', 'bool', 'float', 'int',\n+               'complex', 'complex256'],\n+              [100**n for n in range(0, 2)]]\n+    param_names = ['dtype', 'size']\n \n-    def time_var(self, n):\n-        self.arr.var()\n+    def setup(self, dtype, size):\n+        try:\n+            self.data = np.ones(size, dtype=getattr(np, dtype))\n+        except AttributeError:  # builtins throw AttributeError after 1.20\n+            self.data = np.ones(size, dtype=dtype)\n+        if dtype.startswith('complex'):\n+            self.data = np.random.randn(size) + 1j * np.random.randn(size)\n+\n+    def time_min(self, dtype, size):\n+        self.data.min()\n+\n+    def time_max(self, dtype, size):\n+        self.data.max()\n+\n+    def time_mean(self, dtype, size):\n+        self.data.mean()\n+\n+    def time_std(self, dtype, size):\n+        self.data.std()\n+\n+    def time_prod(self, dtype, size):\n+        self.data.prod()\n+\n+    def time_var(self, dtype, size):\n+        self.data.var()\n+\n+    def time_sum(self, dtype, size):\n+        self.data.sum()"
            },
            {
                "filename": "benchmarks/benchmarks/bench_creation.py",
                "patch": "@@ -0,0 +1,81 @@\n+from .common import Benchmark, TYPES1\n+\n+import numpy as np\n+\n+\n+class MeshGrid(Benchmark):\n+    \"\"\" Benchmark meshgrid generation\n+    \"\"\"\n+    params = [[16, 32],\n+              [2, 3, 4],\n+              ['ij', 'xy'], TYPES1]\n+    param_names = ['size', 'ndims', 'ind', 'ndtype']\n+    timeout = 10\n+\n+    def setup(self, size, ndims, ind, ndtype):\n+        self.grid_dims = [(np.random.ranf(size)).astype(ndtype) for\n+                          x in range(ndims)]\n+\n+    def time_meshgrid(self, size, ndims, ind, ndtype):\n+        np.meshgrid(*self.grid_dims, indexing=ind)\n+\n+\n+class Create(Benchmark):\n+    \"\"\" Benchmark for creation functions\n+    \"\"\"\n+    # (64, 64), (128, 128), (256, 256)\n+    # , (512, 512), (1024, 1024)\n+    params = [[16, 32, 128, 256, 512,\n+               (16, 16), (32, 32)],\n+              ['C', 'F'],\n+              TYPES1]\n+    param_names = ['shape', 'order', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, shape, order, npdtypes):\n+        values = get_squares_()\n+        self.xarg = values.get(npdtypes)[0]\n+\n+    def time_full(self, shape, order, npdtypes):\n+        np.full(shape, self.xarg[1], dtype=npdtypes, order=order)\n+\n+    def time_full_like(self, shape, order, npdtypes):\n+        np.full_like(self.xarg, self.xarg[0], order=order)\n+\n+    def time_ones(self, shape, order, npdtypes):\n+        np.ones(shape, dtype=npdtypes, order=order)\n+\n+    def time_ones_like(self, shape, order, npdtypes):\n+        np.ones_like(self.xarg, order=order)\n+\n+    def time_zeros(self, shape, order, npdtypes):\n+        np.zeros(shape, dtype=npdtypes, order=order)\n+\n+    def time_zeros_like(self, shape, order, npdtypes):\n+        np.zeros_like(self.xarg, order=order)\n+\n+    def time_empty(self, shape, order, npdtypes):\n+        np.empty(shape, dtype=npdtypes, order=order)\n+\n+    def time_empty_like(self, shape, order, npdtypes):\n+        np.empty_like(self.xarg, order=order)\n+\n+\n+class UfuncsFromDLP(Benchmark):\n+    \"\"\" Benchmark for creation functions\n+    \"\"\"\n+    params = [[16, 32, (16, 16),\n+               (32, 32), (64, 64)],\n+              TYPES1]\n+    param_names = ['shape', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, shape, npdtypes):\n+        if npdtypes in ['longdouble', 'clongdouble']:\n+            raise NotImplementedError(\n+                'Only IEEE dtypes are supported')\n+        values = get_squares_()\n+        self.xarg = values.get(npdtypes)[0]\n+\n+    def time_from_dlpack(self, shape, npdtypes):\n+        np.from_dlpack(self.xarg)"
            },
            {
                "filename": "benchmarks/benchmarks/bench_lib.py",
                "patch": "@@ -132,11 +132,26 @@ def setup(self, array_size, percent_nans):\n         # produce a randomly shuffled array with the\n         # approximate desired percentage np.nan content\n         base_array = np.random.uniform(size=array_size)\n-        base_array[base_array < percent_nans / 100.] = np.nan\n+        n_nan = int(percent_nans * array_size)\n+        nan_indices = np.random.choice(np.arange(array_size), size=n_nan)\n+        base_array[nan_indices] = np.nan\n         self.arr = base_array\n \n-    def time_unique(self, array_size, percent_nans):\n-        np.unique(self.arr)\n+    def time_unique_values(self, array_size, percent_nans):\n+        np.unique(self.arr, return_index=False,\n+                  return_inverse=False, return_counts=False)\n+\n+    def time_unique_counts(self, array_size, percent_nans):\n+        np.unique(self.arr, return_index=False,\n+                  return_inverse=False, return_counts=True)\n+\n+    def time_unique_inverse(self, array_size, percent_nans):\n+        np.unique(self.arr, return_index=False,\n+                  return_inverse=True, return_counts=False)\n+\n+    def time_unique_all(self, array_size, percent_nans):\n+        np.unique(self.arr, return_index=True,\n+                  return_inverse=True, return_counts=True)\n \n \n class Isin(Benchmark):"
            },
            {
                "filename": "benchmarks/benchmarks/bench_linalg.py",
                "patch": "@@ -190,3 +190,27 @@ def time_einsum_noncon_contig_contig(self, dtype):\n     # sum_of_products_contig_outstride0_one\uff1anon_contiguous arrays\n     def time_einsum_noncon_contig_outstride0(self, dtype):\n         np.einsum(\"i->\", self.non_contiguous_dim1, optimize=True)\n+\n+\n+class LinAlgTransposeVdot(Benchmark):\n+    # Smaller for speed\n+    # , (128, 128), (256, 256), (512, 512),\n+    # (1024, 1024)\n+    params = [[(16, 16), (32, 32),\n+               (64, 64)], TYPES1]\n+    param_names = ['shape', 'npdtypes']\n+\n+    def setup(self, shape, npdtypes):\n+        self.xarg = np.random.uniform(-1, 1, np.dot(*shape)).reshape(shape)\n+        self.xarg = self.xarg.astype(npdtypes)\n+        self.x2arg = np.random.uniform(-1, 1, np.dot(*shape)).reshape(shape)\n+        self.x2arg = self.x2arg.astype(npdtypes)\n+        if npdtypes.startswith('complex'):\n+            self.xarg += self.xarg.T*1j\n+            self.x2arg += self.x2arg.T*1j\n+\n+    def time_transpose(self, shape, npdtypes):\n+        np.transpose(self.xarg)\n+\n+    def time_vdot(self, shape, npdtypes):\n+        np.vdot(self.xarg, self.x2arg)"
            },
            {
                "filename": "benchmarks/benchmarks/bench_manipulate.py",
                "patch": "@@ -0,0 +1,107 @@\n+from .common import Benchmark, get_squares_, TYPES1, DLPACK_TYPES\n+\n+import numpy as np\n+from collections import deque\n+\n+class BroadcastArrays(Benchmark):\n+    params = [[(16, 32), (32, 64),\n+               (64, 128), (128, 256),\n+               (256, 512), (512, 1024)],\n+              TYPES1]\n+    param_names = ['shape', 'ndtype']\n+    timeout = 10\n+\n+    def setup(self, shape, ndtype):\n+        self.xarg = np.random.ranf(shape[0]*shape[1]).reshape(shape)\n+        self.xarg = self.xarg.astype(ndtype)\n+        if ndtype.startswith('complex'):\n+            self.xarg += np.random.ranf(1)*1j\n+\n+    def time_broadcast_arrays(self, shape, ndtype):\n+        np.broadcast_arrays(self.xarg, np.ones(1))\n+\n+\n+class BroadcastArraysTo(Benchmark):\n+    params = [[16, 32, 64, 128, 256, 512],\n+              TYPES1]\n+    param_names = ['size', 'ndtype']\n+    timeout = 10\n+\n+    def setup(self, size, ndtype):\n+        self.rng = np.random.default_rng()\n+        self.xarg = self.rng.random(size)\n+        self.xarg = self.xarg.astype(ndtype)\n+        if ndtype.startswith('complex'):\n+            self.xarg += self.rng.random(1)*1j\n+\n+    def time_broadcast_to(self, size, ndtype):\n+        np.broadcast_to(self.xarg, (size, size))\n+\n+\n+class ConcatenateStackArrays(Benchmark):\n+    # (64, 128), (128, 256), (256, 512)\n+    params = [[(16, 32), (32, 64)],\n+              [2, 3, 4, 5],\n+              TYPES1]\n+    param_names = ['shape', 'narrays', 'ndtype']\n+    timeout = 10\n+\n+    def setup(self, shape, narrays, ndtype):\n+        self.xarg = [np.random.ranf(shape[0]*shape[1]).reshape(shape)\n+                     for x in range(narrays)]\n+        self.xarg = [x.astype(ndtype) for x in self.xarg]\n+        if ndtype.startswith('complex'):\n+            [x + np.random.ranf(1)*1j for x in self.xarg]\n+\n+    def time_concatenate_ax0(self, size, narrays, ndtype):\n+        np.concatenate(self.xarg, axis=0)\n+\n+    def time_concatenate_ax1(self, size, narrays, ndtype):\n+        np.concatenate(self.xarg, axis=1)\n+\n+    def time_stack_ax0(self, size, narrays, ndtype):\n+        np.stack(self.xarg, axis=0)\n+\n+    def time_stack_ax1(self, size, narrays, ndtype):\n+        np.stack(self.xarg, axis=1)\n+\n+\n+class DimsManipulations(Benchmark):\n+    params = [\n+        [(2, 1, 4), (2, 1), (5, 2, 3, 1)],\n+    ]\n+    param_names = ['shape']\n+    timeout = 10\n+\n+    def setup(self, shape):\n+        self.xarg = np.ones(shape=shape)\n+        self.reshaped = deque(shape)\n+        self.reshaped.rotate(1)\n+        self.reshaped = tuple(self.reshaped)\n+\n+    def time_expand_dims(self, shape):\n+        np.expand_dims(self.xarg, axis=1)\n+\n+    def time_expand_dims_neg(self, shape):\n+        np.expand_dims(self.xarg, axis=-1)\n+\n+    def time_squeeze_dims(self, shape):\n+        np.squeeze(self.xarg)\n+\n+    def time_flip_all(self, shape):\n+        np.flip(self.xarg, axis=None)\n+\n+    def time_flip_one(self, shape):\n+        np.flip(self.xarg, axis=1)\n+\n+    def time_flip_neg(self, shape):\n+        np.flip(self.xarg, axis=-1)\n+\n+    def time_moveaxis(self, shape):\n+        np.moveaxis(self.xarg, [0, 1], [-1, -2])\n+\n+    def time_roll(self, shape):\n+        np.roll(self.xarg, 3)\n+\n+    def time_reshape(self, shape):\n+        np.reshape(self.xarg, self.reshaped)"
            },
            {
                "filename": "benchmarks/benchmarks/bench_reduce.py",
                "patch": "@@ -45,19 +45,40 @@ def time_any_slow(self):\n         self.zeros.any()\n \n \n-class MinMax(Benchmark):\n-    params = [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32,\n-              np.int64, np.uint64, np.float32, np.float64, np.intp]\n+class StatsReductions(Benchmark):\n+    # Not testing, but in array_api (redundant)\n+    # 8, 16, 32 bit variants, and 128 complexes\n+    params = ['int64', 'uint64', 'float64', 'intp',\n+               'complex64', 'bool', 'float', 'int',\n+               'complex', 'complex256'],\n     param_names = ['dtype']\n \n     def setup(self, dtype):\n-        self.d = np.ones(20000, dtype=dtype)\n+        try:\n+            self.data = np.ones(200, dtype=getattr(np, dtype))\n+        except AttributeError:  # builtins throw AttributeError after 1.20\n+            self.data = np.ones(200, dtype=dtype)\n+        if dtype.startswith('complex'):\n+            self.data = self.data * self.data.T*1j\n \n     def time_min(self, dtype):\n-        np.min(self.d)\n+        np.min(self.data)\n \n     def time_max(self, dtype):\n-        np.max(self.d)\n+        np.max(self.data)\n+\n+    def time_mean(self, dtype):\n+        np.mean(self.data)\n+\n+    def time_std(self, dtype):\n+        np.std(self.data)\n+\n+    def time_prod(self, dtype):\n+        np.prod(self.data)\n+\n+    def time_var(self, dtype):\n+        np.var(self.data)\n+\n \n class FMinMax(Benchmark):\n     params = [np.float32, np.float64]\n@@ -72,6 +93,7 @@ def time_min(self, dtype):\n     def time_max(self, dtype):\n         np.fmax.reduce(self.d)\n \n+\n class ArgMax(Benchmark):\n     params = [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32,\n               np.int64, np.uint64, np.float32, np.float64, bool]\n@@ -83,6 +105,7 @@ def setup(self, dtype):\n     def time_argmax(self, dtype):\n         np.argmax(self.d)\n \n+\n class ArgMin(Benchmark):\n     params = [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32,\n               np.int64, np.uint64, np.float32, np.float64, bool]\n@@ -94,6 +117,7 @@ def setup(self, dtype):\n     def time_argmin(self, dtype):\n         np.argmin(self.d)\n \n+\n class SmallReduction(Benchmark):\n     def setup(self):\n         self.d = np.ones(100, dtype=np.float32)"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -1,6 +1,9 @@\n-from .common import Benchmark, get_squares_\n+from .common import Benchmark, get_squares_, TYPES1, DLPACK_TYPES\n \n import numpy as np\n+import itertools\n+from packaging import version\n+import operator\n \n \n ufuncs = ['abs', 'absolute', 'add', 'arccos', 'arccosh', 'arcsin', 'arcsinh',\n@@ -13,18 +16,44 @@\n           'isinf', 'isnan', 'isnat', 'lcm', 'ldexp', 'left_shift', 'less',\n           'less_equal', 'log', 'log10', 'log1p', 'log2', 'logaddexp',\n           'logaddexp2', 'logical_and', 'logical_not', 'logical_or',\n-          'logical_xor', 'matmul', 'maximum', 'minimum', 'mod', 'modf', 'multiply',\n-          'negative', 'nextafter', 'not_equal', 'positive', 'power',\n-          'rad2deg', 'radians', 'reciprocal', 'remainder', 'right_shift',\n-          'rint', 'sign', 'signbit', 'sin', 'sinh', 'spacing', 'sqrt',\n-          'square', 'subtract', 'tan', 'tanh', 'true_divide', 'trunc']\n+          'logical_xor', 'matmul', 'maximum', 'minimum', 'mod', 'modf',\n+          'multiply', 'negative', 'nextafter', 'not_equal', 'positive',\n+          'power', 'rad2deg', 'radians', 'reciprocal', 'remainder',\n+          'right_shift', 'rint', 'sign', 'signbit', 'sin',\n+          'sinh', 'spacing', 'sqrt', 'square', 'subtract', 'tan', 'tanh',\n+          'true_divide', 'trunc']\n+arrayfuncdisp = ['real', 'round']\n \n \n for name in dir(np):\n     if isinstance(getattr(np, name, None), np.ufunc) and name not in ufuncs:\n         print(\"Missing ufunc %r\" % (name,))\n \n \n+class ArrayFunctionDispatcher(Benchmark):\n+    params = [arrayfuncdisp]\n+    param_names = ['func']\n+    timeout = 10\n+\n+    def setup(self, ufuncname):\n+        np.seterr(all='ignore')\n+        try:\n+            self.afdn = getattr(np, ufuncname)\n+        except AttributeError:\n+            raise NotImplementedError()\n+        self.args = []\n+        for _, aarg in get_squares_().items():\n+            arg = (aarg,) * 1  # no nin\n+            try:\n+                self.afdn(*arg)\n+            except TypeError:\n+                continue\n+            self.args.append(arg)\n+\n+    def time_afdn_types(self, ufuncname):\n+        [self.afdn(*arg) for arg in self.args]\n+\n+\n class Broadcast(Benchmark):\n     def setup(self):\n         self.d = np.ones((50000, 100), dtype=np.float64)\n@@ -56,23 +85,179 @@ class UFunc(Benchmark):\n     def setup(self, ufuncname):\n         np.seterr(all='ignore')\n         try:\n-            self.f = getattr(np, ufuncname)\n+            self.ufn = getattr(np, ufuncname)\n         except AttributeError:\n             raise NotImplementedError()\n         self.args = []\n-        for t, a in get_squares_().items():\n-            arg = (a,) * self.f.nin\n+        for _, aarg in get_squares_().items():\n+            arg = (aarg,) * self.ufn.nin\n             try:\n-                self.f(*arg)\n+                self.ufn(*arg)\n             except TypeError:\n                 continue\n             self.args.append(arg)\n \n     def time_ufunc_types(self, ufuncname):\n-        [self.f(*arg) for arg in self.args]\n+        [self.ufn(*arg) for arg in self.args]\n+\n+\n+class MethodsV0(Benchmark):\n+    \"\"\" Benchmark for the methods which do not take any arguments\n+    \"\"\"\n+    params = [['__abs__', '__neg__', '__pos__'], TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_()\n+        self.xarg = values.get(npdtypes)[0]\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(self.xarg)\n+\n+\n+class NDArrayLRShifts(Benchmark):\n+    \"\"\" Benchmark for the shift methods\n+    \"\"\"\n+    params = [['__lshift__', '__rshift__'],\n+              ['intp', 'int8', 'int16',\n+                'int32', 'int64', 'uint8',\n+                'uint16', 'uint32', 'uint64']]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        self.vals = np.ones(1000,\n+                            dtype=getattr(np, npdtypes)) * \\\n+                            np.random.randint(9)\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(*[self.vals, 2])\n+\n+\n+class Methods0D(Benchmark):\n+    \"\"\"Zero dimension array methods\n+    \"\"\"\n+    params = [['__bool__', '__complex__', '__invert__',\n+               '__float__', '__int__'], TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        self.xarg = np.array(3, dtype=npdtypes)\n+        if (npdtypes.startswith('complex') and\n+           methname in ['__float__', '__int__']) or \\\n+           (npdtypes.startswith('int') and methname == '__invert__'):\n+            # Skip\n+            raise NotImplementedError\n+\n+    def time_ndarray__0d__(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        meth()\n+\n+\n+class MethodsV1(Benchmark):\n+    \"\"\" Benchmark for the methods which take an argument\n+    \"\"\"\n+    params = [['__and__', '__add__', '__eq__', '__floordiv__', '__ge__',\n+               '__gt__', '__le__', '__lt__', '__matmul__',\n+               '__mod__', '__mul__', '__ne__', '__or__',\n+               '__pow__', '__sub__', '__truediv__', '__xor__'],\n+              TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        if (\n+            npdtypes.startswith(\"complex\")\n+                and methname in [\"__floordiv__\", \"__mod__\"]\n+        ) or (\n+            not npdtypes.startswith(\"int\")\n+            and methname in [\"__and__\", \"__or__\", \"__xor__\"]\n+        ):\n+            raise NotImplementedError  # skip\n+        values = get_squares_().get(npdtypes)\n+        self.xargs = [values[0], values[1]]\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(*self.xargs)\n+\n+\n+class NDArrayGetItem(Benchmark):\n+    param_names = ['margs', 'msize']\n+    params = [[0, (0, 0), (-1, 0), [0, -1]],\n+              ['small', 'big']]\n+\n+    def setup(self, margs, msize):\n+        self.xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        self.xl = np.random.uniform(-1, 1, 50*50).reshape(50, 50)\n+\n+    def time_methods_getitem(self, margs, msize):\n+        if msize == 'small':\n+            mdat = self.xs\n+        elif msize == 'big':\n+            mdat = self.xl\n+        getattr(mdat, '__getitem__')(margs)\n+\n+\n+class NDArraySetItem(Benchmark):\n+    param_names = ['margs', 'msize']\n+    params = [[0, (0, 0), (-1, 0), [0, -1]],\n+              ['small', 'big']]\n+\n+    def setup(self, margs, msize):\n+        self.xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        self.xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+\n+    def time_methods_setitem(self, margs, msize):\n+        if msize == 'small':\n+            mdat = self.xs\n+        elif msize == 'big':\n+            mdat = self.xl\n+            mdat[margs] = 17\n+\n+\n+class DLPMethods(Benchmark):\n+    \"\"\" Benchmark for DLPACK helpers\n+    \"\"\"\n+    params = [['__dlpack__', '__dlpack_device__'], DLPACK_TYPES]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_()\n+        if npdtypes == 'bool':\n+            if version.parse(np.__version__) > version.parse(\"1.25\"):\n+                self.xarg = values.get('int16')[0].astype('bool')\n+            else:\n+                raise NotImplementedError(\"Not supported before v1.25\")\n+        else:\n+            self.xarg = values.get('int16')[0]\n+\n+    def time_ndarray_dlp(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        meth()\n+\n+\n+class NDArrayAsType(Benchmark):\n+    \"\"\" Benchmark for type conversion\n+    \"\"\"\n+    params = [list(itertools.combinations(TYPES1, 2))]\n+    param_names = ['typeconv']\n+    timeout = 10\n+\n+    def setup(self, typeconv):\n+        if typeconv[0] == typeconv[1]:\n+            raise NotImplementedError(\n+                    \"Skipping test for converting to the same dtype\")\n+        self.xarg = get_squares_().get(typeconv[0])\n+\n+    def time_astype(self, typeconv):\n+        self.xarg.astype(typeconv[1])\n+\n \n class UFuncSmall(Benchmark):\n-    \"\"\"  Benchmark for a selection of ufuncs on a small arrays and scalars \n+    \"\"\"  Benchmark for a selection of ufuncs on a small arrays and scalars\n \n     Since the arrays and scalars are small, we are benchmarking the overhead \n     of the numpy ufunc functionality"
            },
            {
                "filename": "benchmarks/benchmarks/common.py",
                "patch": "@@ -1,15 +1,16 @@\n-import numpy\n+import numpy as np\n import random\n import os\n-import functools\n+from functools import lru_cache\n+from pathlib import Path\n \n # Various pre-crafted datasets/variables for testing\n # !!! Must not be changed -- only appended !!!\n # while testing numpy we better not rely on numpy to produce random\n # sequences\n random.seed(1)\n # but will seed it nevertheless\n-numpy.random.seed(1)\n+np.random.seed(1)\n \n nx, ny = 1000, 1000\n # reduced squares based on indexes_rand, primarily for testing more\n@@ -21,37 +22,37 @@\n     'int16', 'float16',\n     'int32', 'float32',\n     'int64', 'float64',  'complex64',\n-    'longfloat', 'complex128',\n+    'longdouble', 'complex128',\n ]\n-if 'complex256' in numpy.sctypeDict:\n-    TYPES1.append('complex256')\n+if 'complex256' in np.sctypeDict:\n+    TYPES1.append('clongdouble')\n \n+DLPACK_TYPES = [\n+    'int16', 'float16',\n+    'int32', 'float32',\n+    'int64', 'float64',  'complex64',\n+    'complex128', 'bool',\n+]\n \n-def memoize(func):\n-    result = []\n-    def wrapper():\n-        if not result:\n-            result.append(func())\n-        return result[0]\n-    return wrapper\n-\n+# Path for caching\n+CACHE_ROOT = Path(__file__).resolve().parent.parent / 'env' / 'numpy_benchdata'\n \n # values which will be used to construct our sample data matrices\n # replicate 10 times to speed up initial imports of this helper\n # and generate some redundancy\n \n-@memoize\n+@lru_cache(typed=True)\n def get_values():\n-    rnd = numpy.random.RandomState(1)\n-    values = numpy.tile(rnd.uniform(0, 100, size=nx*ny//10), 10)\n+    rnd = np.random.RandomState(1)\n+    values = np.tile(rnd.uniform(0, 100, size=nx*ny//10), 10)\n     return values\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_squares():\n     values = get_values()\n-    squares = {t: numpy.array(values,\n-                              dtype=getattr(numpy, t)).reshape((nx, ny))\n+    squares = {t: np.array(values,\n+                              dtype=getattr(np, t)).reshape((nx, ny))\n                for t in TYPES1}\n \n     # adjust complex ones to have non-degenerated imagery part -- use\n@@ -62,63 +63,57 @@ def get_squares():\n     return squares\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_squares_():\n     # smaller squares\n     squares_ = {t: s[:nxs, :nys] for t, s in get_squares().items()}\n     return squares_\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_vectors():\n     # vectors\n     vectors = {t: s[0] for t, s in get_squares().items()}\n     return vectors\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_indexes():\n     indexes = list(range(nx))\n     # so we do not have all items\n     indexes.pop(5)\n     indexes.pop(95)\n \n-    indexes = numpy.array(indexes)\n+    indexes = np.array(indexes)\n     return indexes\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_indexes_rand():\n     rnd = random.Random(1)\n \n     indexes_rand = get_indexes().tolist()       # copy\n     rnd.shuffle(indexes_rand)         # in-place shuffle\n-    indexes_rand = numpy.array(indexes_rand)\n+    indexes_rand = np.array(indexes_rand)\n     return indexes_rand\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_indexes_():\n     # smaller versions\n     indexes = get_indexes()\n     indexes_ = indexes[indexes < nxs]\n     return indexes_\n \n \n-@memoize\n+@lru_cache(typed=True)\n def get_indexes_rand_():\n     indexes_rand = get_indexes_rand()\n     indexes_rand_ = indexes_rand[indexes_rand < nxs]\n     return indexes_rand_\n \n \n-CACHE_ROOT = os.path.dirname(__file__)\n-CACHE_ROOT = os.path.abspath(\n-    os.path.join(CACHE_ROOT, '..', 'env', 'numpy_benchdata')\n-)\n-\n-\n-@functools.cache\n+@lru_cache(typed=True)\n def get_data(size, dtype, ip_num=0, zeros=False, finite=True, denormal=False):\n     \"\"\"\n     Generates a cached random array that covers several scenarios that\n@@ -144,15 +139,14 @@ def get_data(size, dtype, ip_num=0, zeros=False, finite=True, denormal=False):\n     denormal:\n         Spreading subnormal numbers along with generated data.\n     \"\"\"\n-    np = numpy\n     dtype = np.dtype(dtype)\n     dname = dtype.name\n     cache_name = f'{dname}_{size}_{ip_num}_{int(zeros)}'\n     if dtype.kind in 'fc':\n         cache_name += f'{int(finite)}{int(denormal)}'\n     cache_name += '.bin'\n-    cache_path = os.path.join(CACHE_ROOT, cache_name)\n-    if os.path.exists(cache_path):\n+    cache_path = CACHE_ROOT / cache_name\n+    if cache_path.exists():\n         return np.fromfile(cache_path, dtype)\n \n     array = np.ones(size, dtype)\n@@ -214,8 +208,8 @@ def get_data(size, dtype, ip_num=0, zeros=False, finite=True, denormal=False):\n         for start, r in enumerate(rands):\n             array[start:len(r)*stride:stride] = r\n \n-    if not os.path.exists(CACHE_ROOT):\n-        os.mkdir(CACHE_ROOT)\n+    if not CACHE_ROOT.exists():\n+        CACHE_ROOT.mkdir(parents=True)\n     array.tofile(cache_path)\n     return array\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18940,
        "body": "Closes #18938\r\n\r\nMaybe. I don't have a 64-bit Windows test environment, but experiments in Compiler Explorer suggest this will help.\r\n\r\n`clang` reference using `__uint128_t` arithmetic: https://godbolt.org/z/Wdj7daqaK\r\nCompiler-inlined MSVC code: https://godbolt.org/z/dxsEWbbd8\r\nManually-inlined MSVC code: https://godbolt.org/z/fYWrrMfjj\r\n\r\n@bashtage Can you check the performance on this branch?",
        "changed_files": [
            {
                "filename": "numpy/random/src/pcg64/pcg64.h",
                "patch": "@@ -229,17 +229,6 @@ static inline void pcg_cm_step_r(pcg_state_setseq_128 *rng) {\n #endif\n }\n \n-static inline uint64_t pcg_output_cm_128_64(pcg128_t state) {\n-  uint64_t hi = state.high;\n-  uint64_t lo = state.low;\n-\n-  lo |= 1;\n-  hi ^= hi >> 32;\n-  hi *= 0xda942042e4dd58b5ULL;\n-  hi ^= hi >> 48;\n-  hi *= lo;\n-  return hi;\n-}\n \n static inline void pcg_cm_srandom_r(pcg_state_setseq_128 *rng, pcg128_t initstate, pcg128_t initseq) {\n   rng->state = PCG_128BIT_CONSTANT(0ULL, 0ULL);\n@@ -253,9 +242,35 @@ static inline void pcg_cm_srandom_r(pcg_state_setseq_128 *rng, pcg128_t initstat\n \n static inline uint64_t pcg_cm_random_r(pcg_state_setseq_128* rng)\n {\n-    uint64_t ret = pcg_output_cm_128_64(rng->state);\n-    pcg_cm_step_r(rng);\n-    return ret;\n+  /* Lots of manual inlining to help out certain compilers to generate\n+   * performant code. */\n+  uint64_t hi = rng->state.high;\n+  uint64_t lo = rng->state.low;\n+\n+  /* Run the DXSM output function on the pre-iterated state. */\n+  lo |= 1;\n+  hi ^= hi >> 32;\n+  hi *= 0xda942042e4dd58b5ULL;\n+  hi ^= hi >> 48;\n+  hi *= lo;\n+\n+  /* Run the CM step. */\n+#if defined _WIN32 && _MSC_VER >= 1900 && _M_AMD64\n+  uint64_t h1;\n+  pcg128_t product;\n+\n+  /* Manually inline the multiplication and addition using intrinsics */\n+  h1 = rng->state.high * PCG_CHEAP_MULTIPLIER_128;\n+  product.low =\n+      _umul128(rng->state.low, PCG_CHEAP_MULTIPLIER_128, &(product.high));\n+  product.high += h1;\n+  _addcarry_u64(_addcarry_u64(0, product.low, rng->inc.low, &(rng->state.low)),\n+                product.high, rng->inc.high, &(rng->state.high));\n+#else\n+  rng->state = pcg128_add(pcg128_mult_64(rng->state, PCG_CHEAP_MULTIPLIER_128),\n+                           rng->inc);\n+#endif\n+  return hi;\n }\n #else /* PCG_EMULATED_128BIT_MATH */\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18137,
        "body": "The linalg.matrix_power function allocates new space\r\nfor each matrix multiplication that it performs.\r\nFor large matrices, creating and using a buffer\r\ncan lead to performance benefits.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/linalg/linalg.py",
                "patch": "@@ -651,17 +651,19 @@ def matrix_power(a, n):\n         return fmatmul(a, a)\n \n     elif n == 3:\n-        return fmatmul(fmatmul(a, a), a)\n+        # create and use buffered space\n+        buffer = fmatmul(a, a)\n+        return fmatmul(buffer, a, out=buffer)\n \n     # Use binary decomposition to reduce the number of matrix multiplications.\n     # Here, we iterate over the bits of n, from LSB to MSB, raise `a` to\n     # increasing powers of 2, and multiply into the result as needed.\n     z = result = None\n     while n > 0:\n-        z = a if z is None else fmatmul(z, z)\n+        z = a.copy() if z is None else fmatmul(z, z, out=z)\n         n, bit = divmod(n, 2)\n         if bit:\n-            result = z if result is None else fmatmul(result, z)\n+            result = z.copy() if result is None else fmatmul(result, z, out=result)\n \n     return result\n "
            },
            {
                "filename": "numpy/linalg/tests/test_linalg.py",
                "patch": "@@ -1039,6 +1039,18 @@ def tz(mat):\n             if dt != object:\n                 tz(self.stacked.astype(dt))\n \n+    def test_power_is_three(self, dt):\n+        def tz(mat):\n+            mz = matrix_power(mat, 3)\n+            mmul = matmul if mat.dtype != object else dot\n+            assert_equal(mz, mmul(mat, mmul(mat, mat)))\n+            assert_equal(mz.dtype, mat.dtype)\n+\n+        for mat in self.rshft_all:\n+            tz(mat.astype(dt))\n+            if dt != object:\n+                tz(self.stacked.astype(dt))\n+\n     def test_power_is_minus_one(self, dt):\n         def tz(mat):\n             invmat = matrix_power(mat, -1)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23061,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\nThis closes #9477 and #23021, as part of a small project suggested by @seberg.",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/23061.new_feature.rst",
                "patch": "@@ -0,0 +1,6 @@\n+``vectorize`` can now be used as a decorator\n+--------------------------------------------\n+When using ``vectorize`` as a decorator, the user\n+needs to specify the keywords for the arguments.\n+``vectorize`` will continue to accept arguments\n+positionally when used normally."
            },
            {
                "filename": "numpy/lib/function_base.py",
                "patch": "@@ -2119,10 +2119,10 @@ def _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes,\n @set_module('numpy')\n class vectorize:\n     \"\"\"\n-    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n-              signature=None)\n+    vectorize(pyfunc=np._NoValue, otypes=None, doc=None, excluded=None,\n+    cache=False, signature=None)\n \n-    Generalized function class.\n+    Returns an object that acts like pyfunc, but takes arrays as input.\n \n     Define a vectorized function which takes a nested sequence of objects or\n     numpy arrays as inputs and returns a single numpy array or a tuple of numpy\n@@ -2136,8 +2136,9 @@ class vectorize:\n \n     Parameters\n     ----------\n-    pyfunc : callable\n+    pyfunc : callable, optional\n         A python function or method.\n+        Can be omitted to produce a decorator with keyword arguments.\n     otypes : str or list of dtypes, optional\n         The output data type. It must be specified as either a string of\n         typecode characters or a list of data type specifiers. There should\n@@ -2169,8 +2170,9 @@ class vectorize:\n \n     Returns\n     -------\n-    vectorized : callable\n-        Vectorized function.\n+    out : callable\n+        A vectorized function if ``pyfunc`` was provided,\n+        a decorator otherwise.\n \n     See Also\n     --------\n@@ -2267,18 +2269,44 @@ class vectorize:\n            [0., 0., 1., 2., 1., 0.],\n            [0., 0., 0., 1., 2., 1.]])\n \n+    Decorator syntax is supported.  The decorator can be called as\n+    a function to provide keyword arguments.\n+    >>>@np.vectorize\n+    ...def identity(x):\n+    ...    return x\n+    ...\n+    >>>identity([0, 1, 2])\n+    array([0, 1, 2])\n+    >>>@np.vectorize(otypes=[float])\n+    ...def as_float(x):\n+    ...    return x\n+    ...\n+    >>>as_float([0, 1, 2])\n+    array([0., 1., 2.])\n     \"\"\"\n-    def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n-                 cache=False, signature=None):\n+    def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,\n+                 excluded=None, cache=False, signature=None):\n+\n+        if (pyfunc != np._NoValue) and (not callable(pyfunc)):\n+            #Splitting the error message to keep\n+            #the length below 79 characters.\n+            part1 = \"When used as a decorator, \"\n+            part2 = \"only accepts keyword arguments.\"\n+            raise TypeError(part1 + part2)\n+\n         self.pyfunc = pyfunc\n         self.cache = cache\n         self.signature = signature\n-        self._ufunc = {}    # Caching to improve default performance\n+        if pyfunc != np._NoValue:\n+            self.__name__ = pyfunc.__name__\n \n+        self._ufunc = {}    # Caching to improve default performance\n+        self._doc = None\n+        self.__doc__ = doc\n         if doc is None:\n             self.__doc__ = pyfunc.__doc__\n         else:\n-            self.__doc__ = doc\n+            self._doc = doc\n \n         if isinstance(otypes, str):\n             for char in otypes:\n@@ -2300,7 +2328,15 @@ def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n         else:\n             self._in_and_out_core_dims = None\n \n-    def __call__(self, *args, **kwargs):\n+    def _init_stage_2(self, pyfunc, *args, **kwargs):\n+        self.__name__ = pyfunc.__name__\n+        self.pyfunc = pyfunc\n+        if self._doc is None:\n+            self.__doc__ = pyfunc.__doc__\n+        else:\n+            self.__doc__ = self._doc\n+\n+    def _call_as_normal(self, *args, **kwargs):\n         \"\"\"\n         Return arrays with the results of `pyfunc` broadcast (vectorized) over\n         `args` and `kwargs` not in `excluded`.\n@@ -2330,6 +2366,13 @@ def func(*vargs):\n \n         return self._vectorize_call(func=func, args=vargs)\n \n+    def __call__(self, *args, **kwargs):\n+        if self.pyfunc is np._NoValue:\n+            self._init_stage_2(*args, **kwargs)\n+            return self\n+\n+        return self._call_as_normal(*args, **kwargs)\n+\n     def _get_ufunc_and_otypes(self, func, args):\n         \"\"\"Return (ufunc, otypes).\"\"\"\n         # frompyfunc will fail if args is empty"
            },
            {
                "filename": "numpy/lib/tests/test_function_base.py",
                "patch": "@@ -1780,6 +1780,61 @@ class subclass(np.ndarray):\n         assert_equal(type(r), subclass)\n         assert_equal(r, m * v)\n \n+    def test_name(self):\n+        #See gh-23021\n+        @np.vectorize\n+        def f2(a, b):\n+            return a + b\n+\n+        assert f2.__name__ == 'f2'\n+\n+    def test_decorator(self):\n+        @vectorize\n+        def addsubtract(a, b):\n+            if a > b:\n+                return a - b\n+            else:\n+                return a + b\n+\n+        r = addsubtract([0, 3, 6, 9], [1, 3, 5, 7])\n+        assert_array_equal(r, [1, 6, 1, 2])\n+\n+    def test_docstring(self):\n+        @vectorize\n+        def f(x):\n+            \"\"\"Docstring\"\"\"\n+            return x\n+\n+        assert f.__doc__ == \"Docstring\"\n+\n+    def test_signature_otypes_decorator(self):\n+        @vectorize(signature='(n)->(n)', otypes=['float64'])\n+        def f(x):\n+            return x\n+\n+        r = f([1, 2, 3])\n+        assert_equal(r.dtype, np.dtype('float64'))\n+        assert_array_equal(r, [1, 2, 3])\n+        assert f.__name__ == 'f'\n+\n+    def test_bad_input(self):\n+        with assert_raises(TypeError):\n+            A = np.vectorize(pyfunc = 3)\n+\n+    def test_no_keywords(self):\n+        with assert_raises(TypeError):\n+            @np.vectorize(\"string\")\n+            def foo():\n+                return \"bar\"\n+\n+    def test_positional_regression_9477(self):\n+        # This supplies the first keyword argument as a positional,\n+        # to ensure that they are still properly forwarded after the\n+        # enhancement for #9477\n+        f = vectorize((lambda x: x), ['float64'])\n+        r = f([2])\n+        assert_equal(r.dtype, np.dtype('float64'))\n+\n \n class TestLeaks:\n     class A:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23190,
        "body": "This adds a new `_is_numeric` attribute to dtype classes which is true for numeric dtypes and false otherwise. \r\n\r\nConcretely, adding this will make it easier for me to add support in pandas for new dtypes. Currently pandas checks if `dtype.kind` is one of the letter codes corresponding to numeric types, but that won't be easily extendible for new custom dtypes. This will be. See #23180 for more information about the motivation for this feature.\r\n\r\nCurrently this only handles builtin dtypes and new-style custom dtypes, I'm not sure how legacy custom dtypes should be handled (if at all).\r\n\r\nI don't have a strong opinion about how this should be spelled. Perhaps it makes sense to make it a \"public\" attribute without the underscore.",
        "changed_files": [
            {
                "filename": "numpy/core/include/numpy/_dtype_api.h",
                "patch": "@@ -5,7 +5,7 @@\n #ifndef NUMPY_CORE_INCLUDE_NUMPY___DTYPE_API_H_\n #define NUMPY_CORE_INCLUDE_NUMPY___DTYPE_API_H_\n \n-#define __EXPERIMENTAL_DTYPE_API_VERSION 7\n+#define __EXPERIMENTAL_DTYPE_API_VERSION 8\n \n struct PyArrayMethodObject_tag;\n \n@@ -263,6 +263,7 @@ typedef int translate_loop_descrs_func(int nin, int nout,\n \n #define NPY_DT_ABSTRACT 1 << 1\n #define NPY_DT_PARAMETRIC 1 << 2\n+#define NPY_DT_NUMERIC 1 << 3\n \n #define NPY_DT_discover_descr_from_pyobject 1\n #define _NPY_DT_is_known_scalar_type 2"
            },
            {
                "filename": "numpy/core/src/multiarray/dtypemeta.c",
                "patch": "@@ -899,6 +899,10 @@ dtypemeta_wrap_legacy_descriptor(PyArray_Descr *descr)\n         }\n     }\n \n+    if (PyTypeNum_ISNUMBER(descr->type_num)) {\n+        dtype_class->flags |= NPY_DT_NUMERIC;\n+    }\n+\n     if (_PyArray_MapPyTypeToDType(dtype_class, descr->typeobj,\n             PyTypeNum_ISUSERDEF(dtype_class->type_num)) < 0) {\n         Py_DECREF(dtype_class);\n@@ -927,13 +931,19 @@ dtypemeta_get_parametric(PyArray_DTypeMeta *self) {\n     return PyBool_FromLong(NPY_DT_is_parametric(self));\n }\n \n+static PyObject *\n+dtypemeta_get_is_numeric(PyArray_DTypeMeta *self) {\n+    return PyBool_FromLong(NPY_DT_is_numeric(self));\n+}\n+\n /*\n  * Simple exposed information, defined for each DType (class).\n  */\n static PyGetSetDef dtypemeta_getset[] = {\n         {\"_abstract\", (getter)dtypemeta_get_abstract, NULL, NULL, NULL},\n         {\"_legacy\", (getter)dtypemeta_get_legacy, NULL, NULL, NULL},\n         {\"_parametric\", (getter)dtypemeta_get_parametric, NULL, NULL, NULL},\n+        {\"_is_numeric\", (getter)dtypemeta_get_is_numeric, NULL, NULL, NULL},\n         {NULL, NULL, NULL, NULL, NULL}\n };\n "
            },
            {
                "filename": "numpy/core/src/multiarray/dtypemeta.h",
                "patch": "@@ -7,10 +7,9 @@ extern \"C\" {\n \n #include \"numpy/_dtype_api.h\"\n \n-/* DType flags, currently private, since we may just expose functions */\n+/* DType flags, currently private, since we may just expose functions \n+   Other publicly visible flags are in _dtype_api.h                   */\n #define NPY_DT_LEGACY 1 << 0\n-#define NPY_DT_ABSTRACT 1 << 1\n-#define NPY_DT_PARAMETRIC 1 << 2\n \n \n typedef struct {\n@@ -53,6 +52,7 @@ typedef struct {\n #define NPY_DT_is_legacy(dtype) (((dtype)->flags & NPY_DT_LEGACY) != 0)\n #define NPY_DT_is_abstract(dtype) (((dtype)->flags & NPY_DT_ABSTRACT) != 0)\n #define NPY_DT_is_parametric(dtype) (((dtype)->flags & NPY_DT_PARAMETRIC) != 0)\n+#define NPY_DT_is_numeric(dtype) (((dtype)->flags & NPY_DT_NUMERIC) != 0)\n #define NPY_DT_is_user_defined(dtype) (((dtype)->type_num == -1))\n \n /*"
            },
            {
                "filename": "numpy/core/src/umath/_scaled_float_dtype.c",
                "patch": "@@ -248,7 +248,7 @@ static PyArray_DTypeMeta PyArray_SFloatDType = {{{\n     }},\n     .type_num = -1,\n     .scalar_type = NULL,\n-    .flags = NPY_DT_PARAMETRIC,\n+    .flags = NPY_DT_PARAMETRIC | NPY_DT_NUMERIC,\n     .dt_slots = &sfloat_slots,\n };\n "
            },
            {
                "filename": "numpy/core/tests/test_custom_dtypes.py",
                "patch": "@@ -231,3 +231,7 @@ def test_type_pickle():\n     assert res is SF\n \n     del np._ScaledFloatTestDType\n+\n+\n+def test_is_numeric():\n+    assert SF._is_numeric"
            },
            {
                "filename": "numpy/core/tests/test_dtype.py",
                "patch": "@@ -1579,6 +1579,18 @@ def test_dtype_superclass(self):\n         assert type(np.dtype).__module__ == \"numpy\"\n         assert np.dtype._abstract\n \n+    def test_is_numeric(self):\n+        all_codes = set(np.typecodes['All'])\n+        numeric_codes = set(np.typecodes['AllInteger'] +\n+                            np.typecodes['AllFloat'] + '?')\n+        non_numeric_codes = all_codes - numeric_codes\n+\n+        for code in numeric_codes:\n+            assert type(np.dtype(code))._is_numeric\n+\n+        for code in non_numeric_codes:\n+            assert not type(np.dtype(code))._is_numeric\n+\n \n class TestFromCTypes:\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 19770,
        "body": "The benchmark run in the CI gave the following response.\r\n```\r\n    before           after         delta\r\n    [79a8986b]       [bdd62b3]\r\n    <master>         <simd>\r\n\r\n-   546\u00b10\u03bcs          490\u00b10\u03bcs        10.26%  bench_core.PackBits.time_copysign\r\n```\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -112,6 +112,9 @@ def time_double_add(self):\n \n     def time_double_add_temp(self):\n         1. + self.d + 1.\n+    \n+    def time_copysign(self):\n+        np.copysign(self.d, self.d)\n \n \n class CustomScalar(Benchmark):\n@@ -166,7 +169,6 @@ def time_add_scalar_conv(self):\n     def time_add_scalar_conv_complex(self):\n         (self.y + self.z)\n \n-\n class ArgPack:\n     __slots__ = ['args', 'kwargs']\n     def __init__(self, *args, **kwargs):"
            },
            {
                "filename": "numpy/core/src/_simd/_simd.dispatch.c.src",
                "patch": "@@ -395,7 +395,7 @@ SIMD_IMPL_INTRIN_2(@intrin@_@sfx@, v@sfx@, v@sfx@, v@sfx@)\n \n #if @fp_only@\n /**begin repeat1\n- * #intrin = maxp, minp#\n+ * #intrin = maxp, minp, copysign#\n  */\n SIMD_IMPL_INTRIN_2(@intrin@_@sfx@, v@sfx@, v@sfx@, v@sfx@)\n /**end repeat1**/\n@@ -629,7 +629,7 @@ SIMD_INTRIN_DEF(@intrin@_@sfx@)\n \n #if @fp_only@\n /**begin repeat1\n- * #intrin = maxp, minp#\n+ * #intrin = maxp, minp, copysign#\n  */\n SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/"
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/math.h",
                "patch": "@@ -105,4 +105,14 @@ NPY_FINLINE npyv_s64 npyv_min_s64(npyv_s64 a, npyv_s64 b)\n     return _mm256_blendv_epi8(a, b, _mm256_cmpgt_epi64(a, b));\n }\n \n+// copysign\n+NPY_FINLINE npyv_f32 npyv_copysign_f32(npyv_f32 a, npyv_f32 b)\n+{\n+    return _mm256_or_ps(a, _mm256_and_ps(b, _mm256_set1_ps(-0.0)));\n+}\n+NPY_FINLINE npyv_f64 npyv_copysign_f64(npyv_f64 a, npyv_f64 b)\n+{\n+    return _mm256_or_pd(a, _mm256_and_pd(b, _mm256_set1_pd(-0.0)));\n+}\n+\n #endif // _NPY_SIMD_AVX2_MATH_H"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/operators.h",
                "patch": "@@ -321,4 +321,14 @@ NPY_FINLINE npyv_b32 npyv_notnan_f32(npyv_f32 a)\n NPY_FINLINE npyv_b64 npyv_notnan_f64(npyv_f64 a)\n { return _mm512_cmp_pd_mask(a, a, _CMP_ORD_Q); }\n \n+// copysign\n+NPY_FINLINE npyv_f32 npyv_copysign_f32(npyv_f32 a, npyv_f32 b)\n+{\n+    return npyv_or_f32(a, npyv_and_f32(b, _mm512_set1_ps(-0.0)));\n+}\n+NPY_FINLINE npyv_f64 npyv_copysign_f64(npyv_f64 a, npyv_f64 b)\n+{\n+    return npyv_or_f64(a, npyv_and_f64(b, _mm512_set1_pd(-0.0)));\n+}\n+\n #endif // _NPY_SIMD_AVX512_OPERATORS_H"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/math.h",
                "patch": "@@ -153,4 +153,16 @@ NPY_FINLINE npyv_s64 npyv_min_s64(npyv_s64 a, npyv_s64 b)\n     return vbslq_s64(npyv_cmplt_s64(a, b), a, b);\n }\n \n+// copysign\n+NPY_FINLINE npyv_f32 npyv_copysign_f32(npyv_f32 a, npyv_f32 b)\n+{\n+    return vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(a), vandq_u32(vreinterpretq_u32_f32(b), vdupq_n_u32(0x80000000))));\n+}\n+#if NPY_SIMD_F64\n+    NPY_FINLINE npyv_f64 npyv_copysign_f64(npyv_f64 a, npyv_f64 b)\n+    {\n+        return vreinterpretq_f64_u64(vorrq_u64(vreinterpretq_u64_f64(a), vandq_u64(vreinterpretq_u64_f64(b), vdupq_n_u64(0x8000000000000000))));\n+    }\n+#endif\n+\n #endif // _NPY_SIMD_NEON_MATH_H"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/math.h",
                "patch": "@@ -143,4 +143,14 @@ NPY_FINLINE npyv_s64 npyv_min_s64(npyv_s64 a, npyv_s64 b)\n     return npyv_select_s64(npyv_cmplt_s64(a, b), a, b);\n }\n \n+// copysign\n+NPY_FINLINE npyv_f32 npyv_copysign_f32(npyv_f32 a, npyv_f32 b)\n+{\n+    return _mm_or_ps(a, _mm_and_ps(b, _mm_set1_ps(-0.0)));\n+}\n+NPY_FINLINE npyv_f64 npyv_copysign_f64(npyv_f64 a, npyv_f64 b)\n+{\n+    return _mm_or_pd(a, _mm_and_pd(b, _mm_set1_pd(-0.0)));\n+}\n+\n #endif // _NPY_SIMD_SSE_MATH_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vsx/math.h",
                "patch": "@@ -69,4 +69,14 @@ NPY_FINLINE npyv_f64 npyv_square_f64(npyv_f64 a)\n #define npyv_min_u64 vec_min\n #define npyv_min_s64 vec_min\n \n+// copysign\n+NPY_FINLINE npyv_f32 npyv_copysign_f32(npyv_f32 a, npyv_f32 b)\n+{\n+    return npyv_or_f32(a, npyv_and_f32(b, npyv_setall_f32(-0.0)));\n+}\n+NPY_FINLINE npyv_f64 npyv_copysign_f64(npyv_f64 a, npyv_f64 b)\n+{\n+    return npyv_or_f64(a, npyv_and_f64(b, npyv_setall_f64(-0.0)));\n+}\n+\n #endif // _NPY_SIMD_VSX_MATH_H"
            },
            {
                "filename": "numpy/core/tests/test_simd.py",
                "patch": "@@ -415,6 +415,16 @@ def test_special_cases(self):\n         nnan = self.notnan(self.setall(self._nan()))\n         assert nnan == [0]*self.nlanes\n \n+    def test_copysign(self):\n+        data_a = self._data()\n+        data_b = self._data(reverse=True)\n+        vdata_a = self.load(data_a)\n+        vdata_b = self.load(data_b)\n+\n+        data_copysign = [math.copysign(a, b) for a, b in zip(data_a, data_b)]\n+        vcopysign = self.copysign(vdata_a, vdata_b)\n+        assert vcopysign == data_copysign\n+\n class _SIMD_ALL(_Test_Utility):\n     \"\"\"\n     To test all vector types at once"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 9055,
        "body": "Find the indices into an array `a` whose values match those queried in `v`.\r\n\r\nThis is a PR to go with issue #9052. I'm not at all sure where this should live. It's in `fromnumeric.py` right now, which is probably wrong. Of course, there would also need to be documentation changes and some kind of tests added, but I thought I'd put this here now to get some feedback.",
        "changed_files": [
            {
                "filename": "numpy/core/fromnumeric.py",
                "patch": "@@ -1019,6 +1019,74 @@ def argmin(a, axis=None, out=None):\n     return _wrapfunc(a, 'argmin', axis=axis, out=out)\n \n \n+def search(a, v, fill_value=None, which='first'):\n+    \"\"\"\n+    Find indices into flattened array `a` where elements in `v` match those\n+    in `a`, such that`a[indices] == v`.\n+\n+    Parameters\n+    ----------\n+    a : 1-D array_like\n+        Input array to search.\n+    v : array_like\n+        Values to search for in `a`.\n+    fill_value : scalar or `None`\n+        Index value to return for elements in `v` that are not in `a`.\n+        If `None`, raise an error for such missing elements.\n+    which : {'first', 'last'}\n+        If an element in `v` matches multiple occurrences of the same\n+        element in `a`, return only the index of the first or last matching\n+        element in `a`.\n+\n+    Returns\n+    -------\n+    indices : array of ints\n+        Array of indices into `a` with the same shape as `v`.\n+\n+    See Also\n+    --------\n+    searchsorted: Find indices into sorted array `a` where elements in `v`\n+                  should be inserted to maintain order.\n+\n+    Notes\n+    -----\n+    ``np.search(a, v)`` is roughly equivalent to\n+    ``np.array([a.index(item) for item in v])`` if `a` and `v` are 1-D sequences.\n+\n+    Adapted from http://stackoverflow.com/a/8251668\n+\n+    Examples\n+    --------\n+    >>> a = [3, -1, -2, -4, 1, 1, -1, 0, -3, 3]\n+    >>> v = [[15, 1, -3, -2, -4], [-1, 3, 2, 1, 0]]\n+    >>> np.search(a, v, fill_value=-1)\n+    array([[-1,  4,  8,  2,  3],\n+           [ 1,  0, -1,  4,  7]])\n+    >>> np.search(a, v, fill_value=-1, which='last')\n+    array([[-1,  5,  8,  2,  3],\n+           [ 6,  9, -1,  5,  7]])\n+    >>> np.search(a, v)\n+    ValueError: array `v` is not a subset of input array `a`\n+\n+    \"\"\"\n+    a = np.ravel(a)\n+    sortis = a.argsort()\n+    lis = a.searchsorted(v, side='left', sorter=sortis)\n+    ris = a.searchsorted(v, side='right', sorter=sortis)\n+    sideis = {'first':lis, 'last':ris-1}[which]\n+    hits = lis != ris # elements in v that are in a\n+    misses = np.logical_not(hits) # elements in v that are not in a\n+    if fill_value is None:\n+        if misses.any():\n+            raise ValueError(\"array `v` is not a subset of input array `a`\")\n+        indices = sortis[sideis]\n+    else:\n+        indices = np.zeros_like(v, dtype=int)\n+        indices[hits] = sortis[sideis[hits]]\n+        indices[misses] = fill_value\n+    return indices\n+\n+\n def searchsorted(a, v, side='left', sorter=None):\n     \"\"\"\n     Find indices where elements should be inserted to maintain order."
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 8924,
        "body": "Scalar logical loops just boil down to memcpy or memset. While not very\r\nuseful in general, some cases like masked arrays can profit when\r\ncreating zero stride boolean arrays for readonly views.",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -703,6 +703,27 @@ BOOL_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED\n }\n /**end repeat**/\n \n+#define BOOL_SCALAR_OP(cond, in_idx, inp, value) \\\n+    if (!(cond)) { \\\n+        if (steps[in_idx] == 1 && steps[2] == 1) { \\\n+            memcpy(args[2], args[in_idx], dimensions[0]); \\\n+        } \\\n+        else { \\\n+            BINARY_LOOP { \\\n+                *op1 = *inp; \\\n+            } \\\n+        } \\\n+    } \\\n+    else { \\\n+        if (steps[2] == 1) { \\\n+            memset(args[2], value, dimensions[0]); \\\n+        } \\\n+        else { \\\n+            BINARY_LOOP { \\\n+                *op1 = value; \\\n+            } \\\n+        } \\\n+    }\n \n /**begin repeat\n  * #kind = logical_and, logical_or#\n@@ -763,7 +784,24 @@ BOOL_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED\n         }\n     }\n     else {\n-        if (run_binary_simd_@kind@_BOOL(args, dimensions, steps)) {\n+        /*\n+         * scalar operations can occur e.g. in masked arrays where a\n+         * stride 0 array is used to represent no or full mask\n+         */\n+        if (steps[0] == 0) {\n+            if (steps[1] == 0) {\n+                /* only the memset part of the loop */\n+                const npy_bool res = (*args[0] @OP@ *args[1]);\n+                BOOL_SCALAR_OP(1, 1, ip2, res);\n+            }\n+            else {\n+                BOOL_SCALAR_OP((*args[0] @SC@ 0), 1, ip2, !@and@);\n+            }\n+        }\n+        else if (steps[1] == 0) {\n+            BOOL_SCALAR_OP((*args[1] @SC@ 0), 0, ip1, !@and@);\n+        }\n+        else if (run_binary_simd_@kind@_BOOL(args, dimensions, steps)) {\n             return;\n         }\n         else {\n@@ -788,9 +826,14 @@ BOOL_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED\n         return;\n     }\n     else {\n-        UNARY_LOOP {\n-            npy_bool in1 = *(npy_bool *)ip1;\n-            *((npy_bool *)op1) = in1 @OP@ 0;\n+        if (steps[0] == 0 && steps[1] == 1) {\n+            memset(args[1], *args[0] @OP@ 0, dimensions[0]);\n+        }\n+        else {\n+            UNARY_LOOP {\n+                npy_bool in1 = *(npy_bool *)ip1;\n+                *((npy_bool *)op1) = in1 @OP@ 0;\n+            }\n         }\n     }\n }"
            },
            {
                "filename": "numpy/core/tests/test_numeric.py",
                "patch": "@@ -288,6 +288,17 @@ def test_logical_not_abs(self):\n         np.abs(self.t, out=self.o)\n         assert_array_equal(self.o, self.t)\n \n+        # test scalar array specialization\n+        norm = np.zeros(114, dtype=bool)[1:]\n+        scalar_f = np.ndarray(buffer=np.zeros((), dtype=bool), strides=(0,),\n+                              shape=norm.shape, dtype=bool)\n+        scalar_t = np.ndarray(buffer=np.ones((), dtype=bool), strides=(0,),\n+                              shape=norm.shape, dtype=bool)\n+        assert_array_equal(~scalar_f, ~norm)\n+        assert_array_equal(np.abs(scalar_f), norm)\n+        assert_array_equal(~scalar_t, norm)\n+        assert_array_equal(np.abs(scalar_t), ~norm)\n+\n     def test_logical_and_or_xor(self):\n         assert_array_equal(self.t | self.t, self.t)\n         assert_array_equal(self.f | self.f, self.f)\n@@ -310,16 +321,44 @@ def test_logical_and_or_xor(self):\n \n         assert_array_equal(self.nm & self.t, self.nm)\n         assert_array_equal(self.im & self.f, False)\n+        assert_array_equal(self.t & self.nm, self.nm)\n+        assert_array_equal(self.f & self.im, False)\n         assert_array_equal(self.nm & True, self.nm)\n         assert_array_equal(self.im & False, self.f)\n+        assert_array_equal(True  & self.nm, self.nm)\n+        assert_array_equal(False & self.im, self.f)\n         assert_array_equal(self.nm | self.t, self.t)\n         assert_array_equal(self.im | self.f, self.im)\n+        assert_array_equal(self.t | self.nm, self.t)\n+        assert_array_equal(self.f | self.im, self.im)\n         assert_array_equal(self.nm | True, self.t)\n         assert_array_equal(self.im | False, self.im)\n+        assert_array_equal(True  | self.nm , self.t)\n+        assert_array_equal(False | self.im, self.im)\n         assert_array_equal(self.nm ^ self.t, self.im)\n         assert_array_equal(self.im ^ self.f, self.im)\n+        assert_array_equal(self.t ^ self.nm, self.im)\n+        assert_array_equal(self.f ^ self.im, self.im)\n         assert_array_equal(self.nm ^ True, self.im)\n         assert_array_equal(self.im ^ False, self.im)\n+        assert_array_equal(True  ^ self.nm, self.im)\n+        assert_array_equal(False ^ self.im, self.im)\n+\n+        # test scalar array specialization\n+        norm = np.zeros(114, dtype=bool)[1:]\n+        scalar_f = np.ndarray(buffer=np.zeros((), dtype=bool), strides=(0,),\n+                              shape=norm.shape, dtype=bool)\n+        scalar_t = np.ndarray(buffer=np.ones((), dtype=bool), strides=(0,),\n+                              shape=norm.shape, dtype=bool)\n+        assert_array_equal(scalar_f & scalar_t, norm)\n+        assert_array_equal(scalar_t & scalar_t, ~norm)\n+        assert_array_equal(scalar_f & scalar_f, norm)\n+        assert_array_equal(scalar_f | scalar_t, ~norm)\n+        assert_array_equal(scalar_t | scalar_t, ~norm)\n+        assert_array_equal(scalar_f | scalar_f, norm)\n+        assert_array_equal(scalar_f ^ scalar_t, ~norm)\n+        assert_array_equal(scalar_t ^ scalar_t, norm)\n+        assert_array_equal(scalar_f ^ scalar_f, norm)\n \n \n class TestBoolCmp(TestCase):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 17677,
        "body": "Based on [this issue](https://github.com/numpy/numpy/issues/17676), this PR modifies `histogramdd` to directly calculate bin indices instead of using `searchsorted` when uniform binning is specified (via `bins` and `range` kwargs). This optimization is already being done for 1D `histogram`. For large enough inputs, this can result in 4-5x speedups for 2D uniform binning over the current `searchsorted` method:\r\n\r\n![image](https://user-images.githubusercontent.com/5760027/97631496-5130de00-19ee-11eb-802f-719b314b69b7.png)\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_function_base.py",
                "patch": "@@ -20,6 +20,7 @@ def time_fine_binning(self):\n class Histogram2D(Benchmark):\n     def setup(self):\n         self.d = np.linspace(0, 100, 200000).reshape((-1,2))\n+        self.normal = np.random.normal(0, 1, 5000000).reshape((-1,2))\n \n     def time_full_coverage(self):\n         np.histogramdd(self.d, (200, 200), ((0, 100), (0, 100)))\n@@ -30,6 +31,9 @@ def time_small_coverage(self):\n     def time_fine_binning(self):\n         np.histogramdd(self.d, (10000, 10000), ((0, 100), (0, 100)))\n \n+    def time_normal_distribution(self):\n+        np.histogramdd(self.normal, (1000, 1000), ((-10, 10), (-10, 10)))\n+\n \n class Bincount(Benchmark):\n     def setup(self):"
            },
            {
                "filename": "numpy/lib/histograms.py",
                "patch": "@@ -1019,6 +1019,7 @@ def histogramdd(sample, bins=10, range=None, normed=None, weights=None,\n         N, D = sample.shape\n \n     nbin = np.empty(D, int)\n+    uniform_binnings = D*[False]\n     edges = D*[None]\n     dedges = D*[None]\n     if weights is not None:\n@@ -1056,6 +1057,7 @@ def histogramdd(sample, bins=10, range=None, normed=None, weights=None,\n                 ) from e\n                 \n             edges[i] = np.linspace(smin, smax, n + 1)    \n+            uniform_binnings[i] = True\n         elif np.ndim(bins[i]) == 1:\n             edges[i] = np.asarray(bins[i])\n             if np.any(edges[i][:-1] > edges[i][1:]):\n@@ -1070,11 +1072,19 @@ def histogramdd(sample, bins=10, range=None, normed=None, weights=None,\n         dedges[i] = np.diff(edges[i])\n \n     # Compute the bin number each sample falls into.\n-    Ncount = tuple(\n-        # avoid np.digitize to work around gh-11022\n-        np.searchsorted(edges[i], sample[:, i], side='right')\n-        for i in _range(D)\n-    )\n+    Ncount = D*[None]\n+    for i in _range(D):\n+        if uniform_binnings[i]:\n+            # avoid np.searchsorted if dimension has uniform binning\n+            left, right = edges[i][0], edges[i][-1]\n+            nbins = len(edges[i]) - 1\n+            ib = 1 + (nbins * (sample[:, i] - left) / (right - left))\n+            ib[sample[:, i] < left] = 0\n+            ib[sample[:, i] > right] = nbins + 1\n+            Ncount[i] = ib.astype(int)\n+        else:\n+            # avoid np.digitize to work around gh-11022\n+            Ncount[i] = np.searchsorted(edges[i], sample[:, i], side='right')\n \n     # Using digitize, values that fall on an edge are put in the right bin.\n     # For the rightmost bin, we want values equal to the right edge to be"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18968,
        "body": "Co-authored-by: alinanesen <alina.nesen@gmail.com> / GitHub username: [@alinanesen](https://github.com/alinanesen)\r\n\r\nModified np.ma.unique to call the np.unique with the array without the masked values.\r\n\r\nCloses gh-14804",
        "changed_files": [
            {
                "filename": "numpy/ma/extras.py",
                "patch": "@@ -1075,7 +1075,7 @@ def unique(ar1, return_index=False, return_inverse=False):\n     numpy.unique : Equivalent function for ndarrays.\n \n     \"\"\"\n-    output = np.unique(ar1,\n+    output = np.unique(ar1.data[~ar1.mask],\n                        return_index=return_index,\n                        return_inverse=return_inverse)\n     if isinstance(output, tuple):"
            },
            {
                "filename": "numpy/ma/tests/test_extras.py",
                "patch": "@@ -1548,6 +1548,12 @@ def test_setdiff1d_char_array(self):\n         a = np.array(['a', 'b', 'c'])\n         b = np.array(['a', 'b', 's'])\n         assert_array_equal(setdiff1d(a, b), np.array(['c']))\n+    \n+    def test_uint8_unique_mask(self):\n+        # Regression test for gh-14804\n+        m = np.ma.masked_array(np.array([50, 255, 1, 255, 1], dtype=np.uint8), mask=[False, False, True, False, True])\n+        expected =[ 50, 255]\n+        assert_equal(np.ma.unique(m).data, expected)\n \n \n class TestShapeBase:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23186,
        "body": "This tries to clean up the bounds checking for random integers a bit.  By doing the following things:\r\n\r\n1. Add paths to check whether `np.can_cast` is true and casting is safe.  In that case, the values cannot possibly be out of bounds.\r\n2. When they _can_ be out of bounds and we are dealing with int64/uint64, assume overflows are plausible/likely.  In that case always use the slow path by manually going via Python floats `int` (manual casting using `int()`) to ensure correct bounds checking.\r\n3. Since we converted the arrays, we can then safely compare them.  This also means I move the `is_open` into the loop in the int64 path (to match the other path).\r\n\r\nSince conversion errors don't match the `ValueError` adds a `ValueError` explicitly on the lower (path is hit in tests).\r\n\r\nDoing this lets the test suite pass largely with weak promotion enabled.  For the upcasting paths I need to add `np.int32({{ub}})` to ensure correct casting when weak.\r\n\r\n(Test suite still chokes in a few places in random due to `isnan(very_large_integer)` failing no the expected way.)\r\n\r\n---\r\n\r\nI suspect this might make some outrageous inputs a bit safer, but not sure.  Unfortunately, this is a bit tricky code :(.",
        "changed_files": [
            {
                "filename": "numpy/random/_bounded_integers.pyx.in",
                "patch": "@@ -99,17 +99,24 @@ cdef object _rand_{{nptype}}_broadcast(np.ndarray low, np.ndarray high, object s\n     is_open = not closed\n     low_arr = <np.ndarray>low\n     high_arr = <np.ndarray>high\n-    if np.any(np.less(low_arr, {{lb}})):\n+\n+    if np.can_cast(low_arr, np.{{otype}}):\n+        pass  # cannot be out-of-bounds\n+    elif np.any(np.less(low_arr, np.{{otype}}({{lb}}))):\n         raise ValueError('low is out of bounds for {{nptype}}')\n+\n     if closed:\n         high_comp = np.greater_equal\n         low_high_comp = np.greater\n     else:\n         high_comp = np.greater\n         low_high_comp = np.greater_equal\n \n-    if np.any(high_comp(high_arr, {{ub}})):\n+    if np.can_cast(high_arr, np.{{otype}}):\n+        pass  # cannot be out-of-bounds\n+    elif np.any(high_comp(high_arr, np.{{nptype_up}}({{ub}}))):\n         raise ValueError('high is out of bounds for {{nptype}}')\n+\n     if np.any(low_high_comp(low_arr, high_arr)):\n         raise ValueError(format_bounds_error(closed, low_arr))\n \n@@ -165,50 +172,69 @@ cdef object _rand_{{nptype}}_broadcast(object low, object high, object size,\n     64 bit integer type.\n     \"\"\"\n \n-    cdef np.ndarray low_arr, high_arr, out_arr, highm1_arr\n+    cdef np.ndarray low_arr, low_arr_orig, high_arr, high_arr_orig, out_arr\n     cdef np.npy_intp i, cnt, n\n     cdef np.broadcast it\n     cdef object closed_upper\n     cdef uint64_t *out_data\n     cdef {{nptype}}_t *highm1_data\n     cdef {{nptype}}_t low_v, high_v\n-    cdef uint64_t rng, last_rng, val, mask, off, out_val\n+    cdef uint64_t rng, last_rng, val, mask, off, out_val, is_open\n \n-    low_arr = <np.ndarray>low\n-    high_arr = <np.ndarray>high\n+    low_arr_orig = <np.ndarray>low\n+    high_arr_orig = <np.ndarray>high\n \n-    if np.any(np.less(low_arr, {{lb}})):\n-        raise ValueError('low is out of bounds for {{nptype}}')\n-    dt = high_arr.dtype\n-    if closed or np.issubdtype(dt, np.integer):\n-        # Avoid object dtype path if already an integer\n-        high_lower_comp = np.less if closed else np.less_equal\n-        if np.any(high_lower_comp(high_arr, {{lb}})):\n-            raise ValueError(format_bounds_error(closed, low_arr))\n-        high_m1 = high_arr if closed else high_arr - dt.type(1)\n-        if np.any(np.greater(high_m1, {{ub}})):\n-            raise ValueError('high is out of bounds for {{nptype}}')\n-        highm1_arr = <np.ndarray>np.PyArray_FROM_OTF(high_m1, np.{{npctype}}, np.NPY_ALIGNED | np.NPY_FORCECAST)\n+    is_open = not closed\n+\n+    # The following code tries to cast safely, but failing that goes via\n+    # Python `int()` because it is very difficult to cast integers in a\n+    # truly safe way (i.e. so it raises on out-of-bound).\n+    # We correct if the interval is not closed in this step if we go the long\n+    # route.  (Not otherwise, since the -1 could overflow in theory.)\n+    if np.can_cast(low_arr_orig, np.{{otype}}):\n+        low_arr = <np.ndarray>np.PyArray_FROM_OTF(low_arr_orig, np.{{npctype}}, np.NPY_ALIGNED)\n+    else:\n+        low_arr = <np.ndarray>np.empty_like(low_arr_orig, dtype=np.{{otype}})\n+        flat = low_arr_orig.flat\n+        low_data = <{{nptype}}_t *>np.PyArray_DATA(low_arr)\n+        cnt = np.PyArray_SIZE(low_arr)\n+        for i in range(cnt):\n+            lower = int(flat[i])\n+            if lower < {{lb}} or lower > {{ub}}:\n+                raise ValueError('low is out of bounds for {{nptype}}')\n+            low_data[i] = lower\n+\n+    del low_arr_orig\n+\n+    if np.can_cast(high_arr_orig, np.{{otype}}):\n+        high_arr = <np.ndarray>np.PyArray_FROM_OTF(high_arr_orig, np.{{npctype}}, np.NPY_ALIGNED)\n     else:\n-        # If input is object or a floating type\n-        highm1_arr = <np.ndarray>np.empty_like(high_arr, dtype=np.{{otype}})\n-        highm1_data = <{{nptype}}_t *>np.PyArray_DATA(highm1_arr)\n+        high_arr = np.empty_like(high_arr_orig, dtype=np.{{otype}})\n+        flat = high_arr_orig.flat\n+        high_data = <{{nptype}}_t *>np.PyArray_DATA(high_arr)\n         cnt = np.PyArray_SIZE(high_arr)\n-        flat = high_arr.flat\n         for i in range(cnt):\n-            # Subtract 1 since generator produces values on the closed int [off, off+rng]\n-            closed_upper = int(flat[i]) - 1\n+            closed_upper = int(flat[i]) - is_open\n             if closed_upper > {{ub}}:\n                 raise ValueError('high is out of bounds for {{nptype}}')\n             if closed_upper < {{lb}}:\n                 raise ValueError(format_bounds_error(closed, low_arr))\n-            highm1_data[i] = <{{nptype}}_t>closed_upper\n+            high_data[i] = closed_upper\n \n-    if np.any(np.greater(low_arr, highm1_arr)):\n-        raise ValueError(format_bounds_error(closed, low_arr))\n+        is_open = 0  # we applied is_open in this path already\n \n-    high_arr = highm1_arr\n-    low_arr = <np.ndarray>np.PyArray_FROM_OTF(low, np.{{npctype}}, np.NPY_ALIGNED | np.NPY_FORCECAST)\n+    del high_arr_orig\n+\n+    # Since we have the largest supported integer dtypes, they must be within\n+    # range at this point; otherwise conversion would have failed.  Check that\n+    # it is never true that `high <= low`` if closed and `high < low` if not\n+    if not is_open:\n+        low_high_comp = np.greater\n+    else:\n+        low_high_comp = np.greater_equal\n+\n+    if np.any(low_high_comp(low_arr, high_arr)):\n+        raise ValueError(format_bounds_error(closed, low_arr))\n \n     if size is not None:\n         out_arr = <np.ndarray>np.empty(size, np.{{otype}})\n@@ -224,8 +250,8 @@ cdef object _rand_{{nptype}}_broadcast(object low, object high, object size,\n         for i in range(n):\n             low_v = (<{{nptype}}_t*>np.PyArray_MultiIter_DATA(it, 0))[0]\n             high_v = (<{{nptype}}_t*>np.PyArray_MultiIter_DATA(it, 1))[0]\n-            # Generator produces values on the closed int [off, off+rng], -1 subtracted above\n-            rng = <{{utype}}_t>(high_v - low_v)\n+            # Generator produces values on the closed int [off, off+rng]\n+            rng = <{{utype}}_t>((high_v - is_open) - low_v)\n             off = <{{utype}}_t>(<{{nptype}}_t>low_v)\n \n             if rng != last_rng:"
            },
            {
                "filename": "numpy/random/tests/test_generator_mt19937.py",
                "patch": "@@ -345,6 +345,8 @@ def test_bounds_checking(self, endpoint):\n                           endpoint=endpoint, dtype=dt)\n             assert_raises(ValueError, self.rfunc, 1, [0],\n                           endpoint=endpoint, dtype=dt)\n+            assert_raises(ValueError, self.rfunc, [ubnd+1], [ubnd],\n+                          endpoint=endpoint, dtype=dt)\n \n     def test_bounds_checking_array(self, endpoint):\n         for dt in self.itype:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22315,
        "body": "This patch adds AVX512 based 64-bit on AVX512-SKX and 16-bit sorting on AVX512-ICL. All the AVX512 sorting code has been reformatted as a separate header files and put in a separate folder. The AVX512 64-bit sorting is nearly 10x faster and AVX512 16-bit sorting is nearly 16x faster when compared to `std::sort`. ~~Still working on running NumPy benchmarks to get exact benchmark numbers~~\r\n\r\n16-bit int sped up by **17x** and float64 by nearly **10x** for random arrays. Benchmarked on a 11th Gen Tigerlake i7-1165G7. \r\n\r\n```\r\n      before           after         ratio\r\n     [41b6ac0e]       [2b384ac6]\r\n     <main>           <avxsort> \r\n-        44.2\u00b11\u03bcs       41.8\u00b10.5\u03bcs     0.95  bench_function_base.Sort.time_sort('quick', 'float32', ('sorted_block', 100))\r\n-      39.7\u00b10.1\u03bcs      37.1\u00b10.02\u03bcs     0.93  bench_function_base.Sort.time_sort('quick', 'int32', ('sorted_block', 1000))\r\n-        45.7\u00b13\u03bcs       42.6\u00b10.4\u03bcs     0.93  bench_function_base.Sort.time_sort('quick', 'float32', ('random',))\r\n-      39.8\u00b10.1\u03bcs       37.1\u00b10.4\u03bcs     0.93  bench_function_base.Sort.time_sort('quick', 'int32', ('random',))\r\n-     39.1\u00b10.03\u03bcs      36.4\u00b10.03\u03bcs     0.93  bench_function_base.Sort.time_sort('quick', 'int32', ('sorted_block', 100))\r\n-      39.9\u00b10.1\u03bcs       37.1\u00b10.2\u03bcs     0.93  bench_function_base.Sort.time_sort('quick', 'int32', ('reversed',))\r\n-      39.4\u00b10.2\u03bcs       36.3\u00b10.2\u03bcs     0.92  bench_function_base.Sort.time_sort('quick', 'int32', ('ordered',))\r\n-     40.6\u00b10.03\u03bcs       37.1\u00b10.3\u03bcs     0.91  bench_function_base.Sort.time_sort('quick', 'uint32', ('sorted_block', 100))\r\n-      46.4\u00b10.7\u03bcs       41.8\u00b10.4\u03bcs     0.90  bench_function_base.Sort.time_sort('quick', 'float32', ('ordered',))\r\n-      40.5\u00b10.4\u03bcs       36.4\u00b10.1\u03bcs     0.90  bench_function_base.Sort.time_sort('quick', 'int32', ('sorted_block', 10))\r\n-        42.5\u00b11\u03bcs      37.5\u00b10.08\u03bcs     0.88  bench_function_base.Sort.time_sort('quick', 'uint32', ('sorted_block', 1000))\r\n-        42.4\u00b11\u03bcs       36.8\u00b10.4\u03bcs     0.87  bench_function_base.Sort.time_sort('quick', 'uint32', ('ordered',))\r\n-        43.4\u00b13\u03bcs       37.4\u00b10.2\u03bcs     0.86  bench_function_base.Sort.time_sort('quick', 'uint32', ('sorted_block', 10))\r\n-        44.5\u00b11\u03bcs         38.0\u00b11\u03bcs     0.85  bench_function_base.Sort.time_sort('quick', 'uint32', ('reversed',))\r\n-        81.7\u00b14\u03bcs       67.9\u00b10.2\u03bcs     0.83  bench_function_base.Sort.time_sort('quick', 'float64', ('ordered',))\r\n-        45.7\u00b11\u03bcs       37.6\u00b10.2\u03bcs     0.82  bench_function_base.Sort.time_sort('quick', 'uint32', ('random',))\r\n-       119\u00b10.6\u03bcs       77.7\u00b10.2\u03bcs     0.65  bench_function_base.Sort.time_sort('quick', 'int64', ('reversed',))\r\n-         136\u00b15\u03bcs       67.1\u00b10.3\u03bcs     0.49  bench_function_base.Sort.time_sort('quick', 'float64', ('reversed',))\r\n-     70.6\u00b10.03\u03bcs      33.9\u00b10.07\u03bcs     0.48  bench_function_base.Sort.time_sort('quick', 'int16', ('ordered',))\r\n-         113\u00b11\u03bcs       34.8\u00b10.1\u03bcs     0.31  bench_function_base.Sort.time_sort('quick', 'int16', ('reversed',))\r\n-         325\u00b19\u03bcs      80.9\u00b10.07\u03bcs     0.25  bench_function_base.Sort.time_sort('quick', 'int64', ('sorted_block', 1000))\r\n-         452\u00b18\u03bcs         85.7\u00b14\u03bcs     0.19  bench_function_base.Sort.time_sort('quick', 'int64', ('sorted_block', 100))\r\n-         386\u00b15\u03bcs       70.3\u00b10.6\u03bcs     0.18  bench_function_base.Sort.time_sort('quick', 'float64', ('sorted_block', 1000))\r\n-         460\u00b15\u03bcs         82.0\u00b13\u03bcs     0.18  bench_function_base.Sort.time_sort('quick', 'int64', ('sorted_block', 10))\r\n-      94.5\u00b10.5ms       14.2\u00b10.4ms     0.15  bench_function_base.Sort.time_sort_worst\r\n-        537\u00b110\u03bcs         80.8\u00b12\u03bcs     0.15  bench_function_base.Sort.time_sort('quick', 'int64', ('random',))\r\n-        518\u00b110\u03bcs         73.9\u00b13\u03bcs     0.14  bench_function_base.Sort.time_sort('quick', 'float64', ('sorted_block', 100))\r\n-        533\u00b120\u03bcs       74.5\u00b10.3\u03bcs     0.14  bench_function_base.Sort.time_sort('quick', 'float64', ('sorted_block', 10))\r\n-        70.3\u00b11\u03bcs       8.54\u00b10.2\u03bcs     0.12  bench_function_base.Sort.time_sort('quick', 'int64', ('uniform',))\r\n-        652\u00b130\u03bcs         70.6\u00b11\u03bcs     0.11  bench_function_base.Sort.time_sort('quick', 'float64', ('random',))\r\n-         317\u00b13\u03bcs       32.5\u00b10.2\u03bcs     0.10  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 1000))\r\n-         439\u00b12\u03bcs         34.4\u00b12\u03bcs     0.08  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 100))\r\n-         133\u00b16\u03bcs       9.99\u00b10.4\u03bcs     0.07  bench_function_base.Sort.time_sort('quick', 'float64', ('uniform',))\r\n-         452\u00b11\u03bcs       33.4\u00b10.1\u03bcs     0.07  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 10))\r\n-      63.5\u00b10.8\u03bcs       4.44\u00b10.2\u03bcs     0.07  bench_function_base.Sort.time_sort('quick', 'int16', ('uniform',))\r\n-         555\u00b17\u03bcs       32.8\u00b10.1\u03bcs     0.06  bench_function_base.Sort.time_sort('quick', 'int16', ('random',))\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\n```\r\n\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": ".gitmodules",
                "patch": "@@ -4,3 +4,6 @@\n [submodule \"numpy/core/src/umath/svml\"]\n \tpath = numpy/core/src/umath/svml\n \turl = https://github.com/numpy/SVML.git\n+[submodule \"numpy/core/src/npysort/x86-simd-sort\"]\n+\tpath = numpy/core/src/npysort/x86-simd-sort\n+\turl = https://github.com/intel/x86-simd-sort"
            },
            {
                "filename": "azure-pipelines.yml",
                "patch": "@@ -184,6 +184,9 @@ stages:\n     - script: /bin/bash -c \"! vulture . --min-confidence 100 --exclude doc/,numpy/distutils/ | grep 'unreachable'\"\n       displayName: 'Check for unreachable code paths in Python modules'\n \n+    - script: git submodule update --init\n+      displayName: 'Fetch submodules'\n+\n     # prefer usage of clang over gcc proper\n     # to match likely scenario on many user mac machines\n     - script: python setup.py build -j 4 build_src --verbose-cfg install"
            },
            {
                "filename": "azure-steps-windows.yml",
                "patch": "@@ -1,4 +1,6 @@\n steps:\n+- script: git submodule update --init\n+  displayName: 'Fetch submodules'\n - task: UsePythonVersion@0\n   inputs:\n     versionSpec: $(PYTHON_VERSION)"
            },
            {
                "filename": "benchmarks/benchmarks/bench_function_base.py",
                "patch": "@@ -248,7 +248,7 @@ class Sort(Benchmark):\n         # In NumPy 1.17 and newer, 'merge' can be one of several\n         # stable sorts, it isn't necessarily merge sort.\n         ['quick', 'merge', 'heap'],\n-        ['float64', 'int64', 'float32', 'uint32', 'int32', 'int16'],\n+        ['float64', 'int64', 'float32', 'uint32', 'int32', 'int16', 'float16'],\n         [\n             ('random',),\n             ('ordered',),"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -453,6 +453,11 @@ if cc.get_id() == 'msvc'\n      staticlib_cflags +=  '-d2VolatileMetadata-'\n    endif\n endif\n+# TODO: change to \"feature\" option in meson_options.txt? See\n+# https://mesonbuild.com/Build-options.html#build-options\n+if get_option('disable-simd-optimizations')\n+  staticlib_cflags += '-DNPY_DISABLE_OPTIMIZATION'\n+endif\n \n npy_math_internal_h = custom_target(\n   output: 'npy_math_internal.h',\n@@ -594,7 +599,8 @@ np_core_dep = declare_dependency(\n     '.',\n     'include',\n     'src/common',\n-  ]\n+  ],\n+  compile_args: disable_simd_optimizations\n )\n \n \n@@ -718,7 +724,8 @@ src_multiarray = [\n   'src/multiarray/usertypes.c',\n   'src/multiarray/vdot.c',\n   src_file.process('src/common/npy_sort.h.src'),\n-  'src/npysort/x86-qsort.dispatch.cpp',\n+  'src/npysort/simd_qsort.dispatch.cpp',\n+  'src/npysort/simd_qsort_16bit.dispatch.cpp',\n   'src/npysort/quicksort.cpp',\n   'src/npysort/mergesort.cpp',\n   'src/npysort/timsort.cpp',"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -942,7 +942,6 @@ def get_mathlib_info(*args):\n             join('src', 'multiarray', 'usertypes.c'),\n             join('src', 'multiarray', 'vdot.c'),\n             join('src', 'common', 'npy_sort.h.src'),\n-            join('src', 'npysort', 'x86-qsort.dispatch.cpp'),\n             join('src', 'npysort', 'quicksort.cpp'),\n             join('src', 'npysort', 'mergesort.cpp'),\n             join('src', 'npysort', 'timsort.cpp'),\n@@ -964,6 +963,8 @@ def get_mathlib_info(*args):\n             # links to the arm64 npymath library,\n             # see gh-22673\n             join('src', 'npymath', 'arm64_exports.c'),\n+            join('src', 'npysort', 'simd_qsort.dispatch.cpp'),\n+            join('src', 'npysort', 'simd_qsort_16bit.dispatch.cpp'),\n             ]\n \n     #######################################################################"
            },
            {
                "filename": "numpy/core/src/common/common.hpp",
                "patch": "@@ -0,0 +1,11 @@\n+#ifndef NUMPY_CORE_SRC_COMMON_COMMON_HPP\n+#define NUMPY_CORE_SRC_COMMON_COMMON_HPP\n+/*\n+ * The following C++ headers are safe to be used standalone, however,\n+ * they are gathered to make it easy for us and for the future need to support PCH.\n+ */\n+#include \"npstd.hpp\"\n+#include \"half.hpp\"\n+#include \"meta.hpp\"\n+\n+#endif // NUMPY_CORE_SRC_COMMON_COMMON_HPP"
            },
            {
                "filename": "numpy/core/src/common/half.hpp",
                "patch": "@@ -0,0 +1,63 @@\n+#ifndef NUMPY_CORE_SRC_COMMON_HALF_HPP\n+#define NUMPY_CORE_SRC_COMMON_HALF_HPP\n+\n+#include \"npstd.hpp\"\n+\n+// TODO(@seiko2plus):\n+// - covers half-precision operations that being supported by numpy/halffloat.h\n+// - support __fp16\n+// - optimize x86 half<->single via cpu_fp16\n+// - optimize ppc64 half<->single via cpu_vsx3\n+\n+namespace np {\n+\n+/// @addtogroup cpp_core_types\n+/// @{\n+\n+/// Provides a type that implements 16-bit floating point (half-precision).\n+/// This type is ensured to be 16-bit size.\n+class Half final {\n+ public:\n+    /// @name Public Constructors\n+    /// @{\n+\n+    /// Default constructor. initialize nothing.\n+    Half() = default;\n+    /// Copy.\n+    Half(const Half &r)\n+    {\n+        data_.u = r.data_.u;\n+    }\n+\n+    /// @}\n+\n+    /// Returns a new Half constracted from the IEEE 754 binary16.\n+    /// @param b the value of binary16.\n+    static Half FromBits(uint16_t b)\n+    {\n+        Half f;\n+        f.data_.u = b;\n+        return f;\n+    }\n+    /// Returns the IEEE 754 binary16 representation.\n+    uint16_t Bits() const\n+    {\n+        return data_.u;\n+    }\n+\n+ private:\n+    union {\n+        uint16_t u;\n+/*\n+TODO(@seiko2plus): support __fp16\n+#ifdef NPY_HAVE_HW_FP16\n+        __fp16 f;\n+#endif\n+*/\n+    } data_;\n+};\n+\n+/// @} cpp_core_types\n+\n+} // namespace np\n+#endif // NUMPY_CORE_SRC_COMMON_HALF_HPP"
            },
            {
                "filename": "numpy/core/src/common/meta.hpp",
                "patch": "@@ -0,0 +1,54 @@\n+#ifndef NUMPY_CORE_SRC_COMMON_META_HPP\n+#define NUMPY_CORE_SRC_COMMON_META_HPP\n+\n+#include \"npstd.hpp\"\n+\n+namespace np { namespace meta {\n+/// @addtogroup cpp_core_meta\n+/// @{\n+\n+namespace details {\n+template<int size, bool unsig>\n+struct IntBySize;\n+\n+template<bool unsig>\n+struct IntBySize<sizeof(uint8_t), unsig> {\n+    using Type = typename std::conditional<\n+        unsig, uint8_t, int8_t>::type;\n+};\n+template<bool unsig>\n+struct IntBySize<sizeof(uint16_t), unsig> {\n+    using Type = typename std::conditional<\n+        unsig, uint16_t, int16_t>::type;\n+};\n+template<bool unsig>\n+struct IntBySize<sizeof(uint32_t), unsig> {\n+    using Type = typename std::conditional<\n+        unsig, uint32_t, int32_t>::type;\n+};\n+template<bool unsig>\n+struct IntBySize<sizeof(uint64_t), unsig> {\n+    using Type = typename std::conditional<\n+        unsig, uint64_t, int64_t>::type;\n+};\n+} // namespace details\n+\n+/// Provides safe conversion of any integer type synonyms\n+/// to a fixed-width integer type.\n+template<typename T>\n+struct FixedWidth {\n+    using TF_ = typename details::IntBySize<\n+        sizeof(T), std::is_unsigned<T>::value\n+    >::Type;\n+\n+    using Type = typename std::conditional<\n+        std::is_integral<T>::value, TF_, T\n+    >::type;\n+};\n+\n+/// @} cpp_core_meta\n+\n+}} // namespace np::meta\n+\n+#endif // NUMPY_CORE_SRC_COMMON_META_HPP\n+"
            },
            {
                "filename": "numpy/core/src/common/npstd.hpp",
                "patch": "@@ -0,0 +1,54 @@\n+#ifndef NUMPY_CORE_SRC_COMMON_NPSTD_HPP\n+#define NUMPY_CORE_SRC_COMMON_NPSTD_HPP\n+\n+#include <cstddef>\n+#include <cstring>\n+#include <cctype>\n+\n+#include <string>\n+#include <algorithm>\n+#include <utility>\n+#include <cstdlib>\n+#include <cmath>\n+#include <complex>\n+#include <type_traits>\n+\n+#include <numpy/npy_common.h>\n+\n+#include \"npy_config.h\"\n+\n+namespace np {\n+/// @addtogroup cpp_core_types\n+/// @{\n+using std::uint8_t;\n+using std::int8_t;\n+using std::uint16_t;\n+using std::int16_t;\n+using std::uint32_t;\n+using std::int32_t;\n+using std::uint64_t;\n+using std::int64_t;\n+using std::uintptr_t;\n+using std::intptr_t;\n+using std::complex;\n+\n+/** Guard for long double.\n+ *\n+ * The C implementation defines long double as double\n+ * on MinGW to provide compatibility with MSVC to unify\n+ * one behavior under Windows OS, which makes npy_longdouble\n+ * not fit to be used with template specialization or overloading.\n+ *\n+ * This type will be set to `void` when `npy_longdouble` is not defined\n+ * as `long double`.\n+ */\n+using LongDouble = typename std::conditional<\n+    !std::is_same<npy_longdouble, long double>::value,\n+     void, npy_longdouble\n+>::type;\n+/// @} cpp_core_types\n+\n+} // namespace np\n+\n+#endif // NUMPY_CORE_SRC_COMMON_NPSTD_HPP\n+"
            },
            {
                "filename": "numpy/core/src/common/npy_config.h",
                "patch": "@@ -2,6 +2,7 @@\n #define NUMPY_CORE_SRC_COMMON_NPY_CONFIG_H_\n \n #include \"config.h\"\n+#include \"npy_cpu_dispatch.h\" // brings NPY_HAVE_[CPU features]\n #include \"numpy/numpyconfig.h\"\n #include \"numpy/utils.h\"\n #include \"numpy/npy_os.h\""
            },
            {
                "filename": "numpy/core/src/npysort/quicksort.cpp",
                "patch": "@@ -54,15 +54,11 @@\n #include \"npysort_common.h\"\n #include \"npysort_heapsort.h\"\n #include \"numpy_tag.h\"\n+#include \"simd_qsort.hpp\"\n \n-#include \"x86-qsort.h\"\n #include <cstdlib>\n #include <utility>\n \n-#ifndef NPY_DISABLE_OPTIMIZATION\n-#include \"x86-qsort.dispatch.h\"\n-#endif\n-\n #define NOT_USED NPY_UNUSED(unused)\n /*\n  * pushing largest partition has upper bound of log2(n) space\n@@ -73,70 +69,50 @@\n #define SMALL_MERGESORT 20\n #define SMALL_STRING 16\n \n+// Temporarily disable AVX512 sorting on WIN32 and CYGWIN until we can figure\n+// out why it has test failures\n+#if defined(_MSC_VER) || defined(__CYGWIN__)\n+template<typename T>\n+inline bool quicksort_dispatch(T*, npy_intp)\n+{\n+    return false;\n+}\n+#else\n+template<typename T>\n+inline bool quicksort_dispatch(T *start, npy_intp num)\n+{\n+    using TF = typename np::meta::FixedWidth<T>::Type;\n+    void (*dispfunc)(TF*, intptr_t) = nullptr;\n+    if (sizeof(T) == sizeof(uint16_t)) {\n+        #ifndef NPY_DISABLE_OPTIMIZATION\n+            #include \"simd_qsort_16bit.dispatch.h\"\n+        #endif\n+        NPY_CPU_DISPATCH_CALL_XB(dispfunc = np::qsort_simd::template QSort, <TF>);\n+    }\n+    else if (sizeof(T) == sizeof(uint32_t) || sizeof(T) == sizeof(uint64_t)) {\n+        #ifndef NPY_DISABLE_OPTIMIZATION\n+            #include \"simd_qsort.dispatch.h\"\n+        #endif\n+        NPY_CPU_DISPATCH_CALL_XB(dispfunc = np::qsort_simd::template QSort, <TF>);\n+    }\n+    if (dispfunc) {\n+        (*dispfunc)(reinterpret_cast<TF*>(start), static_cast<intptr_t>(num));\n+        return true;\n+    }\n+    return false;\n+}\n+#endif // _MSC_VER || CYGWIN\n+\n /*\n  *****************************************************************************\n  **                            NUMERIC SORTS                                **\n  *****************************************************************************\n  */\n \n-namespace {\n-\n-template <typename Tag>\n-struct x86_dispatch {\n-    static bool quicksort(typename Tag::type *, npy_intp) { return false; }\n-};\n-\n-template <>\n-struct x86_dispatch<npy::int_tag> {\n-    static bool quicksort(npy_int *start, npy_intp num)\n-    {\n-        void (*dispfunc)(void *, npy_intp) = nullptr;\n-        NPY_CPU_DISPATCH_CALL_XB(dispfunc = x86_quicksort_int);\n-        if (dispfunc) {\n-            (*dispfunc)(start, num);\n-            return true;\n-        }\n-        return false;\n-    }\n-};\n-\n-template <>\n-struct x86_dispatch<npy::uint_tag> {\n-    static bool quicksort(npy_uint *start, npy_intp num)\n-    {\n-        void (*dispfunc)(void *, npy_intp) = nullptr;\n-        NPY_CPU_DISPATCH_CALL_XB(dispfunc = x86_quicksort_uint);\n-        if (dispfunc) {\n-            (*dispfunc)(start, num);\n-            return true;\n-        }\n-        return false;\n-    }\n-};\n-\n-template <>\n-struct x86_dispatch<npy::float_tag> {\n-    static bool quicksort(npy_float *start, npy_intp num)\n-    {\n-        void (*dispfunc)(void *, npy_intp) = nullptr;\n-        NPY_CPU_DISPATCH_CALL_XB(dispfunc = x86_quicksort_float);\n-        if (dispfunc) {\n-            (*dispfunc)(start, num);\n-            return true;\n-        }\n-        return false;\n-    }\n-};\n-\n-}  // namespace\n-\n template <typename Tag, typename type>\n static int\n quicksort_(type *start, npy_intp num)\n {\n-    if (x86_dispatch<Tag>::quicksort(start, num))\n-        return 0;\n-\n     type vp;\n     type *pl = start;\n     type *pr = pl + num - 1;\n@@ -729,56 +705,89 @@ quicksort_ubyte(void *start, npy_intp n, void *NPY_UNUSED(varr))\n NPY_NO_EXPORT int\n quicksort_short(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_short *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::short_tag>((npy_short *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_ushort(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_ushort *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::ushort_tag>((npy_ushort *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_int(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_int *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::int_tag>((npy_int *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_uint(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_uint *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::uint_tag>((npy_uint *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_long(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_long *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::long_tag>((npy_long *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_ulong(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_ulong *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::ulong_tag>((npy_ulong *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_longlong(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_longlong *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::longlong_tag>((npy_longlong *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_ulonglong(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_ulonglong *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::ulonglong_tag>((npy_ulonglong *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_half(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((np::Half *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::half_tag>((npy_half *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_float(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_float *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::float_tag>((npy_float *)start, n);\n }\n NPY_NO_EXPORT int\n quicksort_double(void *start, npy_intp n, void *NPY_UNUSED(varr))\n {\n+    if (quicksort_dispatch((npy_double *)start, n)) {\n+        return 0;\n+    }\n     return quicksort_<npy::double_tag>((npy_double *)start, n);\n }\n NPY_NO_EXPORT int"
            },
            {
                "filename": "numpy/core/src/npysort/simd_qsort.dispatch.cpp",
                "patch": "@@ -0,0 +1,44 @@\n+/*@targets\n+ * $maxopt $keep_baseline avx512_skx\n+ */\n+// policy $keep_baseline is used to avoid skip building avx512_skx\n+// when its part of baseline features (--cpu-baseline), since\n+// 'baseline' option isn't specified within targets.\n+\n+#include \"simd_qsort.hpp\"\n+\n+#if defined(NPY_HAVE_AVX512_SKX) && !defined(_MSC_VER)\n+    #include \"x86-simd-sort/src/avx512-32bit-qsort.hpp\"\n+    #include \"x86-simd-sort/src/avx512-64bit-qsort.hpp\"\n+#endif\n+\n+namespace np { namespace qsort_simd {\n+\n+#if defined(NPY_HAVE_AVX512_SKX) && !defined(_MSC_VER)\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(int32_t *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(uint32_t *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(int64_t *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(uint64_t *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(float *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(double *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+#endif  // NPY_HAVE_AVX512_SKX\n+\n+}} // namespace np::simd"
            },
            {
                "filename": "numpy/core/src/npysort/simd_qsort.hpp",
                "patch": "@@ -0,0 +1,19 @@\n+#ifndef NUMPY_SRC_COMMON_NPYSORT_SIMD_QSORT_HPP\n+#define NUMPY_SRC_COMMON_NPYSORT_SIMD_QSORT_HPP\n+\n+#include \"common.hpp\"\n+\n+namespace np { namespace qsort_simd {\n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"simd_qsort.dispatch.h\"\n+#endif\n+NPY_CPU_DISPATCH_DECLARE(template <typename T> void QSort, (T *arr, intptr_t size))\n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"simd_qsort_16bit.dispatch.h\"\n+#endif\n+NPY_CPU_DISPATCH_DECLARE(template <typename T> void QSort, (T *arr, intptr_t size))\n+\n+} } // np::qsort_simd\n+#endif // NUMPY_SRC_COMMON_NPYSORT_SIMD_QSORT_HPP"
            },
            {
                "filename": "numpy/core/src/npysort/simd_qsort_16bit.dispatch.cpp",
                "patch": "@@ -0,0 +1,31 @@\n+/*@targets\n+ * $maxopt $keep_baseline avx512_icl\n+ */\n+// policy $keep_baseline is used to avoid skip building avx512_skx\n+// when its part of baseline features (--cpu-baseline), since\n+// 'baseline' option isn't specified within targets.\n+\n+#include \"simd_qsort.hpp\"\n+\n+#if defined(NPY_HAVE_AVX512_ICL) && !defined(_MSC_VER)\n+    #include \"x86-simd-sort/src/avx512-16bit-qsort.hpp\"\n+#endif\n+\n+namespace np { namespace qsort_simd {\n+\n+#if defined(NPY_HAVE_AVX512_ICL) && !defined(_MSC_VER)\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(Half *arr, intptr_t size)\n+{\n+    avx512_qsort_fp16(reinterpret_cast<uint16_t*>(arr), size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(uint16_t *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+template<> void NPY_CPU_DISPATCH_CURFX(QSort)(int16_t *arr, intptr_t size)\n+{\n+    avx512_qsort(arr, size);\n+}\n+#endif // NPY_HAVE_AVX512_ICL\n+\n+}} // namespace np::qsort_simd"
            },
            {
                "filename": "numpy/core/src/npysort/x86-qsort.dispatch.cpp",
                "patch": "@@ -1,835 +0,0 @@\n-/*@targets\n- * $maxopt $keep_baseline avx512_skx\n- */\n-// policy $keep_baseline is used to avoid skip building avx512_skx\n-// when its part of baseline features (--cpu-baseline), since\n-// 'baseline' option isn't specified within targets.\n-\n-#include \"x86-qsort.h\"\n-#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n-\n-#ifdef NPY_HAVE_AVX512_SKX\n-#include \"numpy/npy_math.h\"\n-\n-#include \"npy_sort.h\"\n-#include \"numpy_tag.h\"\n-\n-#include \"simd/simd.h\"\n-#include <immintrin.h>\n-\n-template <typename Tag, typename type>\n-NPY_NO_EXPORT int\n-heapsort_(type *start, npy_intp n);\n-\n-/*\n- * Quicksort using AVX-512 for int, uint32 and float. The ideas and code are\n- * based on these two research papers:\n- * (1) Fast and Robust Vectorized In-Place Sorting of Primitive Types\n- *     https://drops.dagstuhl.de/opus/volltexte/2021/13775/\n- * (2) A Novel Hybrid Quicksort Algorithm Vectorized using AVX-512 on Intel\n- * Skylake https://arxiv.org/pdf/1704.08579.pdf\n- *\n- * High level idea: Vectorize the quicksort partitioning using AVX-512\n- * compressstore instructions. The algorithm to pick the pivot is to use median\n- * of 72 elements picked at random. If the array size is < 128, then use\n- * Bitonic sorting network. Good resource for bitonic sorting network:\n- * http://mitp-content-server.mit.edu:18180/books/content/sectbyfn?collid=books_pres_0&fn=Chapter%2027.pdf&id=8030\n- *\n- * Refer to https://github.com/numpy/numpy/pull/20133#issuecomment-958110340\n- * for potential problems when converting this code to universal intrinsics\n- * framework.\n- */\n-\n-/*\n- * Constants used in sorting 16 elements in a ZMM registers. Based on Bitonic\n- * sorting network (see\n- * https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort.svg)\n- */\n-#define NETWORK1 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1\n-#define NETWORK2 12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3\n-#define NETWORK3 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7\n-#define NETWORK4 13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2\n-#define NETWORK5 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n-#define NETWORK6 11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4\n-#define NETWORK7 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8\n-#define ZMM_MAX_FLOAT _mm512_set1_ps(NPY_INFINITYF)\n-#define ZMM_MAX_UINT _mm512_set1_epi32(NPY_MAX_UINT32)\n-#define ZMM_MAX_INT _mm512_set1_epi32(NPY_MAX_INT32)\n-#define SHUFFLE_MASK(a, b, c, d) (a << 6) | (b << 4) | (c << 2) | d\n-#define SHUFFLE_ps(ZMM, MASK) _mm512_shuffle_ps(zmm, zmm, MASK)\n-#define SHUFFLE_epi32(ZMM, MASK) _mm512_shuffle_epi32(zmm, MASK)\n-\n-#define MAX(x, y) (((x) > (y)) ? (x) : (y))\n-#define MIN(x, y) (((x) < (y)) ? (x) : (y))\n-\n-/*\n- * Vectorized random number generator xoroshiro128+. Broken into 2 parts:\n- * (1) vnext generates 2 64-bit random integers\n- * (2) rnd_epu32 converts this to 4 32-bit random integers and bounds it to\n- *     the length of the array\n- */\n-#define VROTL(x, k) /* rotate each uint64_t value in vector */ \\\n-    _mm256_or_si256(_mm256_slli_epi64((x), (k)),               \\\n-                    _mm256_srli_epi64((x), 64 - (k)))\n-\n-static inline __m256i\n-vnext(__m256i *s0, __m256i *s1)\n-{\n-    *s1 = _mm256_xor_si256(*s0, *s1); /* modify vectors s1 and s0 */\n-    *s0 = _mm256_xor_si256(_mm256_xor_si256(VROTL(*s0, 24), *s1),\n-                           _mm256_slli_epi64(*s1, 16));\n-    *s1 = VROTL(*s1, 37);\n-    return _mm256_add_epi64(*s0, *s1); /* return random vector */\n-}\n-\n-/* transform random numbers to the range between 0 and bound - 1 */\n-static inline __m256i\n-rnd_epu32(__m256i rnd_vec, __m256i bound)\n-{\n-    __m256i even = _mm256_srli_epi64(_mm256_mul_epu32(rnd_vec, bound), 32);\n-    __m256i odd = _mm256_mul_epu32(_mm256_srli_epi64(rnd_vec, 32), bound);\n-    return _mm256_blend_epi32(odd, even, 0b01010101);\n-}\n-\n-template <typename type>\n-struct vector;\n-\n-template <>\n-struct vector<npy_int> {\n-    using tag = npy::int_tag;\n-    using type_t = npy_int;\n-    using zmm_t = __m512i;\n-    using ymm_t = __m256i;\n-\n-    static type_t type_max() { return NPY_MAX_INT32; }\n-    static type_t type_min() { return NPY_MIN_INT32; }\n-    static zmm_t zmm_max() { return _mm512_set1_epi32(type_max()); }\n-\n-    static __mmask16 ge(zmm_t x, zmm_t y)\n-    {\n-        return _mm512_cmp_epi32_mask(x, y, _MM_CMPINT_NLT);\n-    }\n-    template <int scale>\n-    static ymm_t i64gather(__m512i index, void const *base)\n-    {\n-        return _mm512_i64gather_epi32(index, base, scale);\n-    }\n-    static zmm_t loadu(void const *mem) { return _mm512_loadu_si512(mem); }\n-    static zmm_t max(zmm_t x, zmm_t y) { return _mm512_max_epi32(x, y); }\n-    static void mask_compressstoreu(void *mem, __mmask16 mask, zmm_t x)\n-    {\n-        return _mm512_mask_compressstoreu_epi32(mem, mask, x);\n-    }\n-    static zmm_t mask_loadu(zmm_t x, __mmask16 mask, void const *mem)\n-    {\n-        return _mm512_mask_loadu_epi32(x, mask, mem);\n-    }\n-    static zmm_t mask_mov(zmm_t x, __mmask16 mask, zmm_t y)\n-    {\n-        return _mm512_mask_mov_epi32(x, mask, y);\n-    }\n-    static void mask_storeu(void *mem, __mmask16 mask, zmm_t x)\n-    {\n-        return _mm512_mask_storeu_epi32(mem, mask, x);\n-    }\n-    static zmm_t min(zmm_t x, zmm_t y) { return _mm512_min_epi32(x, y); }\n-    static zmm_t permutexvar(__m512i idx, zmm_t zmm)\n-    {\n-        return _mm512_permutexvar_epi32(idx, zmm);\n-    }\n-    static type_t reducemax(zmm_t v) { return npyv_reduce_max_s32(v); }\n-    static type_t reducemin(zmm_t v) { return npyv_reduce_min_s32(v); }\n-    static zmm_t set1(type_t v) { return _mm512_set1_epi32(v); }\n-    template<__mmask16 mask>\n-    static zmm_t shuffle(zmm_t zmm)\n-    {\n-        return _mm512_shuffle_epi32(zmm, (_MM_PERM_ENUM)mask);\n-    }\n-    static void storeu(void *mem, zmm_t x)\n-    {\n-        return _mm512_storeu_si512(mem, x);\n-    }\n-\n-    static ymm_t max(ymm_t x, ymm_t y) { return _mm256_max_epi32(x, y); }\n-    static ymm_t min(ymm_t x, ymm_t y) { return _mm256_min_epi32(x, y); }\n-};\n-template <>\n-struct vector<npy_uint> {\n-    using tag = npy::uint_tag;\n-    using type_t = npy_uint;\n-    using zmm_t = __m512i;\n-    using ymm_t = __m256i;\n-\n-    static type_t type_max() { return NPY_MAX_UINT32; }\n-    static type_t type_min() { return 0; }\n-    static zmm_t zmm_max() { return _mm512_set1_epi32(type_max()); }\n-\n-    template<int scale>\n-    static ymm_t i64gather(__m512i index, void const *base)\n-    {\n-        return _mm512_i64gather_epi32(index, base, scale);\n-    }\n-    static __mmask16 ge(zmm_t x, zmm_t y)\n-    {\n-        return _mm512_cmp_epu32_mask(x, y, _MM_CMPINT_NLT);\n-    }\n-    static zmm_t loadu(void const *mem) { return _mm512_loadu_si512(mem); }\n-    static zmm_t max(zmm_t x, zmm_t y) { return _mm512_max_epu32(x, y); }\n-    static void mask_compressstoreu(void *mem, __mmask16 mask, zmm_t x)\n-    {\n-        return _mm512_mask_compressstoreu_epi32(mem, mask, x);\n-    }\n-    static zmm_t mask_loadu(zmm_t x, __mmask16 mask, void const *mem)\n-    {\n-        return _mm512_mask_loadu_epi32(x, mask, mem);\n-    }\n-    static zmm_t mask_mov(zmm_t x, __mmask16 mask, zmm_t y)\n-    {\n-        return _mm512_mask_mov_epi32(x, mask, y);\n-    }\n-    static void mask_storeu(void *mem, __mmask16 mask, zmm_t x)\n-    {\n-        return _mm512_mask_storeu_epi32(mem, mask, x);\n-    }\n-    static zmm_t min(zmm_t x, zmm_t y) { return _mm512_min_epu32(x, y); }\n-    static zmm_t permutexvar(__m512i idx, zmm_t zmm)\n-    {\n-        return _mm512_permutexvar_epi32(idx, zmm);\n-    }\n-    static type_t reducemax(zmm_t v) { return npyv_reduce_max_u32(v); }\n-    static type_t reducemin(zmm_t v) { return npyv_reduce_min_u32(v); }\n-    static zmm_t set1(type_t v) { return _mm512_set1_epi32(v); }\n-    template<__mmask16 mask>\n-    static zmm_t shuffle(zmm_t zmm)\n-    {\n-        return _mm512_shuffle_epi32(zmm, (_MM_PERM_ENUM)mask);\n-    }\n-    static void storeu(void *mem, zmm_t x)\n-    {\n-        return _mm512_storeu_si512(mem, x);\n-    }\n-\n-    static ymm_t max(ymm_t x, ymm_t y) { return _mm256_max_epu32(x, y); }\n-    static ymm_t min(ymm_t x, ymm_t y) { return _mm256_min_epu32(x, y); }\n-};\n-template <>\n-struct vector<npy_float> {\n-    using tag = npy::float_tag;\n-    using type_t = npy_float;\n-    using zmm_t = __m512;\n-    using ymm_t = __m256;\n-\n-    static type_t type_max() { return NPY_INFINITYF; }\n-    static type_t type_min() { return -NPY_INFINITYF; }\n-    static zmm_t zmm_max() { return _mm512_set1_ps(type_max()); }\n-\n-    static __mmask16 ge(zmm_t x, zmm_t y)\n-    {\n-        return _mm512_cmp_ps_mask(x, y, _CMP_GE_OQ);\n-    }\n-    template<int scale>\n-    static ymm_t i64gather(__m512i index, void const *base)\n-    {\n-        return _mm512_i64gather_ps(index, base, scale);\n-    }\n-    static zmm_t loadu(void const *mem) { return _mm512_loadu_ps(mem); }\n-    static zmm_t max(zmm_t x, zmm_t y) { return _mm512_max_ps(x, y); }\n-    static void mask_compressstoreu(void *mem, __mmask16 mask, zmm_t x)\n-    {\n-        return _mm512_mask_compressstoreu_ps(mem, mask, x);\n-    }\n-    static zmm_t mask_loadu(zmm_t x, __mmask16 mask, void const *mem)\n-    {\n-        return _mm512_mask_loadu_ps(x, mask, mem);\n-    }\n-    static zmm_t mask_mov(zmm_t x, __mmask16 mask, zmm_t y)\n-    {\n-        return _mm512_mask_mov_ps(x, mask, y);\n-    }\n-    static void mask_storeu(void *mem, __mmask16 mask, zmm_t x)\n-    {\n-        return _mm512_mask_storeu_ps(mem, mask, x);\n-    }\n-    static zmm_t min(zmm_t x, zmm_t y) { return _mm512_min_ps(x, y); }\n-    static zmm_t permutexvar(__m512i idx, zmm_t zmm)\n-    {\n-        return _mm512_permutexvar_ps(idx, zmm);\n-    }\n-    static type_t reducemax(zmm_t v) { return npyv_reduce_max_f32(v); }\n-    static type_t reducemin(zmm_t v) { return npyv_reduce_min_f32(v); }\n-    static zmm_t set1(type_t v) { return _mm512_set1_ps(v); }\n-    template<__mmask16 mask>\n-    static zmm_t shuffle(zmm_t zmm)\n-    {\n-        return _mm512_shuffle_ps(zmm, zmm, (_MM_PERM_ENUM)mask);\n-    }\n-    static void storeu(void *mem, zmm_t x) { return _mm512_storeu_ps(mem, x); }\n-\n-    static ymm_t max(ymm_t x, ymm_t y) { return _mm256_max_ps(x, y); }\n-    static ymm_t min(ymm_t x, ymm_t y) { return _mm256_min_ps(x, y); }\n-};\n-\n-/*\n- * COEX == Compare and Exchange two registers by swapping min and max values\n- */\n-template <typename vtype, typename mm_t>\n-void\n-COEX(mm_t &a, mm_t &b)\n-{\n-    mm_t temp = a;\n-    a = vtype::min(a, b);\n-    b = vtype::max(temp, b);\n-}\n-\n-template <typename vtype, typename zmm_t = typename vtype::zmm_t>\n-static inline zmm_t\n-cmp_merge(zmm_t in1, zmm_t in2, __mmask16 mask)\n-{\n-    zmm_t min = vtype::min(in2, in1);\n-    zmm_t max = vtype::max(in2, in1);\n-    return vtype::mask_mov(min, mask, max);  // 0 -> min, 1 -> max\n-}\n-\n-/*\n- * Assumes zmm is random and performs a full sorting network defined in\n- * https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort.svg\n- */\n-template <typename vtype, typename zmm_t = typename vtype::zmm_t>\n-static inline zmm_t\n-sort_zmm(zmm_t zmm)\n-{\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(2, 3, 0, 1)>(zmm),\n-                           0xAAAA);\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(0, 1, 2, 3)>(zmm),\n-                           0xCCCC);\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(2, 3, 0, 1)>(zmm),\n-                           0xAAAA);\n-    zmm = cmp_merge<vtype>(\n-            zmm, vtype::permutexvar(_mm512_set_epi32(NETWORK3), zmm), 0xF0F0);\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(1, 0, 3, 2)>(zmm),\n-                           0xCCCC);\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(2, 3, 0, 1)>(zmm),\n-                           0xAAAA);\n-    zmm = cmp_merge<vtype>(\n-            zmm, vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm), 0xFF00);\n-    zmm = cmp_merge<vtype>(\n-            zmm, vtype::permutexvar(_mm512_set_epi32(NETWORK6), zmm), 0xF0F0);\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(1, 0, 3, 2)>(zmm),\n-                           0xCCCC);\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(2, 3, 0, 1)>(zmm),\n-                           0xAAAA);\n-    return zmm;\n-}\n-\n-// Assumes zmm is bitonic and performs a recursive half cleaner\n-template <typename vtype, typename zmm_t = typename vtype::zmm_t>\n-static inline zmm_t\n-bitonic_merge_zmm(zmm_t zmm)\n-{\n-    // 1) half_cleaner[16]: compare 1-9, 2-10, 3-11 etc ..\n-    zmm = cmp_merge<vtype>(\n-            zmm, vtype::permutexvar(_mm512_set_epi32(NETWORK7), zmm), 0xFF00);\n-    // 2) half_cleaner[8]: compare 1-5, 2-6, 3-7 etc ..\n-    zmm = cmp_merge<vtype>(\n-            zmm, vtype::permutexvar(_mm512_set_epi32(NETWORK6), zmm), 0xF0F0);\n-    // 3) half_cleaner[4]\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(1, 0, 3, 2)>(zmm),\n-                           0xCCCC);\n-    // 3) half_cleaner[1]\n-    zmm = cmp_merge<vtype>(zmm, vtype::template shuffle<SHUFFLE_MASK(2, 3, 0, 1)>(zmm),\n-                           0xAAAA);\n-    return zmm;\n-}\n-\n-// Assumes zmm1 and zmm2 are sorted and performs a recursive half cleaner\n-template <typename vtype, typename zmm_t = typename vtype::zmm_t>\n-static inline void\n-bitonic_merge_two_zmm(zmm_t *zmm1, zmm_t *zmm2)\n-{\n-    // 1) First step of a merging network: coex of zmm1 and zmm2 reversed\n-    *zmm2 = vtype::permutexvar(_mm512_set_epi32(NETWORK5), *zmm2);\n-    zmm_t zmm3 = vtype::min(*zmm1, *zmm2);\n-    zmm_t zmm4 = vtype::max(*zmm1, *zmm2);\n-    // 2) Recursive half cleaner for each\n-    *zmm1 = bitonic_merge_zmm<vtype>(zmm3);\n-    *zmm2 = bitonic_merge_zmm<vtype>(zmm4);\n-}\n-\n-// Assumes [zmm0, zmm1] and [zmm2, zmm3] are sorted and performs a recursive\n-// half cleaner\n-template <typename vtype, typename zmm_t = typename vtype::zmm_t>\n-static inline void\n-bitonic_merge_four_zmm(zmm_t *zmm)\n-{\n-    zmm_t zmm2r = vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm[2]);\n-    zmm_t zmm3r = vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm[3]);\n-    zmm_t zmm_t1 = vtype::min(zmm[0], zmm3r);\n-    zmm_t zmm_t2 = vtype::min(zmm[1], zmm2r);\n-    zmm_t zmm_t3 = vtype::permutexvar(_mm512_set_epi32(NETWORK5),\n-                                      vtype::max(zmm[1], zmm2r));\n-    zmm_t zmm_t4 = vtype::permutexvar(_mm512_set_epi32(NETWORK5),\n-                                      vtype::max(zmm[0], zmm3r));\n-    zmm_t zmm0 = vtype::min(zmm_t1, zmm_t2);\n-    zmm_t zmm1 = vtype::max(zmm_t1, zmm_t2);\n-    zmm_t zmm2 = vtype::min(zmm_t3, zmm_t4);\n-    zmm_t zmm3 = vtype::max(zmm_t3, zmm_t4);\n-    zmm[0] = bitonic_merge_zmm<vtype>(zmm0);\n-    zmm[1] = bitonic_merge_zmm<vtype>(zmm1);\n-    zmm[2] = bitonic_merge_zmm<vtype>(zmm2);\n-    zmm[3] = bitonic_merge_zmm<vtype>(zmm3);\n-}\n-\n-template <typename vtype, typename zmm_t = typename vtype::zmm_t>\n-static inline void\n-bitonic_merge_eight_zmm(zmm_t *zmm)\n-{\n-    zmm_t zmm4r = vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm[4]);\n-    zmm_t zmm5r = vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm[5]);\n-    zmm_t zmm6r = vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm[6]);\n-    zmm_t zmm7r = vtype::permutexvar(_mm512_set_epi32(NETWORK5), zmm[7]);\n-    zmm_t zmm_t1 = vtype::min(zmm[0], zmm7r);\n-    zmm_t zmm_t2 = vtype::min(zmm[1], zmm6r);\n-    zmm_t zmm_t3 = vtype::min(zmm[2], zmm5r);\n-    zmm_t zmm_t4 = vtype::min(zmm[3], zmm4r);\n-    zmm_t zmm_t5 = vtype::permutexvar(_mm512_set_epi32(NETWORK5),\n-                                      vtype::max(zmm[3], zmm4r));\n-    zmm_t zmm_t6 = vtype::permutexvar(_mm512_set_epi32(NETWORK5),\n-                                      vtype::max(zmm[2], zmm5r));\n-    zmm_t zmm_t7 = vtype::permutexvar(_mm512_set_epi32(NETWORK5),\n-                                      vtype::max(zmm[1], zmm6r));\n-    zmm_t zmm_t8 = vtype::permutexvar(_mm512_set_epi32(NETWORK5),\n-                                      vtype::max(zmm[0], zmm7r));\n-    COEX<vtype>(zmm_t1, zmm_t3);\n-    COEX<vtype>(zmm_t2, zmm_t4);\n-    COEX<vtype>(zmm_t5, zmm_t7);\n-    COEX<vtype>(zmm_t6, zmm_t8);\n-    COEX<vtype>(zmm_t1, zmm_t2);\n-    COEX<vtype>(zmm_t3, zmm_t4);\n-    COEX<vtype>(zmm_t5, zmm_t6);\n-    COEX<vtype>(zmm_t7, zmm_t8);\n-    zmm[0] = bitonic_merge_zmm<vtype>(zmm_t1);\n-    zmm[1] = bitonic_merge_zmm<vtype>(zmm_t2);\n-    zmm[2] = bitonic_merge_zmm<vtype>(zmm_t3);\n-    zmm[3] = bitonic_merge_zmm<vtype>(zmm_t4);\n-    zmm[4] = bitonic_merge_zmm<vtype>(zmm_t5);\n-    zmm[5] = bitonic_merge_zmm<vtype>(zmm_t6);\n-    zmm[6] = bitonic_merge_zmm<vtype>(zmm_t7);\n-    zmm[7] = bitonic_merge_zmm<vtype>(zmm_t8);\n-}\n-\n-template <typename vtype, typename type_t>\n-static inline void\n-sort_16(type_t *arr, npy_int N)\n-{\n-    __mmask16 load_mask = (0x0001 << N) - 0x0001;\n-    typename vtype::zmm_t zmm =\n-            vtype::mask_loadu(vtype::zmm_max(), load_mask, arr);\n-    vtype::mask_storeu(arr, load_mask, sort_zmm<vtype>(zmm));\n-}\n-\n-template <typename vtype, typename type_t>\n-static inline void\n-sort_32(type_t *arr, npy_int N)\n-{\n-    if (N <= 16) {\n-        sort_16<vtype>(arr, N);\n-        return;\n-    }\n-    using zmm_t = typename vtype::zmm_t;\n-    zmm_t zmm1 = vtype::loadu(arr);\n-    __mmask16 load_mask = (0x0001 << (N - 16)) - 0x0001;\n-    zmm_t zmm2 = vtype::mask_loadu(vtype::zmm_max(), load_mask, arr + 16);\n-    zmm1 = sort_zmm<vtype>(zmm1);\n-    zmm2 = sort_zmm<vtype>(zmm2);\n-    bitonic_merge_two_zmm<vtype>(&zmm1, &zmm2);\n-    vtype::storeu(arr, zmm1);\n-    vtype::mask_storeu(arr + 16, load_mask, zmm2);\n-}\n-\n-template <typename vtype, typename type_t>\n-static inline void\n-sort_64(type_t *arr, npy_int N)\n-{\n-    if (N <= 32) {\n-        sort_32<vtype>(arr, N);\n-        return;\n-    }\n-    using zmm_t = typename vtype::zmm_t;\n-    zmm_t zmm[4];\n-    zmm[0] = vtype::loadu(arr);\n-    zmm[1] = vtype::loadu(arr + 16);\n-    __mmask16 load_mask1 = 0xFFFF, load_mask2 = 0xFFFF;\n-    if (N < 48) {\n-        load_mask1 = (0x0001 << (N - 32)) - 0x0001;\n-        load_mask2 = 0x0000;\n-    }\n-    else if (N < 64) {\n-        load_mask2 = (0x0001 << (N - 48)) - 0x0001;\n-    }\n-    zmm[2] = vtype::mask_loadu(vtype::zmm_max(), load_mask1, arr + 32);\n-    zmm[3] = vtype::mask_loadu(vtype::zmm_max(), load_mask2, arr + 48);\n-    zmm[0] = sort_zmm<vtype>(zmm[0]);\n-    zmm[1] = sort_zmm<vtype>(zmm[1]);\n-    zmm[2] = sort_zmm<vtype>(zmm[2]);\n-    zmm[3] = sort_zmm<vtype>(zmm[3]);\n-    bitonic_merge_two_zmm<vtype>(&zmm[0], &zmm[1]);\n-    bitonic_merge_two_zmm<vtype>(&zmm[2], &zmm[3]);\n-    bitonic_merge_four_zmm<vtype>(zmm);\n-    vtype::storeu(arr, zmm[0]);\n-    vtype::storeu(arr + 16, zmm[1]);\n-    vtype::mask_storeu(arr + 32, load_mask1, zmm[2]);\n-    vtype::mask_storeu(arr + 48, load_mask2, zmm[3]);\n-}\n-\n-template <typename vtype, typename type_t>\n-static inline void\n-sort_128(type_t *arr, npy_int N)\n-{\n-    if (N <= 64) {\n-        sort_64<vtype>(arr, N);\n-        return;\n-    }\n-    using zmm_t = typename vtype::zmm_t;\n-    zmm_t zmm[8];\n-    zmm[0] = vtype::loadu(arr);\n-    zmm[1] = vtype::loadu(arr + 16);\n-    zmm[2] = vtype::loadu(arr + 32);\n-    zmm[3] = vtype::loadu(arr + 48);\n-    zmm[0] = sort_zmm<vtype>(zmm[0]);\n-    zmm[1] = sort_zmm<vtype>(zmm[1]);\n-    zmm[2] = sort_zmm<vtype>(zmm[2]);\n-    zmm[3] = sort_zmm<vtype>(zmm[3]);\n-    __mmask16 load_mask1 = 0xFFFF, load_mask2 = 0xFFFF;\n-    __mmask16 load_mask3 = 0xFFFF, load_mask4 = 0xFFFF;\n-    if (N < 80) {\n-        load_mask1 = (0x0001 << (N - 64)) - 0x0001;\n-        load_mask2 = 0x0000;\n-        load_mask3 = 0x0000;\n-        load_mask4 = 0x0000;\n-    }\n-    else if (N < 96) {\n-        load_mask2 = (0x0001 << (N - 80)) - 0x0001;\n-        load_mask3 = 0x0000;\n-        load_mask4 = 0x0000;\n-    }\n-    else if (N < 112) {\n-        load_mask3 = (0x0001 << (N - 96)) - 0x0001;\n-        load_mask4 = 0x0000;\n-    }\n-    else {\n-        load_mask4 = (0x0001 << (N - 112)) - 0x0001;\n-    }\n-    zmm[4] = vtype::mask_loadu(vtype::zmm_max(), load_mask1, arr + 64);\n-    zmm[5] = vtype::mask_loadu(vtype::zmm_max(), load_mask2, arr + 80);\n-    zmm[6] = vtype::mask_loadu(vtype::zmm_max(), load_mask3, arr + 96);\n-    zmm[7] = vtype::mask_loadu(vtype::zmm_max(), load_mask4, arr + 112);\n-    zmm[4] = sort_zmm<vtype>(zmm[4]);\n-    zmm[5] = sort_zmm<vtype>(zmm[5]);\n-    zmm[6] = sort_zmm<vtype>(zmm[6]);\n-    zmm[7] = sort_zmm<vtype>(zmm[7]);\n-    bitonic_merge_two_zmm<vtype>(&zmm[0], &zmm[1]);\n-    bitonic_merge_two_zmm<vtype>(&zmm[2], &zmm[3]);\n-    bitonic_merge_two_zmm<vtype>(&zmm[4], &zmm[5]);\n-    bitonic_merge_two_zmm<vtype>(&zmm[6], &zmm[7]);\n-    bitonic_merge_four_zmm<vtype>(zmm);\n-    bitonic_merge_four_zmm<vtype>(zmm + 4);\n-    bitonic_merge_eight_zmm<vtype>(zmm);\n-    vtype::storeu(arr, zmm[0]);\n-    vtype::storeu(arr + 16, zmm[1]);\n-    vtype::storeu(arr + 32, zmm[2]);\n-    vtype::storeu(arr + 48, zmm[3]);\n-    vtype::mask_storeu(arr + 64, load_mask1, zmm[4]);\n-    vtype::mask_storeu(arr + 80, load_mask2, zmm[5]);\n-    vtype::mask_storeu(arr + 96, load_mask3, zmm[6]);\n-    vtype::mask_storeu(arr + 112, load_mask4, zmm[7]);\n-}\n-\n-template <typename type_t>\n-static inline void\n-swap(type_t *arr, npy_intp ii, npy_intp jj)\n-{\n-    type_t temp = arr[ii];\n-    arr[ii] = arr[jj];\n-    arr[jj] = temp;\n-}\n-\n-// Median of 3 strategy\n-// template<typename type_t>\n-// static inline\n-// npy_intp get_pivot_index(type_t *arr, const npy_intp left, const npy_intp\n-// right) {\n-//    return (rand() % (right + 1 - left)) + left;\n-//    //npy_intp middle = ((right-left)/2) + left;\n-//    //type_t a = arr[left], b = arr[middle], c = arr[right];\n-//    //if ((b >= a && b <= c) || (b <= a && b >= c))\n-//    //    return middle;\n-//    //if ((a >= b && a <= c) || (a <= b && a >= c))\n-//    //    return left;\n-//    //else\n-//    //    return right;\n-//}\n-\n-/*\n- * Picking the pivot: Median of 72 array elements chosen at random.\n- */\n-\n-template <typename vtype, typename type_t>\n-static inline type_t\n-get_pivot(type_t *arr, const npy_intp left, const npy_intp right)\n-{\n-    /* seeds for vectorized random number generator */\n-    __m256i s0 = _mm256_setr_epi64x(8265987198341093849, 3762817312854612374,\n-                                    1324281658759788278, 6214952190349879213);\n-    __m256i s1 = _mm256_setr_epi64x(2874178529384792648, 1257248936691237653,\n-                                    7874578921548791257, 1998265912745817298);\n-    s0 = _mm256_add_epi64(s0, _mm256_set1_epi64x(left));\n-    s1 = _mm256_sub_epi64(s1, _mm256_set1_epi64x(right));\n-\n-    npy_intp arrsize = right - left + 1;\n-    __m256i bound =\n-            _mm256_set1_epi32(arrsize > INT32_MAX ? INT32_MAX : arrsize);\n-    __m512i left_vec = _mm512_set1_epi64(left);\n-    __m512i right_vec = _mm512_set1_epi64(right);\n-    using ymm_t = typename vtype::ymm_t;\n-    ymm_t v[9];\n-    /* fill 9 vectors with random numbers */\n-    for (npy_int i = 0; i < 9; ++i) {\n-        __m256i rand_64 = vnext(&s0, &s1); /* vector with 4 random uint64_t */\n-        __m512i rand_32 = _mm512_cvtepi32_epi64(rnd_epu32(\n-                rand_64, bound)); /* random numbers between 0 and bound - 1 */\n-        __m512i indices;\n-        if (i < 5)\n-            indices =\n-                    _mm512_add_epi64(left_vec, rand_32); /* indices for arr */\n-        else\n-            indices =\n-                    _mm512_sub_epi64(right_vec, rand_32); /* indices for arr */\n-\n-        v[i] = vtype::template i64gather<sizeof(type_t)>(indices, arr);\n-    }\n-\n-    /* median network for 9 elements */\n-    COEX<vtype>(v[0], v[1]);\n-    COEX<vtype>(v[2], v[3]);\n-    COEX<vtype>(v[4], v[5]);\n-    COEX<vtype>(v[6], v[7]);\n-    COEX<vtype>(v[0], v[2]);\n-    COEX<vtype>(v[1], v[3]);\n-    COEX<vtype>(v[4], v[6]);\n-    COEX<vtype>(v[5], v[7]);\n-    COEX<vtype>(v[0], v[4]);\n-    COEX<vtype>(v[1], v[2]);\n-    COEX<vtype>(v[5], v[6]);\n-    COEX<vtype>(v[3], v[7]);\n-    COEX<vtype>(v[1], v[5]);\n-    COEX<vtype>(v[2], v[6]);\n-    COEX<vtype>(v[3], v[5]);\n-    COEX<vtype>(v[2], v[4]);\n-    COEX<vtype>(v[3], v[4]);\n-    COEX<vtype>(v[3], v[8]);\n-    COEX<vtype>(v[4], v[8]);\n-\n-    // technically v[4] needs to be sorted before we pick the correct median,\n-    // picking the 4th element works just as well for performance\n-    type_t *temp = (type_t *)&v[4];\n-\n-    return temp[4];\n-}\n-\n-/*\n- * Partition one ZMM register based on the pivot and returns the index of the\n- * last element that is less than equal to the pivot.\n- */\n-template <typename vtype, typename type_t, typename zmm_t>\n-static inline npy_int\n-partition_vec(type_t *arr, npy_intp left, npy_intp right, const zmm_t curr_vec,\n-              const zmm_t pivot_vec, zmm_t *smallest_vec, zmm_t *biggest_vec)\n-{\n-    /* which elements are larger than the pivot */\n-    __mmask16 gt_mask = vtype::ge(curr_vec, pivot_vec);\n-    npy_int amount_gt_pivot = _mm_popcnt_u32((npy_int)gt_mask);\n-    vtype::mask_compressstoreu(arr + left, _mm512_knot(gt_mask), curr_vec);\n-    vtype::mask_compressstoreu(arr + right - amount_gt_pivot, gt_mask,\n-                               curr_vec);\n-    *smallest_vec = vtype::min(curr_vec, *smallest_vec);\n-    *biggest_vec = vtype::max(curr_vec, *biggest_vec);\n-    return amount_gt_pivot;\n-}\n-\n-/*\n- * Partition an array based on the pivot and returns the index of the\n- * last element that is less than equal to the pivot.\n- */\n-template <typename vtype, typename type_t>\n-static inline npy_intp\n-partition_avx512(type_t *arr, npy_intp left, npy_intp right, type_t pivot,\n-                 type_t *smallest, type_t *biggest)\n-{\n-    /* make array length divisible by 16 , shortening the array */\n-    for (npy_int i = (right - left) % 16; i > 0; --i) {\n-        *smallest = MIN(*smallest, arr[left]);\n-        *biggest = MAX(*biggest, arr[left]);\n-        if (arr[left] > pivot) {\n-            swap(arr, left, --right);\n-        }\n-        else {\n-            ++left;\n-        }\n-    }\n-\n-    if (left == right)\n-        return left; /* less than 16 elements in the array */\n-\n-    using zmm_t = typename vtype::zmm_t;\n-    zmm_t pivot_vec = vtype::set1(pivot);\n-    zmm_t min_vec = vtype::set1(*smallest);\n-    zmm_t max_vec = vtype::set1(*biggest);\n-\n-    if (right - left == 16) {\n-        zmm_t vec = vtype::loadu(arr + left);\n-        npy_int amount_gt_pivot = partition_vec<vtype>(\n-                arr, left, left + 16, vec, pivot_vec, &min_vec, &max_vec);\n-        *smallest = vtype::reducemin(min_vec);\n-        *biggest = vtype::reducemax(max_vec);\n-        return left + (16 - amount_gt_pivot);\n-    }\n-\n-    // first and last 16 values are partitioned at the end\n-    zmm_t vec_left = vtype::loadu(arr + left);\n-    zmm_t vec_right = vtype::loadu(arr + (right - 16));\n-    // store points of the vectors\n-    npy_intp r_store = right - 16;\n-    npy_intp l_store = left;\n-    // indices for loading the elements\n-    left += 16;\n-    right -= 16;\n-    while (right - left != 0) {\n-        zmm_t curr_vec;\n-        /*\n-         * if fewer elements are stored on the right side of the array,\n-         * then next elements are loaded from the right side,\n-         * otherwise from the left side\n-         */\n-        if ((r_store + 16) - right < left - l_store) {\n-            right -= 16;\n-            curr_vec = vtype::loadu(arr + right);\n-        }\n-        else {\n-            curr_vec = vtype::loadu(arr + left);\n-            left += 16;\n-        }\n-        // partition the current vector and save it on both sides of the array\n-        npy_int amount_gt_pivot =\n-                partition_vec<vtype>(arr, l_store, r_store + 16, curr_vec,\n-                                     pivot_vec, &min_vec, &max_vec);\n-        ;\n-        r_store -= amount_gt_pivot;\n-        l_store += (16 - amount_gt_pivot);\n-    }\n-\n-    /* partition and save vec_left and vec_right */\n-    npy_int amount_gt_pivot =\n-            partition_vec<vtype>(arr, l_store, r_store + 16, vec_left,\n-                                 pivot_vec, &min_vec, &max_vec);\n-    l_store += (16 - amount_gt_pivot);\n-    amount_gt_pivot =\n-            partition_vec<vtype>(arr, l_store, l_store + 16, vec_right,\n-                                 pivot_vec, &min_vec, &max_vec);\n-    l_store += (16 - amount_gt_pivot);\n-    *smallest = vtype::reducemin(min_vec);\n-    *biggest = vtype::reducemax(max_vec);\n-    return l_store;\n-}\n-\n-template <typename vtype, typename type_t>\n-static inline void\n-qsort_(type_t *arr, npy_intp left, npy_intp right, npy_int max_iters)\n-{\n-    /*\n-     * Resort to heapsort if quicksort isn't making any progress\n-     */\n-    if (max_iters <= 0) {\n-        heapsort_<typename vtype::tag>(arr + left, right + 1 - left);\n-        return;\n-    }\n-    /*\n-     * Base case: use bitonic networks to sort arrays <= 128\n-     */\n-    if (right + 1 - left <= 128) {\n-        sort_128<vtype>(arr + left, (npy_int)(right + 1 - left));\n-        return;\n-    }\n-\n-    type_t pivot = get_pivot<vtype>(arr, left, right);\n-    type_t smallest = vtype::type_max();\n-    type_t biggest = vtype::type_min();\n-    npy_intp pivot_index = partition_avx512<vtype>(arr, left, right + 1, pivot,\n-                                                   &smallest, &biggest);\n-    if (pivot != smallest)\n-        qsort_<vtype>(arr, left, pivot_index - 1, max_iters - 1);\n-    if (pivot != biggest)\n-        qsort_<vtype>(arr, pivot_index, right, max_iters - 1);\n-}\n-\n-static inline npy_intp\n-replace_nan_with_inf(npy_float *arr, npy_intp arrsize)\n-{\n-    npy_intp nan_count = 0;\n-    __mmask16 loadmask = 0xFFFF;\n-    while (arrsize > 0) {\n-        if (arrsize < 16) {\n-            loadmask = (0x0001 << arrsize) - 0x0001;\n-        }\n-        __m512 in_zmm = _mm512_maskz_loadu_ps(loadmask, arr);\n-        __mmask16 nanmask = _mm512_cmp_ps_mask(in_zmm, in_zmm, _CMP_NEQ_UQ);\n-        nan_count += _mm_popcnt_u32((npy_int)nanmask);\n-        _mm512_mask_storeu_ps(arr, nanmask, ZMM_MAX_FLOAT);\n-        arr += 16;\n-        arrsize -= 16;\n-    }\n-    return nan_count;\n-}\n-\n-static inline void\n-replace_inf_with_nan(npy_float *arr, npy_intp arrsize, npy_intp nan_count)\n-{\n-    for (npy_intp ii = arrsize - 1; nan_count > 0; --ii) {\n-        arr[ii] = NPY_NANF;\n-        nan_count -= 1;\n-    }\n-}\n-\n-/***************************************\n- * C > C++ dispatch\n- ***************************************/\n-\n-NPY_NO_EXPORT void\n-NPY_CPU_DISPATCH_CURFX(x86_quicksort_int)(void *arr, npy_intp arrsize)\n-{\n-    if (arrsize > 1) {\n-        qsort_<vector<npy_int>, npy_int>((npy_int *)arr, 0, arrsize - 1,\n-                                         2 * (npy_int)log2(arrsize));\n-    }\n-}\n-\n-NPY_NO_EXPORT void\n-NPY_CPU_DISPATCH_CURFX(x86_quicksort_uint)(void *arr, npy_intp arrsize)\n-{\n-    if (arrsize > 1) {\n-        qsort_<vector<npy_uint>, npy_uint>((npy_uint *)arr, 0, arrsize - 1,\n-                                           2 * (npy_int)log2(arrsize));\n-    }\n-}\n-\n-NPY_NO_EXPORT void\n-NPY_CPU_DISPATCH_CURFX(x86_quicksort_float)(void *arr, npy_intp arrsize)\n-{\n-    if (arrsize > 1) {\n-        npy_intp nan_count = replace_nan_with_inf((npy_float *)arr, arrsize);\n-        qsort_<vector<npy_float>, npy_float>((npy_float *)arr, 0, arrsize - 1,\n-                                             2 * (npy_int)log2(arrsize));\n-        replace_inf_with_nan((npy_float *)arr, arrsize, nan_count);\n-    }\n-}\n-\n-#endif  // NPY_HAVE_AVX512_SKX"
            },
            {
                "filename": "numpy/core/src/npysort/x86-qsort.h",
                "patch": "@@ -1,28 +0,0 @@\n-#include \"numpy/npy_common.h\"\n-\n-#include \"npy_cpu_dispatch.h\"\n-\n-#ifndef NPY_NO_EXPORT\n-#define NPY_NO_EXPORT NPY_VISIBILITY_HIDDEN\n-#endif\n-\n-#ifndef NPY_DISABLE_OPTIMIZATION\n-#include \"x86-qsort.dispatch.h\"\n-#endif\n-\n-#ifdef __cplusplus\n-extern \"C\" {\n-#endif\n-\n-NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void x86_quicksort_int,\n-                         (void *start, npy_intp num))\n-\n-NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void x86_quicksort_uint,\n-                         (void *start, npy_intp num))\n-\n-NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void x86_quicksort_float,\n-                         (void *start, npy_intp num))\n-\n-#ifdef __cplusplus\n-}\n-#endif"
            },
            {
                "filename": "numpy/core/src/npysort/x86-simd-sort",
                "patch": "@@ -0,0 +1 @@\n+Subproject commit 7d7591cf5927e83e4a1e7c4b6f2c4dc91a97889f"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -9858,39 +9858,48 @@ def test_non_c_contiguous(self):\n \n \n # Test various array sizes that hit different code paths in quicksort-avx512\n-@pytest.mark.parametrize(\"N\", [8, 16, 24, 32, 48, 64, 96, 128, 151, 191,\n-                               256, 383, 512, 1023, 2047])\n-def test_sort_float(N):\n+@pytest.mark.parametrize(\"N\", np.arange(1, 512))\n+@pytest.mark.parametrize(\"dtype\", ['e', 'f', 'd'])\n+def test_sort_float(N, dtype):\n     # Regular data with nan sprinkled\n     np.random.seed(42)\n-    arr = -0.5 + np.random.sample(N).astype('f')\n+    arr = -0.5 + np.random.sample(N).astype(dtype)\n     arr[np.random.choice(arr.shape[0], 3)] = np.nan\n     assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))\n \n     # (2) with +INF\n-    infarr = np.inf*np.ones(N, dtype='f')\n+    infarr = np.inf*np.ones(N, dtype=dtype)\n     infarr[np.random.choice(infarr.shape[0], 5)] = -1.0\n     assert_equal(np.sort(infarr, kind='quick'), np.sort(infarr, kind='heap'))\n \n     # (3) with -INF\n-    neginfarr = -np.inf*np.ones(N, dtype='f')\n+    neginfarr = -np.inf*np.ones(N, dtype=dtype)\n     neginfarr[np.random.choice(neginfarr.shape[0], 5)] = 1.0\n     assert_equal(np.sort(neginfarr, kind='quick'),\n                  np.sort(neginfarr, kind='heap'))\n \n     # (4) with +/-INF\n-    infarr = np.inf*np.ones(N, dtype='f')\n+    infarr = np.inf*np.ones(N, dtype=dtype)\n     infarr[np.random.choice(infarr.shape[0], (int)(N/2))] = -np.inf\n     assert_equal(np.sort(infarr, kind='quick'), np.sort(infarr, kind='heap'))\n \n-\n-def test_sort_int():\n-    # Random data with NPY_MAX_INT32 and NPY_MIN_INT32 sprinkled\n-    rng = np.random.default_rng(42)\n-    N = 2047\n-    minv = np.iinfo(np.int32).min\n-    maxv = np.iinfo(np.int32).max\n-    arr = rng.integers(low=minv, high=maxv, size=N).astype('int32')\n+def test_sort_float16():\n+    arr = np.arange(65536, dtype=np.int16)\n+    temp = np.frombuffer(arr.tobytes(), dtype=np.float16)\n+    data = np.copy(temp)\n+    np.random.shuffle(data)\n+    data_backup = data\n+    assert_equal(np.sort(data, kind='quick'),\n+            np.sort(data_backup, kind='heap'))\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(1, 512))\n+@pytest.mark.parametrize(\"dtype\", ['h', 'H', 'i', 'I', 'l', 'L'])\n+def test_sort_int(N, dtype):\n+    # Random data with MAX and MIN sprinkled\n+    minv = np.iinfo(dtype).min\n+    maxv = np.iinfo(dtype).max\n+    arr = np.random.randint(low=minv, high=maxv-1, size=N, dtype=dtype)\n     arr[np.random.choice(arr.shape[0], 10)] = minv\n     arr[np.random.choice(arr.shape[0], 10)] = maxv\n     assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22907,
        "body": "This PR adds optional parameters `initial` and `out` to `np.bincount`, mostly based on discussion in #22471 and a prior attempt at this #9424. \r\n- `initial` is used to set the initial values for the output array.\r\n- `out` is an alternative output parameter.  if `initial is out`, then `np.bincount` will reuse and directly accumulate onto `out`. (this allows `bincount` to reuse `out` across multiple calls, rather than overwriting it.)\r\n\r\nIt's a bit niche, but a feature like this has been requested several times in the past. Including these: #22471, #8495, #9424. It's sort of a faster alternative to `np.add.at`.\r\n\r\nThe intended use case is essentially to do bincounts over large chunks of data:\r\n~~~~\r\nhigh_res_map = np.zeros(10000**2)\r\nfor indices, quantities in next_big_chunk_of_data():\r\n    np.bincount(indices, quantities, initial=high_res_map, out=high_res_map)\r\n~~~~\r\n\r\nThis is my first time trying to contribute to Numpy, so apologies in advance if I've missed something. Of course, I am happy to make any changes / improvements :-)\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/multiarray.py",
                "patch": "@@ -889,16 +889,17 @@ def vdot(a, b):\n \n \n @array_function_from_c_func_and_dispatcher(_multiarray_umath.bincount)\n-def bincount(x, weights=None, minlength=None):\n+def bincount(x, weights=None, minlength=None, initial=None, out=None):\n     \"\"\"\n-    bincount(x, /, weights=None, minlength=0)\n+    bincount(x, /, weights=None, minlength=0, initial=None, out=None)\n \n     Count number of occurrences of each value in array of non-negative ints.\n \n     The number of bins (of size 1) is one larger than the largest value in\n     `x`. If `minlength` is specified, there will be at least this number\n     of bins in the output array (though it will be longer if necessary,\n-    depending on the contents of `x`).\n+    depending on the contents of `x`). If `out` is specified, its size must\n+    be large enough to contain all of the bins.\n     Each bin gives the number of occurrences of its index value in `x`.\n     If `weights` is specified the input array is weighted by it, i.e. if a\n     value ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\n@@ -912,22 +913,32 @@ def bincount(x, weights=None, minlength=None):\n         Weights, array of the same shape as `x`.\n     minlength : int, optional\n         A minimum number of bins for the output array.\n+    initial: ndarray, 1 dimension, optional\n+        Array of initial values for each bin. It must have the same shape and\n+        buffer length as the expected output\n+    out: ndarray, 1 dimenion, optional\n+        Alternative output array in which to place the result. It must have the\n+        same shape and buffer length as the expected output but the type will\n+        be cast when safe.\n \n         .. versionadded:: 1.6.0\n \n     Returns\n     -------\n     out : ndarray of ints\n         The result of binning the input array.\n-        The length of `out` is equal to ``np.amax(x)+1``.\n+        The length of `out` is equal to ``max(minlength, np.amax(x)+1)``.\n \n     Raises\n     ------\n     ValueError\n         If the input is not 1-dimensional, or contains elements with negative\n-        values, or if `minlength` is negative.\n+        values, or if `minlength` is negative, or initial or out do not have\n+        sufficient sizes, or initial and out have different sizes.\n     TypeError\n-        If the type of the input is float or complex.\n+        If the type of the input is float or complex, or `minlength` cannot be\n+        converted to int, or `initial` or `out` cannot be safely converted to \n+        the expected type.\n \n     See Also\n     --------"
            },
            {
                "filename": "numpy/core/multiarray.pyi",
                "patch": "@@ -417,6 +417,8 @@ def bincount(\n     /,\n     weights: None | ArrayLike = ...,\n     minlength: SupportsIndex = ...,\n+    initial: None | NDArray[Any] = ...,\n+    out: None | NDArray[Any] = ...,\n ) -> NDArray[intp]: ...\n \n def copyto("
            },
            {
                "filename": "numpy/core/src/multiarray/compiled_base.c",
                "patch": "@@ -14,9 +14,11 @@\n #include \"alloc.h\"\n #include \"ctors.h\"\n #include \"common.h\"\n+#include \"convert.h\"\n #include \"simd/simd.h\"\n \n #include <string.h>\n+#include <stdio.h>\n \n typedef enum {\n     PACK_ORDER_LITTLE = 0,\n@@ -74,152 +76,277 @@ check_array_monotonic(const double *a, npy_intp lena)\n     }\n }\n \n-/* Find the minimum and maximum of an integer array */\n+/* Find the minimum and maximum of an intp strided array */\n static void\n-minmax(const npy_intp *data, npy_intp data_len, npy_intp *mn, npy_intp *mx)\n+minmax(const char *data, npy_intp len, npy_intp stride, npy_intp *mn,\n+        npy_intp *mx)\n {\n-    npy_intp min = *data;\n-    npy_intp max = *data;\n+    *mn = *(npy_intp *)data;\n+    *mx = *(npy_intp *)data;\n+    data += stride;\n+    while (--len) {\n+        const npy_intp val = *(npy_intp *)data;\n+        if (val < *mn) {\n+            *mn = val;\n+        }\n+        else if (val > *mx) {\n+            *mx = val;\n+        }\n+        data += stride;\n+    }\n+}\n \n-    while (--data_len) {\n-        const npy_intp val = *(++data);\n-        if (val < min) {\n-            min = val;\n+/* Converts an array-like object into a behaved array of the right type. */\n+static PyArrayObject*\n+_array_from_object(PyObject *object, enum NPY_TYPES type, int extra_flags)\n+{\n+    int flags = NPY_ARRAY_ALIGNED | NPY_ARRAY_NOTSWAPPED;\n+    PyArray_Descr *dtype = PyArray_DescrFromType(type);\n+    return (PyArrayObject *)PyArray_CheckFromAny(object, dtype, 1, 1,\n+                                                flags | extra_flags, NULL);\n+}\n+\n+/* Initializes and checks the 'out' array used by bincount. */\n+static PyArrayObject*\n+_build_bincount_out_arr(PyObject *out_obj, enum NPY_TYPES out_type,\n+        PyObject *initial_obj, npy_intp minlength,\n+        npy_intp *list_max)\n+{\n+    PyArrayObject *out_arr = NULL;\n+    PyArrayObject *initial_arr = NULL;\n+    npy_intp out_len = 0, initial_len = 0;\n+    const int out_flags = NPY_ARRAY_WRITEABLE | NPY_ARRAY_WRITEBACKIFCOPY;\n+    const npy_intp out_required_len = list_max == NULL ? 0 : *list_max + 1;\n+\n+    /* If specified, get the initial and out arrays. Check their lengths */\n+    if (out_obj != NULL) {\n+        out_arr = _array_from_object(out_obj, out_type, out_flags);\n+        if (out_arr == NULL) {\n+            goto fail;\n         }\n-        else if (val > max) {\n-            max = val;\n+        out_len = PyArray_DIM(out_arr, 0);\n+    }\n+    if (initial_obj != NULL) {\n+        initial_arr = _array_from_object(initial_obj, out_type, 0);\n+        if (initial_arr == NULL) {\n+            goto fail;\n+        }\n+        initial_len = PyArray_DIM(initial_arr, 0);\n+        if (minlength > initial_len) {\n+            PyErr_SetString(PyExc_ValueError,\n+                            \"'minlength' cannot be larger than the size of 'initial'\");\n+            goto fail;\n+        }\n+    }\n+\n+    /* Check that initial and out have same lengths */\n+    if (out_obj != NULL && initial_obj != NULL && out_len != initial_len) {\n+        PyErr_SetString(PyExc_ValueError,\n+                        \"'out' and 'initial' must have the same size\");\n+        goto fail;\n+    }\n+\n+    /* Create the output array if not specified by user */\n+    if (out_obj == NULL) {\n+        if (initial_arr != NULL) {\n+            out_len = initial_len;\n+        }\n+        else {\n+            out_len = out_required_len > minlength ? out_required_len \n+                                                   : minlength;\n+        }\n+        out_arr = (PyArrayObject *)PyArray_EMPTY(1, &out_len, out_type, 0);\n+        if (out_arr == NULL) {\n+            goto fail;\n+        }\n+    }\n+\n+    /* Check that the output array satisfies its size requirements */\n+    if (minlength > out_len) {\n+        PyErr_SetString(PyExc_ValueError,\n+                        \"'minlength' cannot be larger than the size of 'out'\");\n+        goto fail;\n+    }\n+    if (out_required_len > out_len) {\n+        PyErr_SetString(PyExc_ValueError,\n+                        \"'out' is too small to count the max item in 'list'\");\n+        goto fail;\n+    }\n+\n+    /* Set the output array's initial values */\n+    if (initial_arr != NULL) {\n+        if (initial_arr != out_arr && \n+                PyArray_CopyInto(out_arr, initial_arr) != 0) {\n+            PyErr_SetString(PyExc_ValueError,\n+                    \"Failed to copy 'initial' values into 'out'\");\n+            goto fail;\n         }\n     }\n+    else {\n+        PyArray_AssignZero(out_arr, NULL);\n+    }\n+\n+    Py_XDECREF(initial_arr);\n+    return out_arr;\n \n-    *mn = min;\n-    *mx = max;\n+fail:\n+    Py_XDECREF(initial_arr);\n+    if (out_arr) {\n+        PyArray_ResolveWritebackIfCopy(out_arr);\n+    }\n+    Py_XDECREF(out_arr);\n+    return NULL;\n }\n \n /*\n  * arr_bincount is registered as bincount.\n  *\n- * bincount accepts one, two or three arguments. The first is an array of\n- * non-negative integers The second, if present, is an array of weights,\n- * which must be promotable to double. Call these arguments list and\n+ * bincount accepts one, two, three, four, or five arguments. The first is an\n+ * array of non-negative integers The second, if present, is an array of \n+ * weights, which must be promotable to double. Call these arguments list and\n  * weight. Both must be one-dimensional with len(weight) == len(list). If\n  * weight is not present then bincount(list)[i] is the number of occurrences\n  * of i in list.  If weight is present then bincount(self,list, weight)[i]\n  * is the sum of all weight[j] where list [j] == i.  Self is not used.\n  * The third argument, if present, is a minimum length desired for the\n- * output array.\n+ * output array. The fourth argument, if present, is an array of initial\n+ * values that are used to initialize the output array. The fifth argument,\n+ * if present, is an alternative output array. The caller must ensure that\n+ * it is large enough to contain the expected output.\n  */\n NPY_NO_EXPORT PyObject *\n arr_bincount(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwds)\n {\n-    PyObject *list = NULL, *weight = Py_None, *mlength = NULL;\n-    PyArrayObject *lst = NULL, *ans = NULL, *wts = NULL;\n-    npy_intp *numbers, *ians, len, mx, mn, ans_size;\n+    PyObject *list_obj = NULL;\n+    PyObject *weights_obj = Py_None;\n+    PyObject *minlength_obj = NULL;\n+    PyObject *initial_obj = NULL;\n+    PyObject *out_obj = NULL;\n+    PyArrayObject *list_arr = NULL;\n+    PyArrayObject *weights_arr = NULL;\n     npy_intp minlength = 0;\n+    PyArrayObject *out_arr = NULL;\n+    npy_intp list_len, list_stride, list_min, list_max;\n+    npy_intp weights_stride = 0;\n+    npy_intp out_stride;\n     npy_intp i;\n-    double *weights , *dans;\n-    static char *kwlist[] = {\"list\", \"weights\", \"minlength\", NULL};\n-\n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O|OO:bincount\",\n-                kwlist, &list, &weight, &mlength)) {\n-            goto fail;\n+    const char *list_ptr = NULL;\n+    const char *weights_ptr = NULL;\n+    char *out_ptr = NULL;\n+    static char *kwlist[] = {\"list\", \"weights\", \"minlength\", \"initial\", \"out\", NULL};\n+\n+    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O|OOOO:bincount\",\n+            kwlist, &list_obj, &weights_obj, &minlength_obj, &initial_obj,\n+            &out_obj)) {\n+        goto fail;\n     }\n \n-    lst = (PyArrayObject *)PyArray_ContiguousFromAny(list, NPY_INTP, 1, 1);\n-    if (lst == NULL) {\n+    list_arr = _array_from_object(list_obj, NPY_INTP, 0);    \n+    if (list_arr == NULL) {\n         goto fail;\n     }\n-    len = PyArray_SIZE(lst);\n+    list_len = PyArray_DIM(list_arr, 0);\n+    list_stride = PyArray_STRIDE(list_arr, 0);\n+    list_ptr = PyArray_DATA(list_arr);\n+\n+    if (weights_obj != Py_None) {\n+        weights_arr = _array_from_object(weights_obj, NPY_DOUBLE, 0);\n+        if (weights_arr == NULL) {\n+            goto fail;\n+        }\n+        if (PyArray_DIM(weights_arr, 0) != list_len) {\n+            PyErr_SetString(PyExc_ValueError,\n+                            \"list and weights do not have the same length\");\n+            goto fail;\n+        }\n+        weights_stride = PyArray_STRIDE(weights_arr, 0);\n+        weights_ptr = PyArray_DATA(weights_arr);\n+    }\n \n     /*\n      * This if/else if can be removed by changing the argspec to O|On above,\n      * once we retire the deprecation\n      */\n-    if (mlength == Py_None) {\n+    if (minlength_obj == Py_None) {\n         /* NumPy 1.14, 2017-06-01 */\n         if (DEPRECATE(\"0 should be passed as minlength instead of None; \"\n                       \"this will error in future.\") < 0) {\n             goto fail;\n         }\n     }\n-    else if (mlength != NULL) {\n-        minlength = PyArray_PyIntAsIntp(mlength);\n+    else if (minlength_obj != NULL) {\n+        minlength = PyArray_PyIntAsIntp(minlength_obj);\n         if (error_converting(minlength)) {\n             goto fail;\n         }\n     }\n \n     if (minlength < 0) {\n         PyErr_SetString(PyExc_ValueError,\n-                        \"'minlength' must not be negative\");\n+                \"'minlength' must not be negative\");\n         goto fail;\n     }\n \n-    /* handle empty list */\n-    if (len == 0) {\n-        ans = (PyArrayObject *)PyArray_ZEROS(1, &minlength, NPY_INTP, 0);\n-        if (ans == NULL){\n+    enum NPY_TYPES out_type = weights_obj == Py_None ? NPY_INTP \n+                                                     : NPY_DOUBLE; \n+    if (list_len == 0) {\n+        out_arr = _build_bincount_out_arr(out_obj, out_type, initial_obj,\n+                minlength, NULL);\n+        if (out_arr == NULL) {\n             goto fail;\n         }\n-        Py_DECREF(lst);\n-        return (PyObject *)ans;\n+        out_obj = (PyObject *)out_arr;\n+        goto success;\n     }\n \n-    numbers = (npy_intp *)PyArray_DATA(lst);\n-    minmax(numbers, len, &mn, &mx);\n-    if (mn < 0) {\n+    minmax(list_ptr, list_len, list_stride, &list_min, &list_max);\n+    if (list_min < 0) {\n         PyErr_SetString(PyExc_ValueError,\n                 \"'list' argument must have no negative elements\");\n         goto fail;\n     }\n-    ans_size = mx + 1;\n-    if (mlength != Py_None) {\n-        if (ans_size < minlength) {\n-            ans_size = minlength;\n-        }\n+\n+    out_arr = _build_bincount_out_arr(out_obj, out_type, initial_obj,\n+            minlength, &list_max);\n+    if (out_arr == NULL) {\n+        goto fail;\n     }\n-    if (weight == Py_None) {\n-        ans = (PyArrayObject *)PyArray_ZEROS(1, &ans_size, NPY_INTP, 0);\n-        if (ans == NULL) {\n-            goto fail;\n+    out_obj = (PyObject *)out_arr;\n+    out_stride = PyArray_STRIDE(out_arr, 0);\n+    out_ptr = PyArray_DATA(out_arr);\n+\n+    if (weights_arr == NULL) {\n+        for (i = 0; i < list_len; i++) {\n+            const npy_intp index = *(npy_intp *)list_ptr;\n+            *(npy_intp *)(out_ptr + index * out_stride) += 1;\n+            list_ptr += list_stride;\n         }\n-        ians = (npy_intp *)PyArray_DATA(ans);\n-        NPY_BEGIN_ALLOW_THREADS;\n-        for (i = 0; i < len; i++)\n-            ians[numbers[i]] += 1;\n-        NPY_END_ALLOW_THREADS;\n-        Py_DECREF(lst);\n     }\n     else {\n-        wts = (PyArrayObject *)PyArray_ContiguousFromAny(\n-                                                weight, NPY_DOUBLE, 1, 1);\n-        if (wts == NULL) {\n-            goto fail;\n-        }\n-        weights = (double *)PyArray_DATA(wts);\n-        if (PyArray_SIZE(wts) != len) {\n-            PyErr_SetString(PyExc_ValueError,\n-                    \"The weights and list don't have the same length.\");\n-            goto fail;\n+        for (i = 0; i < list_len; i++) {\n+            const npy_intp index = *(npy_intp *)list_ptr;\n+            const npy_double weight = *(npy_double *)weights_ptr;\n+            *(npy_double *)(out_ptr + index * out_stride) += weight;\n+            list_ptr += list_stride;\n+            weights_ptr += weights_stride;\n         }\n-        ans = (PyArrayObject *)PyArray_ZEROS(1, &ans_size, NPY_DOUBLE, 0);\n-        if (ans == NULL) {\n-            goto fail;\n-        }\n-        dans = (double *)PyArray_DATA(ans);\n-        NPY_BEGIN_ALLOW_THREADS;\n-        for (i = 0; i < len; i++) {\n-            dans[numbers[i]] += weights[i];\n-        }\n-        NPY_END_ALLOW_THREADS;\n-        Py_DECREF(lst);\n-        Py_DECREF(wts);\n     }\n-    return (PyObject *)ans;\n+\n+success:\n+    Py_XDECREF(list_arr);\n+    Py_XDECREF(weights_arr);\n+    Py_INCREF(out_obj);\n+    PyArray_ResolveWritebackIfCopy(out_arr);\n+    Py_XDECREF(out_arr);\n+    return out_obj;\n \n fail:\n-    Py_XDECREF(lst);\n-    Py_XDECREF(wts);\n-    Py_XDECREF(ans);\n+    Py_XDECREF(list_arr);\n+    Py_XDECREF(weights_arr);\n+    if (out_arr) {\n+        PyArray_ResolveWritebackIfCopy(out_arr);\n+    }    \n+    Py_XDECREF(out_arr);\n     return NULL;\n }\n "
            },
            {
                "filename": "numpy/lib/tests/test_function_base.py",
                "patch": "@@ -2656,6 +2656,10 @@ def test_simple2(self):\n         y = np.bincount(np.array([1, 5, 2, 4, 1]))\n         assert_array_equal(y, np.array([0, 2, 1, 0, 1, 1]))\n \n+    def test_simple_strided(self):\n+        y = np.bincount(np.array([1, 5, 2, 4, 1])[::2])\n+        assert_array_equal(y, np.array([0, 2, 1]))\n+\n     def test_simple_weight(self):\n         x = np.arange(4)\n         w = np.array([0.2, 0.3, 0.5, 0.1])\n@@ -2688,6 +2692,63 @@ def test_with_minlength_and_weights(self):\n         w = np.array([0.2, 0.3, 0.5, 0.1, 0.2])\n         y = np.bincount(x, w, 8)\n         assert_array_equal(y, np.array([0, 0.2, 0.5, 0, 0.5, 0.1, 0, 0]))\n+    \n+    def test_with_out(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        out = np.ones(6, dtype=int)\n+        y = np.bincount(x, out=out)\n+        assert_array_equal(out, np.array([0, 2, 1, 0, 1, 1]))\n+\n+    def test_with_minlength_and_out(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        out = np.zeros(6, dtype=int)\n+        np.bincount(x, minlength=2, out=out)\n+        assert_array_equal(out, np.array([0, 2, 1, 0, 1, 1]))\n+\n+    def test_with_weights_and_out(self):\n+        x = np.arange(4)\n+        w = np.array([0.2, 0.3, 0.5, 0.1])\n+        out = np.empty(4)\n+        np.bincount(x, w, out=out)\n+        assert_array_equal(out, w)\n+\n+    def test_with_initial(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        initial = np.ones(6, dtype=int)\n+        y = np.bincount(x, initial=initial)\n+        assert_array_equal(y, np.array([0, 2, 1, 0, 1, 1]) + 1)\n+\n+    def test_with_strided_initial(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        initial = np.ones(12, dtype=int)\n+        y = np.bincount(x, initial=initial[::-2])\n+        assert_array_equal(y, np.array([0, 2, 1, 0, 1, 1]) + 1)\n+\n+    def test_with_minlength_and_initial(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        initial = np.ones(6, dtype=int)\n+        y = np.bincount(x, minlength=3, initial=initial)\n+        assert_array_equal(y, np.array([0, 2, 1, 0, 1, 1]) + 1)\n+\n+    def test_with_initial_and_out(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        out = np.empty(6, dtype=int)\n+        initial = np.ones(6, dtype=int)\n+        np.bincount(x, initial=initial, out=out)\n+        assert_array_equal(out, np.array([0, 2, 1, 0, 1, 1]) + 1)\n+\n+    def test_with_initial_and_out2(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        out = np.zeros(6, dtype=int)\n+        initial = np.ones(6, dtype=int)\n+        np.bincount(x, initial=initial, out=out)\n+        assert_array_equal(out, np.array([0, 2, 1, 0, 1, 1]) + 1)\n+\n+    def test_with_initial_equal_to_out(self):\n+        x = np.array([1, 5, 2, 4, 1])\n+        out = np.ones(6, dtype=int)\n+        y = np.bincount(x, initial=out, out=out)\n+        assert_array_equal(out, np.array([0, 2, 1, 0, 1, 1]) + 1)\n \n     def test_empty(self):\n         x = np.array([], dtype=int)\n@@ -2699,6 +2760,49 @@ def test_empty_with_minlength(self):\n         y = np.bincount(x, minlength=5)\n         assert_array_equal(y, np.zeros(5, dtype=int))\n \n+    def test_empty_with_initial(self):\n+        x = np.array([], dtype=int)\n+        initial = np.ones(6, dtype=int)\n+        y = np.bincount(x, initial=initial)\n+        assert_array_equal(y, initial)\n+\n+    def test_empty_with_initial_and_out(self):\n+        x = np.array([], dtype=int)\n+        out = np.empty(5, dtype=int)\n+        initial = np.arange(5, dtype=int)\n+        np.bincount(x, out=out, initial=initial)\n+        assert_array_equal(out, initial)\n+\n+    def test_empty_with_weights_initial_and_out(self):\n+        x = np.array([], dtype=int)\n+        weights = np.array([])\n+        out = np.empty(5, dtype=float)\n+        initial = np.arange(5, dtype=float)\n+        np.bincount(x, weights=weights, out=out, initial=initial)\n+        assert_array_equal(out, initial)\n+\n+    def test_empty_with_minlength_and_out(self):\n+        x = np.array([], dtype=int)\n+        out = np.empty(5, dtype=int)\n+        np.bincount(x, minlength=4, out=out)\n+        assert_array_equal(out, np.zeros(5, dtype=int))\n+\n+    def test_with_negative_element(self):\n+        x = np.array([1, 2, 4, -1, 5, 1], dtype=int)\n+        assert_raises_regex(ValueError,\n+                            \"must have no negative elements\",\n+                            lambda: np.bincount(x))\n+\n+    def test_with_incorrect_weights(self):\n+        x = np.array([], dtype=int)\n+        assert_raises_regex(ValueError,\n+                            \"list and weights do not have the same\",\n+                            lambda: np.bincount(x, weights=[1]))\n+        x = np.arange(5)\n+        assert_raises_regex(ValueError,\n+                            \"list and weights do not have the same\",\n+                            lambda: np.bincount(x, weights=[1, 2]))\n+\n     def test_with_incorrect_minlength(self):\n         x = np.array([], dtype=int)\n         assert_raises_regex(TypeError,\n@@ -2716,6 +2820,28 @@ def test_with_incorrect_minlength(self):\n                             \"must not be negative\",\n                             lambda: np.bincount(x, minlength=-1))\n \n+        out = np.zeros(5, dtype=int)\n+        assert_raises_regex(ValueError,\n+                            \"cannot be larger than the size of 'out'\",\n+                            lambda: np.bincount(x, minlength=10, out=out))\n+\n+        initial = np.zeros(5, dtype=int)\n+        assert_raises_regex(ValueError,\n+                            \"cannot be larger than the size of 'initial'\",\n+                            lambda: np.bincount(x, minlength=10, \n+                                                initial=initial))\n+\n+    def test_with_incorrect_types(self):\n+        x = np.array([], dtype=int)\n+        out = np.empty(6, dtype=float)\n+        assert_raises_regex(TypeError,\n+                            \"Cannot cast\",\n+                            lambda: np.bincount(x, out=out))\n+        initial = np.empty(6, dtype=float)\n+        assert_raises_regex(TypeError,\n+                            \"Cannot cast\",\n+                            lambda: np.bincount(x, initial=initial))\n+\n     @pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n     def test_dtype_reference_leaks(self):\n         # gh-6805"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23108,
        "body": "* Improve performance of `finfo` by making use of an `lru_cache`. For `np.complex128` this makes a large difference.\r\n* Improve performance of  `_commonType` by re-using the expression for `a.dtype.type` and eliminating variables\r\n\r\nThis is a variation of #23088.\r\n\r\n**Notes about the `lru_cache`**\r\n\r\nThe performance of `finfo` is improved by using an lru_cache\r\n\r\n- The behaviour of the new implementation is subtly different. Before we would have `id(finfo(np.float_))==id(finfo(np.complex_))`. In the new implementation this is no longer guaranteed, but the properties of\r\n`finfo(np.float_)` and `finfo(np.complex_)` will be the same.\r\n\r\nThere are some tests for `getlimits.py` that still pass, but perhaps should be modified. E.g.\r\n```\r\nclass TestDouble:\r\n    def test_singleton(self):\r\n        ftype = finfo(double)\r\n        ftype2 = finfo(double)\r\n        assert_equal(id(ftype), id(ftype2))\r\n```\r\n        \r\n- The performance is good for the common cases.\r\n\r\n```\r\npython -m pyperf timeit -s \"import numpy as np; x=np.array([1.,2.]).dtype;\" \"np.finfo(x)\"\r\npython -m pyperf timeit -s \"import numpy as np; x=1.1;\" \"np.finfo(x)\"\r\npython -m pyperf timeit -s \"import numpy as np; x=np.array([1.,2.], dtype=np.complex_).dtype;\" \"np.finfo(x)\"\r\n```\r\n\r\nMain\r\n```\r\n.....................\r\nMean +- std dev: 272 ns +- 3 ns\r\n.....................\r\nMean +- std dev: 2.40 us +- 0.06 us\r\n.....................\r\nMean +- std dev: 272 ns +- 5 ns\r\n```\r\n\r\nPr:\r\n\r\n```\r\n.....................\r\nMean +- std dev: 212 ns +- 2 ns\r\n.....................\r\nMean +- std dev: 237 ns +- 2 ns\r\n.....................\r\nMean +- std dev: 211 ns +- 2 ns\r\n\r\n```\r\n\r\n There are scenarios where many cache misses occur (for example `np.finfo(x)` is allowed for any float `x`, so something like `[np.finfo(x) for x in np.arange(10000)]` will result in misses), but they are hardly any slower than the old implementation.\r\n\r\n- Just adding an `lru_cache` is sufficient for the performance. But since it is no longer required, the `_finfo_cache` is removed. \r\n- The original implementation contained multiple calls to the cache. They have all been removed to simplify the implementation and avoid any risks of creating circular references.\r\n- The `lru_cache` caches the combination of the `cls` and `dtype` argument. The extra overhead compared to just caching the `dtype` is not very large. A test:\r\n```\r\n%timeit hash((cls,d))\r\n%timeit hash(cls)\r\n%timeit hash(d)\r\n108 ns \u00b1 4.66 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000,000 loops each)\r\n56.3 ns \u00b1 1.55 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000,000 loops each)\r\n61.1 ns \u00b1 0.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000,000 loops each)\r\n```\r\nOne might eliminate this by changing `finfo` from a class into a function `def finfo(dtype)` that returns the class, but that is not implemented in this PR.\r\n\r\n- The deprecation warning for `np.finfo(None)` will only get printed once.\r\n- For integers there is the `iinfo` class. No caching was present there, but performance is improved with a `lru_cache` as well.\r\n\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/getlimits.py",
                "patch": "@@ -4,6 +4,7 @@\n __all__ = ['finfo', 'iinfo']\n \n import warnings\n+from functools import lru_cache\n \n from .._utils import set_module\n from ._machar import MachAr\n@@ -119,7 +120,7 @@ def _float_to_str(self, value):\n         return self.params['fmt'] % array(_fr0(value)[0], self.ftype)\n \n \n-_convert_to_float = {\n+_convert_complex_to_real_float = {\n     ntypes.csingle: ntypes.single,\n     ntypes.complex_: ntypes.float_,\n     ntypes.clongfloat: ntypes.longfloat\n@@ -479,9 +480,9 @@ class finfo:\n \n     \"\"\"\n \n-    _finfo_cache = {}\n-\n+    @lru_cache(typed=True)\n     def __new__(cls, dtype):\n+\n         if dtype is None:\n             # Deprecated in NumPy 1.25, 2023-01-16\n             warnings.warn(\n@@ -496,31 +497,18 @@ def __new__(cls, dtype):\n         except TypeError:\n             # In case a float instance was given\n             dtype = numeric.dtype(type(dtype))\n+            # changed type, so we can recursively call finfo\n+            return finfo(dtype)\n \n-        obj = cls._finfo_cache.get(dtype, None)\n-        if obj is not None:\n-            return obj\n-        dtypes = [dtype]\n-        newdtype = numeric.obj2sctype(dtype)\n-        if newdtype is not dtype:\n-            dtypes.append(newdtype)\n-            dtype = newdtype\n+        # we have a dtype object, convert it to the equivalent scalar dtype\n+        dtype = numeric.obj2sctype(dtype)\n         if not issubclass(dtype, numeric.inexact):\n             raise ValueError(\"data type %r not inexact\" % (dtype))\n-        obj = cls._finfo_cache.get(dtype, None)\n-        if obj is not None:\n-            return obj\n+\n         if not issubclass(dtype, numeric.floating):\n-            newdtype = _convert_to_float[dtype]\n-            if newdtype is not dtype:\n-                dtypes.append(newdtype)\n-                dtype = newdtype\n-        obj = cls._finfo_cache.get(dtype, None)\n-        if obj is not None:\n-            return obj\n+            dtype = _convert_complex_to_real_float[dtype]\n+\n         obj = object.__new__(cls)._init(dtype)\n-        for dt in dtypes:\n-            cls._finfo_cache[dt] = obj\n         return obj\n \n     def _init(self, dtype):\n@@ -668,7 +656,11 @@ class iinfo:\n     _min_vals = {}\n     _max_vals = {}\n \n-    def __init__(self, int_type):\n+    @lru_cache(typed=True)\n+    def __new__(cls, int_type):\n+        return object.__new__(cls)._init(int_type)\n+\n+    def _init(self, int_type):\n         try:\n             self.dtype = numeric.dtype(int_type)\n         except TypeError:\n@@ -678,6 +670,7 @@ def __init__(self, int_type):\n         self.key = \"%s%d\" % (self.kind, self.bits)\n         if self.kind not in 'iu':\n             raise ValueError(\"Invalid integer data type %r.\" % (self.kind,))\n+        return self\n \n     @property\n     def min(self):"
            },
            {
                "filename": "numpy/core/tests/test_deprecations.py",
                "patch": "@@ -900,4 +900,7 @@ def test_attributeerror_includes_info(self, name):\n class TestDeprecatedFinfo(_DeprecationTestCase):\n     # Deprecated in NumPy 1.25, 2023-01-16\n     def test_deprecated_none(self):\n-        self.assert_deprecated(np.finfo, args=(None,))\n+        # the deprecation is only generated once\n+        np.finfo.__new__.cache_clear()\n+        with pytest.warns(DeprecationWarning):\n+            np.finfo(None)"
            },
            {
                "filename": "numpy/core/tests/test_scalarmath.py",
                "patch": "@@ -526,6 +526,10 @@ def test_int_from_infinite_longdouble___int__(self):\n     def test_int_from_huge_longdouble(self):\n         # Produce a longdouble that would overflow a double,\n         # use exponent that avoids bug in Darwin pow function.\n+\n+        a = np.finfo(np.double)\n+        b = np.finfo(np.longdouble)\n+\n         exp = np.finfo(np.double).maxexp - 1\n         huge_ld = 2 * 1234 * np.longdouble(2) ** exp\n         huge_i = 2 * 1234 * 2 ** exp"
            },
            {
                "filename": "numpy/linalg/linalg.py",
                "patch": "@@ -138,24 +138,24 @@ def _commonType(*arrays):\n     result_type = single\n     is_complex = False\n     for a in arrays:\n-        if issubclass(a.dtype.type, inexact):\n-            if isComplexType(a.dtype.type):\n+        type_ = a.dtype.type\n+        if issubclass(type_, inexact):\n+            if isComplexType(type_):\n                 is_complex = True\n-            rt = _realType(a.dtype.type, default=None)\n-            if rt is None:\n+            rt = _realType(type_, default=None)\n+            if rt is double:\n+                result_type = double\n+            elif rt is None:\n                 # unsupported inexact scalar\n                 raise TypeError(\"array type %s is unsupported in linalg\" %\n                         (a.dtype.name,))\n         else:\n-            rt = double\n-        if rt is double:\n             result_type = double\n     if is_complex:\n-        t = cdouble\n         result_type = _complex_types_map[result_type]\n+        return cdouble, result_type\n     else:\n-        t = double\n-    return t, result_type\n+        return double, result_type\n \n \n def _to_native_byte_order(*arrays):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23088,
        "body": "* The `finfo` contains a cache for dtypes, but the `np.complex128` dtype does not end up in the cache. The reason is that the `np.complex128` is converted to `np.float64` which is in the cache.\r\n\r\nPerformance improvement for `finfo(np.complex128)`:\r\n```\r\nMain: 2.07 \u00b5s \u00b1 75 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\nPr: 324 ns \u00b1 28.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n```\r\n\r\n* Improve performance of `finfo` by making the cache check the first action in the `__new__`\r\n* Improve performance of `_commonType` by re-using the expression for `a.dtype.type` and eliminating variables\r\n\r\nThe `finfo` and `_commonType` was part of the computatation time in `lstsq` when using scikit-rf. Since these methods are used in various other methods performance can improve there slightly as well.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/getlimits.py",
                "patch": "@@ -482,6 +482,10 @@ class finfo:\n     _finfo_cache = {}\n \n     def __new__(cls, dtype):\n+        obj = cls._finfo_cache.get(dtype)  # most common path\n+        if obj is not None:\n+            return obj\n+\n         if dtype is None:\n             # Deprecated in NumPy 1.25, 2023-01-16\n             warnings.warn(\n@@ -497,7 +501,7 @@ def __new__(cls, dtype):\n             # In case a float instance was given\n             dtype = numeric.dtype(type(dtype))\n \n-        obj = cls._finfo_cache.get(dtype, None)\n+        obj = cls._finfo_cache.get(dtype)\n         if obj is not None:\n             return obj\n         dtypes = [dtype]\n@@ -507,17 +511,24 @@ def __new__(cls, dtype):\n             dtype = newdtype\n         if not issubclass(dtype, numeric.inexact):\n             raise ValueError(\"data type %r not inexact\" % (dtype))\n-        obj = cls._finfo_cache.get(dtype, None)\n+        obj = cls._finfo_cache.get(dtype)\n         if obj is not None:\n             return obj\n         if not issubclass(dtype, numeric.floating):\n             newdtype = _convert_to_float[dtype]\n             if newdtype is not dtype:\n+                # dtype changed, for example from complex128 to float64\n                 dtypes.append(newdtype)\n                 dtype = newdtype\n-        obj = cls._finfo_cache.get(dtype, None)\n-        if obj is not None:\n-            return obj\n+\n+                obj = cls._finfo_cache.get(dtype, None)\n+                if obj is not None:\n+                    # the original dtype was not in the cache, but the new\n+                    # dtype is in the cache. we add the original dtypes to\n+                    # the cache and return the result\n+                    for dt in dtypes:\n+                        cls._finfo_cache[dt] = obj\n+                    return obj\n         obj = object.__new__(cls)._init(dtype)\n         for dt in dtypes:\n             cls._finfo_cache[dt] = obj"
            },
            {
                "filename": "numpy/linalg/linalg.py",
                "patch": "@@ -138,24 +138,24 @@ def _commonType(*arrays):\n     result_type = single\n     is_complex = False\n     for a in arrays:\n-        if issubclass(a.dtype.type, inexact):\n-            if isComplexType(a.dtype.type):\n+        type_ = a.dtype.type\n+        if issubclass(type_, inexact):\n+            if isComplexType(type_):\n                 is_complex = True\n-            rt = _realType(a.dtype.type, default=None)\n-            if rt is None:\n+            rt = _realType(type_, default=None)\n+            if rt is double:\n+                result_type = double\n+            elif rt is None:\n                 # unsupported inexact scalar\n                 raise TypeError(\"array type %s is unsupported in linalg\" %\n                         (a.dtype.name,))\n         else:\n-            rt = double\n-        if rt is double:\n             result_type = double\n     if is_complex:\n-        t = cdouble\n         result_type = _complex_types_map[result_type]\n+        return cdouble, result_type\n     else:\n-        t = double\n-    return t, result_type\n+        return double, result_type\n \n \n def _to_native_byte_order(*arrays):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23174,
        "body": "Resolve the issue in https://github.com/numpy/numpy/pull/21056 by using volatile. \r\n\r\nWhen partially loading a vector register for a divide operation, the remaining elements are set\r\n to 1 to avoid divide-by-zero.  The partial load is paired with a partial store after the divide operation.  clang notices that the entire register is not needed for the store and optimizes out the fill of 1 to the remaining elements.  This causes either a divide-by-zero or 0/0 with invalid exception that we were trying to avoid by filling.\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/loops_arithm_fp.dispatch.c.src",
                "patch": "@@ -32,6 +32,58 @@\n  ** Defining ufunc inner functions\n  ********************************************************************************/\n \n+/*\n+ * clang has a bug that's present at -O1 or greater.  When partially loading a\n+ * vector register for a divide operation, the remaining elements are set\n+ * to 1 to avoid divide-by-zero.  The partial load is paired with a partial\n+ * store after the divide operation.  clang notices that the entire register\n+ * is not needed for the store and optimizes out the fill of 1 to the remaining\n+ * elements.  This causes either a divide-by-zero or 0/0 with invalid exception\n+ * that we were trying to avoid by filling.\n+ *\n+ * Using a dummy variable marked 'volatile' convinces clang not to ignore\n+ * the explicit fill of remaining elements.  If `-ftrapping-math` is\n+ * supported, then it'll also avoid the bug.  `-ftrapping-math` is supported\n+ * on Apple clang v12+ for x86_64.  It is not currently supported for arm64.\n+ * `-ftrapping-math` is set by default of Numpy builds in\n+ * numpy/distutils/ccompiler.py.\n+ *\n+ * Note: Apple clang and clang upstream have different versions that overlap\n+ */\n+#if defined(__clang__)\n+    #if defined(__apple_build_version__)\n+    // Apple Clang\n+        #if __apple_build_version__ < 12000000\n+        // Apple Clang before v12\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 1\n+        #elif defined(NPY_CPU_X86) || defined(NPY_CPU_AMD64)\n+        // Apple Clang after v12, targeting i386 or x86_64\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 0\n+        #else\n+        // Apple Clang after v12, not targeting i386 or x86_64\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 1\n+        #endif\n+    #else\n+    // Clang, not Apple Clang\n+        #if __clang_major__ < 10\n+        // Clang before v10\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 1\n+        #elif defined(_MSC_VER)\n+        // clang-cl has the same bug\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 1\n+        #elif defined(NPY_CPU_X86) || defined(NPY_CPU_AMD64)\n+        // Clang v10+, targeting i386 or x86_64\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 0\n+        #else\n+        // Clang v10+, not targeting i386 or x86_64\n+        #define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 1\n+        #endif\n+    #endif\n+#else\n+// Not a Clang compiler\n+#define WORKAROUND_CLANG_PARTIAL_LOAD_BUG 0\n+#endif\n+\n /**begin repeat\n  * Float types\n  *  #type = npy_float, npy_double#\n@@ -96,7 +148,12 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n                 npyv_store_@sfx@((@type@*)dst, r0);\n                 npyv_store_@sfx@((@type@*)(dst + vstep), r1);\n             }\n-            for (; len > 0; len -= hstep, src0 += vstep, src1 += vstep, dst += vstep) {\n+        #if @is_div@ && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+            const int vstop = hstep - 1;\n+        #else\n+            const int vstop = 0;\n+        #endif // #if @is_div@ && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+            for (; len > vstop; len -= hstep, src0 += vstep, src1 += vstep, dst += vstep) {\n             #if @is_div@\n                 npyv_@sfx@ a = npyv_load_till_@sfx@((const @type@*)src0, len, 1.0@c@);\n                 npyv_@sfx@ b = npyv_load_till_@sfx@((const @type@*)src1, len, 1.0@c@);\n@@ -107,6 +164,15 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n                 npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n                 npyv_store_till_@sfx@((@type@*)dst, len, r);\n             }\n+        #if @is_div@ && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+            // last partial iteration for divide and working around clang partial load bug\n+            if(len > 0){\n+                npyv_@sfx@ a = npyv_load_till_@sfx@((const @type@*)src0, len, 1.0@c@);\n+                volatile npyv_@sfx@ b = npyv_load_till_@sfx@((const @type@*)src1, len, 1.0@c@);\n+                npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n+                npyv_store_till_@sfx@((@type@*)dst, len, r);\n+            }\n+        #endif // #if @is_div@ && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n         }\n         else if (ssrc0 == 0 && ssrc1 == sizeof(@type@) && sdst == ssrc1) {\n             npyv_@sfx@ a = npyv_setall_@sfx@(*((@type@*)src0));\n@@ -118,7 +184,12 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n                 npyv_store_@sfx@((@type@*)dst, r0);\n                 npyv_store_@sfx@((@type@*)(dst + vstep), r1);\n             }\n-            for (; len > 0; len -= hstep, src1 += vstep, dst += vstep) {\n+        #if (@is_div@ || @is_mul@) && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+            const int vstop = hstep - 1;\n+        #else\n+            const int vstop = 0;\n+        #endif // #if (@is_div@ || @is_mul@) && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+            for (; len > vstop; len -= hstep, src1 += vstep, dst += vstep) {\n             #if @is_div@ || @is_mul@\n                 npyv_@sfx@ b = npyv_load_till_@sfx@((const @type@*)src1, len, 1.0@c@);\n             #else\n@@ -127,6 +198,14 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n                 npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n                 npyv_store_till_@sfx@((@type@*)dst, len, r);\n             }\n+        #if (@is_div@ || @is_mul@) && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+            // last partial iteration for multiply / divide and working around clang partial load bug\n+            if(len > 0){\n+                volatile npyv_@sfx@ b = npyv_load_till_@sfx@((const @type@*)src1, len, 1.0@c@);\n+                npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n+                npyv_store_till_@sfx@((@type@*)dst, len, r);\n+            }\n+        #endif // #if (@is_div@ || @is_mul@) && WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n         }\n         else if (ssrc1 == 0 && ssrc0 == sizeof(@type@) && sdst == ssrc0) {\n             npyv_@sfx@ b = npyv_setall_@sfx@(*((@type@*)src1));\n@@ -164,6 +243,8 @@ loop_scalar:\n /**end repeat1**/\n /**end repeat**/\n \n+#undef WORKAROUND_CLANG_PARTIAL_LOAD_BUG\n+\n //###############################################################################\n //## Complex Single/Double precision\n //###############################################################################"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 16700,
        "body": "This is the first step in addressing [gh-15981](https://github.com/numpy/numpy/issues/15981)\r\n\r\nRelated mailing list thread can be found [here](http://numpy-discussion.10968.n7.nabble.com/Improving-Complex-Comparison-Ordering-in-Numpy-td48209.html)\r\n\r\nAs stated in the original thread, We need to start by having a sort() function for complex numbers that can do it based on keys, rather than plain arithmetic ordering.\r\n\r\nThere are two broad ways to approach a sorting function that supports keys (Not just for complex numbers).\r\n\r\n1. Add a ```key``` kwarg to the ```sort()``` (function and method). To support key based sorting on arrays.\r\n2. Use a new function on the lines off ```sortby(c_arr, key=(c_arr.real, c_arr.imag)```\r\n\r\n\r\nIn this PR I have chosen approach 1 for the following reasons\r\n\r\n1. Approach 1 means it is more easier to deal with both in-place method and the function. Since we can make the change in the c-sort function, have minimal change in the python layer. This I hope results, minimal impact on current code that handles complex sorting. One example within numpy is is ```linalg``` module's ```svd()``` function.\r\n\r\n2. With approach 2 when we deprecate complex arithmetic ordering, existing methods using sort() for complex types, need to update their signature. \r\n\r\nAs it stands the PR does the following 3 things within the Python-C Array method implementation of sort\r\n\r\n1. Checks for complex type- If array is of complex-type, it creates a default ```key```(When no key is passed) which mimics the current arithmethic ordering in Numpy .\r\n2. Uses the keys to perform a ``Py_LexSort`` and generate indices.\r\n3. We perform the ```take_along_axis``` via C call back and copy over the result to the original array (pseudo in-place).\r\n\r\nI am requesting feedback/help on implementing ``take_along_axis`` logic in C level in a in-place manner and the approach in general.\r\n\r\nThis will further feed into ``max()`` and ``min()`` as well. Once we figure this out. Next step would be to deprecate  arithmetic ordering for complex types (Which I think will be a PR on it's own)\r\n\r\n\r\nUPDATE:\r\n   The latest version uses A new Function PyArray_Keysort() to argsort a 1D slice of indices, and then use the indices to move contents of the same 1D slice\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "changed_files": [
            {
                "filename": "numpy/__init__.pyi",
                "patch": "@@ -1065,6 +1065,7 @@ def sort(\n     axis: Optional[int] = ...,\n     kind: Optional[_SortKind] = ...,\n     order: Union[None, str, Sequence[str]] = ...,\n+    by: Optional[Tuple[ArrayLike]] = ...,\n ) -> ndarray: ...\n def argsort(\n     a: ArrayLike,"
            },
            {
                "filename": "numpy/core/fromnumeric.py",
                "patch": "@@ -837,12 +837,12 @@ def argpartition(a, kth, axis=-1, kind='introselect', order=None):\n     return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)\n \n \n-def _sort_dispatcher(a, axis=None, kind=None, order=None):\n+def _sort_dispatcher(a, axis=None, kind=None, order=None, *, by=None):\n     return (a,)\n \n \n @array_function_dispatch(_sort_dispatcher)\n-def sort(a, axis=-1, kind=None, order=None):\n+def sort(a, axis=-1, kind=None, order=None, *, by=None):\n     \"\"\"\n     Return a sorted copy of an array.\n \n@@ -869,6 +869,14 @@ def sort(a, axis=-1, kind=None, order=None):\n         but unspecified fields will still be used, in the order in which\n         they come up in the dtype, to break ties.\n \n+    by: tuple of array-likes, optional\n+        With a given array `a` sort the elements not by arithmetic\n+        order, but by any arbitrary order defined by tuple, there can\n+        be multiple keys in a tuple/list form.\n+        Tuple contains k (N,)-shaped sequences\n+        The `k` different \"columns\" to be sorted.  The last column (or row if\n+        `keys` is a 2D array) is the primary sort key.\n+\n     Returns\n     -------\n     sorted_array : ndarray\n@@ -993,7 +1001,8 @@ def sort(a, axis=-1, kind=None, order=None):\n         axis = -1\n     else:\n         a = asanyarray(a).copy(order=\"K\")\n-    a.sort(axis=axis, kind=kind, order=order)\n+\n+    a.sort(axis=axis, kind=kind, order=order, by=by)\n     return a\n \n "
            },
            {
                "filename": "numpy/core/include/numpy/ndarraytypes.h",
                "patch": "@@ -356,10 +356,12 @@ struct NpyAuxData_tag {\n #define PyArray_malloc PyMem_RawMalloc\n #define PyArray_free PyMem_RawFree\n #define PyArray_realloc PyMem_RawRealloc\n+#define PyArray_calloc  PyMem_RawCalloc\n #else\n #define PyArray_malloc malloc\n #define PyArray_free free\n #define PyArray_realloc realloc\n+#define PyArray_calloc calloc\n #endif\n \n /* Dimensions and strides */"
            },
            {
                "filename": "numpy/core/src/multiarray/item_selection.c",
                "patch": "@@ -1749,6 +1749,286 @@ PyArray_LexSort(PyObject *sort_keys, int axis)\n     return NULL;\n }\n \n+/*\n+ * Helper function for PyArray_KeySort\n+ * Sorts 1D slice based on argsort indices\n+ * takes in raw bytes of data, indices array\n+ * Number of elements, and stride of data\n+ * element size and temporary storage for an element.\n+ */\n+void reorder_by_index(char *data, char *current, npy_intp *index, npy_intp N, npy_intp stride, npy_intp elsize) {\n+    /*\n+     * The following code reorders the data with respect to the index. The inner\n+     * while loop places a single element to the right place until it reaches a \n+     * fully cycle (an already ordered element). \n+     * Each element may be visited twice, but will be sorted on the first visit.\n+     * The second visit finds it already sorted and immediately continues.\n+     */\n+    for(npy_intp i = 0; i < N; i++){\n+        if(i == index[i]) {\n+            continue;\n+        }\n+        memmove(current, data + (i*stride), elsize);\n+        npy_intp current_idx = i;\n+        // Break when index and index buffer are the  same.\n+        while(1) {\n+           npy_intp next_idx = index[current_idx];\n+           index[current_idx] = current_idx;\n+           if (next_idx == i) {\n+              break;\n+           }\n+           memmove(data +(current_idx*stride), data + (next_idx*stride), elsize);\n+           current_idx = next_idx;\n+        }\n+        // Put current element in the right place\n+        memmove(data + (current_idx*stride), current, elsize);\n+    }\n+}\n+\n+NPY_NO_EXPORT PyObject *\n+PyArray_KeySort(PyArrayObject *self, PyObject *sort_keys, int axis)\n+{\n+    PyArrayObject **mps;\n+    PyArrayIterObject **its;\n+    PyArrayIterObject *sit = NULL;\n+    npy_intp n, N, size, i, j;\n+    npy_intp astride, self_stride;\n+    int nd;\n+    int needcopy = 0;\n+    int elsize;\n+    int self_elsize;\n+    int maxelsize;\n+    int any_key_is_object = 0;\n+    PyArray_ArgSortFunc *argsort;\n+    NPY_BEGIN_THREADS_DEF;\n+\n+    if (!PySequence_Check(sort_keys)\n+           || ((n = PySequence_Size(sort_keys)) <0)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"Error when trying to get length of sort keys.\");\n+        return NULL;\n+    }\n+    if (n == 0) {\n+        //No keys provided, do nothing.\n+        Py_RETURN_NONE;\n+    }\n+    mps = PyArray_calloc(n, sizeof(*mps));\n+    if (mps == NULL) {\n+        return PyErr_NoMemory();\n+    }\n+    its = PyArray_calloc(n , sizeof(*its));\n+    if (its == NULL) {\n+        PyArray_free(mps);\n+        return PyErr_NoMemory();\n+    }\n+    for (i = 0; i < n; i++) {\n+        PyObject *obj;\n+        obj = PySequence_GetItem(sort_keys, i);\n+        if (obj == NULL) {\n+            goto fail;\n+        }\n+        mps[i] = (PyArrayObject *)PyArray_FROM_O(obj);\n+        Py_DECREF(obj);\n+        if (mps[i] == NULL) {\n+            goto fail;\n+        }\n+        if (i > 0) {\n+            if ((PyArray_NDIM(mps[i]) != PyArray_NDIM(mps[0]))\n+                    || (!PyArray_CompareLists(PyArray_DIMS(mps[i]),\n+                                       PyArray_DIMS(mps[0]),\n+                                       PyArray_NDIM(mps[0])))) {\n+                PyErr_SetString(PyExc_ValueError,\n+                                \"all keys need to be the same shape\");\n+                goto fail;\n+            }\n+        }\n+        if (!PyArray_DESCR(mps[i])->f->argsort[NPY_STABLESORT]\n+                && !PyArray_DESCR(mps[i])->f->compare) {\n+            PyErr_Format(PyExc_TypeError,\n+                         \"item %zd type does not have compare function\", i);\n+            goto fail;\n+        }\n+        any_key_is_object = any_key_is_object\n+            || PyDataType_FLAGCHK(PyArray_DESCR(mps[i]), NPY_NEEDS_PYAPI);\n+\n+        if ((PyArray_NDIM(mps[i]) != PyArray_NDIM(self))||\n+            (!PyArray_CompareLists(PyArray_DIMS(mps[i]),\n+                                       PyArray_DIMS(self),\n+                                       PyArray_NDIM(self)))) {\n+                PyErr_SetString(PyExc_ValueError,\n+                                \"all keys and input array need to be the same shape\");\n+                goto fail;\n+        }\n+    }\n+\n+    /* Now we can check the axis */\n+    nd = PyArray_NDIM(mps[0]);\n+    /*\n+    * Special case letting axis={-1,0} slip through for scalars,\n+    * for backwards compatibility reasons.\n+    */\n+    if (nd == 0 && (axis == 0 || axis == -1)) {\n+        /* TODO: can we deprecate this? */\n+    }\n+    else if (check_and_adjust_axis(&axis, nd) < 0) {\n+        goto fail;\n+    }\n+    if ((nd == 0) || (PyArray_SIZE(mps[0]) <= 1)) {\n+        /* empty/single element case */\n+        // Nothing to do here just return;\n+        goto finish;\n+    }\n+\n+    for (i = 0; i < n; i++) {\n+        its[i] = (PyArrayIterObject *)PyArray_IterAllButAxis(\n+                (PyObject *)mps[i], &axis);\n+        if (its[i] == NULL) {\n+            goto fail;\n+        }\n+    }\n+    /* Now do the sorting */\n+    sit = (PyArrayIterObject *)\n+            PyArray_IterAllButAxis((PyObject *)self, &axis);\n+    if (sit == NULL) {\n+        goto fail;\n+    }\n+    if (!any_key_is_object) {\n+        NPY_BEGIN_THREADS;\n+    }\n+    size = sit->size;\n+    // Number of elements in a slice is denoted by N\n+    N = PyArray_DIMS(mps[0])[axis];\n+    self_stride = PyArray_STRIDE(self, axis);\n+    self_elsize = PyArray_DESCR(self)->elsize;\n+    maxelsize = PyArray_DESCR(mps[0])->elsize;\n+    needcopy = (self_stride != self_elsize);\n+    for (j = 0; j < n; j++) {\n+        needcopy = needcopy\n+            || PyArray_ISBYTESWAPPED(mps[j])\n+            || !(PyArray_FLAGS(mps[j]) & NPY_ARRAY_ALIGNED)\n+            || (PyArray_STRIDES(mps[j])[axis] != (npy_intp)PyArray_DESCR(mps[j])->elsize);\n+        maxelsize = PyArray_MAX(maxelsize, PyArray_DESCR(mps[j])->elsize);\n+    }\n+    assert(N > 0);  /* Guaranteed and assumed by indbuffer */\n+    npy_intp *indbuffer = PyDataMem_NEW(N * sizeof(*indbuffer));\n+    if (indbuffer == NULL) {\n+       PyErr_NoMemory();\n+       goto fail;\n+    }\n+    if (needcopy) {\n+        char *valbuffer;\n+        // Create a buffer of size of the current 1D slice i.e N\n+        npy_intp valbufsize = N * maxelsize;\n+        if (NPY_UNLIKELY(valbufsize) == 0) {\n+            valbufsize = 1;  /* Ensure allocation is not empty */\n+        }\n+        valbuffer = PyDataMem_NEW(valbufsize);\n+        if (valbuffer == NULL) {\n+            PyDataMem_FREE(indbuffer);\n+            PyErr_NoMemory();\n+            goto fail;\n+        }\n+        while (size--) {\n+            for (i = 0; i < N; i++) {\n+                indbuffer[i] = i;\n+            }\n+            for (j = 0; j < n; j++) {\n+                int rcode;\n+                elsize = PyArray_DESCR(mps[j])->elsize;\n+                astride = PyArray_STRIDES(mps[j])[axis];\n+                argsort = PyArray_DESCR(mps[j])->f->argsort[NPY_STABLESORT];\n+                if(argsort == NULL) {\n+                    argsort = npy_atimsort;\n+                }\n+                _unaligned_strided_byte_copy(valbuffer, (npy_intp) elsize,\n+                                             its[j]->dataptr, astride, N, elsize);\n+                if (PyArray_ISBYTESWAPPED(mps[j])) {\n+                    _strided_byte_swap(valbuffer, (npy_intp) elsize, N, elsize);\n+                }\n+                rcode = argsort(valbuffer, indbuffer, N, mps[j]);\n+                if (rcode < 0 || (PyDataType_REFCHK(PyArray_DESCR(mps[j]))\n+                            && PyErr_Occurred())) {\n+                    PyDataMem_FREE(valbuffer);\n+                    PyDataMem_FREE(indbuffer);\n+                    goto fail;\n+                }\n+                PyArray_ITER_NEXT(its[j]);\n+            }\n+            // For unaligned we use stride to copy\n+            char *el_swap = PyDataMem_NEW(self_elsize);\n+            if (el_swap == NULL){\n+                PyDataMem_FREE(indbuffer);\n+                PyErr_NoMemory();\n+                goto fail;\n+            }\n+            reorder_by_index(sit->dataptr, el_swap, indbuffer, N,\n+                    self_stride, self_elsize);\n+            PyArray_ITER_NEXT(sit);\n+            PyDataMem_FREE(el_swap);\n+        }\n+        PyDataMem_FREE(valbuffer);\n+        PyDataMem_FREE(indbuffer);\n+    }\n+    else {\n+        while (size--) {\n+            for (i = 0; i < N; i++) {\n+                indbuffer[i] = i;\n+            }\n+            for (j = 0; j < n; j++) {\n+                int rcode;\n+                argsort = PyArray_DESCR(mps[j])->f->argsort[NPY_STABLESORT];\n+                if(argsort == NULL) {\n+                    argsort = npy_atimsort;\n+                }\n+                rcode = argsort(its[j]->dataptr,\n+                        indbuffer, N, mps[j]);\n+                if (rcode < 0 || (PyDataType_REFCHK(PyArray_DESCR(mps[j]))\n+                            && PyErr_Occurred())) {\n+                    goto fail;\n+                }\n+\n+                PyArray_ITER_NEXT(its[j]);\n+            }\n+            // Aligned use element size here to do in-place copy\n+            char *el_swap = PyDataMem_NEW(self_elsize);\n+            if (el_swap == NULL){\n+                PyDataMem_FREE(indbuffer);\n+                PyErr_NoMemory();\n+                goto fail;\n+            }\n+            reorder_by_index(sit->dataptr, el_swap, indbuffer, N,\n+                    self_elsize, self_elsize);\n+            PyArray_ITER_NEXT(sit);\n+            PyDataMem_FREE(el_swap);\n+        }\n+        PyDataMem_FREE(indbuffer);\n+    }\n+\n+    if (!any_key_is_object) {\n+        NPY_END_THREADS;\n+    }\n+\n+ finish:\n+    for (i = 0; i < n; i++) {\n+        Py_XDECREF(mps[i]);\n+        Py_XDECREF(its[i]);\n+    }\n+    Py_XDECREF(sit);\n+    PyArray_free(mps);\n+    PyArray_free(its);\n+    Py_RETURN_NONE;\n+\n+ fail:\n+    NPY_END_THREADS;\n+    Py_XDECREF(sit);\n+    for (i = 0; i < n; i++) {\n+        Py_XDECREF(mps[i]);\n+        Py_XDECREF(its[i]);\n+    }\n+    PyArray_free(mps);\n+    PyArray_free(its);\n+    return NULL;\n+}\n \n /*NUMPY_API\n  *"
            },
            {
                "filename": "numpy/core/src/multiarray/item_selection.h",
                "patch": "@@ -27,4 +27,14 @@ NPY_NO_EXPORT int\n PyArray_MultiIndexSetItem(PyArrayObject *self, const npy_intp *multi_index,\n                                                 PyObject *obj);\n \n+/*\n+ *Keysort an array providing indices that will sort a collection of arrays\n+ *lexicographically.  The first key is sorted on first, followed by the second key\n+ *-- requires that arg\"merge\"sort is available for each sort_key\n+ *\n+ *Returns None on success, returns NULL on failure\n+ */\n+NPY_NO_EXPORT PyObject *\n+PyArray_KeySort(PyArrayObject *self, PyObject *sort_keys, int axis);\n+\n #endif"
            },
            {
                "filename": "numpy/core/src/multiarray/methods.c",
                "patch": "@@ -28,7 +28,7 @@\n \n #include \"methods.h\"\n #include \"alloc.h\"\n-\n+#include \"item_selection.h\"\n \n /* NpyArg_ParseKeywords\n  *\n@@ -1229,18 +1229,39 @@ array_sort(PyArrayObject *self, PyObject *args, PyObject *kwds)\n {\n     int axis=-1;\n     int val;\n-    NPY_SORTKIND sortkind = NPY_QUICKSORT;\n+    NPY_SORTKIND sortkind = -1;\n     PyObject *order = NULL;\n+    PyObject *by = NULL;\n     PyArray_Descr *saved = NULL;\n     PyArray_Descr *newd;\n-    static char *kwlist[] = {\"axis\", \"kind\", \"order\", NULL};\n \n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|iO&O:sort\", kwlist,\n+\n+    static char *kwlist[] = {\"axis\", \"kind\", \"order\", \"by\", NULL};\n+    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|iO&O$O:sort\", kwlist,\n                                     &axis,\n                                     PyArray_SortkindConverter, &sortkind,\n-                                    &order)) {\n+                                    &order, &by)) {\n         return NULL;\n     }\n+    if (by != Py_None && by != NULL) {\n+        if (sortkind == -1) {\n+            sortkind = NPY_STABLESORT;\n+        }\n+        if (sortkind != NPY_STABLESORT) {\n+            PyErr_SetString(PyExc_ValueError, \"When using by argument \" \n+                    \"sort-kind can be only \\'stable\\'.\");\n+            return NULL;\n+        }\n+        if (!PyTuple_CheckExact(by)) {\n+            PyErr_SetString(PyExc_TypeError, \"\\'by\\' is expected to be\"\n+                    \"of a sequence type.\");\n+            return NULL;\n+        }\n+        return PyArray_KeySort(self, by, axis);\n+    }\n+    if (sortkind == -1) {\n+        sortkind = NPY_QUICKSORT;\n+    }\n     if (order == Py_None) {\n         order = NULL;\n     }"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -1747,6 +1747,36 @@ def test_transpose(self):\n         assert_raises(ValueError, lambda: a.transpose(0, 0))\n         assert_raises(ValueError, lambda: a.transpose(0, 1, 2))\n \n+    def test_sort_by(self):\n+        a = np.array([-2, -3, -4, -1, -6])\n+        b = np.sort(a, by=(np.absolute(a),))\n+        e = np.array([-1, -2, -3, -4, -6])\n+        assert_equal(b, e)\n+        # Test sorting complex numbers by absolute value\n+        carr = np.arange(4, dtype=np.complex128)[::-1].reshape(2, 2)\n+        carr_out = np.array([[2+0j, 3+0j], [0+0j, 1+0j]], dtype=np.complex128)\n+        carr.sort(axis=1, by=(np.absolute(carr),))\n+        assert_equal(carr, carr_out)\n+        #Testing sort by real, imag\n+        carr = np.array([1 + 1j, 1-1j], dtype=np.complex128)\n+        carr_out = np.array([1-1j, 1+1j], dtype=np.complex128)\n+        carr.sort(by=(carr.imag, carr.real))\n+        assert_equal(carr, carr_out)\n+        # Look for different shape error\n+        carr = np.array([[1+1j, 1-1j]], dtype=np.complex128)\n+        assert_raises(ValueError, carr.sort, by=(carr.imag[0],))\n+        # Only stable sort test\n+        carr = np.array([[1+1j, 1-1j]], dtype=np.complex128)\n+        assert_raises(ValueError, carr.sort,by=(carr.imag, carr.real), kind='quicksort')\n+        # Passing empty tuple test\n+        arr = np.array([4,1,7])\n+        new_arr = np.sort(arr, by=())\n+        assert_equal(arr, new_arr)\n+        # Zero dimensional scalar\n+        arr = np.array(4+4j)\n+        new_arr = np.sort(arr, by=(arr,))\n+        assert_equal(arr, new_arr)\n+\n     def test_sort(self):\n         # test ordering for floats and complex containing nans. It is only\n         # necessary to check the less-than comparison, so sorts that\n@@ -1818,14 +1848,13 @@ def test_sort_complex(self, part, dtype):\n         bi = (b * (1+1j)).astype(cdtype)\n         setattr(ai, part, 1)\n         setattr(bi, part, 1)\n-        for kind in self.sort_kinds:\n-            msg = \"complex sort, %s part == 1, kind=%s\" % (part, kind)\n-            c = ai.copy()\n-            c.sort(kind=kind)\n-            assert_equal(c, ai, msg)\n-            c = bi.copy()\n-            c.sort(kind=kind)\n-            assert_equal(c, ai, msg)\n+        msg = \"complex sort, %s part == 1, kind=%s\" % (part, 'stable')\n+        c = ai.copy()\n+        c.sort(kind='stable')\n+        assert_equal(c, ai, msg)\n+        c = bi.copy()\n+        c.sort(kind='stable')\n+        assert_equal(c, ai, msg)\n \n     def test_sort_complex_byte_swapping(self):\n         # test sorting of complex arrays requiring byte-swapping, gh-5441"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21056,
        "body": "This pull-request:\r\n\r\n- re-implement SIMD kernels of complex operations(multiply, add, sub, absolute, square, conjugate)\r\n\r\n    New kernels implemented via universal intrinsic,\r\n    which leads to bringing SIMD kernels for all supported SIMD extensions,\r\n    also adds support for MSVC.\r\n\r\n    The raw SIMD only supports AVX512,  still new kernels provide better\r\n    performance for non-contiguous memory access and don't require 128-bit/64-bit alignment\r\n    for complex128/complex64 just 64-bit/32-bit.\r\n\r\n- Remove raw SIMD of f32/f64 arithmetic operations on X86 and\r\n   activate the implementation of universal intrinsics instead.\r\n\r\n- Implement intrinsics for mask division\r\n\r\n- Implement intrinsics for shuffle over 128-bit lane and unzip\r\n    \r\n    shuffle intrinsics support 32-bit/64-bit vector data types,\r\n    unzip(deinterleave) intrinsics supports all data types.\r\n\r\n- Add special intrinsics for better non-contiguous/partial memory access for complex load/store\r\n    \r\n    summarized as follows:\r\n    \r\n    * 64-bit contiguous partial load/store over 32-bit lane\r\n       npyv_load2_till_u32, npyv_load2_till_s32, npyv_load2_till_f32\r\n       npyv_load2_tillz_u32, npyv_load2_tillz_s32, npyv_load2_tillz_f32\r\n       npyv_store2_till_u32, npyv_store2_till_s32, npyv_store2_till_f32\r\n    \r\n    * 128-bit contiguous partial load/store over 64-bit lane\r\n       npyv_load2_till_u64, npyv_load2_till_s64, npyv_load2_till_f64\r\n       npyv_load2_tillz_u64, npyv_load2_tillz_s64, npyv_load2_tillz_f64\r\n       npyv_store2_till_u64, npyv_store2_till_s64, npyv_store2_till_f64\r\n    \r\n    * 64-bit non-contiguous load/store over 32-bit stride\r\n       npyv_loadn2_u32,  npyv_loadn2_s32,  npyv_loadn2_f32\r\n       npyv_storen2_u32, npyv_storen2_s32, npyv_storen2_f32\r\n    \r\n    * 128-bit non-contiguous load/store over 64-bit stride\r\n       npyv_loadn2_u64,  npyv_loadn2_s64,  npyv_loadn2_f64\r\n       npyv_storen2_u64, npyv_storen2_s64, npyv_storen2_f64\r\n    \r\n    * 64-bit non-contiguous partial load/store over 32-bit stride\r\n       npyv_loadn2_till_u32, npyv_loadn2_till_s32, npyv_loadn2_till_f32\r\n       npyv_loadn2_tillz_u32, npyv_loadn2_tillz_s32, npyv_loadn2_tillz_f32\r\n       npyv_storen2_till_u32, npyv_storen2_till_s32, npyv_storen2_till_f32\r\n    \r\n    * 128-bit non-contiguous partial load/store over 64-bit stride\r\n       npyv_loadn2_till_u64, npyv_loadn2_till_s64, npyv_loadn2_till_f64\r\n       npyv_loadn2_tillz_u64, npyv_loadn2_tillz_s64, npyv_loadn2_tillz_f64\r\n       npyv_storen2_till_u64, npyv_storen2_till_s64, npyv_storen2_till_f64\r\n    \r\n    * 2 channels de-interlave/interleave contiguous load/store for all data types\r\n          npyv_load_##sfx##x2, npyv_store_##sfx##x2\r\n\r\n------\r\n\r\n### X86\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         48 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 7700X 8-Core Processor\r\n    CPU family:          25\r\n    Model:               97\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            2\r\n    Frequency boost:     disabled\r\n    CPU(s) scaling MHz:  67%\r\n    CPU max MHz:         5572.2651\r\n    CPU min MHz:         3000.0000\r\n    BogoMIPS:            8986.45\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep\r\n                         mtrr pge mca cmov pat pse36 clflush mmx fxsr\r\n                          sse sse2 ht syscall nx mmxext fxsr_opt pdpe\r\n                         1gb rdtscp lm constant_tsc rep_good amd_lbr_\r\n                         v2 nopl nonstop_tsc cpuid extd_apicid aperfm\r\n                         perf rapl pni pclmulqdq monitor ssse3 fma cx\r\n                         16 sse4_1 sse4_2 x2apic movbe popcnt aes xsa\r\n                         ve avx f16c rdrand lahf_lm cmp_legacy svm ex\r\n                         tapic cr8_legacy abm sse4a misalignsse 3dnow\r\n                         prefetch osvw ibs skinit wdt tce topoext per\r\n                         fctr_core perfctr_nb bpext perfctr_llc mwait\r\n                         x cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfm\r\n                         on_v2 ibrs ibpb stibp vmmcall fsgsbase bmi1\r\n                         avx2 smep bmi2 erms invpcid cqm rdt_a avx512\r\n                         f avx512dq rdseed adx smap avx512ifma clflus\r\n                         hopt clwb avx512cd sha_ni avx512bw avx512vl\r\n                         xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_o\r\n                         ccup_llc cqm_mbm_total cqm_mbm_local avx512_\r\n                         bf16 clzero irperf xsaveerptr rdpru wbnoinvd\r\n                          cppc arat npt lbrv svm_lock nrip_save tsc_s\r\n                         cale vmcb_clean flushbyasid decodeassists pa\r\n                         usefilter pfthreshold avic v_vmsave_vmload v\r\n                         gif x2avic v_spec_ctrl avx512vbmi umip pku o\r\n                         spke avx512_vbmi2 gfni vaes vpclmulqdq avx51\r\n                         2_vnni avx512_bitalg avx512_vpopcntdq rdpid\r\n                         overflow_recov succor smca fsrm flush_l1d\r\nVirtualization features:\r\n  Virtualization:        AMD-V\r\nCaches (sum of all):\r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    8 MiB (8 instances)\r\n  L3:                    32 MiB (1 instance)\r\nNUMA:\r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disable\r\n                         d via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __u\r\n                         ser pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IB\r\n                         RS_FW, STIBP always-on, RSB filling, PBRSB-e\r\n                         IBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux seiko-pc 6.1.1-arch1-1 #1 SMP PREEMPT_DYNAMIC Wed, 21 Dec 2022 22:27:55 +0000 x86_64 GNU/Linux\r\nPython 3.10.9\r\ngcc (GCC) 12.2.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n<details>\r\n <summary>AVX512F(before) vs AVX2(except for absolute)</summary>\r\n\r\n```Bash\r\nunset NPY_DISABLE_CPU_FEATURES\r\npython runtests.py -n --bench-compare parent/main Complex  -- --cpu-affinity=6,7\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [6d474f2d]       [62fef37f]\r\n                      <replace_raw_arithmfp>\r\n-     4.79\u00b10.03\u03bcs      4.55\u00b10.05\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 4, 4, 2, 'F')\r\n-      4.86\u00b10.1\u03bcs      4.61\u00b10.06\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 4, 2, 'F')\r\n-      4.85\u00b10.1\u03bcs      4.60\u00b10.05\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 4, 'F')\r\n-     4.41\u00b10.02\u03bcs      4.18\u00b10.08\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 2, 2, 2, 'F')\r\n-     5.23\u00b10.04\u03bcs      4.96\u00b10.05\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 4, 'F')\r\n-      6.40\u00b10.1\u03bcs      6.05\u00b10.02\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 2, 'D')\r\n-     4.79\u00b10.05\u03bcs      4.53\u00b10.05\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 4, 4, 2, 'F')\r\n-     6.47\u00b10.03\u03bcs      6.11\u00b10.06\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 2, 'D')\r\n-     4.79\u00b10.04\u03bcs      4.52\u00b10.08\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 4, 'F')\r\n-     4.44\u00b10.05\u03bcs      4.18\u00b10.04\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 1, 4, 'F')\r\n-     4.75\u00b10.06\u03bcs      4.47\u00b10.03\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 4, 2, 'F')\r\n-      7.75\u00b10.2\u03bcs      7.28\u00b10.06\u03bcs     0.94  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 4, 1, 'D')\r\n-     4.88\u00b10.06\u03bcs       4.59\u00b10.1\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 4, 'F')\r\n-      6.50\u00b10.1\u03bcs      6.10\u00b10.03\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 4, 'F')\r\n-     5.12\u00b10.09\u03bcs      4.78\u00b10.02\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 4, 1, 2, 'F')\r\n-     5.04\u00b10.02\u03bcs      4.70\u00b10.07\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 2, 2, 'F')\r\n-     6.51\u00b10.08\u03bcs      6.07\u00b10.04\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 2, 'D')\r\n-      6.46\u00b10.1\u03bcs       6.01\u00b10.1\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 4, 'F')\r\n-      4.90\u00b10.1\u03bcs      4.56\u00b10.04\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 4, 'F')\r\n-      6.43\u00b10.1\u03bcs      5.97\u00b10.08\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 2, 'D')\r\n-      6.50\u00b10.1\u03bcs      6.03\u00b10.09\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 4, 'F')\r\n-     4.41\u00b10.05\u03bcs      4.09\u00b10.08\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 4, 1, 'F')\r\n-     4.52\u00b10.09\u03bcs      4.18\u00b10.05\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 2, 2, 2, 'F')\r\n-     14.0\u00b10.04\u03bcs      13.0\u00b10.04\u03bcs     0.93  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 4, 2, 'F')\r\n-     4.87\u00b10.07\u03bcs      4.50\u00b10.08\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 4, 2, 'F')\r\n-     13.8\u00b10.02\u03bcs      12.7\u00b10.02\u03bcs     0.92  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 2, 2, 'F')\r\n-     5.07\u00b10.03\u03bcs      4.68\u00b10.01\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 2, 1, 2, 'F')\r\n-     4.87\u00b10.08\u03bcs       4.48\u00b10.1\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 4, 2, 'F')\r\n-     4.46\u00b10.04\u03bcs      4.10\u00b10.05\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 1, 4, 'F')\r\n-     11.4\u00b10.06\u03bcs      10.5\u00b10.03\u03bcs     0.92  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 4, 1, 'F')\r\n-     5.03\u00b10.02\u03bcs      4.62\u00b10.08\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 2, 1, 'F')\r\n-     11.2\u00b10.01\u03bcs      10.3\u00b10.01\u03bcs     0.92  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 2, 1, 'F')\r\n-     5.18\u00b10.04\u03bcs      4.75\u00b10.06\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 4, 1, 'F')\r\n-     5.07\u00b10.01\u03bcs      4.65\u00b10.03\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 2, 2, 1, 'F')\r\n-      14.0\u00b10.1\u03bcs      12.8\u00b10.07\u03bcs     0.92  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 2, 4, 'F')\r\n-     14.5\u00b10.03\u03bcs      13.3\u00b10.06\u03bcs     0.92  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 4, 4, 'F')\r\n-     5.04\u00b10.02\u03bcs      4.62\u00b10.05\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 2, 1, 1, 'F')\r\n-     5.06\u00b10.03\u03bcs      4.63\u00b10.02\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 2, 'F')\r\n-      4.85\u00b10.2\u03bcs      4.41\u00b10.01\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 4, 2, 'F')\r\n-      4.99\u00b10.1\u03bcs      4.54\u00b10.05\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 4, 1, 2, 'F')\r\n-     4.50\u00b10.07\u03bcs      4.09\u00b10.03\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 4, 1, 1, 'F')\r\n-     5.30\u00b10.04\u03bcs      4.81\u00b10.06\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 4, 1, 1, 'F')\r\n-     5.60\u00b10.03\u03bcs       5.09\u00b10.1\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 4, 1, 2, 'F')\r\n-     5.20\u00b10.03\u03bcs      4.72\u00b10.06\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 2, 'D')\r\n-     5.17\u00b10.04\u03bcs      4.69\u00b10.09\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 2, 'D')\r\n-       138\u00b10.5\u03bcs        125\u00b10.8\u03bcs     0.91  bench_core.VarComplex.time_var(100000)\r\n-      4.51\u00b10.1\u03bcs      4.08\u00b10.02\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 4, 1, 1, 'F')\r\n-     5.19\u00b10.07\u03bcs      4.68\u00b10.07\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 1, 'D')\r\n-     5.17\u00b10.02\u03bcs      4.66\u00b10.03\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 2, 'D')\r\n-     5.23\u00b10.01\u03bcs      4.71\u00b10.08\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 2, 'D')\r\n-     5.23\u00b10.03\u03bcs      4.71\u00b10.08\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 2, 'D')\r\n-     4.00\u00b10.01\u03bcs         3.58\u00b10\u03bcs     0.90  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 2, 2, 'F')\r\n-     5.23\u00b10.04\u03bcs      4.69\u00b10.07\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 1, 'D')\r\n-     5.20\u00b10.03\u03bcs       4.65\u00b10.1\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 1, 'D')\r\n-     5.20\u00b10.02\u03bcs       4.64\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 1, 'D')\r\n-     5.22\u00b10.02\u03bcs       4.66\u00b10.1\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 2, 'D')\r\n-     5.17\u00b10.07\u03bcs       4.61\u00b10.1\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 1, 'D')\r\n-     5.22\u00b10.04\u03bcs       4.65\u00b10.1\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 2, 'D')\r\n-     5.74\u00b10.05\u03bcs      5.09\u00b10.08\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 4, 2, 1, 'F')\r\n-     5.24\u00b10.06\u03bcs       4.63\u00b10.1\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 1, 'D')\r\n-     5.24\u00b10.03\u03bcs      4.63\u00b10.02\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 2, 'D')\r\n-     5.23\u00b10.02\u03bcs      4.61\u00b10.09\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 2, 'D')\r\n-     5.22\u00b10.01\u03bcs      4.58\u00b10.03\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 2, 'D')\r\n-     5.24\u00b10.04\u03bcs      4.59\u00b10.04\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 1, 'D')\r\n-     5.21\u00b10.04\u03bcs      4.55\u00b10.06\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 1, 'D')\r\n-     5.24\u00b10.02\u03bcs      4.57\u00b10.07\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 2, 'D')\r\n-     5.27\u00b10.03\u03bcs      4.60\u00b10.09\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 1, 'D')\r\n-     5.21\u00b10.02\u03bcs      4.54\u00b10.07\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 2, 'D')\r\n-      8.14\u00b10.1\u03bcs      7.09\u00b10.04\u03bcs     0.87  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 4, 1, 'D')\r\n-     5.21\u00b10.03\u03bcs      4.51\u00b10.03\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 1, 'D')\r\n-     5.23\u00b10.04\u03bcs      4.52\u00b10.06\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 1, 'D')\r\n-     5.22\u00b10.03\u03bcs      4.50\u00b10.04\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 1, 'D')\r\n-     4.60\u00b10.04\u03bcs      3.96\u00b10.07\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 4, 'F')\r\n-     4.04\u00b10.03\u03bcs      3.45\u00b10.05\u03bcs     0.85  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 1, 4, 'F')\r\n-     4.62\u00b10.02\u03bcs      3.93\u00b10.07\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 4, 'F')\r\n-     4.64\u00b10.04\u03bcs      3.95\u00b10.08\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 4, 'F')\r\n-     4.60\u00b10.03\u03bcs      3.91\u00b10.02\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 4, 1, 1, 'F')\r\n-     4.22\u00b10.04\u03bcs      3.59\u00b10.06\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 2, 2, 1, 'F')\r\n-     4.58\u00b10.05\u03bcs      3.87\u00b10.03\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 4, 2, 1, 'F')\r\n-     4.23\u00b10.06\u03bcs      3.58\u00b10.04\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 2, 2, 'F')\r\n-     4.63\u00b10.06\u03bcs      3.91\u00b10.03\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 4, 'F')\r\n-     4.67\u00b10.05\u03bcs      3.93\u00b10.06\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 4, 'F')\r\n-     5.47\u00b10.05\u03bcs      4.60\u00b10.07\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 4, 'F')\r\n-     4.63\u00b10.08\u03bcs      3.89\u00b10.05\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 4, 'F')\r\n-     4.63\u00b10.01\u03bcs      3.89\u00b10.03\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 4, 'F')\r\n-     4.61\u00b10.03\u03bcs      3.87\u00b10.01\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 4, 'F')\r\n-     4.63\u00b10.04\u03bcs      3.88\u00b10.04\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 4, 'F')\r\n-     4.65\u00b10.04\u03bcs      3.90\u00b10.09\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 4, 1, 'F')\r\n-     4.62\u00b10.04\u03bcs      3.86\u00b10.02\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 4, 1, 'F')\r\n-     4.64\u00b10.05\u03bcs      3.88\u00b10.04\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 4, 4, 1, 'F')\r\n-      23.4\u00b10.2\u03bcs      19.6\u00b10.07\u03bcs     0.84  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 2, 4, 'D')\r\n-     4.64\u00b10.03\u03bcs      3.88\u00b10.04\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 4, 'F')\r\n-     4.62\u00b10.06\u03bcs      3.86\u00b10.04\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 4, 2, 1, 'F')\r\n-     4.60\u00b10.04\u03bcs      3.84\u00b10.04\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 4, 1, 'F')\r\n-     4.63\u00b10.03\u03bcs      3.86\u00b10.06\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 4, 'F')\r\n-     4.66\u00b10.05\u03bcs      3.88\u00b10.07\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 4, 'F')\r\n-     4.61\u00b10.06\u03bcs      3.84\u00b10.01\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 4, 1, 'F')\r\n-     4.65\u00b10.04\u03bcs      3.87\u00b10.02\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 4, 1, 1, 'F')\r\n-     4.63\u00b10.03\u03bcs      3.86\u00b10.05\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 4, 1, 'F')\r\n-     4.64\u00b10.04\u03bcs       3.86\u00b10.1\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 4, 4, 1, 'F')\r\n-     3.58\u00b10.01\u03bcs      2.97\u00b10.06\u03bcs     0.83  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 1, 1, 'D')\r\n-     18.2\u00b10.03\u03bcs      15.1\u00b10.08\u03bcs     0.83  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 2, 1, 'D')\r\n-     4.21\u00b10.02\u03bcs      3.49\u00b10.02\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 2, 1, 1, 'F')\r\n-     4.27\u00b10.09\u03bcs      3.53\u00b10.03\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 2, 1, 2, 'F')\r\n-     20.2\u00b10.09\u03bcs      16.7\u00b10.08\u03bcs     0.83  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 2, 2, 'D')\r\n-     4.20\u00b10.02\u03bcs      3.47\u00b10.01\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 2, 1, 'F')\r\n-     5.56\u00b10.03\u03bcs      4.60\u00b10.06\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 4, 'F')\r\n-     5.56\u00b10.02\u03bcs      4.59\u00b10.09\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 4, 'F')\r\n-     5.51\u00b10.02\u03bcs      4.53\u00b10.02\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 4, 'F')\r\n-     4.25\u00b10.04\u03bcs      3.49\u00b10.03\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 2, 2, 1, 'F')\r\n-     5.53\u00b10.03\u03bcs      4.54\u00b10.06\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 4, 'F')\r\n-     5.68\u00b10.07\u03bcs       4.67\u00b10.2\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 4, 2, 'F')\r\n-     4.28\u00b10.03\u03bcs      3.51\u00b10.02\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 2, 2, 'F')\r\n-     4.69\u00b10.02\u03bcs      3.85\u00b10.03\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 4, 1, 'F')\r\n-        2.08\u00b10\u03bcs      1.70\u00b10.06\u03bcs     0.82  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 1, 1, 'F')\r\n-     4.26\u00b10.04\u03bcs      3.49\u00b10.02\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 1, 2, 'F')\r\n-     4.31\u00b10.03\u03bcs      3.52\u00b10.03\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 2, 1, 2, 'F')\r\n-     5.55\u00b10.07\u03bcs      4.52\u00b10.06\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 4, 2, 'F')\r\n-     24.4\u00b10.08\u03bcs      19.9\u00b10.07\u03bcs     0.81  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 4, 4, 'D')\r\n-     19.3\u00b10.08\u03bcs       15.7\u00b10.1\u03bcs     0.81  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 4, 1, 'D')\r\n-     5.58\u00b10.04\u03bcs      4.52\u00b10.08\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 4, 'F')\r\n-     5.72\u00b10.06\u03bcs      4.63\u00b10.03\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 2, 'F')\r\n-     5.71\u00b10.05\u03bcs      4.62\u00b10.09\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 2, 'F')\r\n-     4.19\u00b10.01\u03bcs      3.38\u00b10.03\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 2, 1, 1, 'F')\r\n-     4.18\u00b10.02\u03bcs      3.36\u00b10.02\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 2, 1, 'F')\r\n-      21.6\u00b10.1\u03bcs       17.4\u00b10.1\u03bcs     0.80  bench_ufunc_strides.UnaryComplex.time_ufunc('absolute', 4, 2, 'D')\r\n-     5.56\u00b10.02\u03bcs       4.46\u00b10.1\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 4, 2, 'F')\r\n-     5.70\u00b10.06\u03bcs      4.57\u00b10.05\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 2, 'F')\r\n-     4.24\u00b10.02\u03bcs      3.36\u00b10.02\u03bcs     0.79  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 1, 2, 'F')\r\n-     6.08\u00b10.02\u03bcs      4.77\u00b10.05\u03bcs     0.79  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 1, 'D')\r\n-     5.23\u00b10.03\u03bcs      4.08\u00b10.09\u03bcs     0.78  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 1, 2, 'D')\r\n-     6.05\u00b10.03\u03bcs      4.70\u00b10.06\u03bcs     0.78  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 2, 'D')\r\n-     6.11\u00b10.03\u03bcs       4.71\u00b10.1\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 2, 'D')\r\n-     6.10\u00b10.02\u03bcs       4.69\u00b10.1\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 1, 'D')\r\n-     6.11\u00b10.03\u03bcs      4.67\u00b10.08\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 1, 'D')\r\n-     6.10\u00b10.02\u03bcs       4.64\u00b10.2\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 2, 'D')\r\n-     6.06\u00b10.01\u03bcs       4.62\u00b10.1\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 2, 'D')\r\n-     6.11\u00b10.03\u03bcs      4.62\u00b10.09\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 1, 'D')\r\n-     6.10\u00b10.02\u03bcs      4.57\u00b10.09\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 1, 'D')\r\n-     6.12\u00b10.03\u03bcs      4.56\u00b10.07\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 2, 'D')\r\n-     6.10\u00b10.05\u03bcs      4.54\u00b10.05\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 2, 'D')\r\n-     6.09\u00b10.03\u03bcs      4.52\u00b10.04\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 1, 'D')\r\n-     5.09\u00b10.01\u03bcs       3.74\u00b10.2\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 1, 'D')\r\n-     5.11\u00b10.02\u03bcs       3.75\u00b10.2\u03bcs     0.73  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 1, 'D')\r\n-     4.48\u00b10.01\u03bcs      3.26\u00b10.01\u03bcs     0.73  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 1, 'F')\r\n-     5.45\u00b10.04\u03bcs       3.96\u00b10.1\u03bcs     0.73  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 4, 'F')\r\n-     4.47\u00b10.01\u03bcs      3.25\u00b10.01\u03bcs     0.73  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 1, 'F')\r\n-     4.53\u00b10.01\u03bcs      3.29\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 2, 'F')\r\n-     4.53\u00b10.04\u03bcs      3.28\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 2, 'F')\r\n-     4.47\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 1, 'F')\r\n-     4.48\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 1, 'F')\r\n-     4.48\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 1, 'F')\r\n-     5.42\u00b10.02\u03bcs      3.91\u00b10.02\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 4, 1, 'F')\r\n-     4.53\u00b10.06\u03bcs      3.27\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 2, 'F')\r\n-     4.48\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 1, 'F')\r\n-     4.50\u00b10.01\u03bcs         3.25\u00b10\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 1, 'F')\r\n-     4.50\u00b10.01\u03bcs         3.25\u00b10\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 2, 'F')\r\n-     4.50\u00b10.01\u03bcs      3.25\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 1, 'F')\r\n-     4.60\u00b10.03\u03bcs      3.32\u00b10.03\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 2, 'F')\r\n-     4.48\u00b10.01\u03bcs      3.23\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 1, 'F')\r\n-     4.49\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 2, 'F')\r\n-     4.49\u00b10.01\u03bcs      3.24\u00b10.02\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 1, 'F')\r\n-     4.57\u00b10.04\u03bcs      3.29\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 2, 'F')\r\n-     4.59\u00b10.04\u03bcs      3.30\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 2, 'F')\r\n-     4.52\u00b10.02\u03bcs      3.25\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 1, 'F')\r\n-     4.52\u00b10.05\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 2, 'F')\r\n-     4.51\u00b10.01\u03bcs         3.23\u00b10\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 2, 'F')\r\n-     3.62\u00b10.01\u03bcs      2.59\u00b10.04\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 1, 'F')\r\n-     4.57\u00b10.03\u03bcs      3.27\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 2, 'F')\r\n-     5.47\u00b10.04\u03bcs       3.92\u00b10.1\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 4, 1, 'F')\r\n-     4.53\u00b10.01\u03bcs         3.25\u00b10\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 2, 'F')\r\n-     5.45\u00b10.01\u03bcs       3.90\u00b10.1\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 4, 1, 'F')\r\n-     4.52\u00b10.02\u03bcs      3.24\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 1, 'F')\r\n-     4.53\u00b10.01\u03bcs      3.24\u00b10.02\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 2, 'F')\r\n-     4.52\u00b10.01\u03bcs      3.23\u00b10.01\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 2, 'F')\r\n-     4.53\u00b10.03\u03bcs      3.24\u00b10.01\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 2, 'F')\r\n-     4.56\u00b10.01\u03bcs      3.26\u00b10.02\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 2, 'F')\r\n-     5.45\u00b10.03\u03bcs      3.90\u00b10.04\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 4, 'F')\r\n-     4.54\u00b10.02\u03bcs      3.24\u00b10.02\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 2, 'F')\r\n-     4.54\u00b10.03\u03bcs         3.24\u00b10\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 2, 'F')\r\n-     4.55\u00b10.01\u03bcs      3.25\u00b10.01\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 2, 'F')\r\n-     4.57\u00b10.03\u03bcs      3.25\u00b10.03\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 2, 'F')\r\n-     5.45\u00b10.03\u03bcs      3.88\u00b10.04\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 4, 'F')\r\n-     5.45\u00b10.02\u03bcs      3.87\u00b10.03\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 4, 'F')\r\n-     4.57\u00b10.01\u03bcs         3.24\u00b10\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 2, 'F')\r\n-     4.61\u00b10.05\u03bcs      3.26\u00b10.03\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 2, 'F')\r\n-     5.46\u00b10.02\u03bcs      3.86\u00b10.03\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 4, 'F')\r\n-     5.57\u00b10.06\u03bcs      3.94\u00b10.08\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 1, 'F')\r\n-     4.58\u00b10.06\u03bcs         3.24\u00b10\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 2, 'F')\r\n-     4.64\u00b10.06\u03bcs      3.27\u00b10.02\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 2, 'F')\r\n-     5.48\u00b10.04\u03bcs      3.86\u00b10.02\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 4, 'F')\r\n-     5.51\u00b10.05\u03bcs      3.88\u00b10.07\u03bcs     0.70  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 1, 'F')\r\n-     6.67\u00b10.02\u03bcs      4.66\u00b10.09\u03bcs     0.70  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 1, 'D')\r\n-     5.52\u00b10.04\u03bcs      3.86\u00b10.02\u03bcs     0.70  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 1, 'F')\r\n-     5.12\u00b10.06\u03bcs       3.56\u00b10.1\u03bcs     0.70  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 1, 'D')\r\n-     4.00\u00b10.01\u03bcs         2.78\u00b10\u03bcs     0.70  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 1, 2, 'F')\r\n-     5.10\u00b10.04\u03bcs      3.47\u00b10.03\u03bcs     0.68  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 4, 1, 'F')\r\n-     5.12\u00b10.01\u03bcs      3.44\u00b10.03\u03bcs     0.67  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 1, 'D')\r\n-     5.13\u00b10.02\u03bcs      3.41\u00b10.04\u03bcs     0.67  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 1, 'D')\r\n-     5.13\u00b10.02\u03bcs      3.41\u00b10.03\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 1, 'D')\r\n-     5.12\u00b10.01\u03bcs      3.40\u00b10.03\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 1, 'D')\r\n-     5.10\u00b10.01\u03bcs      3.38\u00b10.01\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 1, 'D')\r\n-     5.11\u00b10.01\u03bcs      3.38\u00b10.02\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 1, 'D')\r\n-     5.11\u00b10.01\u03bcs      3.38\u00b10.04\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 1, 'D')\r\n-        5.11\u00b10\u03bcs      3.37\u00b10.03\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 1, 'D')\r\n-        5.12\u00b10\u03bcs      3.35\u00b10.03\u03bcs     0.65  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 1, 'D')\r\n-     5.17\u00b10.02\u03bcs      3.38\u00b10.04\u03bcs     0.65  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 1, 4, 'F')\r\n-     5.37\u00b10.09\u03bcs      3.35\u00b10.02\u03bcs     0.62  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 2, 'F')\r\n-     5.31\u00b10.01\u03bcs      3.31\u00b10.02\u03bcs     0.62  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 2, 'F')\r\n-     5.37\u00b10.02\u03bcs      3.33\u00b10.05\u03bcs     0.62  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 2, 'F')\r\n-     5.32\u00b10.02\u03bcs      3.26\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 1, 'F')\r\n-     5.33\u00b10.01\u03bcs      3.26\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 1, 'F')\r\n-        5.33\u00b10\u03bcs         3.26\u00b10\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 2, 'F')\r\n-        5.34\u00b10\u03bcs         3.26\u00b10\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 1, 'F')\r\n-     5.39\u00b10.03\u03bcs      3.30\u00b10.03\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 2, 'F')\r\n-     5.35\u00b10.01\u03bcs      3.27\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 2, 'F')\r\n-     5.32\u00b10.01\u03bcs      3.25\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 1, 'F')\r\n-     5.34\u00b10.02\u03bcs      3.25\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 1, 'F')\r\n-     5.38\u00b10.02\u03bcs      3.28\u00b10.02\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 2, 'F')\r\n-     5.35\u00b10.02\u03bcs      3.26\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 2, 'F')\r\n-     5.38\u00b10.02\u03bcs      3.28\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 2, 'F')\r\n-     5.36\u00b10.01\u03bcs      3.27\u00b10.02\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 1, 'F')\r\n-     5.35\u00b10.01\u03bcs      3.26\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 2, 'F')\r\n-        5.35\u00b10\u03bcs      3.25\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 2, 'F')\r\n-     5.38\u00b10.03\u03bcs         3.25\u00b10\u03bcs     0.60  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 2, 'F')\r\n-     6.00\u00b10.01\u03bcs      3.56\u00b10.09\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 1, 'D')\r\n-     6.98\u00b10.01\u03bcs      4.11\u00b10.04\u03bcs     0.59  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 2, 1, 'D')\r\n-     6.00\u00b10.01\u03bcs      3.50\u00b10.07\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 1, 'D')\r\n-     6.00\u00b10.03\u03bcs      3.49\u00b10.08\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 1, 'D')\r\n-     5.98\u00b10.02\u03bcs      3.44\u00b10.04\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 1, 'D')\r\n-     5.99\u00b10.01\u03bcs      3.42\u00b10.03\u03bcs     0.57  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 1, 'D')\r\n-     6.00\u00b10.01\u03bcs       3.33\u00b10.1\u03bcs     0.56  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 1, 'D')\r\n-        5.03\u00b10\u03bcs      2.79\u00b10.02\u03bcs     0.55  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 1, 2, 'F')\r\n-     4.44\u00b10.01\u03bcs       2.36\u00b10.1\u03bcs     0.53  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 1, 'F')\r\n-     4.44\u00b10.01\u03bcs       2.35\u00b10.1\u03bcs     0.53  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 1, 'F')\r\n-     6.73\u00b10.04\u03bcs      3.45\u00b10.05\u03bcs     0.51  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 4, 1, 'F')\r\n-     4.46\u00b10.01\u03bcs      2.19\u00b10.02\u03bcs     0.49  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 1, 'F')\r\n-     4.46\u00b10.03\u03bcs      2.18\u00b10.05\u03bcs     0.49  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 1, 'F')\r\n-     4.48\u00b10.03\u03bcs      2.19\u00b10.01\u03bcs     0.49  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 1, 'F')\r\n-     4.49\u00b10.02\u03bcs      2.18\u00b10.09\u03bcs     0.49  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 1, 'F')\r\n-     4.45\u00b10.01\u03bcs      2.16\u00b10.01\u03bcs     0.48  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 1, 'F')\r\n-     4.47\u00b10.01\u03bcs      2.16\u00b10.01\u03bcs     0.48  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 1, 'F')\r\n-     4.49\u00b10.02\u03bcs      2.17\u00b10.01\u03bcs     0.48  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 1, 'F')\r\n-     4.46\u00b10.01\u03bcs      2.15\u00b10.01\u03bcs     0.48  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 1, 'F')\r\n-     4.47\u00b10.01\u03bcs      2.15\u00b10.01\u03bcs     0.48  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 1, 'F')\r\n-     4.46\u00b10.02\u03bcs      2.15\u00b10.01\u03bcs     0.48  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 1, 'F')\r\n-     9.23\u00b10.06\u03bcs      4.37\u00b10.09\u03bcs     0.47  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 2, 1, 'D')\r\n-     5.31\u00b10.03\u03bcs      2.21\u00b10.04\u03bcs     0.42  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 1, 'F')\r\n-      5.35\u00b10.1\u03bcs       2.22\u00b10.1\u03bcs     0.41  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 1, 'F')\r\n-     5.32\u00b10.01\u03bcs      2.20\u00b10.01\u03bcs     0.41  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 1, 'F')\r\n-     5.30\u00b10.01\u03bcs      2.16\u00b10.02\u03bcs     0.41  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 1, 'F')\r\n-      5.34\u00b10.1\u03bcs      2.17\u00b10.07\u03bcs     0.41  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 1, 'F')\r\n-     5.31\u00b10.01\u03bcs      2.15\u00b10.04\u03bcs     0.41  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 1, 'F')\r\n-     6.52\u00b10.02\u03bcs      2.24\u00b10.01\u03bcs     0.34  bench_ufunc_strides.UnaryComplex.time_ufunc('conjugate', 2, 1, 'F')\r\n-        7.16\u00b10\u03bcs       2.43\u00b10.1\u03bcs     0.34  bench_ufunc_strides.UnaryComplex.time_ufunc('square', 2, 1, 'F')\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n <summary>AVX2</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX512F AVX512_SKX\"\r\npython runtests.py -n --bench-compare parent/main Complex  -- --cpu-affinity=6,7\r\n```\r\n```Bash   before           after         ratio\r\n     [2b9851ba]       [4a787e9e]\r\n     <replace_raw_arithmfp~8>       <replace_raw_arithmfp>\r\n+     4.92\u00b10.03\u03bcs       5.26\u00b10.1\u03bcs     1.07  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 2, 4, 1, 'F')\r\n+      17.3\u00b10.2\u03bcs         18.3\u00b10\u03bcs     1.06  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('divide', 1, 4, 1, 'F')\r\n+     17.3\u00b10.07\u03bcs      18.3\u00b10.01\u03bcs     1.05  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('divide', 1, 1, 1, 'F')\r\n+      17.5\u00b10.2\u03bcs      18.4\u00b10.05\u03bcs     1.05  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('divide', 2, 1, 2, 'F')\r\n-     4.79\u00b10.06\u03bcs       4.55\u00b10.1\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 1, 'D')\r\n-      8.10\u00b10.1\u03bcs      7.68\u00b10.04\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 4, 1, 'D')\r\n-      7.93\u00b10.1\u03bcs       7.52\u00b10.1\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 4, 1, 'D')\r\n-      5.82\u00b10.1\u03bcs       5.51\u00b10.1\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 1, 2, 'D')\r\n-     8.09\u00b10.02\u03bcs       7.65\u00b10.1\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 1, 'D')\r\n-      4.83\u00b10.1\u03bcs      4.56\u00b10.03\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 1, 'D')\r\n-      6.44\u00b10.1\u03bcs       6.06\u00b10.1\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 2, 'D')\r\n-      7.98\u00b10.1\u03bcs       7.50\u00b10.1\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 4, 1, 1, 'D')\r\n-     4.81\u00b10.06\u03bcs      4.51\u00b10.06\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 1, 'D')\r\n-      8.14\u00b10.1\u03bcs      7.63\u00b10.05\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 1, 'D')\r\n-      6.46\u00b10.1\u03bcs       6.00\u00b10.1\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 2, 'D')\r\n-     4.51\u00b10.06\u03bcs      4.18\u00b10.09\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 1, 1, 'D')\r\n-     20.4\u00b10.05\u03bcs      18.9\u00b10.06\u03bcs     0.93  bench_core.VarComplex.time_var(10000)\r\n-      8.18\u00b10.1\u03bcs      7.59\u00b10.03\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 4, 1, 'D')\r\n-      6.53\u00b10.2\u03bcs      6.00\u00b10.05\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 2, 'D')\r\n-     4.98\u00b10.04\u03bcs      4.54\u00b10.08\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 2, 'D')\r\n-     5.06\u00b10.08\u03bcs      4.57\u00b10.07\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 2, 'D')\r\n-     4.32\u00b10.02\u03bcs      3.90\u00b10.06\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 4, 'F')\r\n-     5.06\u00b10.09\u03bcs      4.57\u00b10.08\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 2, 'D')\r\n-     4.36\u00b10.06\u03bcs      3.90\u00b10.06\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 4, 'F')\r\n-     4.42\u00b10.04\u03bcs      3.94\u00b10.06\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 4, 1, 'F')\r\n-        3.99\u00b10\u03bcs         3.55\u00b10\u03bcs     0.89  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 2, 2, 'F')\r\n-     5.06\u00b10.08\u03bcs      4.48\u00b10.02\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 2, 'D')\r\n-     4.33\u00b10.01\u03bcs      3.84\u00b10.02\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 4, 'F')\r\n-     4.31\u00b10.03\u03bcs      3.81\u00b10.03\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 4, 'F')\r\n-     3.68\u00b10.06\u03bcs      3.25\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 2, 'F')\r\n-     5.15\u00b10.07\u03bcs      4.55\u00b10.08\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 2, 'D')\r\n-     3.67\u00b10.02\u03bcs      3.24\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 2, 'F')\r\n-      3.70\u00b10.1\u03bcs      3.26\u00b10.02\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs      3.23\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 2, 'F')\r\n-     3.67\u00b10.02\u03bcs      3.23\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 2, 'F')\r\n-        3.66\u00b10\u03bcs         3.21\u00b10\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 1, 'F')\r\n-     3.70\u00b10.01\u03bcs      3.25\u00b10.04\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 2, 'F')\r\n-     4.34\u00b10.03\u03bcs      3.80\u00b10.05\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 4, 'F')\r\n-     3.67\u00b10.01\u03bcs      3.21\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 1, 'F')\r\n-     3.67\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 1, 'F')\r\n-     4.41\u00b10.01\u03bcs      3.86\u00b10.09\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 1, 'F')\r\n-     3.66\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 2, 'F')\r\n-     3.68\u00b10.01\u03bcs      3.22\u00b10.02\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 2, 'F')\r\n-     3.66\u00b10.01\u03bcs         3.20\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 1, 'F')\r\n-     3.67\u00b10.01\u03bcs      3.20\u00b10.01\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 1, 'F')\r\n-        3.68\u00b10\u03bcs         3.22\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 1, 'F')\r\n-     3.65\u00b10.01\u03bcs         3.19\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 1, 'F')\r\n-     3.67\u00b10.01\u03bcs      3.21\u00b10.01\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 2, 'F')\r\n-     4.43\u00b10.07\u03bcs      3.87\u00b10.04\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 1, 'F')\r\n-     3.69\u00b10.06\u03bcs      3.22\u00b10.01\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 2, 'F')\r\n-     3.66\u00b10.01\u03bcs         3.20\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 1, 'F')\r\n-      3.72\u00b10.1\u03bcs      3.25\u00b10.02\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs      3.21\u00b10.01\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 2, 'F')\r\n-     3.69\u00b10.01\u03bcs      3.22\u00b10.01\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 2, 'F')\r\n-     3.66\u00b10.01\u03bcs         3.19\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 2, 'F')\r\n-     3.71\u00b10.02\u03bcs      3.24\u00b10.01\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs         3.20\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 2, 'F')\r\n-     3.67\u00b10.01\u03bcs         3.20\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 2, 'F')\r\n-     3.69\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 2, 'F')\r\n-     3.66\u00b10.01\u03bcs         3.19\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 1, 'F')\r\n-     4.46\u00b10.03\u03bcs      3.87\u00b10.03\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 4, 1, 'F')\r\n-     3.69\u00b10.02\u03bcs         3.21\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 1, 'F')\r\n-     4.44\u00b10.07\u03bcs      3.85\u00b10.06\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 4, 1, 'F')\r\n-     3.71\u00b10.03\u03bcs         3.21\u00b10\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 1, 'F')\r\n-     4.43\u00b10.07\u03bcs      3.84\u00b10.09\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 1, 'F')\r\n-     4.39\u00b10.04\u03bcs      3.80\u00b10.01\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 4, 'F')\r\n-     3.70\u00b10.02\u03bcs      3.19\u00b10.01\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 1, 'F')\r\n-     5.23\u00b10.01\u03bcs      4.51\u00b10.07\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 2, 'D')\r\n-     3.78\u00b10.09\u03bcs      3.25\u00b10.01\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 2, 'F')\r\n-     3.80\u00b10.06\u03bcs      3.26\u00b10.01\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 2, 'F')\r\n-     4.02\u00b10.03\u03bcs      3.45\u00b10.07\u03bcs     0.86  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 4, 'F')\r\n-     3.79\u00b10.05\u03bcs      3.23\u00b10.01\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 2, 'F')\r\n-     4.00\u00b10.02\u03bcs      3.40\u00b10.05\u03bcs     0.85  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 4, 1, 'F')\r\n-     5.36\u00b10.05\u03bcs       4.55\u00b10.1\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 2, 'D')\r\n-     5.39\u00b10.09\u03bcs      4.57\u00b10.06\u03bcs     0.85  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 1, 'D')\r\n-     5.12\u00b10.03\u03bcs       4.32\u00b10.1\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 1, 'D')\r\n-      5.41\u00b10.1\u03bcs       4.56\u00b10.2\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 1, 'D')\r\n-      5.35\u00b10.1\u03bcs      4.51\u00b10.07\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 1, 'D')\r\n-     3.98\u00b10.04\u03bcs      3.36\u00b10.09\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 1, 'D')\r\n-      5.49\u00b10.1\u03bcs       4.61\u00b10.1\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 2, 'D')\r\n-     5.45\u00b10.03\u03bcs      4.56\u00b10.06\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 2, 'D')\r\n-     5.09\u00b10.06\u03bcs      4.25\u00b10.08\u03bcs     0.84  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 2, 1, 'D')\r\n-     4.00\u00b10.04\u03bcs      3.34\u00b10.06\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 1, 'D')\r\n-     5.35\u00b10.09\u03bcs      4.44\u00b10.06\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 1, 'D')\r\n-     3.98\u00b10.03\u03bcs      3.30\u00b10.04\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 1, 'D')\r\n-     4.02\u00b10.02\u03bcs      3.33\u00b10.07\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 1, 'D')\r\n-        4.00\u00b10\u03bcs      3.31\u00b10.03\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 1, 'D')\r\n-     4.07\u00b10.04\u03bcs      3.35\u00b10.03\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 1, 'D')\r\n-     5.46\u00b10.03\u03bcs       4.50\u00b10.1\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 2, 'D')\r\n-     5.45\u00b10.07\u03bcs      4.48\u00b10.05\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 1, 'D')\r\n-     5.46\u00b10.04\u03bcs       4.49\u00b10.1\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 2, 'D')\r\n-     5.50\u00b10.05\u03bcs       4.50\u00b10.1\u03bcs     0.82  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 1, 'D')\r\n-      5.48\u00b10.1\u03bcs      4.44\u00b10.04\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 2, 'D')\r\n-     4.61\u00b10.03\u03bcs       3.73\u00b10.2\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 1, 'D')\r\n-     5.24\u00b10.02\u03bcs       4.14\u00b10.1\u03bcs     0.79  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 2, 'D')\r\n-     4.29\u00b10.02\u03bcs      3.31\u00b10.03\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 2, 'F')\r\n-     4.28\u00b10.02\u03bcs      3.27\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 2, 'F')\r\n-     4.24\u00b10.01\u03bcs      3.22\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 1, 'F')\r\n-     4.30\u00b10.02\u03bcs      3.27\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 2, 'F')\r\n-        4.25\u00b10\u03bcs         3.22\u00b10\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 1, 'F')\r\n-     4.28\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 2, 'F')\r\n-     4.34\u00b10.03\u03bcs      3.29\u00b10.04\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 2, 'F')\r\n-     4.29\u00b10.05\u03bcs      3.25\u00b10.02\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 2, 'F')\r\n-     4.26\u00b10.01\u03bcs      3.22\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 2, 'F')\r\n-        4.24\u00b10\u03bcs      3.20\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 1, 'F')\r\n-     4.25\u00b10.01\u03bcs      3.21\u00b10.01\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 1, 'F')\r\n-     4.25\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 1, 'F')\r\n-     4.25\u00b10.01\u03bcs         3.21\u00b10\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 2, 'F')\r\n-     4.27\u00b10.01\u03bcs      3.22\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 2, 'F')\r\n-     4.26\u00b10.02\u03bcs         3.21\u00b10\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 1, 'F')\r\n-     4.27\u00b10.01\u03bcs      3.22\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 2, 'F')\r\n-     4.27\u00b10.01\u03bcs      3.21\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 2, 'F')\r\n-     4.28\u00b10.02\u03bcs      3.21\u00b10.02\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 2, 'F')\r\n-        3.91\u00b10\u03bcs      2.89\u00b10.03\u03bcs     0.74  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 1, 'D')\r\n-     4.58\u00b10.04\u03bcs       3.38\u00b10.2\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 1, 'D')\r\n-     4.63\u00b10.04\u03bcs       3.39\u00b10.2\u03bcs     0.73  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 1, 'D')\r\n-     4.60\u00b10.03\u03bcs      3.32\u00b10.03\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 1, 'D')\r\n-     4.61\u00b10.03\u03bcs      3.29\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 1, 'D')\r\n-     4.72\u00b10.03\u03bcs      3.36\u00b10.03\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 1, 'D')\r\n-     3.38\u00b10.03\u03bcs      2.39\u00b10.02\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 1, 1, 'F')\r\n-        3.37\u00b10\u03bcs      2.38\u00b10.01\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 1, 1, 'F')\r\n-        3.99\u00b10\u03bcs         2.80\u00b10\u03bcs     0.70  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 2, 'F')\r\n-     4.99\u00b10.03\u03bcs       3.50\u00b10.1\u03bcs     0.70  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 1, 'D')\r\n-     5.09\u00b10.04\u03bcs      3.45\u00b10.08\u03bcs     0.68  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 1, 'D')\r\n-     5.07\u00b10.08\u03bcs      3.39\u00b10.01\u03bcs     0.67  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 4, 1, 'F')\r\n-     5.12\u00b10.02\u03bcs      3.39\u00b10.02\u03bcs     0.66  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 4, 'F')\r\n-     5.00\u00b10.07\u03bcs       3.30\u00b10.2\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 1, 'D')\r\n-     5.00\u00b10.05\u03bcs       3.28\u00b10.1\u03bcs     0.66  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 1, 'D')\r\n-     5.02\u00b10.03\u03bcs      3.28\u00b10.03\u03bcs     0.65  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 1, 'D')\r\n-     5.02\u00b10.01\u03bcs      3.27\u00b10.04\u03bcs     0.65  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 1, 'D')\r\n-     3.67\u00b10.01\u03bcs       2.32\u00b10.1\u03bcs     0.63  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 1, 'F')\r\n-     5.04\u00b10.01\u03bcs       3.07\u00b10.1\u03bcs     0.61  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 1, 'D')\r\n-        3.95\u00b10\u03bcs      2.38\u00b10.02\u03bcs     0.60  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 1, 'F')\r\n-     3.67\u00b10.02\u03bcs      2.15\u00b10.09\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 1, 'F')\r\n-        3.66\u00b10\u03bcs      2.14\u00b10.02\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 1, 'F')\r\n-     3.66\u00b10.01\u03bcs         2.14\u00b10\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 1, 'F')\r\n-        3.65\u00b10\u03bcs      2.13\u00b10.02\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 1, 'F')\r\n-     3.66\u00b10.01\u03bcs         2.13\u00b10\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 1, 'F')\r\n-        3.64\u00b10\u03bcs      2.12\u00b10.09\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 1, 'F')\r\n-     3.65\u00b10.01\u03bcs      2.12\u00b10.01\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 1, 'F')\r\n-     3.68\u00b10.03\u03bcs         2.13\u00b10\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 1, 'F')\r\n-     3.69\u00b10.03\u03bcs      2.14\u00b10.04\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 1, 'F')\r\n-     3.66\u00b10.01\u03bcs      2.11\u00b10.01\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 1, 'F')\r\n-      32.9\u00b10.1\u03bcs      18.9\u00b10.09\u03bcs     0.57  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 4, 'D')\r\n-     3.68\u00b10.02\u03bcs      2.10\u00b10.01\u03bcs     0.57  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 1, 'F')\r\n-     32.8\u00b10.09\u03bcs      18.6\u00b10.08\u03bcs     0.57  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 4, 'D')\r\n-     3.99\u00b10.01\u03bcs      2.25\u00b10.03\u03bcs     0.57  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 2, 1, 'F')\r\n-     5.03\u00b10.01\u03bcs         2.80\u00b10\u03bcs     0.56  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 2, 'F')\r\n-     30.5\u00b10.06\u03bcs      16.7\u00b10.02\u03bcs     0.55  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 2, 'D')\r\n-     29.8\u00b10.05\u03bcs      15.8\u00b10.07\u03bcs     0.53  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 2, 'D')\r\n-      32.2\u00b10.1\u03bcs       16.9\u00b10.1\u03bcs     0.52  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 4, 'D')\r\n-     4.26\u00b10.02\u03bcs       2.23\u00b10.1\u03bcs     0.52  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 1, 'F')\r\n-      28.6\u00b10.1\u03bcs      14.7\u00b10.02\u03bcs     0.52  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 1, 'D')\r\n-     4.23\u00b10.02\u03bcs      2.13\u00b10.02\u03bcs     0.50  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 1, 'F')\r\n-     4.23\u00b10.01\u03bcs      2.13\u00b10.02\u03bcs     0.50  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 1, 'F')\r\n-     4.23\u00b10.03\u03bcs      2.13\u00b10.03\u03bcs     0.50  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 1, 'F')\r\n-     4.22\u00b10.03\u03bcs      2.13\u00b10.03\u03bcs     0.50  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 1, 'F')\r\n-     4.27\u00b10.02\u03bcs      2.13\u00b10.01\u03bcs     0.50  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 1, 'F')\r\n-     28.1\u00b10.05\u03bcs      14.0\u00b10.01\u03bcs     0.50  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 1, 'D')\r\n-     24.9\u00b10.03\u03bcs      12.0\u00b10.04\u03bcs     0.48  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 2, 'F')\r\n-     29.5\u00b10.09\u03bcs       14.2\u00b10.1\u03bcs     0.48  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 2, 'D')\r\n-     25.3\u00b10.09\u03bcs      12.2\u00b10.03\u03bcs     0.48  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 4, 'F')\r\n-     24.8\u00b10.01\u03bcs      11.9\u00b10.01\u03bcs     0.48  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 2, 'F')\r\n-     25.0\u00b10.05\u03bcs      12.0\u00b10.05\u03bcs     0.48  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 4, 'F')\r\n-     28.0\u00b10.05\u03bcs      12.6\u00b10.02\u03bcs     0.45  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 1, 'D')\r\n-     5.04\u00b10.03\u03bcs      2.26\u00b10.09\u03bcs     0.45  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 2, 1, 'F')\r\n-     3.98\u00b10.01\u03bcs      1.72\u00b10.01\u03bcs     0.43  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 1, 'F')\r\n-     22.4\u00b10.03\u03bcs      9.39\u00b10.02\u03bcs     0.42  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 1, 'F')\r\n-     22.3\u00b10.03\u03bcs      9.35\u00b10.03\u03bcs     0.42  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 1, 'F')\r\n-        5.02\u00b10\u03bcs      1.80\u00b10.06\u03bcs     0.36  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 1, 'F')\r\n-     24.9\u00b10.06\u03bcs      8.60\u00b10.08\u03bcs     0.35  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 4, 'F')\r\n-     24.8\u00b10.01\u03bcs      8.57\u00b10.02\u03bcs     0.35  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 2, 'F')\r\n-     22.3\u00b10.03\u03bcs      6.04\u00b10.01\u03bcs     0.27  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 1, 'F')\r\n```\r\n</details>\r\n\r\n<details>\r\n <summary>SSE3</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX2 AVX512F AVX512_SKX\"\r\npython runtests.py -n --bench-compare parent/main Complex  -- --cpu-affinity=6,7\r\n```\r\n```Bash\r\n        before           after         ratio\r\n     [2b9851ba]       [4a787e9e]\r\n     <replace_raw_arithmfp~8>       <replace_raw_arithmfp>\r\n+     17.5\u00b10.02\u03bcs      18.4\u00b10.01\u03bcs     1.05  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('divide', 2, 4, 2, 'F')\r\n+     17.4\u00b10.02\u03bcs      18.2\u00b10.01\u03bcs     1.05  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('divide', 2, 4, 1, 'F')\r\n-     21.3\u00b10.09\u03bcs       20.3\u00b10.1\u03bcs     0.95  bench_core.VarComplex.time_var(10000)\r\n-     4.86\u00b10.08\u03bcs      4.62\u00b10.09\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 4, 2, 'F')\r\n-      4.38\u00b10.1\u03bcs      4.16\u00b10.05\u03bcs     0.95  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 1, 1, 'D')\r\n-     6.10\u00b10.08\u03bcs       5.75\u00b10.1\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 2, 1, 1, 'D')\r\n-      8.21\u00b10.1\u03bcs      7.73\u00b10.07\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 1, 'D')\r\n-     4.80\u00b10.03\u03bcs      4.51\u00b10.08\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 4, 'F')\r\n-     4.84\u00b10.08\u03bcs      4.53\u00b10.05\u03bcs     0.94  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 4, 'F')\r\n-     4.86\u00b10.05\u03bcs      4.51\u00b10.09\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 4, 2, 'F')\r\n-     4.91\u00b10.05\u03bcs      4.54\u00b10.02\u03bcs     0.93  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 4, 'F')\r\n-      4.95\u00b10.1\u03bcs       4.55\u00b10.1\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 4, 2, 'F')\r\n-     4.90\u00b10.05\u03bcs      4.50\u00b10.08\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 2, 'F')\r\n-     4.87\u00b10.07\u03bcs       4.46\u00b10.1\u03bcs     0.92  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 2, 'F')\r\n-      32.7\u00b10.1\u03bcs      29.9\u00b10.09\u03bcs     0.91  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 4, 'D')\r\n-     5.02\u00b10.08\u03bcs      4.58\u00b10.09\u03bcs     0.91  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 2, 'F')\r\n-      33.1\u00b10.1\u03bcs      30.1\u00b10.09\u03bcs     0.91  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 4, 'D')\r\n-     3.66\u00b10.02\u03bcs      3.30\u00b10.04\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 2, 'F')\r\n-     3.66\u00b10.03\u03bcs      3.30\u00b10.02\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 2, 'F')\r\n-     30.0\u00b10.05\u03bcs      27.0\u00b10.08\u03bcs     0.90  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 2, 'D')\r\n-      32.3\u00b10.2\u03bcs       29.0\u00b10.1\u03bcs     0.90  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 4, 'D')\r\n-     3.65\u00b10.03\u03bcs      3.28\u00b10.03\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 2, 'F')\r\n-     3.64\u00b10.04\u03bcs      3.27\u00b10.05\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 2, 'F')\r\n-     3.64\u00b10.01\u03bcs      3.27\u00b10.01\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 2, 'F')\r\n-     3.65\u00b10.02\u03bcs      3.27\u00b10.02\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 2, 'F')\r\n-     3.65\u00b10.02\u03bcs      3.27\u00b10.06\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 2, 'F')\r\n-     28.7\u00b10.08\u03bcs      25.7\u00b10.06\u03bcs     0.90  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 1, 'D')\r\n-     3.62\u00b10.01\u03bcs      3.24\u00b10.02\u03bcs     0.90  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 2, 'F')\r\n-     3.99\u00b10.01\u03bcs      3.57\u00b10.01\u03bcs     0.89  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 2, 2, 'F')\r\n-      30.8\u00b10.2\u03bcs      27.6\u00b10.08\u03bcs     0.89  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 2, 'D')\r\n-     28.1\u00b10.08\u03bcs      25.2\u00b10.02\u03bcs     0.89  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 1, 'D')\r\n-        3.62\u00b10\u03bcs         3.23\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 2, 'F')\r\n-     3.66\u00b10.03\u03bcs      3.27\u00b10.03\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 2, 'F')\r\n-        3.63\u00b10\u03bcs      3.24\u00b10.01\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 1, 'F')\r\n-     3.62\u00b10.01\u03bcs         3.23\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 1, 'F')\r\n-      29.6\u00b10.1\u03bcs      26.3\u00b10.05\u03bcs     0.89  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 2, 'D')\r\n-        3.63\u00b10\u03bcs      3.23\u00b10.01\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 2, 1, 'F')\r\n-     3.64\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 2, 'F')\r\n-        3.64\u00b10\u03bcs         3.24\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 2, 'F')\r\n-     3.63\u00b10.01\u03bcs         3.24\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 1, 'F')\r\n-     3.61\u00b10.02\u03bcs         3.22\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 1, 1, 'F')\r\n-     3.63\u00b10.01\u03bcs         3.23\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 2, 'F')\r\n-     3.65\u00b10.01\u03bcs      3.24\u00b10.01\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 2, 'F')\r\n-     3.67\u00b10.04\u03bcs      3.26\u00b10.01\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 2, 2, 'F')\r\n-     3.64\u00b10.01\u03bcs      3.23\u00b10.01\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 4, 1, 'F')\r\n-     3.63\u00b10.01\u03bcs         3.22\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 1, 'F')\r\n-        3.63\u00b10\u03bcs         3.22\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 2, 1, 'F')\r\n-        3.62\u00b10\u03bcs         3.21\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 2, 'F')\r\n-        3.63\u00b10\u03bcs         3.21\u00b10\u03bcs     0.89  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 2, 'F')\r\n-     3.65\u00b10.02\u03bcs         3.23\u00b10\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 2, 1, 'F')\r\n-     3.63\u00b10.01\u03bcs      3.21\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 2, 1, 1, 'F')\r\n-        3.63\u00b10\u03bcs      3.21\u00b10.02\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 2, 'F')\r\n-     3.65\u00b10.02\u03bcs         3.22\u00b10\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 2, 1, 'F')\r\n-     3.66\u00b10.01\u03bcs      3.23\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 2, 1, 'F')\r\n-     3.64\u00b10.01\u03bcs      3.22\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 2, 'F')\r\n-     3.64\u00b10.02\u03bcs      3.21\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 2, 'F')\r\n-     27.9\u00b10.02\u03bcs      24.6\u00b10.02\u03bcs     0.88  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 1, 'D')\r\n-     3.72\u00b10.05\u03bcs      3.27\u00b10.05\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 2, 4, 2, 'F')\r\n-     3.65\u00b10.02\u03bcs      3.22\u00b10.01\u03bcs     0.88  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 2, 'F')\r\n-     4.82\u00b10.07\u03bcs      4.21\u00b10.08\u03bcs     0.87  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 1, 1, 'F')\r\n-     5.68\u00b10.02\u03bcs      4.90\u00b10.08\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 1, 'D')\r\n-     3.78\u00b10.06\u03bcs      3.26\u00b10.01\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 2, 2, 'F')\r\n-     3.78\u00b10.07\u03bcs      3.26\u00b10.01\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 2, 2, 'F')\r\n-     4.01\u00b10.04\u03bcs      3.45\u00b10.04\u03bcs     0.86  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 4, 'F')\r\n-     4.75\u00b10.03\u03bcs      4.08\u00b10.09\u03bcs     0.86  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 2, 1, 'F')\r\n-     4.71\u00b10.06\u03bcs      3.98\u00b10.07\u03bcs     0.84  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 4, 'F')\r\n-     4.06\u00b10.02\u03bcs      3.40\u00b10.01\u03bcs     0.84  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 4, 1, 'F')\r\n-     4.69\u00b10.05\u03bcs      3.91\u00b10.06\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 4, 1, 'F')\r\n-     4.69\u00b10.02\u03bcs      3.90\u00b10.05\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 4, 'F')\r\n-     4.72\u00b10.04\u03bcs       3.93\u00b10.1\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 4, 1, 'F')\r\n-     4.66\u00b10.03\u03bcs      3.86\u00b10.02\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 4, 'F')\r\n-     4.69\u00b10.02\u03bcs      3.89\u00b10.03\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 4, 'F')\r\n-     4.78\u00b10.04\u03bcs      3.95\u00b10.02\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 4, 4, 1, 'F')\r\n-     4.68\u00b10.01\u03bcs      3.86\u00b10.05\u03bcs     0.83  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 4, 'F')\r\n-     4.77\u00b10.04\u03bcs      3.86\u00b10.07\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 4, 'F')\r\n-     4.82\u00b10.06\u03bcs      3.89\u00b10.04\u03bcs     0.81  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 4, 1, 'F')\r\n-     4.61\u00b10.02\u03bcs      3.68\u00b10.03\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 1, 'F')\r\n-     4.61\u00b10.01\u03bcs         3.68\u00b10\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 1, 'F')\r\n-     4.61\u00b10.01\u03bcs      3.67\u00b10.01\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 1, 'F')\r\n-     4.61\u00b10.01\u03bcs         3.68\u00b10\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 2, 'F')\r\n-     4.63\u00b10.01\u03bcs         3.68\u00b10\u03bcs     0.80  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 2, 'F')\r\n-     4.64\u00b10.02\u03bcs      3.69\u00b10.01\u03bcs     0.79  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 2, 'F')\r\n-     4.64\u00b10.02\u03bcs      3.69\u00b10.02\u03bcs     0.79  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 2, 'F')\r\n-     4.66\u00b10.01\u03bcs      3.70\u00b10.05\u03bcs     0.79  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 2, 'F')\r\n-     4.69\u00b10.06\u03bcs      3.71\u00b10.03\u03bcs     0.79  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 2, 'F')\r\n-     6.03\u00b10.05\u03bcs      4.71\u00b10.06\u03bcs     0.78  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 1, 'D')\r\n-     6.07\u00b10.09\u03bcs      4.68\u00b10.07\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 2, 'D')\r\n-     5.96\u00b10.02\u03bcs      4.59\u00b10.06\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 2, 'D')\r\n-     6.03\u00b10.06\u03bcs       4.65\u00b10.1\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 1, 'D')\r\n-     6.04\u00b10.05\u03bcs      4.64\u00b10.09\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 2, 1, 'D')\r\n-     6.01\u00b10.01\u03bcs      4.61\u00b10.03\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 2, 'D')\r\n-     6.03\u00b10.05\u03bcs       4.63\u00b10.1\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 4, 1, 'D')\r\n-     4.37\u00b10.01\u03bcs      3.35\u00b10.04\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 1, 'D')\r\n-     6.03\u00b10.03\u03bcs       4.62\u00b10.1\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 2, 1, 1, 'D')\r\n-     6.08\u00b10.05\u03bcs      4.65\u00b10.07\u03bcs     0.77  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 1, 'D')\r\n-     6.02\u00b10.02\u03bcs      4.60\u00b10.06\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 2, 'D')\r\n-     4.38\u00b10.02\u03bcs      3.34\u00b10.02\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 1, 'D')\r\n-     6.00\u00b10.04\u03bcs      4.57\u00b10.06\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 2, 'D')\r\n-     4.41\u00b10.04\u03bcs      3.34\u00b10.05\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 1, 'D')\r\n-     6.01\u00b10.03\u03bcs       4.55\u00b10.1\u03bcs     0.76  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 2, 'D')\r\n-     4.43\u00b10.04\u03bcs      3.34\u00b10.04\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 1, 'D')\r\n-     4.66\u00b10.03\u03bcs      3.51\u00b10.04\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 2, 'F')\r\n-     4.36\u00b10.01\u03bcs      3.28\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 1, 'D')\r\n-     4.37\u00b10.02\u03bcs      3.29\u00b10.03\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 1, 'D')\r\n-     4.38\u00b10.01\u03bcs      3.29\u00b10.02\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 1, 'D')\r\n-     4.62\u00b10.01\u03bcs      3.46\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 2, 'F')\r\n-     4.44\u00b10.01\u03bcs      3.32\u00b10.05\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 1, 'D')\r\n-     4.62\u00b10.01\u03bcs         3.45\u00b10\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 2, 'F')\r\n-     4.38\u00b10.03\u03bcs      3.28\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 1, 'D')\r\n-     4.62\u00b10.01\u03bcs         3.45\u00b10\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 1, 'F')\r\n-     4.66\u00b10.01\u03bcs      3.48\u00b10.02\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 2, 'F')\r\n-     4.63\u00b10.04\u03bcs         3.46\u00b10\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 2, 1, 'F')\r\n-        4.63\u00b10\u03bcs      3.46\u00b10.01\u03bcs     0.75  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 2, 'F')\r\n-     4.43\u00b10.01\u03bcs      3.29\u00b10.02\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 1, 'D')\r\n-     4.64\u00b10.02\u03bcs         3.45\u00b10\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 2, 1, 'F')\r\n-     5.94\u00b10.03\u03bcs      4.38\u00b10.02\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 1, 'D')\r\n-     5.94\u00b10.03\u03bcs      4.38\u00b10.01\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 1, 'D')\r\n-     4.71\u00b10.02\u03bcs      3.46\u00b10.01\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 2, 2, 'F')\r\n-     4.46\u00b10.06\u03bcs      3.28\u00b10.04\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 1, 'D')\r\n-     5.94\u00b10.01\u03bcs      4.37\u00b10.01\u03bcs     0.74  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 1, 'D')\r\n-     4.47\u00b10.04\u03bcs      3.28\u00b10.03\u03bcs     0.73  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 1, 'D')\r\n-     3.91\u00b10.01\u03bcs      2.84\u00b10.01\u03bcs     0.73  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 1, 'D')\r\n-     5.90\u00b10.02\u03bcs      4.23\u00b10.02\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 1, 'D')\r\n-     3.33\u00b10.01\u03bcs         2.39\u00b10\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc('add', 1, 1, 1, 'F')\r\n-     3.34\u00b10.03\u03bcs         2.39\u00b10\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc('subtract', 1, 1, 1, 'F')\r\n-     5.90\u00b10.04\u03bcs      4.22\u00b10.01\u03bcs     0.72  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 1, 'D')\r\n-     5.97\u00b10.03\u03bcs      4.26\u00b10.03\u03bcs     0.71  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 1, 'D')\r\n-        3.99\u00b10\u03bcs      2.80\u00b10.01\u03bcs     0.70  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 2, 'F')\r\n-     5.12\u00b10.04\u03bcs      3.45\u00b10.01\u03bcs     0.68  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 4, 1, 'F')\r\n-     5.16\u00b10.03\u03bcs      3.44\u00b10.04\u03bcs     0.67  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 4, 'F')\r\n-        5.03\u00b10\u03bcs         3.27\u00b10\u03bcs     0.65  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 2, 'F')\r\n-     4.33\u00b10.01\u03bcs      2.78\u00b10.01\u03bcs     0.64  bench_ufunc_strides.BinaryComplex.time_ufunc('multiply', 1, 1, 1, 'F')\r\n-     25.0\u00b10.07\u03bcs      16.0\u00b10.09\u03bcs     0.64  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 2, 'F')\r\n-     25.4\u00b10.09\u03bcs      16.3\u00b10.07\u03bcs     0.64  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 4, 'F')\r\n-     24.8\u00b10.03\u03bcs      15.9\u00b10.07\u03bcs     0.64  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 2, 'F')\r\n-     25.0\u00b10.05\u03bcs      15.9\u00b10.05\u03bcs     0.64  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 4, 'F')\r\n-        5.02\u00b10\u03bcs         3.19\u00b10\u03bcs     0.64  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 2, 1, 'F')\r\n-     4.61\u00b10.03\u03bcs      2.82\u00b10.01\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 4, 1, 'F')\r\n-        4.61\u00b10\u03bcs         2.82\u00b10\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 2, 1, 'F')\r\n-     4.64\u00b10.02\u03bcs         2.81\u00b10\u03bcs     0.61  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('multiply', 1, 1, 1, 'F')\r\n-     22.3\u00b10.02\u03bcs      13.4\u00b10.08\u03bcs     0.60  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 4, 1, 'F')\r\n-     3.62\u00b10.01\u03bcs      2.15\u00b10.02\u03bcs     0.60  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 4, 1, 1, 'F')\r\n-     3.62\u00b10.01\u03bcs      2.15\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 1, 1, 1, 'F')\r\n-     3.62\u00b10.01\u03bcs      2.15\u00b10.02\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('add', 2, 1, 1, 'F')\r\n-     3.63\u00b10.03\u03bcs      2.16\u00b10.02\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 4, 1, 'F')\r\n-     22.3\u00b10.04\u03bcs      13.3\u00b10.04\u03bcs     0.59  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 2, 1, 'F')\r\n-     3.62\u00b10.01\u03bcs      2.15\u00b10.02\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 2, 1, 'F')\r\n-     3.62\u00b10.02\u03bcs      2.15\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 4, 1, 1, 'F')\r\n-     3.66\u00b10.02\u03bcs      2.16\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 4, 1, 'F')\r\n-     3.63\u00b10.01\u03bcs      2.15\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 1, 1, 1, 'F')\r\n-        3.63\u00b10\u03bcs      2.14\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('subtract', 2, 1, 1, 'F')\r\n-     3.65\u00b10.02\u03bcs      2.15\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 2, 1, 'F')\r\n-     3.64\u00b10.04\u03bcs      2.14\u00b10.03\u03bcs     0.59  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('subtract', 1, 1, 1, 'F')\r\n-     3.65\u00b10.03\u03bcs      2.13\u00b10.03\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in1('add', 1, 1, 1, 'F')\r\n-     4.60\u00b10.01\u03bcs         2.67\u00b10\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 1, 1, 1, 'F')\r\n-     24.8\u00b10.08\u03bcs      14.4\u00b10.05\u03bcs     0.58  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 4, 'F')\r\n-     4.61\u00b10.01\u03bcs      2.67\u00b10.01\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 2, 1, 1, 'F')\r\n-     24.8\u00b10.01\u03bcs      14.3\u00b10.01\u03bcs     0.58  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 2, 'F')\r\n-     4.62\u00b10.03\u03bcs      2.67\u00b10.01\u03bcs     0.58  bench_ufunc_strides.BinaryComplex.time_ufunc_scalar_in0('multiply', 4, 1, 1, 'F')\r\n-        3.99\u00b10\u03bcs         2.26\u00b10\u03bcs     0.57  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 2, 1, 'F')\r\n-        5.02\u00b10\u03bcs         2.67\u00b10\u03bcs     0.53  bench_ufunc_strides.UnrayComplex.time_ufunc('square', 1, 1, 'F')\r\n-     22.3\u00b10.02\u03bcs      11.8\u00b10.03\u03bcs     0.53  bench_ufunc_strides.UnrayComplex.time_ufunc('absolute', 1, 1, 'F')\r\n-        3.99\u00b10\u03bcs         1.70\u00b10\u03bcs     0.43  bench_ufunc_strides.UnrayComplex.time_ufunc('conjugate', 1, 1, 'F')\r\n```\r\n</details>\r\n----\r\n\r\n\r\nTODO:\r\n- [x] benchmark\r\n",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -227,6 +227,7 @@ numpy/core/src/umath/loops_umath_fp.dispatch.c\n numpy/core/src/umath/loops_hyperbolic.dispatch.c\n numpy/core/src/umath/loops_modulo.dispatch.c\n numpy/core/src/umath/loops_comparison.dispatch.c\n+numpy/core/src/umath/loops_unary_complex.dispatch.c\n # npysort module\n numpy/core/src/npysort/x86-qsort.dispatch.c\n numpy/core/src/npysort/x86-qsort.dispatch.*.cpp"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc_strides.py",
                "patch": "@@ -102,45 +102,59 @@ def time_ufunc(self, dtype, stride):\n cmplxstride = [1, 2, 4]\n cmplxdtype  = ['F', 'D']\n \n-class AVX_cmplx_arithmetic(Benchmark):\n-    params = [cmplx_bfuncs, cmplxstride, cmplxdtype]\n-    param_names = ['bfunc', 'stride', 'dtype']\n+class BinaryComplex(Benchmark):\n+    params = [cmplx_bfuncs, cmplxstride, cmplxstride, cmplxstride, cmplxdtype]\n+    param_names = ['bfunc', 'stride_in0', 'stride_in1' 'stride_out', 'dtype']\n     timeout = 10\n \n-    def setup(self, bfuncname, stride, dtype):\n+    def setup(self, bfuncname, stride_in0, stride_in1, stride_out, dtype):\n         np.seterr(all='ignore')\n         try:\n             self.f = getattr(np, bfuncname)\n         except AttributeError:\n             raise NotImplementedError(f\"No bfunc {bfuncname} found\") from None\n         N = 10000\n-        self.arr1 = np.ones(stride*N, dtype)\n-        self.arr2 = np.ones(stride*N, dtype)\n+        self.arr1 = np.ones(stride_in0*N, dtype)\n+        self.arr2 = np.ones(stride_in1*N, dtype)\n+        self.arr_out = np.empty(stride_out*N, dtype)\n \n-    def time_ufunc(self, bfuncname, stride, dtype):\n-        self.f(self.arr1[::stride], self.arr2[::stride])\n+    def time_ufunc(self, bfuncname, stride_in0, stride_in1, stride_out,\n+                   dtype):\n+        self.f(self.arr1[::stride_in0], self.arr2[::stride_in1],\n+               self.arr_out[::stride_out])\n+\n+    def time_ufunc_scalar_in0(self, bfuncname, stride_in0, stride_in1,\n+                              stride_out, dtype):\n+        self.f(self.arr1[0], self.arr2[::stride_in1],\n+               self.arr_out[::stride_out])\n+\n+    def time_ufunc_scalar_in1(self, bfuncname, stride_in0, stride_in1,\n+                              stride_out, dtype):\n+        self.f(self.arr1[::stride_in0], self.arr2[0],\n+               self.arr_out[::stride_out])\n \n cmplx_ufuncs = ['reciprocal',\n                 'absolute',\n                 'square',\n                 'conjugate']\n \n-class AVX_cmplx_funcs(Benchmark):\n-    params = [cmplx_ufuncs, cmplxstride, cmplxdtype]\n-    param_names = ['bfunc', 'stride', 'dtype']\n+class UnaryComplex(Benchmark):\n+    params = [cmplx_ufuncs, cmplxstride, cmplxstride, cmplxdtype]\n+    param_names = ['bfunc', 'stride_in', 'stride_out', 'dtype']\n     timeout = 10\n \n-    def setup(self, bfuncname, stride, dtype):\n+    def setup(self, bfuncname, stride_in, stride_out, dtype):\n         np.seterr(all='ignore')\n         try:\n             self.f = getattr(np, bfuncname)\n         except AttributeError:\n             raise NotImplementedError(f\"No bfunc {bfuncname} found\") from None\n         N = 10000\n-        self.arr1 = np.ones(stride*N, dtype)\n+        self.arr1 = np.ones(stride_in*N, dtype)\n+        self.arr_out = np.empty(stride_out*N, dtype)\n \n-    def time_ufunc(self, bfuncname, stride, dtype):\n-        self.f(self.arr1[::stride])\n+    def time_ufunc(self, bfuncname, stride_in, stride_out, dtype):\n+        self.f(self.arr1[::stride_in], self.arr_out[::stride_out])\n \n class Mandelbrot(Benchmark):\n     def f(self,z):"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -404,7 +404,8 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.conjugate'),\n           None,\n-          TD(ints+flts+cmplx, simd=[('avx2', ints), ('avx512f', cmplxvec)]),\n+          TD(ints+flts+cmplx, simd=[('avx2', ints)],\n+            dispatch=[('loops_arithm_fp', 'FD')]),\n           TD(P, f='conjugate'),\n           ),\n 'fmod':\n@@ -419,7 +420,8 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.square'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints), ('avx512f', 'FD')], dispatch=[('loops_unary_fp', 'fd')]),\n+          TD(ints+inexact, simd=[('avx2', ints)],\n+             dispatch=[('loops_unary_fp', 'fd'), ('loops_arithm_fp', 'FD')]),\n           TD(O, f='Py_square'),\n           ),\n 'reciprocal':\n@@ -460,7 +462,8 @@ def english_upper(s):\n           'PyUFunc_AbsoluteTypeResolver',\n           TD(bints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd'),\n                                                  ('loops_logical', '?')]),\n-          TD(cmplx, simd=[('avx512f', cmplxvec)], out=('f', 'd', 'g')),\n+          TD(cmplx, dispatch=[('loops_unary_complex', 'FD')],\n+             out=('f', 'd', 'g')),\n           TD(O, f='PyNumber_Absolute'),\n           ),\n '_arg':"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -759,9 +759,9 @@ src_umath = [\n   src_file.process('src/umath/loops_unary.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary_fp.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary_fp_le.dispatch.c.src'),\n+  src_file.process('src/umath/loops_unary_complex.dispatch.c.src'),\n   src_file.process('src/umath/matmul.c.src'),\n   src_file.process('src/umath/matmul.h.src'),\n-  src_file.process('src/umath/simd.inc.src'),\n   'src/umath/ufunc_type_resolution.c',\n   'src/umath/clip.cpp',\n   'src/umath/clip.h',"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1001,7 +1001,6 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'umathmodule.c'),\n             join('src', 'umath', 'reduction.c'),\n             join('src', 'umath', 'funcs.inc.src'),\n-            join('src', 'umath', 'simd.inc.src'),\n             join('src', 'umath', 'loops.h.src'),\n             join('src', 'umath', 'loops_utils.h.src'),\n             join('src', 'umath', 'loops.c.src'),\n@@ -1018,6 +1017,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops_hyperbolic.dispatch.c.src'),\n             join('src', 'umath', 'loops_modulo.dispatch.c.src'),\n             join('src', 'umath', 'loops_comparison.dispatch.c.src'),\n+            join('src', 'umath', 'loops_unary_complex.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),\n             join('src', 'umath', 'matmul.c.src'),\n             join('src', 'umath', 'clip.h'),\n@@ -1042,7 +1042,6 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'multiarray', 'common.h'),\n             join('src', 'multiarray', 'number.h'),\n             join('src', 'common', 'templ_common.h.src'),\n-            join('src', 'umath', 'simd.inc.src'),\n             join('src', 'umath', 'override.h'),\n             join(codegen_dir, 'generate_ufunc_api.py'),\n             join(codegen_dir, 'ufunc_docstrings.py'),"
            },
            {
                "filename": "numpy/core/src/_simd/_simd.dispatch.c.src",
                "patch": "@@ -42,23 +42,26 @@\n  */\n SIMD_IMPL_INTRIN_1(@intrin@_@sfx@, v@sfx@, q@sfx@)\n /**end repeat1**/\n+SIMD_IMPL_INTRIN_1(load_@sfx@x2, v@sfx@x2, q@sfx@)\n+\n /**begin repeat1\n- * # intrin = store, storea, stores, storel, storeh#\n+ * # intrin = store, storea, stores, storel, storeh, store#\n+ * # x = ,,,,, x2#\n  */\n // special definition due to the nature of @intrin@\n static PyObject *\n-simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n+simd__intrin_@intrin@_@sfx@@x@(PyObject* NPY_UNUSED(self), PyObject *args)\n {\n     simd_arg seq_arg = {.dtype = simd_data_q@sfx@};\n-    simd_arg vec_arg = {.dtype = simd_data_v@sfx@};\n+    simd_arg vec_arg = {.dtype = simd_data_v@sfx@@x@};\n     if (!PyArg_ParseTuple(\n-        args, \"O&O&:@intrin@_@sfx@\",\n+        args, \"O&O&:@intrin@_@sfx@@x@\",\n         simd_arg_converter, &seq_arg,\n         simd_arg_converter, &vec_arg\n     )) {\n         return NULL;\n     }\n-    npyv_@intrin@_@sfx@(seq_arg.data.q@sfx@, vec_arg.data.v@sfx@);\n+    npyv_@intrin@_@sfx@@x@(seq_arg.data.q@sfx@, vec_arg.data.v@sfx@@x@);\n     // write-back\n     if (simd_sequence_fill_iterable(seq_arg.obj, seq_arg.data.q@sfx@, simd_data_q@sfx@)) {\n         simd_arg_free(&seq_arg);\n@@ -76,23 +79,35 @@ simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n // Partial Load\n SIMD_IMPL_INTRIN_3(load_till_@sfx@, v@sfx@, q@sfx@, u32, @sfx@)\n SIMD_IMPL_INTRIN_2(load_tillz_@sfx@, v@sfx@, q@sfx@, u32)\n+#if @size@ == 32\n+    SIMD_IMPL_INTRIN_4(load2_till_@sfx@, v@sfx@, q@sfx@, u32, @sfx@, @sfx@)\n+    SIMD_IMPL_INTRIN_2(load2_tillz_@sfx@, v@sfx@, q@sfx@, u32)\n+#else\n+    SIMD_IMPL_INTRIN_4(load2_till_@sfx@, v@sfx@, q@sfx@, u32, @sfx@, @sfx@)\n+    SIMD_IMPL_INTRIN_2(load2_tillz_@sfx@, v@sfx@, q@sfx@, u32)\n+#endif\n \n // Partial Store\n+/**begin repeat1\n+ * #intrin = store_till, store2_till, store2_till#\n+ * #chksize= 0,          32,           64#\n+ */\n+#if !@chksize@ || @chksize@ == @size@\n static PyObject *\n-simd__intrin_store_till_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n+simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n {\n     simd_arg seq_arg = {.dtype = simd_data_q@sfx@};\n     simd_arg nlane_arg = {.dtype = simd_data_u32};\n     simd_arg vec_arg = {.dtype = simd_data_v@sfx@};\n     if (!PyArg_ParseTuple(\n-        args, \"O&O&O&:store_till_@sfx@\",\n+        args, \"O&O&O&:@intrin@_@sfx@\",\n         simd_arg_converter, &seq_arg,\n         simd_arg_converter, &nlane_arg,\n         simd_arg_converter, &vec_arg\n     )) {\n         return NULL;\n     }\n-    npyv_store_till_@sfx@(\n+    npyv_@intrin@_@sfx@(\n         seq_arg.data.q@sfx@, nlane_arg.data.u32, vec_arg.data.v@sfx@\n     );\n     // write-back\n@@ -103,14 +118,22 @@ simd__intrin_store_till_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n     simd_arg_free(&seq_arg);\n     Py_RETURN_NONE;\n }\n+#endif // chksize\n \n+/**end repeat1**/\n // Non-contiguous Load\n /**begin repeat1\n- * #intrin = loadn, loadn_till, loadn_tillz#\n- * #till   = 0,     1,          1#\n- * #fill   = 0,     1,          0#\n- * #format = ,    O&O&,         O&#\n- */\n+ * #intrin = loadn,       loadn2,       loadn2,\n+ *           loadn_till,  loadn2_till,  loadn2_till,\n+ *           loadn_tillz, loadn2_tillz, loadn2_tillz#\n+ * #scale  = 1,2,2,       1,2,2,         1,2,2#\n+ * #till   = 0*3,         1*3,           1*3#\n+ * #fill   = 0*3,         1*3,           0*3#\n+ # #fill2  = 0*3,         0,1,1,         0*3#\n+ * #format = ,,,          O&O&, O&O&O&*2,O&*3#\n+ * #chksize= 0,32,64,     0,32,64,       0,32,64#\n+ */\n+#if !@chksize@ || @chksize@ == @size@\n static PyObject *\n simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n {\n@@ -121,6 +144,9 @@ simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n #endif // till\n #if @fill@\n     simd_arg fill_arg = {.dtype = simd_data_@sfx@};\n+#endif\n+#if @fill2@\n+    simd_arg fill2_arg = {.dtype = simd_data_@sfx@};\n #endif\n     if (!PyArg_ParseTuple(\n         args, \"@format@O&O&:@intrin@_@sfx@\",\n@@ -131,6 +157,9 @@ simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n #endif\n #if @fill@\n         ,simd_arg_converter, &fill_arg\n+#endif\n+#if @fill2@\n+        ,simd_arg_converter, &fill2_arg\n #endif\n     )) {\n         return NULL;\n@@ -140,7 +169,7 @@ simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n     Py_ssize_t cur_seq_len = simd_sequence_len(seq_ptr);\n     Py_ssize_t min_seq_len = stride * npyv_nlanes_@sfx@;\n     if (stride < 0) {\n-        seq_ptr += cur_seq_len -1;\n+        seq_ptr += cur_seq_len - 1 * @scale@;\n         min_seq_len = -min_seq_len;\n     }\n     if (cur_seq_len < min_seq_len) {\n@@ -159,6 +188,9 @@ simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n     #if @fill@\n         , fill_arg.data.@sfx@\n     #endif\n+    #if @fill2@\n+        , fill2_arg.data.@sfx@\n+    #endif\n     );\n     simd_arg ret = {\n         .dtype = simd_data_v@sfx@, .data = {.v@sfx@=rvec}\n@@ -169,14 +201,19 @@ err:\n     simd_arg_free(&seq_arg);\n     return NULL;\n }\n+#endif // chksize\n /**end repeat1**/\n \n // Non-contiguous Store\n /**begin repeat1\n- * #intrin = storen, storen_till#\n- * #till   = 0,      1#\n- * #format = ,       O&#\n+ * #intrin = storen,      storen2,      storen2,\n+             storen_till, storen2_till, storen2_till#\n+ * #scale  = 1,2,2,       1,2,2#\n+ * #till   = 0*3,         1*3#\n+ * #format = ,,,          O&*3#\n+ * #chksize= 0,32,64,     0,32,64#\n  */\n+#if !@chksize@ || @chksize@ == @size@\n static PyObject *\n simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n {\n@@ -202,7 +239,7 @@ simd__intrin_@intrin@_@sfx@(PyObject* NPY_UNUSED(self), PyObject *args)\n     Py_ssize_t cur_seq_len = simd_sequence_len(seq_ptr);\n     Py_ssize_t min_seq_len = stride * npyv_nlanes_@sfx@;\n     if (stride < 0) {\n-        seq_ptr += cur_seq_len -1;\n+        seq_ptr += cur_seq_len - 1*@scale@;\n         min_seq_len = -min_seq_len;\n     }\n     // overflow guard\n@@ -231,6 +268,7 @@ err:\n     simd_arg_free(&seq_arg);\n     return NULL;\n }\n+#endif // chksize\n /**end repeat1**/\n #endif // @ncont_sup@\n \n@@ -300,7 +338,7 @@ SIMD_IMPL_INTRIN_2(@intrin@_@sfx@, v@sfx@, v@sfx@, v@sfx@)\n /**end repeat1**/\n \n /**begin repeat1\n- * # intrin = combine, zip#\n+ * # intrin = combine, zip, unzip#\n  */\n SIMD_IMPL_INTRIN_2(@intrin@_@sfx@, v@sfx@x2, v@sfx@, v@sfx@)\n /**end repeat1**/\n@@ -309,6 +347,60 @@ SIMD_IMPL_INTRIN_2(@intrin@_@sfx@, v@sfx@x2, v@sfx@, v@sfx@)\n SIMD_IMPL_INTRIN_1(rev64_@sfx@, v@sfx@, v@sfx@)\n #endif\n \n+// special implementation to convert runtime constants to immediate values\n+#if @size@ == 32\n+// one call for element index then gather them within one vector\n+// instead of unroll the 255 possible cases.\n+NPY_FINLINE npyv_@sfx@\n+npyv_permi128_@sfx@_(npyv_@sfx@ a, unsigned e0, unsigned e1, unsigned e2, unsigned e3)\n+{\n+   /**begin repeat1\n+    * # en = e0, e1, e2, e3#\n+    */\n+    npyv_@sfx@ v@en@;\n+    npyv_lanetype_@sfx@ d@en@[npyv_nlanes_@sfx@];\n+    if (0) {}\n+   /**begin repeat2\n+    * # imm = 1, 2, 3#\n+    */\n+    else if (@en@ == @imm@) {\n+        v@en@ = npyv_permi128_@sfx@(a, @imm@, @imm@, @imm@, @imm@);\n+    }\n+    /**end repeat2**/\n+    else {\n+        v@en@ = npyv_permi128_@sfx@(a, 0, 0, 0, 0);\n+    }\n+    npyv_store_@sfx@(d@en@, v@en@);\n+    /**end repeat1**/\n+    if (e0 == e1 && e0 == e2 && e0 == e3) {\n+        return ve0;\n+    }\n+    for (int i = 0; i < npyv_nlanes_@sfx@; i += 4) {\n+        de0[i+1] = de1[i+1];\n+        de0[i+2] = de2[i+2];\n+        de0[i+3] = de3[i+3];\n+    }\n+    return npyv_load_@sfx@(de0);\n+}\n+SIMD_IMPL_INTRIN_5(permi128_@sfx@_, v@sfx@, v@sfx@, u8, u8, u8, u8)\n+#elif @size@ == 64\n+NPY_FINLINE npyv_@sfx@\n+npyv_permi128_@sfx@_(npyv_@sfx@ a, unsigned e0, unsigned e1)\n+{\n+    if (e0 == 1 && e1 == 0) {\n+        return npyv_permi128_@sfx@(a, 1, 0);\n+    }\n+    else if (e0 == 0 && e1 == 1) {\n+        return npyv_permi128_@sfx@(a, 0, 1);\n+    }\n+    else if (e0 == 1 && e1 == 1) {\n+        return npyv_permi128_@sfx@(a, 1, 1);\n+    }\n+    return npyv_permi128_@sfx@(a, 0, 0);\n+}\n+SIMD_IMPL_INTRIN_3(permi128_@sfx@_, v@sfx@, v@sfx@, u8, u8)\n+#endif\n+\n /***************************\n  * Operators\n  ***************************/\n@@ -387,7 +479,7 @@ SIMD_IMPL_INTRIN_2(divc_@sfx@, v@sfx@, v@sfx@, v@sfx@x3)\n \n #if @fused_sup@\n /**begin repeat1\n- * #intrin = muladd, mulsub, nmuladd, nmulsub#\n+ * #intrin = muladd, mulsub, nmuladd, nmulsub, muladdsub#\n  */\n SIMD_IMPL_INTRIN_3(@intrin@_@sfx@, v@sfx@, v@sfx@, v@sfx@, v@sfx@)\n /**end repeat1**/\n@@ -438,6 +530,11 @@ SIMD_IMPL_INTRIN_1(reduce_@intrin@_@sfx@, @sfx@, v@sfx@)\n  SIMD_IMPL_INTRIN_4(@intrin@_@sfx@, v@sfx@, v@bsfx@, v@sfx@, v@sfx@, v@sfx@)\n /**end repeat1**/\n \n+#if @fp_only@\n+SIMD_IMPL_INTRIN_4(ifdiv_@sfx@, v@sfx@, v@bsfx@, v@sfx@, v@sfx@, v@sfx@)\n+SIMD_IMPL_INTRIN_3(ifdivz_@sfx@, v@sfx@, v@bsfx@, v@sfx@, v@sfx@)\n+#endif\n+\n #endif // simd_sup\n /**end repeat**/\n /*************************************************************************\n@@ -541,6 +638,12 @@ static PyMethodDef simd__intrinsics_methods[] = {\n SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n \n+/**begin repeat1\n+ * # intrin = load, store#\n+ */\n+SIMD_INTRIN_DEF(@intrin@_@sfx@x2)\n+/**end repeat1**/\n+\n /****************************************\n  * Non-contiguous/Partial Memory access\n  ****************************************/\n@@ -551,6 +654,21 @@ SIMD_INTRIN_DEF(@intrin@_@sfx@)\n  */\n SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n+#if @size@ == 32\n+    /**begin repeat1\n+     * #intrin = load2_till, load2_tillz, loadn2, loadn2_till, loadn2_tillz,\n+     *           store2_till, storen2, storen2_till#\n+     */\n+    SIMD_INTRIN_DEF(@intrin@_@sfx@)\n+    /**end repeat1**/\n+#else\n+    /**begin repeat1\n+     * #intrin = load2_till, load2_tillz, loadn2, loadn2_till, loadn2_tillz,\n+     *           store2_till, storen2, storen2_till#\n+     */\n+    SIMD_INTRIN_DEF(@intrin@_@sfx@)\n+    /**end repeat1**/\n+#endif\n #endif // ncont_sup\n \n /****************************\n@@ -584,7 +702,7 @@ SIMD_INTRIN_DEF(@intrin@_@sfx@)\n  * Reorder\n  ***************************/\n /**begin repeat1\n- * # intrin = combinel, combineh, combine, zip#\n+ * # intrin = combinel, combineh, combine, zip, unzip#\n  */\n SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n@@ -593,6 +711,10 @@ SIMD_INTRIN_DEF(@intrin@_@sfx@)\n SIMD_INTRIN_DEF(rev64_@sfx@)\n #endif\n \n+#if @size@ > 16\n+{ \"permi128_@sfx@\", simd__intrin_permi128_@sfx@_, METH_VARARGS, NULL },\n+#endif\n+\n /***************************\n  * Operators\n  ***************************/\n@@ -658,7 +780,7 @@ SIMD_INTRIN_DEF(divc_@sfx@)\n \n #if @fused_sup@\n /**begin repeat1\n- * #intrin = muladd, mulsub, nmuladd, nmulsub#\n+ * #intrin = muladd, mulsub, nmuladd, nmulsub, muladdsub#\n  */\n SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n@@ -708,6 +830,14 @@ SIMD_INTRIN_DEF(reduce_@intrin@_@sfx@)\n  SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n \n+#if @fp_only@\n+/**begin repeat1\n+ * #intrin = ifdiv, ifdivz#\n+ */\n+SIMD_INTRIN_DEF(@intrin@_@sfx@)\n+/**end repeat1**/\n+#endif\n+\n #endif // simd_sup\n /**end repeat**/\n /*************************************************************************"
            },
            {
                "filename": "numpy/core/src/_simd/_simd_easyintrin.inc",
                "patch": "@@ -197,6 +197,39 @@\n         return simd_arg_to_obj(&ret);                     \\\n     }\n \n+#define SIMD_IMPL_INTRIN_5(NAME, RET, IN0, IN1, IN2, IN3, IN4) \\\n+    static PyObject *simd__intrin_##NAME                  \\\n+    (PyObject* NPY_UNUSED(self), PyObject *args)          \\\n+    {                                                     \\\n+        simd_arg arg1 = {.dtype = simd_data_##IN0};       \\\n+        simd_arg arg2 = {.dtype = simd_data_##IN1};       \\\n+        simd_arg arg3 = {.dtype = simd_data_##IN2};       \\\n+        simd_arg arg4 = {.dtype = simd_data_##IN3};       \\\n+        simd_arg arg5 = {.dtype = simd_data_##IN4};       \\\n+        if (!PyArg_ParseTuple(                            \\\n+            args, \"O&O&O&O&O&:\"NPY_TOSTRING(NAME),        \\\n+            simd_arg_converter, &arg1,                    \\\n+            simd_arg_converter, &arg2,                    \\\n+            simd_arg_converter, &arg3,                    \\\n+            simd_arg_converter, &arg4,                    \\\n+            simd_arg_converter, &arg5                     \\\n+        )) return NULL;                                   \\\n+        simd_data data = {.RET = npyv_##NAME(             \\\n+            arg1.data.IN0, arg2.data.IN1,                 \\\n+            arg3.data.IN2, arg4.data.IN3,                 \\\n+            arg5.data.IN4                                 \\\n+        )};                                               \\\n+        simd_arg_free(&arg1);                             \\\n+        simd_arg_free(&arg2);                             \\\n+        simd_arg_free(&arg3);                             \\\n+        simd_arg_free(&arg4);                             \\\n+        simd_arg_free(&arg5);                             \\\n+        simd_arg ret = {                                  \\\n+            .data = data, .dtype = simd_data_##RET        \\\n+        };                                                \\\n+        return simd_arg_to_obj(&ret);                     \\\n+    }\n+\n /**\n  * Helper macros for repeating and expand a certain macro.\n  * Mainly used for converting a scalar to an immediate constant."
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/arithmetic.h",
                "patch": "@@ -247,6 +247,10 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     // negate multiply and subtract, -(a*b) - c\n     #define npyv_nmulsub_f32 _mm256_fnmsub_ps\n     #define npyv_nmulsub_f64 _mm256_fnmsub_pd\n+    // multiply, add for odd elements and subtract even elements.\n+    // (a * b) -+ c\n+    #define npyv_muladdsub_f32 _mm256_fmaddsub_ps\n+    #define npyv_muladdsub_f64 _mm256_fmaddsub_pd\n #else\n     // multiply and add, a*b + c\n     NPY_FINLINE npyv_f32 npyv_muladd_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n@@ -274,6 +278,13 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n         npyv_f64 neg_a = npyv_xor_f64(a, npyv_setall_f64(-0.0));\n         return npyv_sub_f64(npyv_mul_f64(neg_a, b), c);\n     }\n+    // multiply, add for odd elements and subtract even elements.\n+    // (a * b) -+ c\n+    NPY_FINLINE npyv_f32 npyv_muladdsub_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n+    { return _mm256_addsub_ps(npyv_mul_f32(a, b), c); }\n+    NPY_FINLINE npyv_f64 npyv_muladdsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+    { return _mm256_addsub_pd(npyv_mul_f64(a, b), c); }\n+\n #endif // !NPY_HAVE_FMA3\n \n /***************************"
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/memory.h",
                "patch": "@@ -84,17 +84,6 @@ NPY_FINLINE npyv_s32 npyv_loadn_s32(const npy_int32 *ptr, npy_intp stride)\n NPY_FINLINE npyv_f32 npyv_loadn_f32(const float *ptr, npy_intp stride)\n { return _mm256_castsi256_ps(npyv_loadn_u32((const npy_uint32*)ptr, stride)); }\n //// 64\n-#if 0 // slower\n-NPY_FINLINE npyv_u64 npyv_loadn_u64(const npy_uint64 *ptr, npy_intp stride)\n-{\n-    const __m256i idx = npyv_set_s64(0, 1*stride, 2*stride, 3*stride);\n-    return _mm256_i64gather_epi64((const void*)ptr, idx, 8);\n-}\n-NPY_FINLINE npyv_s64 npyv_loadn_s64(const npy_int64 *ptr, npy_intp stride)\n-{ return npyv_loadn_u64((const npy_uint64*)ptr, stride); }\n-NPY_FINLINE npyv_f64 npyv_loadn_f64(const double *ptr, npy_intp stride)\n-{ return _mm256_castsi256_pd(npyv_loadn_u64((const npy_uint64*)ptr, stride)); }\n-#endif\n NPY_FINLINE npyv_f64 npyv_loadn_f64(const double *ptr, npy_intp stride)\n {\n     __m128d a0 = _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)ptr));\n@@ -107,6 +96,32 @@ NPY_FINLINE npyv_u64 npyv_loadn_u64(const npy_uint64 *ptr, npy_intp stride)\n { return _mm256_castpd_si256(npyv_loadn_f64((const double*)ptr, stride)); }\n NPY_FINLINE npyv_s64 npyv_loadn_s64(const npy_int64 *ptr, npy_intp stride)\n { return _mm256_castpd_si256(npyv_loadn_f64((const double*)ptr, stride)); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_f32 npyv_loadn2_f32(const float *ptr, npy_intp stride)\n+{\n+    __m128d a0 = _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)ptr));\n+    __m128d a2 = _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)(ptr + stride*2)));\n+    __m128d a01 = _mm_loadh_pd(a0, (const double*)(ptr + stride));\n+    __m128d a23 = _mm_loadh_pd(a2, (const double*)(ptr + stride*3));\n+    return _mm256_castpd_ps(_mm256_insertf128_pd(_mm256_castpd128_pd256(a01), a23, 1));\n+}\n+NPY_FINLINE npyv_u32 npyv_loadn2_u32(const npy_uint32 *ptr, npy_intp stride)\n+{ return _mm256_castps_si256(npyv_loadn2_f32((const float*)ptr, stride)); }\n+NPY_FINLINE npyv_s32 npyv_loadn2_s32(const npy_int32 *ptr, npy_intp stride)\n+{ return _mm256_castps_si256(npyv_loadn2_f32((const float*)ptr, stride)); }\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_f64 npyv_loadn2_f64(const double *ptr, npy_intp stride)\n+{\n+    __m256d a = npyv_loadl_f64(ptr);\n+    return _mm256_insertf128_pd(a, _mm_loadu_pd(ptr + stride), 1);\n+}\n+NPY_FINLINE npyv_u64 npyv_loadn2_u64(const npy_uint64 *ptr, npy_intp stride)\n+{ return _mm256_castpd_si256(npyv_loadn2_f64((const double*)ptr, stride)); }\n+NPY_FINLINE npyv_s64 npyv_loadn2_s64(const npy_int64 *ptr, npy_intp stride)\n+{ return _mm256_castpd_si256(npyv_loadn2_f64((const double*)ptr, stride)); }\n+\n /***************************\n  * Non-contiguous Store\n  ***************************/\n@@ -143,6 +158,32 @@ NPY_FINLINE void npyv_storen_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n NPY_FINLINE void npyv_storen_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n { npyv_storen_f64((double*)ptr, stride, _mm256_castsi256_pd(a)); }\n \n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_u32(npy_uint32 *ptr, npy_intp stride, npyv_u32 a)\n+{\n+    __m128d a0 = _mm256_castpd256_pd128(_mm256_castsi256_pd(a));\n+    __m128d a1 = _mm256_extractf128_pd(_mm256_castsi256_pd(a), 1);\n+    _mm_storel_pd((double*)ptr, a0);\n+    _mm_storeh_pd((double*)(ptr + stride), a0);\n+    _mm_storel_pd((double*)(ptr + stride*2), a1);\n+    _mm_storeh_pd((double*)(ptr + stride*3), a1);\n+}\n+NPY_FINLINE void npyv_storen2_s32(npy_int32 *ptr, npy_intp stride, npyv_s32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, a); }\n+NPY_FINLINE void npyv_storen2_f32(float *ptr, npy_intp stride, npyv_f32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, _mm256_castps_si256(a)); }\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n+{\n+    npyv_storel_u64(ptr, a);\n+    npyv_storeh_u64(ptr + stride, a);\n+}\n+NPY_FINLINE void npyv_storen2_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n+{ npyv_storen2_u64((npy_uint64*)ptr, stride, a); }\n+NPY_FINLINE void npyv_storen2_f64(double *ptr, npy_intp stride, npyv_f64 a)\n+{ npyv_storen2_u64((npy_uint64*)ptr, stride, _mm256_castpd_si256(a)); }\n+\n /*********************************\n  * Partial Load\n  *********************************/\n@@ -186,6 +227,44 @@ NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n     __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n     return _mm256_maskload_epi64((const void*)ptr, mask);\n }\n+\n+//// 64-bit nlane\n+NPY_FINLINE npyv_s32 npyv_load2_till_s32(const npy_int32 *ptr, npy_uintp nlane,\n+                                          npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    const __m256i vfill = npyv_set_s32(\n+        fill_lo, fill_hi, fill_lo, fill_hi,\n+        fill_lo, fill_hi, fill_lo, fill_hi\n+    );\n+    const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n+    __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n+    __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n+    __m256i payload = _mm256_maskload_epi64((const void*)ptr, mask);\n+    return _mm256_blendv_epi8(vfill, payload, mask);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_load2_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n+{ return npyv_load_tillz_s64((const npy_int64*)ptr, nlane); }\n+\n+/// 128-bit nlane\n+NPY_FINLINE npyv_u64 npyv_load2_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n+{\n+    assert(nlane > 0);\n+    npy_int64 m = -((npy_int64)(nlane > 1));\n+    __m256i mask = npyv_set_s64(-1, -1, m, m);\n+    return _mm256_maskload_epi64((const void*)ptr, mask);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_u64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n+                                           npy_int64 fill_lo, npy_int64 fill_hi)\n+{\n+    const __m256i vfill = npyv_set_s64(0, 0, fill_lo, fill_hi);\n+    npy_int64 m = -((npy_int64)(nlane > 1));\n+    __m256i mask = npyv_set_s64(-1, -1, m, m);\n+    __m256i payload = _mm256_maskload_epi64((const void*)ptr, mask);\n+    return _mm256_blendv_epi8(vfill, payload, mask);\n+}\n /*********************************\n  * Non-contiguous partial load\n  *********************************/\n@@ -222,6 +301,47 @@ npyv_loadn_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npy_\n NPY_FINLINE npyv_s64\n npyv_loadn_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n { return npyv_loadn_till_s64(ptr, stride, nlane, 0); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                 npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    const __m256i vfill = npyv_set_s32(\n+        fill_lo, fill_hi, fill_lo, fill_hi,\n+        fill_lo, fill_hi, fill_lo, fill_hi\n+    );\n+    const __m256i idx   = npyv_set_s64(0, 1*stride, 2*stride, 3*stride);\n+    const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n+    __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n+    __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n+    return _mm256_mask_i64gather_epi64(vfill, (const void*)ptr, idx, mask, 4);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_loadn2_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n+{ return npyv_loadn2_till_s32(ptr, stride, nlane, 0, 0); }\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                  npy_int64 fill_lo, npy_int64 fill_hi)\n+{\n+    assert(nlane > 0);\n+    __m256i a = npyv_loadl_s64(ptr);\n+#if defined(_MSC_VER) && defined(_M_IX86)\n+    __m128i fill =_mm_setr_epi32(\n+        (int)fill_lo, (int)(fill_lo >> 32),\n+        (int)fill_hi, (int)(fill_hi >> 32)\n+    );\n+#else\n+    __m128i fill = _mm_set_epi64x(fill_hi, fill_lo);\n+#endif\n+    __m128i b = nlane > 1 ? _mm_loadu_si128((const __m128i*)(ptr + stride)) : fill;\n+    return _mm256_inserti128_si256(a, b, 1);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s64 npyv_loadn2_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n+{ return npyv_loadn2_till_s64(ptr, stride, nlane, 0, 0); }\n+\n /*********************************\n  * Partial store\n  *********************************/\n@@ -243,6 +363,20 @@ NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a\n     __m256i mask   = _mm256_cmpgt_epi64(vnlane, steps);\n     _mm256_maskstore_epi64((void*)ptr, mask, a);\n }\n+\n+//// 64-bit nlane\n+NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n+{ npyv_store_till_s64((npy_int64*)ptr, nlane, a); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0);\n+    npyv_storel_s64(ptr, a);\n+    if (nlane > 1) {\n+        npyv_storeh_s64(ptr + 2, a);\n+    }\n+}\n /*********************************\n  * Non-contiguous partial store\n  *********************************/\n@@ -252,23 +386,52 @@ NPY_FINLINE void npyv_storen_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp\n     assert(nlane > 0);\n     __m128i a0 = _mm256_castsi256_si128(a);\n     __m128i a1 = _mm256_extracti128_si256(a, 1);\n+\n+    ptr[stride*0] = _mm_extract_epi32(a0, 0);\n     switch(nlane) {\n-    default:\n-        ptr[stride*7] = _mm_extract_epi32(a1, 3);\n-    case 7:\n-        ptr[stride*6] = _mm_extract_epi32(a1, 2);\n-    case 6:\n-        ptr[stride*5] = _mm_extract_epi32(a1, 1);\n+    case 1:\n+        return;\n+    case 2:\n+        ptr[stride*1] = _mm_extract_epi32(a0, 1);\n+        return;\n+    case 3:\n+        ptr[stride*1] = _mm_extract_epi32(a0, 1);\n+        ptr[stride*2] = _mm_extract_epi32(a0, 2);\n+        return;\n+    case 4:\n+        ptr[stride*1] = _mm_extract_epi32(a0, 1);\n+        ptr[stride*2] = _mm_extract_epi32(a0, 2);\n+        ptr[stride*3] = _mm_extract_epi32(a0, 3);\n+        return;\n     case 5:\n+        ptr[stride*1] = _mm_extract_epi32(a0, 1);\n+        ptr[stride*2] = _mm_extract_epi32(a0, 2);\n+        ptr[stride*3] = _mm_extract_epi32(a0, 3);\n         ptr[stride*4] = _mm_extract_epi32(a1, 0);\n-    case 4:\n+        return;\n+    case 6:\n+        ptr[stride*1] = _mm_extract_epi32(a0, 1);\n+        ptr[stride*2] = _mm_extract_epi32(a0, 2);\n         ptr[stride*3] = _mm_extract_epi32(a0, 3);\n-    case 3:\n+        ptr[stride*4] = _mm_extract_epi32(a1, 0);\n+        ptr[stride*5] = _mm_extract_epi32(a1, 1);\n+        return;\n+    case 7:\n+        ptr[stride*1] = _mm_extract_epi32(a0, 1);\n         ptr[stride*2] = _mm_extract_epi32(a0, 2);\n-    case 2:\n+        ptr[stride*3] = _mm_extract_epi32(a0, 3);\n+        ptr[stride*4] = _mm_extract_epi32(a1, 0);\n+        ptr[stride*5] = _mm_extract_epi32(a1, 1);\n+        ptr[stride*6] = _mm_extract_epi32(a1, 2);\n+        return;\n+    default:\n         ptr[stride*1] = _mm_extract_epi32(a0, 1);\n-    case 1:\n-        ptr[stride*0] = _mm_extract_epi32(a0, 0);\n+        ptr[stride*2] = _mm_extract_epi32(a0, 2);\n+        ptr[stride*3] = _mm_extract_epi32(a0, 3);\n+        ptr[stride*4] = _mm_extract_epi32(a1, 0);\n+        ptr[stride*5] = _mm_extract_epi32(a1, 1);\n+        ptr[stride*6] = _mm_extract_epi32(a1, 2);\n+        ptr[stride*7] = _mm_extract_epi32(a1, 3);\n     }\n }\n //// 64\n@@ -277,19 +440,60 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n     assert(nlane > 0);\n     __m128d a0 = _mm256_castpd256_pd128(_mm256_castsi256_pd(a));\n     __m128d a1 = _mm256_extractf128_pd(_mm256_castsi256_pd(a), 1);\n+\n     double *dptr = (double*)ptr;\n+    _mm_storel_pd(dptr, a0);\n     switch(nlane) {\n-    default:\n-        _mm_storeh_pd(dptr + stride * 3, a1);\n+    case 1:\n+        return;\n+    case 2:\n+        _mm_storeh_pd(dptr + stride * 1, a0);\n+        return;\n     case 3:\n+        _mm_storeh_pd(dptr + stride * 1, a0);\n         _mm_storel_pd(dptr + stride * 2, a1);\n-    case 2:\n+        return;\n+    default:\n         _mm_storeh_pd(dptr + stride * 1, a0);\n+        _mm_storel_pd(dptr + stride * 2, a1);\n+        _mm_storeh_pd(dptr + stride * 3, a1);\n+    }\n+}\n+\n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+    __m128d a0 = _mm256_castpd256_pd128(_mm256_castsi256_pd(a));\n+    __m128d a1 = _mm256_extractf128_pd(_mm256_castsi256_pd(a), 1);\n+\n+    _mm_storel_pd((double*)ptr, a0);\n+    switch(nlane) {\n     case 1:\n-        _mm_storel_pd(dptr + stride * 0, a0);\n+        return;\n+    case 2:\n+        _mm_storeh_pd((double*)(ptr + stride * 1), a0);\n+        return;\n+    case 3:\n+        _mm_storeh_pd((double*)(ptr + stride * 1), a0);\n+        _mm_storel_pd((double*)(ptr + stride * 2), a1);\n+        return;\n+    default:\n+        _mm_storeh_pd((double*)(ptr + stride * 1), a0);\n+        _mm_storel_pd((double*)(ptr + stride * 2), a1);\n+        _mm_storeh_pd((double*)(ptr + stride * 3), a1);\n     }\n }\n \n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0);\n+    npyv_storel_s64(ptr, a);\n+    if (nlane > 1) {\n+        npyv_storeh_s64(ptr + stride, a);\n+    }\n+}\n /*****************************************************************************\n  * Implement partial load/store for u32/f32/u64/f64... via reinterpret cast\n  *****************************************************************************/\n@@ -300,7 +504,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load_till_##T_SFX(                   \\\n             (const npyv_lanetype_##T_SFX *)ptr, nlane, pun.to_##T_SFX                       \\\n         ));                                                                                 \\\n@@ -312,7 +517,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn_till_##T_SFX(                  \\\n             (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun.to_##T_SFX               \\\n         ));                                                                                 \\\n@@ -353,6 +559,110 @@ NPYV_IMPL_AVX2_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_AVX2_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_AVX2_REST_PARTIAL_TYPES(f64, s64)\n \n+// 128-bit/64-bit stride (load/store pair)\n+#define NPYV_IMPL_AVX2_REST_PARTIAL_TYPES_PAIR(F_SFX, T_SFX)                                \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_till_##F_SFX                                        \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane,                                     \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_till_##T_SFX(                  \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane, pun_lo.to_##T_SFX, pun_hi.to_##T_SFX \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_till_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane,                    \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_till_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun_lo.to_##T_SFX,           \\\n+            pun_hi.to_##T_SFX                                                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_tillz_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane)                                     \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_tillz_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane                                       \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_tillz_##F_SFX                                      \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane)                    \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_tillz_##T_SFX(                \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_store2_till_##F_SFX                                               \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_uintp nlane, npyv_##F_SFX a)                           \\\n+    {                                                                                       \\\n+        npyv_store2_till_##T_SFX(                                                           \\\n+            (npyv_lanetype_##T_SFX *)ptr, nlane,                                            \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_storen2_till_##F_SFX                                              \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane, npyv_##F_SFX a)          \\\n+    {                                                                                       \\\n+        npyv_storen2_till_##T_SFX(                                                          \\\n+            (npyv_lanetype_##T_SFX *)ptr, stride, nlane,                                    \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }\n+\n+NPYV_IMPL_AVX2_REST_PARTIAL_TYPES_PAIR(u32, s32)\n+NPYV_IMPL_AVX2_REST_PARTIAL_TYPES_PAIR(f32, s32)\n+NPYV_IMPL_AVX2_REST_PARTIAL_TYPES_PAIR(u64, s64)\n+NPYV_IMPL_AVX2_REST_PARTIAL_TYPES_PAIR(f64, s64)\n+\n+/************************************************************\n+ *  de-interlave load / interleave contiguous store\n+ ************************************************************/\n+// two channels\n+#define NPYV_IMPL_AVX2_MEM_INTERLEAVE(SFX, ZSFX)                             \\\n+    NPY_FINLINE npyv_##ZSFX##x2 npyv_zip_##ZSFX(npyv_##ZSFX, npyv_##ZSFX);   \\\n+    NPY_FINLINE npyv_##ZSFX##x2 npyv_unzip_##ZSFX(npyv_##ZSFX, npyv_##ZSFX); \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_load_##SFX##x2(                          \\\n+        const npyv_lanetype_##SFX *ptr                                       \\\n+    ) {                                                                      \\\n+        return npyv_unzip_##ZSFX(                                            \\\n+            npyv_load_##SFX(ptr), npyv_load_##SFX(ptr+npyv_nlanes_##SFX)     \\\n+        );                                                                   \\\n+    }                                                                        \\\n+    NPY_FINLINE void npyv_store_##SFX##x2(                                   \\\n+        npyv_lanetype_##SFX *ptr, npyv_##SFX##x2 v                           \\\n+    ) {                                                                      \\\n+        npyv_##SFX##x2 zip = npyv_zip_##ZSFX(v.val[0], v.val[1]);            \\\n+        npyv_store_##SFX(ptr, zip.val[0]);                                   \\\n+        npyv_store_##SFX(ptr + npyv_nlanes_##SFX, zip.val[1]);               \\\n+    }\n+\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(u8, u8)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(s8, u8)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(u16, u16)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(s16, u16)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(u32, u32)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(s32, u32)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(u64, u64)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(s64, u64)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(f32, f32)\n+NPYV_IMPL_AVX2_MEM_INTERLEAVE(f64, f64)\n+\n /*********************************\n  * Lookup tables\n  *********************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/reorder.h",
                "patch": "@@ -94,6 +94,75 @@ NPY_FINLINE npyv_f64x2 npyv_zip_f64(__m256d a, __m256d b)\n     return npyv_combine_f64(ab0, ab1);\n }\n \n+// deinterleave two vectors\n+NPY_FINLINE npyv_u8x2 npyv_unzip_u8(npyv_u8 ab0, npyv_u8 ab1)\n+{\n+    const __m256i idx = _mm256_setr_epi8(\n+        0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15,\n+        0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15\n+    );\n+    __m256i ab_03 = _mm256_shuffle_epi8(ab0, idx);\n+    __m256i ab_12 = _mm256_shuffle_epi8(ab1, idx);\n+    npyv_u8x2 ab_lh = npyv_combine_u8(ab_03, ab_12);\n+    npyv_u8x2 r;\n+    r.val[0] = _mm256_unpacklo_epi64(ab_lh.val[0], ab_lh.val[1]);\n+    r.val[1] = _mm256_unpackhi_epi64(ab_lh.val[0], ab_lh.val[1]);\n+    return r;\n+}\n+#define npyv_unzip_s8 npyv_unzip_u8\n+\n+NPY_FINLINE npyv_u16x2 npyv_unzip_u16(npyv_u16 ab0, npyv_u16 ab1)\n+{\n+    const __m256i idx = _mm256_setr_epi8(\n+        0,1, 4,5, 8,9, 12,13, 2,3, 6,7, 10,11, 14,15,\n+        0,1, 4,5, 8,9, 12,13, 2,3, 6,7, 10,11, 14,15\n+    );\n+    __m256i ab_03 = _mm256_shuffle_epi8(ab0, idx);\n+    __m256i ab_12 = _mm256_shuffle_epi8(ab1, idx);\n+    npyv_u16x2 ab_lh = npyv_combine_u16(ab_03, ab_12);\n+    npyv_u16x2 r;\n+    r.val[0] = _mm256_unpacklo_epi64(ab_lh.val[0], ab_lh.val[1]);\n+    r.val[1] = _mm256_unpackhi_epi64(ab_lh.val[0], ab_lh.val[1]);\n+    return r;\n+}\n+#define npyv_unzip_s16 npyv_unzip_u16\n+\n+NPY_FINLINE npyv_u32x2 npyv_unzip_u32(npyv_u32 ab0, npyv_u32 ab1)\n+{\n+    const __m256i idx = npyv_set_u32(0, 2, 4, 6, 1, 3, 5, 7);\n+    __m256i abl = _mm256_permutevar8x32_epi32(ab0, idx);\n+    __m256i abh = _mm256_permutevar8x32_epi32(ab1, idx);\n+    return npyv_combine_u32(abl, abh);\n+}\n+#define npyv_unzip_s32 npyv_unzip_u32\n+\n+NPY_FINLINE npyv_u64x2 npyv_unzip_u64(npyv_u64 ab0, npyv_u64 ab1)\n+{\n+    npyv_u64x2 ab_lh = npyv_combine_u64(ab0, ab1);\n+    npyv_u64x2 r;\n+    r.val[0] = _mm256_unpacklo_epi64(ab_lh.val[0], ab_lh.val[1]);\n+    r.val[1] = _mm256_unpackhi_epi64(ab_lh.val[0], ab_lh.val[1]);\n+    return r;\n+}\n+#define npyv_unzip_s64 npyv_unzip_u64\n+\n+NPY_FINLINE npyv_f32x2 npyv_unzip_f32(npyv_f32 ab0, npyv_f32 ab1)\n+{\n+    const __m256i idx = npyv_set_u32(0, 2, 4, 6, 1, 3, 5, 7);\n+    __m256 abl = _mm256_permutevar8x32_ps(ab0, idx);\n+    __m256 abh = _mm256_permutevar8x32_ps(ab1, idx);\n+    return npyv_combine_f32(abl, abh);\n+}\n+\n+NPY_FINLINE npyv_f64x2 npyv_unzip_f64(npyv_f64 ab0, npyv_f64 ab1)\n+{\n+    npyv_f64x2 ab_lh = npyv_combine_f64(ab0, ab1);\n+    npyv_f64x2 r;\n+    r.val[0] = _mm256_unpacklo_pd(ab_lh.val[0], ab_lh.val[1]);\n+    r.val[1] = _mm256_unpackhi_pd(ab_lh.val[0], ab_lh.val[1]);\n+    return r;\n+}\n+\n // Reverse elements of each 64-bit lane\n NPY_FINLINE npyv_u8 npyv_rev64_u8(npyv_u8 a)\n {\n@@ -126,4 +195,22 @@ NPY_FINLINE npyv_f32 npyv_rev64_f32(npyv_f32 a)\n     return _mm256_shuffle_ps(a, a, _MM_SHUFFLE(2, 3, 0, 1));\n }\n \n+// Permuting the elements of each 128-bit lane by immediate index for\n+// each element.\n+#define npyv_permi128_u32(A, E0, E1, E2, E3) \\\n+    _mm256_shuffle_epi32(A, _MM_SHUFFLE(E3, E2, E1, E0))\n+\n+#define npyv_permi128_s32 npyv_permi128_u32\n+\n+#define npyv_permi128_u64(A, E0, E1) \\\n+    _mm256_shuffle_epi32(A, _MM_SHUFFLE(((E1)<<1)+1, ((E1)<<1), ((E0)<<1)+1, ((E0)<<1)))\n+\n+#define npyv_permi128_s64 npyv_permi128_u64\n+\n+#define npyv_permi128_f32(A, E0, E1, E2, E3) \\\n+    _mm256_permute_ps(A, _MM_SHUFFLE(E3, E2, E1, E0))\n+\n+#define npyv_permi128_f64(A, E0, E1) \\\n+    _mm256_permute_pd(A, ((E1)<<3) | ((E0)<<2) | ((E1)<<1) | (E0))\n+\n #endif // _NPY_SIMD_AVX2_REORDER_H"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/arithmetic.h",
                "patch": "@@ -349,6 +349,10 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n // negate multiply and subtract, -(a*b) - c\n #define npyv_nmulsub_f32 _mm512_fnmsub_ps\n #define npyv_nmulsub_f64 _mm512_fnmsub_pd\n+// multiply, add for odd elements and subtract even elements.\n+// (a * b) -+ c\n+#define npyv_muladdsub_f32 _mm512_fmaddsub_ps\n+#define npyv_muladdsub_f64 _mm512_fmaddsub_pd\n \n /***************************\n  * Summation: Calculates the sum of all vector elements."
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/maskop.h",
                "patch": "@@ -51,4 +51,17 @@ NPYV_IMPL_AVX512_MASK_ADDSUB(s64, b64, epi64)\n NPYV_IMPL_AVX512_MASK_ADDSUB(f32, b32, ps)\n NPYV_IMPL_AVX512_MASK_ADDSUB(f64, b64, pd)\n \n+// division, m ? a / b : c\n+NPY_FINLINE npyv_f32 npyv_ifdiv_f32(npyv_b32 m, npyv_f32 a, npyv_f32 b, npyv_f32 c)\n+{ return _mm512_mask_div_ps(c, m, a, b); }\n+// conditional division, m ? a / b : 0\n+NPY_FINLINE npyv_f32 npyv_ifdivz_f32(npyv_b32 m, npyv_f32 a, npyv_f32 b)\n+{ return _mm512_maskz_div_ps(m, a, b); }\n+// division, m ? a / b : c\n+NPY_FINLINE npyv_f64 npyv_ifdiv_f64(npyv_b32 m, npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+{ return _mm512_mask_div_pd(c, m, a, b); }\n+// conditional division, m ? a / b : 0\n+NPY_FINLINE npyv_f64 npyv_ifdivz_f64(npyv_b32 m, npyv_f64 a, npyv_f64 b)\n+{ return _mm512_maskz_div_pd(m, a, b); }\n+\n #endif // _NPY_SIMD_AVX512_MASKOP_H"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/memory.h",
                "patch": "@@ -120,6 +120,52 @@ NPY_FINLINE npyv_s64 npyv_loadn_s64(const npy_int64 *ptr, npy_intp stride)\n { return npyv_loadn_u64((const npy_uint64*)ptr, stride); }\n NPY_FINLINE npyv_f64 npyv_loadn_f64(const double *ptr, npy_intp stride)\n { return _mm512_castsi512_pd(npyv_loadn_u64((const npy_uint64*)ptr, stride)); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_u32 npyv_loadn2_u32(const npy_uint32 *ptr, npy_intp stride)\n+{\n+    __m128d a = _mm_loadh_pd(\n+        _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)ptr)),\n+        (const double*)(ptr + stride)\n+    );\n+    __m128d b = _mm_loadh_pd(\n+        _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)(ptr + stride*2))),\n+        (const double*)(ptr + stride*3)\n+    );\n+    __m128d c = _mm_loadh_pd(\n+        _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)(ptr + stride*4))),\n+        (const double*)(ptr + stride*5)\n+    );\n+    __m128d d = _mm_loadh_pd(\n+        _mm_castsi128_pd(_mm_loadl_epi64((const __m128i*)(ptr + stride*6))),\n+        (const double*)(ptr + stride*7)\n+    );\n+    return _mm512_castpd_si512(npyv512_combine_pd256(\n+        _mm256_insertf128_pd(_mm256_castpd128_pd256(a), b, 1),\n+        _mm256_insertf128_pd(_mm256_castpd128_pd256(c), d, 1)\n+    ));\n+}\n+NPY_FINLINE npyv_s32 npyv_loadn2_s32(const npy_int32 *ptr, npy_intp stride)\n+{ return npyv_loadn2_u32((const npy_uint32*)ptr, stride); }\n+NPY_FINLINE npyv_f32 npyv_loadn2_f32(const float *ptr, npy_intp stride)\n+{ return _mm512_castsi512_ps(npyv_loadn2_u32((const npy_uint32*)ptr, stride)); }\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_f64 npyv_loadn2_f64(const double *ptr, npy_intp stride)\n+{\n+    __m128d a = _mm_loadu_pd(ptr);\n+    __m128d b = _mm_loadu_pd(ptr + stride);\n+    __m128d c = _mm_loadu_pd(ptr + stride * 2);\n+    __m128d d = _mm_loadu_pd(ptr + stride * 3);\n+    return npyv512_combine_pd256(\n+        _mm256_insertf128_pd(_mm256_castpd128_pd256(a), b, 1),\n+        _mm256_insertf128_pd(_mm256_castpd128_pd256(c), d, 1)\n+    );\n+}\n+NPY_FINLINE npyv_u64 npyv_loadn2_u64(const npy_uint64 *ptr, npy_intp stride)\n+{ return npyv_reinterpret_u64_f64(npyv_loadn2_f64((const double*)ptr, stride)); }\n+NPY_FINLINE npyv_s64 npyv_loadn2_s64(const npy_int64 *ptr, npy_intp stride)\n+{ return npyv_loadn2_u64((const npy_uint64*)ptr, stride); }\n /***************************\n  * Non-contiguous Store\n  ***************************/\n@@ -151,6 +197,48 @@ NPY_FINLINE void npyv_storen_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n NPY_FINLINE void npyv_storen_f64(double *ptr, npy_intp stride, npyv_f64 a)\n { npyv_storen_u64((npy_uint64*)ptr, stride, _mm512_castpd_si512(a)); }\n \n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_u32(npy_uint32 *ptr, npy_intp stride, npyv_u32 a)\n+{\n+    __m256d lo = _mm512_castpd512_pd256(_mm512_castsi512_pd(a));\n+    __m256d hi = _mm512_extractf64x4_pd(_mm512_castsi512_pd(a), 1);\n+    __m128d e0 = _mm256_castpd256_pd128(lo);\n+    __m128d e1 = _mm256_extractf128_pd(lo, 1);\n+    __m128d e2 = _mm256_castpd256_pd128(hi);\n+    __m128d e3 = _mm256_extractf128_pd(hi, 1);\n+    _mm_storel_pd((double*)(ptr + stride * 0), e0);\n+    _mm_storeh_pd((double*)(ptr + stride * 1), e0);\n+    _mm_storel_pd((double*)(ptr + stride * 2), e1);\n+    _mm_storeh_pd((double*)(ptr + stride * 3), e1);\n+    _mm_storel_pd((double*)(ptr + stride * 4), e2);\n+    _mm_storeh_pd((double*)(ptr + stride * 5), e2);\n+    _mm_storel_pd((double*)(ptr + stride * 6), e3);\n+    _mm_storeh_pd((double*)(ptr + stride * 7), e3);\n+}\n+NPY_FINLINE void npyv_storen2_s32(npy_int32 *ptr, npy_intp stride, npyv_s32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, a); }\n+NPY_FINLINE void npyv_storen2_f32(float *ptr, npy_intp stride, npyv_f32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, _mm512_castps_si512(a)); }\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n+{\n+    __m256i lo = npyv512_lower_si256(a);\n+    __m256i hi = npyv512_higher_si256(a);\n+    __m128i e0 = _mm256_castsi256_si128(lo);\n+    __m128i e1 = _mm256_extracti128_si256(lo, 1);\n+    __m128i e2 = _mm256_castsi256_si128(hi);\n+    __m128i e3 = _mm256_extracti128_si256(hi, 1);\n+    _mm_storeu_si128((__m128i*)(ptr + stride * 0), e0);\n+    _mm_storeu_si128((__m128i*)(ptr + stride * 1), e1);\n+    _mm_storeu_si128((__m128i*)(ptr + stride * 2), e2);\n+    _mm_storeu_si128((__m128i*)(ptr + stride * 3), e3);\n+}\n+NPY_FINLINE void npyv_storen2_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n+{ npyv_storen2_u64((npy_uint64*)ptr, stride, a); }\n+NPY_FINLINE void npyv_storen2_f64(double *ptr, npy_intp stride, npyv_f64 a)\n+{ npyv_storen2_u64((npy_uint64*)ptr, stride, _mm512_castpd_si512(a)); }\n+\n /*********************************\n  * Partial Load\n  *********************************/\n@@ -159,29 +247,59 @@ NPY_FINLINE npyv_s32 npyv_load_till_s32(const npy_int32 *ptr, npy_uintp nlane, n\n {\n     assert(nlane > 0);\n     const __m512i vfill = _mm512_set1_epi32(fill);\n-    const __mmask16 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask16 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n     return _mm512_mask_loadu_epi32(vfill, mask, (const __m512i*)ptr);\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s32 npyv_load_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n {\n     assert(nlane > 0);\n-    const __mmask16 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask16 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n     return _mm512_maskz_loadu_epi32(mask, (const __m512i*)ptr);\n }\n //// 64\n NPY_FINLINE npyv_s64 npyv_load_till_s64(const npy_int64 *ptr, npy_uintp nlane, npy_int64 fill)\n {\n     assert(nlane > 0);\n     const __m512i vfill = npyv_setall_s64(fill);\n-    const __mmask8 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n     return _mm512_mask_loadu_epi64(vfill, mask, (const __m512i*)ptr);\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n {\n     assert(nlane > 0);\n-    const __mmask8 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    return _mm512_maskz_loadu_epi64(mask, (const __m512i*)ptr);\n+}\n+\n+//// 64-bit nlane\n+NPY_FINLINE npyv_s32 npyv_load2_till_s32(const npy_int32 *ptr, npy_uintp nlane,\n+                                          npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    const __m512i vfill = _mm512_set4_epi32(fill_hi, fill_lo, fill_hi, fill_lo);\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    return _mm512_mask_loadu_epi64(vfill, mask, (const __m512i*)ptr);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_load2_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n+{ return npyv_load_tillz_s64((const npy_int64*)ptr, nlane); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE npyv_u64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n+                                           npy_int64 fill_lo, npy_int64 fill_hi)\n+{\n+    assert(nlane > 0);\n+    const __m512i vfill = _mm512_set4_epi64(fill_hi, fill_lo, fill_hi, fill_lo);\n+    const __mmask8 mask = nlane > 3 ? -1 : (1 << (nlane*2)) - 1;\n+    return _mm512_mask_loadu_epi64(vfill, mask, (const __m512i*)ptr);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s64 npyv_load2_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n+{\n+    assert(nlane > 0);\n+    const __mmask8 mask = nlane > 3 ? -1 : (1 << (nlane*2)) - 1;\n     return _mm512_maskz_loadu_epi64(mask, (const __m512i*)ptr);\n }\n /*********************************\n@@ -198,7 +316,7 @@ npyv_loadn_till_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npy_\n     );\n     const __m512i idx = _mm512_mullo_epi32(steps, _mm512_set1_epi32((int)stride));\n     const __m512i vfill = _mm512_set1_epi32(fill);\n-    const __mmask16 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask16 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n     return _mm512_mask_i32gather_epi32(vfill, mask, idx, (const __m512i*)ptr, 4);\n }\n // fill zero to rest lanes\n@@ -215,28 +333,79 @@ npyv_loadn_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npy_\n         4*stride, 5*stride, 6*stride, 7*stride\n     );\n     const __m512i vfill = npyv_setall_s64(fill);\n-    const __mmask8 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask8 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n     return _mm512_mask_i64gather_epi64(vfill, mask, idx, (const __m512i*)ptr, 8);\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s64\n npyv_loadn_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n { return npyv_loadn_till_s64(ptr, stride, nlane, 0); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                 npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    const __m512i idx = npyv_set_s64(\n+        0*stride, 1*stride, 2*stride, 3*stride,\n+        4*stride, 5*stride, 6*stride, 7*stride\n+    );\n+    const __m512i vfill = _mm512_set4_epi32(fill_hi, fill_lo, fill_hi, fill_lo);\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    return _mm512_mask_i64gather_epi64(vfill, mask, idx, (const __m512i*)ptr, 4);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_loadn2_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n+{ return npyv_loadn2_till_s32(ptr, stride, nlane, 0, 0); }\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                  npy_int64 fill_lo, npy_int64 fill_hi)\n+{\n+    assert(nlane > 0);\n+    const __m512i idx = npyv_set_s64(\n+       0,        1,          stride,   stride+1,\n+       stride*2, stride*2+1, stride*3, stride*3+1\n+    );\n+    const __mmask8 mask = nlane > 3 ? -1 : (1 << (nlane*2)) - 1;\n+    const __m512i vfill = _mm512_set4_epi64(fill_hi, fill_lo, fill_hi, fill_lo);\n+    return _mm512_mask_i64gather_epi64(vfill, mask, idx, (const __m512i*)ptr, 8);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s64 npyv_loadn2_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n+{ return npyv_loadn2_till_s64(ptr, stride, nlane, 0, 0); }\n+\n /*********************************\n  * Partial store\n  *********************************/\n //// 32\n NPY_FINLINE void npyv_store_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n {\n     assert(nlane > 0);\n-    const __mmask16 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask16 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n     _mm512_mask_storeu_epi32((__m512i*)ptr, mask, a);\n }\n //// 64\n NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n {\n     assert(nlane > 0);\n-    const __mmask8 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    _mm512_mask_storeu_epi64((__m512i*)ptr, mask, a);\n+}\n+\n+//// 64-bit nlane\n+NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    _mm512_mask_storeu_epi64((__m512i*)ptr, mask, a);\n+}\n+\n+//// 128-bit nlane\n+NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0);\n+    const __mmask8 mask = nlane > 3 ? -1 : (1 << (nlane*2)) - 1;\n     _mm512_mask_storeu_epi64((__m512i*)ptr, mask, a);\n }\n /*********************************\n@@ -251,7 +420,7 @@ NPY_FINLINE void npyv_storen_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp\n         0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n     );\n     const __m512i idx = _mm512_mullo_epi32(steps, _mm512_set1_epi32((int)stride));\n-    const __mmask16 mask = nlane > 31 ? -1 : (1 << nlane) - 1;\n+    const __mmask16 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n     _mm512_mask_i32scatter_epi32((__m512i*)ptr, mask, idx, a, 4);\n }\n //// 64\n@@ -262,7 +431,31 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         0*stride, 1*stride, 2*stride, 3*stride,\n         4*stride, 5*stride, 6*stride, 7*stride\n     );\n-    const __mmask8 mask = nlane > 15 ? -1 : (1 << nlane) - 1;\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    _mm512_mask_i64scatter_epi64((__m512i*)ptr, mask, idx, a, 8);\n+}\n+\n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+    const __m512i idx = npyv_set_s64(\n+        0*stride, 1*stride, 2*stride, 3*stride,\n+        4*stride, 5*stride, 6*stride, 7*stride\n+    );\n+    const __mmask8 mask = nlane > 7 ? -1 : (1 << nlane) - 1;\n+    _mm512_mask_i64scatter_epi64((__m512i*)ptr, mask, idx, a, 4);\n+}\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0);\n+    const __m512i idx = npyv_set_s64(\n+        0,        1,            stride,   stride+1,\n+        2*stride, 2*stride+1, 3*stride, 3*stride+1\n+    );\n+    const __mmask8 mask = nlane > 3 ? -1 : (1 << (nlane*2)) - 1;\n     _mm512_mask_i64scatter_epi64((__m512i*)ptr, mask, idx, a, 8);\n }\n \n@@ -331,6 +524,110 @@ NPYV_IMPL_AVX512_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_AVX512_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_AVX512_REST_PARTIAL_TYPES(f64, s64)\n \n+// 128-bit/64-bit stride (pair load/store)\n+#define NPYV_IMPL_AVX512_REST_PARTIAL_TYPES_PAIR(F_SFX, T_SFX)                              \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_till_##F_SFX                                        \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane,                                     \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_till_##T_SFX(                  \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane, pun_lo.to_##T_SFX, pun_hi.to_##T_SFX \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_till_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane,                    \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_till_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun_lo.to_##T_SFX,           \\\n+            pun_hi.to_##T_SFX                                                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_tillz_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane)                                     \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_tillz_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane                                       \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_tillz_##F_SFX                                      \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane)                    \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_tillz_##T_SFX(                \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_store2_till_##F_SFX                                               \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_uintp nlane, npyv_##F_SFX a)                           \\\n+    {                                                                                       \\\n+        npyv_store2_till_##T_SFX(                                                           \\\n+            (npyv_lanetype_##T_SFX *)ptr, nlane,                                            \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_storen2_till_##F_SFX                                              \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane, npyv_##F_SFX a)          \\\n+    {                                                                                       \\\n+        npyv_storen2_till_##T_SFX(                                                          \\\n+            (npyv_lanetype_##T_SFX *)ptr, stride, nlane,                                    \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }\n+\n+NPYV_IMPL_AVX512_REST_PARTIAL_TYPES_PAIR(u32, s32)\n+NPYV_IMPL_AVX512_REST_PARTIAL_TYPES_PAIR(f32, s32)\n+NPYV_IMPL_AVX512_REST_PARTIAL_TYPES_PAIR(u64, s64)\n+NPYV_IMPL_AVX512_REST_PARTIAL_TYPES_PAIR(f64, s64)\n+\n+/************************************************************\n+ *  de-interlave load / interleave contiguous store\n+ ************************************************************/\n+// two channels\n+#define NPYV_IMPL_AVX512_MEM_INTERLEAVE(SFX, ZSFX)                           \\\n+    NPY_FINLINE npyv_##ZSFX##x2 npyv_zip_##ZSFX(npyv_##ZSFX, npyv_##ZSFX);   \\\n+    NPY_FINLINE npyv_##ZSFX##x2 npyv_unzip_##ZSFX(npyv_##ZSFX, npyv_##ZSFX); \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_load_##SFX##x2(                          \\\n+        const npyv_lanetype_##SFX *ptr                                       \\\n+    ) {                                                                      \\\n+        return npyv_unzip_##ZSFX(                                            \\\n+            npyv_load_##SFX(ptr), npyv_load_##SFX(ptr+npyv_nlanes_##SFX)     \\\n+        );                                                                   \\\n+    }                                                                        \\\n+    NPY_FINLINE void npyv_store_##SFX##x2(                                   \\\n+        npyv_lanetype_##SFX *ptr, npyv_##SFX##x2 v                           \\\n+    ) {                                                                      \\\n+        npyv_##SFX##x2 zip = npyv_zip_##ZSFX(v.val[0], v.val[1]);            \\\n+        npyv_store_##SFX(ptr, zip.val[0]);                                   \\\n+        npyv_store_##SFX(ptr + npyv_nlanes_##SFX, zip.val[1]);               \\\n+    }\n+\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(u8, u8)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(s8, u8)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(u16, u16)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(s16, u16)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(u32, u32)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(s32, u32)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(u64, u64)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(s64, u64)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(f32, f32)\n+NPYV_IMPL_AVX512_MEM_INTERLEAVE(f64, f64)\n+\n /**************************************************\n  * Lookup table\n  *************************************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/reorder.h",
                "patch": "@@ -167,6 +167,140 @@ NPY_FINLINE npyv_f64x2 npyv_zip_f64(__m512d a, __m512d b)\n     return r;\n }\n \n+// deinterleave two vectors\n+NPY_FINLINE npyv_u8x2 npyv_unzip_u8(npyv_u8 ab0, npyv_u8 ab1)\n+{\n+    npyv_u8x2 r;\n+#ifdef NPY_HAVE_AVX512VBMI\n+    const __m512i idx_a = npyv_set_u8(\n+        0,  2,  4,   6,   8,   10,  12,  14,  16,  18,  20,  22,  24,  26,  28,  30,\n+        32, 34, 36,  38,  40,  42,  44,  46,  48,  50,  52,  54,  56,  58,  60,  62,\n+        64, 66, 68,  70,  72,  74,  76,  78,  80,  82,  84,  86,  88,  90,  92,  94,\n+        96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126\n+    );\n+    const __m512i idx_b = npyv_set_u8(\n+        1,  3,  5,   7,   9,   11,  13,  15,  17,  19,  21,  23,  25,  27,  29,  31,\n+        33, 35, 37,  39,  41,  43,  45,  47,  49,  51,  53,  55,  57,  59,  61,  63,\n+        65, 67, 69,  71,  73,  75,  77,  79,  81,  83,  85,  87,  89,  91,  93,  95,\n+        97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127\n+    );\n+    r.val[0] = _mm512_permutex2var_epi8(ab0, idx_a, ab1);\n+    r.val[1] = _mm512_permutex2var_epi8(ab0, idx_b, ab1);\n+#else\n+    #ifdef NPY_HAVE_AVX512BW\n+        const __m512i idx = npyv_set_u8(\n+            0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15,\n+            0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15,\n+            0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15,\n+            0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15\n+        );\n+        __m512i abl = _mm512_shuffle_epi8(ab0, idx);\n+        __m512i abh = _mm512_shuffle_epi8(ab1, idx);\n+    #else\n+        const __m256i idx = _mm256_setr_epi8(\n+            0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15,\n+            0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15\n+        );\n+        __m256i abl_lo = _mm256_shuffle_epi8(npyv512_lower_si256(ab0),  idx);\n+        __m256i abl_hi = _mm256_shuffle_epi8(npyv512_higher_si256(ab0), idx);\n+        __m256i abh_lo = _mm256_shuffle_epi8(npyv512_lower_si256(ab1),  idx);\n+        __m256i abh_hi = _mm256_shuffle_epi8(npyv512_higher_si256(ab1), idx);\n+        __m512i abl = npyv512_combine_si256(abl_lo, abl_hi);\n+        __m512i abh = npyv512_combine_si256(abh_lo, abh_hi);\n+    #endif\n+    const __m512i idx_a = npyv_set_u64(0, 2, 4, 6, 8, 10, 12, 14);\n+    const __m512i idx_b = npyv_set_u64(1, 3, 5, 7, 9, 11, 13, 15);\n+    r.val[0] = _mm512_permutex2var_epi64(abl, idx_a, abh);\n+    r.val[1] = _mm512_permutex2var_epi64(abl, idx_b, abh);\n+#endif\n+    return r;\n+}\n+#define npyv_unzip_s8 npyv_unzip_u8\n+\n+NPY_FINLINE npyv_u16x2 npyv_unzip_u16(npyv_u16 ab0, npyv_u16 ab1)\n+{\n+    npyv_u16x2 r;\n+#ifdef NPY_HAVE_AVX512BW\n+    const __m512i idx_a = npyv_set_u16(\n+        0,  2,  4,  6,  8,  10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,\n+        32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62\n+    );\n+    const __m512i idx_b = npyv_set_u16(\n+        1,  3,  5,  7,  9,  11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,\n+        33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63\n+    );\n+    r.val[0] = _mm512_permutex2var_epi16(ab0, idx_a, ab1);\n+    r.val[1] = _mm512_permutex2var_epi16(ab0, idx_b, ab1);\n+#else\n+    const __m256i idx = _mm256_setr_epi8(\n+        0,1, 4,5, 8,9, 12,13, 2,3, 6,7, 10,11, 14,15,\n+        0,1, 4,5, 8,9, 12,13, 2,3, 6,7, 10,11, 14,15\n+    );\n+    __m256i abl_lo = _mm256_shuffle_epi8(npyv512_lower_si256(ab0),  idx);\n+    __m256i abl_hi = _mm256_shuffle_epi8(npyv512_higher_si256(ab0), idx);\n+    __m256i abh_lo = _mm256_shuffle_epi8(npyv512_lower_si256(ab1),  idx);\n+    __m256i abh_hi = _mm256_shuffle_epi8(npyv512_higher_si256(ab1), idx);\n+    __m512i abl = npyv512_combine_si256(abl_lo, abl_hi);\n+    __m512i abh = npyv512_combine_si256(abh_lo, abh_hi);\n+\n+    const __m512i idx_a = npyv_set_u64(0, 2, 4, 6, 8, 10, 12, 14);\n+    const __m512i idx_b = npyv_set_u64(1, 3, 5, 7, 9, 11, 13, 15);\n+    r.val[0] = _mm512_permutex2var_epi64(abl, idx_a, abh);\n+    r.val[1] = _mm512_permutex2var_epi64(abl, idx_b, abh);\n+#endif\n+    return r;\n+}\n+#define npyv_unzip_s16 npyv_unzip_u16\n+\n+NPY_FINLINE npyv_u32x2 npyv_unzip_u32(npyv_u32 ab0, npyv_u32 ab1)\n+{\n+    const __m512i idx_a = npyv_set_u32(\n+        0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30\n+    );\n+    const __m512i idx_b = npyv_set_u32(\n+        1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31\n+    );\n+    npyv_u32x2 r;\n+    r.val[0] = _mm512_permutex2var_epi32(ab0, idx_a, ab1);\n+    r.val[1] = _mm512_permutex2var_epi32(ab0, idx_b, ab1);\n+    return r;\n+}\n+#define npyv_unzip_s32 npyv_unzip_u32\n+\n+NPY_FINLINE npyv_u64x2 npyv_unzip_u64(npyv_u64 ab0, npyv_u64 ab1)\n+{\n+    const __m512i idx_a = npyv_set_u64(0, 2, 4, 6, 8, 10, 12, 14);\n+    const __m512i idx_b = npyv_set_u64(1, 3, 5, 7, 9, 11, 13, 15);\n+    npyv_u64x2 r;\n+    r.val[0] = _mm512_permutex2var_epi64(ab0, idx_a, ab1);\n+    r.val[1] = _mm512_permutex2var_epi64(ab0, idx_b, ab1);\n+    return r;\n+}\n+#define npyv_unzip_s64 npyv_unzip_u64\n+\n+NPY_FINLINE npyv_f32x2 npyv_unzip_f32(npyv_f32 ab0, npyv_f32 ab1)\n+{\n+    const __m512i idx_a = npyv_set_u32(\n+        0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30\n+    );\n+    const __m512i idx_b = npyv_set_u32(\n+        1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31\n+    );\n+    npyv_f32x2 r;\n+    r.val[0] = _mm512_permutex2var_ps(ab0, idx_a, ab1);\n+    r.val[1] = _mm512_permutex2var_ps(ab0, idx_b, ab1);\n+    return r;\n+}\n+NPY_FINLINE npyv_f64x2 npyv_unzip_f64(npyv_f64 ab0, npyv_f64 ab1)\n+{\n+    const __m512i idx_a = npyv_set_u64(0, 2, 4, 6, 8, 10, 12, 14);\n+    const __m512i idx_b = npyv_set_u64(1, 3, 5, 7, 9, 11, 13, 15);\n+    npyv_f64x2 r;\n+    r.val[0] = _mm512_permutex2var_pd(ab0, idx_a, ab1);\n+    r.val[1] = _mm512_permutex2var_pd(ab0, idx_b, ab1);\n+    return r;\n+}\n+\n // Reverse elements of each 64-bit lane\n NPY_FINLINE npyv_u8 npyv_rev64_u8(npyv_u8 a)\n {\n@@ -223,4 +357,22 @@ NPY_FINLINE npyv_f32 npyv_rev64_f32(npyv_f32 a)\n     return _mm512_shuffle_ps(a, a, (_MM_PERM_ENUM)_MM_SHUFFLE(2, 3, 0, 1));\n }\n \n+// Permuting the elements of each 128-bit lane by immediate index for\n+// each element.\n+#define npyv_permi128_u32(A, E0, E1, E2, E3) \\\n+    _mm512_shuffle_epi32(A, _MM_SHUFFLE(E3, E2, E1, E0))\n+\n+#define npyv_permi128_s32 npyv_permi128_u32\n+\n+#define npyv_permi128_u64(A, E0, E1) \\\n+    _mm512_shuffle_epi32(A, _MM_SHUFFLE(((E1)<<1)+1, ((E1)<<1), ((E0)<<1)+1, ((E0)<<1)))\n+\n+#define npyv_permi128_s64 npyv_permi128_u64\n+\n+#define npyv_permi128_f32(A, E0, E1, E2, E3) \\\n+    _mm512_permute_ps(A, _MM_SHUFFLE(E3, E2, E1, E0))\n+\n+#define npyv_permi128_f64(A, E0, E1) \\\n+    _mm512_permute_pd(A, (((E1)<<7) | ((E0)<<6) | ((E1)<<5) | ((E0)<<4) | ((E1)<<3) | ((E0)<<2) | ((E1)<<1) | (E0)))\n+\n #endif // _NPY_SIMD_AVX512_REORDER_H"
            },
            {
                "filename": "numpy/core/src/common/simd/emulate_maskop.h",
                "patch": "@@ -42,5 +42,39 @@ NPYV_IMPL_EMULATE_MASK_ADDSUB(s64, b64)\n #if NPY_SIMD_F64\n     NPYV_IMPL_EMULATE_MASK_ADDSUB(f64, b64)\n #endif\n+#if NPY_SIMD_F32\n+    // conditional division, m ? a / b : c\n+    NPY_FINLINE npyv_f32\n+    npyv_ifdiv_f32(npyv_b32 m, npyv_f32 a, npyv_f32 b, npyv_f32 c)\n+    {\n+        const npyv_f32 one = npyv_setall_f32(1.0f);\n+        npyv_f32 div = npyv_div_f32(a, npyv_select_f32(m, b, one));\n+        return npyv_select_f32(m, div, c);\n+    }\n+    // conditional division, m ? a / b : 0\n+    NPY_FINLINE npyv_f32\n+    npyv_ifdivz_f32(npyv_b32 m, npyv_f32 a, npyv_f32 b)\n+    {\n+        const npyv_f32 zero = npyv_zero_f32();\n+        return npyv_ifdiv_f32(m, a, b, zero);\n+    }\n+#endif\n+#if NPY_SIMD_F64\n+    // conditional division, m ? a / b : c\n+    NPY_FINLINE npyv_f64\n+    npyv_ifdiv_f64(npyv_b64 m, npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+    {\n+        const npyv_f64 one = npyv_setall_f64(1.0);\n+        npyv_f64 div = npyv_div_f64(a, npyv_select_f64(m, b, one));\n+        return npyv_select_f64(m, div, c);\n+    }\n+    // conditional division, m ? a / b : 0\n+    NPY_FINLINE npyv_f64\n+    npyv_ifdivz_f64(npyv_b64 m, npyv_f64 a, npyv_f64 b)\n+    {\n+        const npyv_f64 zero = npyv_zero_f64();\n+        return npyv_ifdiv_f64(m, a, b, zero);\n+    }\n+#endif\n \n #endif // _NPY_SIMD_EMULATE_MASKOP_H"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/arithmetic.h",
                "patch": "@@ -265,6 +265,14 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     NPY_FINLINE npyv_f32 npyv_nmulsub_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n     { return vmlsq_f32(vnegq_f32(c), a, b); }\n #endif\n+// multiply, add for odd elements and subtract even elements.\n+// (a * b) -+ c\n+NPY_FINLINE npyv_f32 npyv_muladdsub_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n+{\n+    const npyv_f32 msign = npyv_set_f32(-0.0f, 0.0f, -0.0f, 0.0f);\n+    return npyv_muladd_f32(a, b, npyv_xor_f32(msign, c));\n+}\n+\n /***************************\n  * FUSED F64\n  ***************************/\n@@ -277,6 +285,11 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     { return vfmsq_f64(c, a, b); }\n     NPY_FINLINE npyv_f64 npyv_nmulsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n     { return vfmsq_f64(vnegq_f64(c), a, b); }\n+    NPY_FINLINE npyv_f64 npyv_muladdsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+    {\n+        const npyv_f64 msign = npyv_set_f64(-0.0, 0.0);\n+        return npyv_muladd_f64(a, b, npyv_xor_f64(msign, c));\n+    }\n #endif // NPY_SIMD_F64\n \n /***************************"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/memory.h",
                "patch": "@@ -102,6 +102,29 @@ NPY_FINLINE npyv_f64 npyv_loadn_f64(const double *ptr, npy_intp stride)\n     );\n }\n #endif\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_u32 npyv_loadn2_u32(const npy_uint32 *ptr, npy_intp stride)\n+{\n+    return vcombine_u32(\n+        vld1_u32((const uint32_t*)ptr), vld1_u32((const uint32_t*)ptr + stride)\n+    );\n+}\n+NPY_FINLINE npyv_s32 npyv_loadn2_s32(const npy_int32 *ptr, npy_intp stride)\n+{ return npyv_reinterpret_s32_u32(npyv_loadn2_u32((const npy_uint32*)ptr, stride)); }\n+NPY_FINLINE npyv_f32 npyv_loadn2_f32(const float *ptr, npy_intp stride)\n+{ return npyv_reinterpret_f32_u32(npyv_loadn2_u32((const npy_uint32*)ptr, stride)); }\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_u64 npyv_loadn2_u64(const npy_uint64 *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_u64(ptr); }\n+NPY_FINLINE npyv_s64 npyv_loadn2_s64(const npy_int64 *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_s64(ptr); }\n+#if NPY_SIMD_F64\n+NPY_FINLINE npyv_f64 npyv_loadn2_f64(const double *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_f64(ptr); }\n+#endif\n+\n /***************************\n  * Non-contiguous Store\n  ***************************/\n@@ -131,6 +154,32 @@ NPY_FINLINE void npyv_storen_f64(double *ptr, npy_intp stride, npyv_f64 a)\n { npyv_storen_s64((npy_int64*)ptr, stride, npyv_reinterpret_s64_f64(a)); }\n #endif\n \n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_u32(npy_uint32 *ptr, npy_intp stride, npyv_u32 a)\n+{\n+#if NPY_SIMD_F64\n+    vst1q_lane_u64((uint64_t*)ptr, npyv_reinterpret_u64_u32(a), 0);\n+    vst1q_lane_u64((uint64_t*)(ptr + stride), npyv_reinterpret_u64_u32(a), 1);\n+#else\n+    // armhf strict to alignment\n+    vst1_u32((uint32_t*)ptr, vget_low_u32(a));\n+    vst1_u32((uint32_t*)ptr + stride, vget_high_u32(a));\n+#endif\n+}\n+NPY_FINLINE void npyv_storen2_s32(npy_int32 *ptr, npy_intp stride, npyv_s32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, npyv_reinterpret_u32_s32(a)); }\n+NPY_FINLINE void npyv_storen2_f32(float *ptr, npy_intp stride, npyv_f32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, npyv_reinterpret_u32_f32(a)); }\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n+{ (void)stride; npyv_store_u64(ptr, a); }\n+NPY_FINLINE void npyv_storen2_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n+{ (void)stride; npyv_store_s64(ptr, a); }\n+#if NPY_SIMD_F64\n+NPY_FINLINE void npyv_storen2_f64(double *ptr, npy_intp stride, npyv_f64 a)\n+{ (void)stride; npyv_store_f64(ptr, a); }\n+#endif\n /*********************************\n  * Partial Load\n  *********************************/\n@@ -168,6 +217,29 @@ NPY_FINLINE npyv_s64 npyv_load_till_s64(const npy_int64 *ptr, npy_uintp nlane, n\n NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n { return npyv_load_till_s64(ptr, nlane, 0); }\n \n+//// 64-bit nlane\n+NPY_FINLINE npyv_s32 npyv_load2_till_s32(const npy_int32 *ptr, npy_uintp nlane,\n+                                          npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        const int32_t NPY_DECL_ALIGNED(16) fill[2] = {fill_lo, fill_hi};\n+        return vcombine_s32(vld1_s32((const int32_t*)ptr), vld1_s32(fill));\n+    }\n+    return npyv_load_s32(ptr);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_load2_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n+{ return vreinterpretq_s32_s64(npyv_load_tillz_s64((const npy_int64*)ptr, nlane)); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE npyv_s64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n+                                           npy_int64 fill_lo, npy_int64 fill_hi)\n+{ (void)nlane; (void)fill_lo; (void)fill_hi; return npyv_load_s64(ptr); }\n+\n+NPY_FINLINE npyv_s64 npyv_load2_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n+{ (void)nlane; return npyv_load_s64(ptr); }\n+\n /*********************************\n  * Non-contiguous partial load\n  *********************************/\n@@ -206,6 +278,34 @@ npyv_loadn_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npy_\n NPY_FINLINE npyv_s64 npyv_loadn_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n { return npyv_loadn_till_s64(ptr, stride, nlane, 0); }\n \n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_s32 npyv_loadn2_till_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                 npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        const int32_t NPY_DECL_ALIGNED(16) fill[2] = {fill_lo, fill_hi};\n+        return vcombine_s32(vld1_s32((const int32_t*)ptr), vld1_s32(fill));\n+    }\n+    return npyv_loadn2_s32(ptr, stride);\n+}\n+NPY_FINLINE npyv_s32 npyv_loadn2_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        return vcombine_s32(vld1_s32((const int32_t*)ptr), vdup_n_s32(0));\n+    }\n+    return npyv_loadn2_s32(ptr, stride);\n+}\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane,\n+                                          npy_int64 fill_lo, npy_int64 fill_hi)\n+{ assert(nlane > 0); (void)stride; (void)nlane; (void)fill_lo; (void)fill_hi; return npyv_load_s64(ptr); }\n+\n+NPY_FINLINE npyv_s64 npyv_loadn2_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n+{ assert(nlane > 0); (void)stride; (void)nlane; return npyv_load_s64(ptr); }\n+\n /*********************************\n  * Partial store\n  *********************************/\n@@ -238,23 +338,52 @@ NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a\n     }\n     npyv_store_s64(ptr, a);\n }\n+\n+//// 64-bit nlane\n+NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        // armhf strict to alignment, may cause bus error\n+    #if NPY_SIMD_F64\n+        vst1q_lane_s64((int64_t*)ptr, npyv_reinterpret_s64_s32(a), 0);\n+    #else\n+        npyv_storel_s32(ptr, a);\n+    #endif\n+        return;\n+    }\n+    npyv_store_s32(ptr, a);\n+}\n+\n+//// 128-bit nlane\n+NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0); (void)nlane;\n+    npyv_store_s64(ptr, a);\n+}\n+\n /*********************************\n  * Non-contiguous partial store\n  *********************************/\n //// 32\n NPY_FINLINE void npyv_storen_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n {\n     assert(nlane > 0);\n+    vst1q_lane_s32((int32_t*)ptr, a, 0);\n     switch(nlane) {\n-    default:\n-        vst1q_lane_s32((int32_t*)ptr + stride*3, a, 3);\n+    case 1:\n+        return;\n+    case 2:\n+        vst1q_lane_s32((int32_t*)ptr + stride, a, 1);\n+        return;\n     case 3:\n+        vst1q_lane_s32((int32_t*)ptr + stride, a, 1);\n         vst1q_lane_s32((int32_t*)ptr + stride*2, a, 2);\n-    case 2:\n+        return;\n+    default:\n         vst1q_lane_s32((int32_t*)ptr + stride, a, 1);\n-    case 1:\n-        vst1q_lane_s32((int32_t*)ptr, a, 0);\n-        break;\n+        vst1q_lane_s32((int32_t*)ptr + stride*2, a, 2);\n+        vst1q_lane_s32((int32_t*)ptr + stride*3, a, 3);\n     }\n }\n //// 64\n@@ -268,6 +397,27 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n     npyv_storen_s64(ptr, stride, a);\n }\n \n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+#if NPY_SIMD_F64\n+    vst1q_lane_s64((int64_t*)ptr, npyv_reinterpret_s64_s32(a), 0);\n+    if (nlane > 1) {\n+        vst1q_lane_s64((int64_t*)(ptr + stride), npyv_reinterpret_s64_s32(a), 1);\n+    }\n+#else\n+    npyv_storel_s32(ptr, a);\n+    if (nlane > 1) {\n+        npyv_storeh_s32(ptr + stride, a);\n+    }\n+#endif\n+}\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npyv_s64 a)\n+{ assert(nlane > 0); (void)stride; (void)nlane; npyv_store_s64(ptr, a); }\n+\n /*****************************************************************\n  * Implement partial load/store for u32/f32/u64/f64... via casting\n  *****************************************************************/\n@@ -278,7 +428,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load_till_##T_SFX(                   \\\n             (const npyv_lanetype_##T_SFX *)ptr, nlane, pun.to_##T_SFX                       \\\n         ));                                                                                 \\\n@@ -290,7 +441,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn_till_##T_SFX(                  \\\n             (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun.to_##T_SFX               \\\n         ));                                                                                 \\\n@@ -332,6 +484,131 @@ NPYV_IMPL_NEON_REST_PARTIAL_TYPES(u64, s64)\n #if NPY_SIMD_F64\n NPYV_IMPL_NEON_REST_PARTIAL_TYPES(f64, s64)\n #endif\n+\n+// 128-bit/64-bit stride\n+#define NPYV_IMPL_NEON_REST_PARTIAL_TYPES_PAIR(F_SFX, T_SFX)                                \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_till_##F_SFX                                        \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane,                                     \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_till_##T_SFX(                  \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane, pun_lo.to_##T_SFX, pun_hi.to_##T_SFX \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_till_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane,                    \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_till_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun_lo.to_##T_SFX,           \\\n+            pun_hi.to_##T_SFX                                                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_tillz_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane)                                     \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_tillz_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane                                       \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_tillz_##F_SFX                                      \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane)                    \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_tillz_##T_SFX(                \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_store2_till_##F_SFX                                               \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_uintp nlane, npyv_##F_SFX a)                           \\\n+    {                                                                                       \\\n+        npyv_store2_till_##T_SFX(                                                           \\\n+            (npyv_lanetype_##T_SFX *)ptr, nlane,                                            \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_storen2_till_##F_SFX                                              \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane, npyv_##F_SFX a)          \\\n+    {                                                                                       \\\n+        npyv_storen2_till_##T_SFX(                                                          \\\n+            (npyv_lanetype_##T_SFX *)ptr, stride, nlane,                                    \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }\n+\n+NPYV_IMPL_NEON_REST_PARTIAL_TYPES_PAIR(u32, s32)\n+NPYV_IMPL_NEON_REST_PARTIAL_TYPES_PAIR(f32, s32)\n+NPYV_IMPL_NEON_REST_PARTIAL_TYPES_PAIR(u64, s64)\n+#if NPY_SIMD_F64\n+NPYV_IMPL_NEON_REST_PARTIAL_TYPES_PAIR(f64, s64)\n+#endif\n+\n+/************************************************************\n+ *  de-interlave load / interleave contiguous store\n+ ************************************************************/\n+// two channels\n+#define NPYV_IMPL_NEON_MEM_INTERLEAVE(SFX, T_PTR)                        \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_load_##SFX##x2(                      \\\n+        const npyv_lanetype_##SFX *ptr                                   \\\n+    ) {                                                                  \\\n+        return vld2q_##SFX((const T_PTR*)ptr);                           \\\n+    }                                                                    \\\n+    NPY_FINLINE void npyv_store_##SFX##x2(                               \\\n+        npyv_lanetype_##SFX *ptr, npyv_##SFX##x2 v                       \\\n+    ) {                                                                  \\\n+        vst2q_##SFX((T_PTR*)ptr, v);                                     \\\n+    }\n+\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(u8,  uint8_t)\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(s8,  int8_t)\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(u16, uint16_t)\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(s16, int16_t)\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(u32, uint32_t)\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(s32, int32_t)\n+NPYV_IMPL_NEON_MEM_INTERLEAVE(f32, float)\n+\n+#if NPY_SIMD_F64\n+    NPYV_IMPL_NEON_MEM_INTERLEAVE(f64, double)\n+    NPYV_IMPL_NEON_MEM_INTERLEAVE(u64, uint64_t)\n+    NPYV_IMPL_NEON_MEM_INTERLEAVE(s64, int64_t)\n+#else\n+    #define NPYV_IMPL_NEON_MEM_INTERLEAVE_64(SFX)                               \\\n+        NPY_FINLINE npyv_##SFX##x2 npyv_load_##SFX##x2(                         \\\n+            const npyv_lanetype_##SFX *ptr)                                     \\\n+        {                                                                       \\\n+            npyv_##SFX a = npyv_load_##SFX(ptr);                                \\\n+            npyv_##SFX b = npyv_load_##SFX(ptr + 2);                            \\\n+            npyv_##SFX##x2 r;                                                   \\\n+            r.val[0] = vcombine_##SFX(vget_low_##SFX(a),  vget_low_##SFX(b));   \\\n+            r.val[1] = vcombine_##SFX(vget_high_##SFX(a), vget_high_##SFX(b));  \\\n+            return r;                                                           \\\n+        }                                                                       \\\n+        NPY_FINLINE void npyv_store_##SFX##x2(                                  \\\n+            npyv_lanetype_##SFX *ptr, npyv_##SFX##x2 v)                         \\\n+        {                                                                       \\\n+            npyv_store_##SFX(ptr, vcombine_##SFX(                               \\\n+                vget_low_##SFX(v.val[0]),  vget_low_##SFX(v.val[1])));          \\\n+            npyv_store_##SFX(ptr + 2, vcombine_##SFX(                           \\\n+                vget_high_##SFX(v.val[0]),  vget_high_##SFX(v.val[1])));        \\\n+        }\n+        NPYV_IMPL_NEON_MEM_INTERLEAVE_64(u64)\n+        NPYV_IMPL_NEON_MEM_INTERLEAVE_64(s64)\n+#endif\n /*********************************\n  * Lookup table\n  *********************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/reorder.h",
                "patch": "@@ -76,36 +76,45 @@ NPYV_IMPL_NEON_COMBINE(npyv_f32, f32)\n NPYV_IMPL_NEON_COMBINE(npyv_f64, f64)\n #endif\n \n-// interleave two vectors\n-#define NPYV_IMPL_NEON_ZIP(T_VEC, SFX)                       \\\n-    NPY_FINLINE T_VEC##x2 npyv_zip_##SFX(T_VEC a, T_VEC b)   \\\n-    {                                                        \\\n-        T_VEC##x2 r;                                         \\\n-        r.val[0] = vzip1q_##SFX(a, b);                       \\\n-        r.val[1] = vzip2q_##SFX(a, b);                       \\\n-        return r;                                            \\\n-    }\n-\n+// interleave & deinterleave two vectors\n #ifdef __aarch64__\n-    NPYV_IMPL_NEON_ZIP(npyv_u8,  u8)\n-    NPYV_IMPL_NEON_ZIP(npyv_s8,  s8)\n-    NPYV_IMPL_NEON_ZIP(npyv_u16, u16)\n-    NPYV_IMPL_NEON_ZIP(npyv_s16, s16)\n-    NPYV_IMPL_NEON_ZIP(npyv_u32, u32)\n-    NPYV_IMPL_NEON_ZIP(npyv_s32, s32)\n-    NPYV_IMPL_NEON_ZIP(npyv_f32, f32)\n-    NPYV_IMPL_NEON_ZIP(npyv_f64, f64)\n+    #define NPYV_IMPL_NEON_ZIP(T_VEC, SFX)                       \\\n+        NPY_FINLINE T_VEC##x2 npyv_zip_##SFX(T_VEC a, T_VEC b)   \\\n+        {                                                        \\\n+            T_VEC##x2 r;                                         \\\n+            r.val[0] = vzip1q_##SFX(a, b);                       \\\n+            r.val[1] = vzip2q_##SFX(a, b);                       \\\n+            return r;                                            \\\n+        }                                                        \\\n+        NPY_FINLINE T_VEC##x2 npyv_unzip_##SFX(T_VEC a, T_VEC b) \\\n+        {                                                        \\\n+            T_VEC##x2 r;                                         \\\n+            r.val[0] = vuzp1q_##SFX(a, b);                       \\\n+            r.val[1] = vuzp2q_##SFX(a, b);                       \\\n+            return r;                                            \\\n+        }\n #else\n-    #define npyv_zip_u8  vzipq_u8\n-    #define npyv_zip_s8  vzipq_s8\n-    #define npyv_zip_u16 vzipq_u16\n-    #define npyv_zip_s16 vzipq_s16\n-    #define npyv_zip_u32 vzipq_u32\n-    #define npyv_zip_s32 vzipq_s32\n-    #define npyv_zip_f32 vzipq_f32\n+    #define NPYV_IMPL_NEON_ZIP(T_VEC, SFX)                       \\\n+        NPY_FINLINE T_VEC##x2 npyv_zip_##SFX(T_VEC a, T_VEC b)   \\\n+        { return vzipq_##SFX(a, b); }                            \\\n+        NPY_FINLINE T_VEC##x2 npyv_unzip_##SFX(T_VEC a, T_VEC b) \\\n+        { return vuzpq_##SFX(a, b); }\n #endif\n+\n+NPYV_IMPL_NEON_ZIP(npyv_u8,  u8)\n+NPYV_IMPL_NEON_ZIP(npyv_s8,  s8)\n+NPYV_IMPL_NEON_ZIP(npyv_u16, u16)\n+NPYV_IMPL_NEON_ZIP(npyv_s16, s16)\n+NPYV_IMPL_NEON_ZIP(npyv_u32, u32)\n+NPYV_IMPL_NEON_ZIP(npyv_s32, s32)\n+NPYV_IMPL_NEON_ZIP(npyv_f32, f32)\n+\n #define npyv_zip_u64 npyv_combine_u64\n #define npyv_zip_s64 npyv_combine_s64\n+#define npyv_zip_f64 npyv_combine_f64\n+#define npyv_unzip_u64 npyv_combine_u64\n+#define npyv_unzip_s64 npyv_combine_s64\n+#define npyv_unzip_f64 npyv_combine_f64\n \n // Reverse elements of each 64-bit lane\n #define npyv_rev64_u8  vrev64q_u8\n@@ -116,4 +125,65 @@ NPYV_IMPL_NEON_COMBINE(npyv_f64, f64)\n #define npyv_rev64_s32 vrev64q_s32\n #define npyv_rev64_f32 vrev64q_f32\n \n+// Permuting the elements of each 128-bit lane by immediate index for\n+// each element.\n+#ifdef __clang__\n+    #define npyv_permi128_u32(A, E0, E1, E2, E3) \\\n+        __builtin_shufflevector(A, A, E0, E1, E2, E3)\n+#elif defined(__GNUC__)\n+    #define npyv_permi128_u32(A, E0, E1, E2, E3) \\\n+        __builtin_shuffle(A, npyv_set_u32(E0, E1, E2, E3))\n+#else\n+    #define npyv_permi128_u32(A, E0, E1, E2, E3)          \\\n+        npyv_set_u32(                                     \\\n+            vgetq_lane_u32(A, E0), vgetq_lane_u32(A, E1), \\\n+            vgetq_lane_u32(A, E2), vgetq_lane_u32(A, E3)  \\\n+        )\n+    #define npyv_permi128_s32(A, E0, E1, E2, E3)          \\\n+        npyv_set_s32(                                     \\\n+            vgetq_lane_s32(A, E0), vgetq_lane_s32(A, E1), \\\n+            vgetq_lane_s32(A, E2), vgetq_lane_s32(A, E3)  \\\n+        )\n+    #define npyv_permi128_f32(A, E0, E1, E2, E3)          \\\n+        npyv_set_f32(                                     \\\n+            vgetq_lane_f32(A, E0), vgetq_lane_f32(A, E1), \\\n+            vgetq_lane_f32(A, E2), vgetq_lane_f32(A, E3)  \\\n+        )\n+#endif\n+\n+#if defined(__clang__) || defined(__GNUC__)\n+    #define npyv_permi128_s32 npyv_permi128_u32\n+    #define npyv_permi128_f32 npyv_permi128_u32\n+#endif\n+\n+#ifdef __clang__\n+    #define npyv_permi128_u64(A, E0, E1) \\\n+        __builtin_shufflevector(A, A, E0, E1)\n+#elif defined(__GNUC__)\n+    #define npyv_permi128_u64(A, E0, E1) \\\n+        __builtin_shuffle(A, npyv_set_u64(E0, E1))\n+#else\n+    #define npyv_permi128_u64(A, E0, E1)                  \\\n+        npyv_set_u64(                                     \\\n+            vgetq_lane_u64(A, E0), vgetq_lane_u64(A, E1)  \\\n+        )\n+    #define npyv_permi128_s64(A, E0, E1)                  \\\n+        npyv_set_s64(                                     \\\n+            vgetq_lane_s64(A, E0), vgetq_lane_s64(A, E1)  \\\n+        )\n+    #define npyv_permi128_f64(A, E0, E1)                  \\\n+        npyv_set_f64(                                     \\\n+            vgetq_lane_f64(A, E0), vgetq_lane_f64(A, E1)  \\\n+        )\n+#endif\n+\n+#if defined(__clang__) || defined(__GNUC__)\n+    #define npyv_permi128_s64 npyv_permi128_u64\n+    #define npyv_permi128_f64 npyv_permi128_u64\n+#endif\n+\n+#if !NPY_SIMD_F64\n+    #undef npyv_permi128_f64\n+#endif\n+\n #endif // _NPY_SIMD_NEON_REORDER_H"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/arithmetic.h",
                "patch": "@@ -282,6 +282,10 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     // negate multiply and subtract, -(a*b) - c\n     #define npyv_nmulsub_f32 _mm_fnmsub_ps\n     #define npyv_nmulsub_f64 _mm_fnmsub_pd\n+    // multiply, add for odd elements and subtract even elements.\n+    // (a * b) -+ c\n+    #define npyv_muladdsub_f32 _mm_fmaddsub_ps\n+    #define npyv_muladdsub_f64 _mm_fmaddsub_pd\n #elif defined(NPY_HAVE_FMA4)\n     // multiply and add, a*b + c\n     #define npyv_muladd_f32 _mm_macc_ps\n@@ -292,6 +296,10 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     // negate multiply and add, -(a*b) + c\n     #define npyv_nmuladd_f32 _mm_nmacc_ps\n     #define npyv_nmuladd_f64 _mm_nmacc_pd\n+    // multiply, add for odd elements and subtract even elements.\n+    // (a * b) -+ c\n+    #define npyv_muladdsub_f32 _mm_maddsub_ps\n+    #define npyv_muladdsub_f64 _mm_maddsub_pd\n #else\n     // multiply and add, a*b + c\n     NPY_FINLINE npyv_f32 npyv_muladd_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n@@ -308,6 +316,28 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     { return npyv_sub_f32(c, npyv_mul_f32(a, b)); }\n     NPY_FINLINE npyv_f64 npyv_nmuladd_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n     { return npyv_sub_f64(c, npyv_mul_f64(a, b)); }\n+    // multiply, add for odd elements and subtract even elements.\n+    // (a * b) -+ c\n+    NPY_FINLINE npyv_f32 npyv_muladdsub_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n+    {\n+        npyv_f32 m = npyv_mul_f32(a, b);\n+    #if NPY_HAVE_SSE3\n+        return _mm_addsub_ps(m, c);\n+    #else\n+        const npyv_f32 msign = npyv_set_f32(-0.0f, 0.0f, -0.0f, 0.0f);\n+        return npyv_add_f32(m, npyv_xor_f32(msign, c));\n+    #endif\n+    }\n+    NPY_FINLINE npyv_f64 npyv_muladdsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+    {\n+        npyv_f64 m = npyv_mul_f64(a, b);\n+    #if NPY_HAVE_SSE3\n+        return _mm_addsub_pd(m, c);\n+    #else\n+        const npyv_f64 msign = npyv_set_f64(-0.0, 0.0);\n+        return npyv_add_f64(m, npyv_xor_f64(msign, c));\n+    #endif\n+    }\n #endif // NPY_HAVE_FMA3\n #ifndef NPY_HAVE_FMA3 // for FMA4 and NON-FMA3\n     // negate multiply and subtract, -(a*b) - c"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/memory.h",
                "patch": "@@ -103,6 +103,28 @@ NPY_FINLINE npyv_u64 npyv_loadn_u64(const npy_uint64 *ptr, npy_intp stride)\n { return _mm_castpd_si128(npyv_loadn_f64((const double*)ptr, stride)); }\n NPY_FINLINE npyv_s64 npyv_loadn_s64(const npy_int64 *ptr, npy_intp stride)\n { return _mm_castpd_si128(npyv_loadn_f64((const double*)ptr, stride)); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_f32 npyv_loadn2_f32(const float *ptr, npy_intp stride)\n+{\n+    __m128d r = _mm_loadh_pd(\n+        npyv_loadl_f64((const double*)ptr), (const double*)(ptr + stride)\n+    );\n+    return _mm_castpd_ps(r);\n+}\n+NPY_FINLINE npyv_u32 npyv_loadn2_u32(const npy_uint32 *ptr, npy_intp stride)\n+{ return _mm_castps_si128(npyv_loadn2_f32((const float*)ptr, stride)); }\n+NPY_FINLINE npyv_s32 npyv_loadn2_s32(const npy_int32 *ptr, npy_intp stride)\n+{ return _mm_castps_si128(npyv_loadn2_f32((const float*)ptr, stride)); }\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_f64 npyv_loadn2_f64(const double *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_f64(ptr); }\n+NPY_FINLINE npyv_u64 npyv_loadn2_u64(const npy_uint64 *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_u64(ptr); }\n+NPY_FINLINE npyv_s64 npyv_loadn2_s64(const npy_int64 *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_s64(ptr); }\n+\n /***************************\n  * Non-contiguous Store\n  ***************************/\n@@ -135,6 +157,24 @@ NPY_FINLINE void npyv_storen_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n NPY_FINLINE void npyv_storen_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n { npyv_storen_f64((double*)ptr, stride, _mm_castsi128_pd(a)); }\n \n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_u32(npy_uint32 *ptr, npy_intp stride, npyv_u32 a)\n+{\n+    _mm_storel_pd((double*)ptr, _mm_castsi128_pd(a));\n+    _mm_storeh_pd((double*)(ptr + stride), _mm_castsi128_pd(a));\n+}\n+NPY_FINLINE void npyv_storen2_s32(npy_int32 *ptr, npy_intp stride, npyv_s32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, a); }\n+NPY_FINLINE void npyv_storen2_f32(float *ptr, npy_intp stride, npyv_f32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, _mm_castps_si128(a)); }\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n+{ (void)stride; npyv_store_u64(ptr, a); }\n+NPY_FINLINE void npyv_storen2_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n+{ (void)stride; npyv_store_s64(ptr, a); }\n+NPY_FINLINE void npyv_storen2_f64(double *ptr, npy_intp stride, npyv_f64 a)\n+{ (void)stride; npyv_store_f64(ptr, a); }\n /*********************************\n  * Partial Load\n  *********************************/\n@@ -246,6 +286,32 @@ NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n     }\n     return npyv_load_s64(ptr);\n }\n+\n+//// 64-bit nlane\n+NPY_FINLINE npyv_s32 npyv_load2_till_s32(const npy_int32 *ptr, npy_uintp nlane,\n+                                          npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        const __m128i vfill = npyv_set_s32(fill_lo, fill_hi, fill_lo, fill_hi);\n+        return _mm_castpd_si128(\n+            _mm_loadl_pd(_mm_castsi128_pd(vfill), (double*)ptr)\n+        );\n+    }\n+    return npyv_load_s32(ptr);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_load2_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n+{ return npyv_load_tillz_s64((const npy_int64*)ptr, nlane); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE npyv_s64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n+                                           npy_int64 fill_lo, npy_int64 fill_hi)\n+{ (void)nlane; (void)fill_lo; (void)fill_hi; return npyv_load_s64(ptr); }\n+\n+NPY_FINLINE npyv_s64 npyv_load2_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n+{ (void)nlane; return npyv_load_s64(ptr); }\n+\n /*********************************\n  * Non-contiguous partial load\n  *********************************/\n@@ -358,6 +424,37 @@ NPY_FINLINE npyv_s64 npyv_loadn_tillz_s64(const npy_int64 *ptr, npy_intp stride,\n     }\n     return npyv_loadn_s64(ptr, stride);\n }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_s32 npyv_loadn2_till_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                 npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        const __m128i vfill = npyv_set_s32(0, 0, fill_lo, fill_hi);\n+        return _mm_castpd_si128(\n+            _mm_loadl_pd(_mm_castsi128_pd(vfill), (double*)ptr)\n+        );\n+    }\n+    return npyv_loadn2_s32(ptr, stride);\n+}\n+NPY_FINLINE npyv_s32 npyv_loadn2_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        return _mm_loadl_epi64((const __m128i*)ptr);\n+    }\n+    return npyv_loadn2_s32(ptr, stride);\n+}\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                  npy_int64 fill_lo, npy_int64 fill_hi)\n+{ assert(nlane > 0); (void)stride; (void)nlane; (void)fill_lo; (void)fill_hi; return npyv_load_s64(ptr); }\n+\n+NPY_FINLINE npyv_s64 npyv_loadn2_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n+{ assert(nlane > 0); (void)stride; (void)nlane; return npyv_load_s64(ptr); }\n+\n /*********************************\n  * Partial store\n  *********************************/\n@@ -394,32 +491,53 @@ NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a\n     }\n     npyv_store_s64(ptr, a);\n }\n+//// 64-bit nlane\n+NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n+{ npyv_store_till_s64((npy_int64*)ptr, nlane, a); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0); (void)nlane;\n+    npyv_store_s64(ptr, a);\n+}\n+\n /*********************************\n  * Non-contiguous partial store\n  *********************************/\n //// 32\n NPY_FINLINE void npyv_storen_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n {\n     assert(nlane > 0);\n+    ptr[stride*0] = _mm_cvtsi128_si32(a);\n     switch(nlane) {\n+    case 1:\n+        return;\n #ifdef NPY_HAVE_SSE41\n-    default:\n-        ptr[stride*3] = _mm_extract_epi32(a, 3);\n+    case 2:\n+        ptr[stride*1] = _mm_extract_epi32(a, 1);\n+        return;\n     case 3:\n+        ptr[stride*1] = _mm_extract_epi32(a, 1);\n         ptr[stride*2] = _mm_extract_epi32(a, 2);\n-    case 2:\n+        return;\n+    default:\n         ptr[stride*1] = _mm_extract_epi32(a, 1);\n+        ptr[stride*2] = _mm_extract_epi32(a, 2);\n+        ptr[stride*3] = _mm_extract_epi32(a, 3);\n #else\n-    default:\n-        ptr[stride*3] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 3)));\n+    case 2:\n+        ptr[stride*1] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 1)));\n+        return;\n     case 3:\n+        ptr[stride*1] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 1)));\n         ptr[stride*2] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 2)));\n-    case 2:\n+        return;\n+    default:\n         ptr[stride*1] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 1)));\n+        ptr[stride*2] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 2)));\n+        ptr[stride*3] = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 3)));\n #endif\n-    case 1:\n-        ptr[stride*0] = _mm_cvtsi128_si32(a);\n-        break;\n     }\n }\n //// 64\n@@ -432,6 +550,21 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n     }\n     npyv_storen_s64(ptr, stride, a);\n }\n+\n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+    npyv_storel_s32(ptr, a);\n+    if (nlane > 1) {\n+        npyv_storeh_s32(ptr + stride, a);\n+    }\n+}\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npyv_s64 a)\n+{ assert(nlane > 0); (void)stride; (void)nlane; npyv_store_s64(ptr, a); }\n+\n /*****************************************************************\n  * Implement partial load/store for u32/f32/u64/f64... via casting\n  *****************************************************************/\n@@ -442,7 +575,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load_till_##T_SFX(                   \\\n             (const npyv_lanetype_##T_SFX *)ptr, nlane, pun.to_##T_SFX                       \\\n         ));                                                                                 \\\n@@ -454,7 +588,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn_till_##T_SFX(                  \\\n             (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun.to_##T_SFX               \\\n         ));                                                                                 \\\n@@ -495,6 +630,110 @@ NPYV_IMPL_SSE_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_SSE_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_SSE_REST_PARTIAL_TYPES(f64, s64)\n \n+// 128-bit/64-bit stride\n+#define NPYV_IMPL_SSE_REST_PARTIAL_TYPES_PAIR(F_SFX, T_SFX)                                 \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_till_##F_SFX                                        \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane,                                     \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_till_##T_SFX(                  \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane, pun_lo.to_##T_SFX, pun_hi.to_##T_SFX \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_till_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane,                    \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_till_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun_lo.to_##T_SFX,           \\\n+            pun_hi.to_##T_SFX                                                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_tillz_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane)                                     \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_tillz_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane                                       \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_tillz_##F_SFX                                      \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane)                    \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_tillz_##T_SFX(                \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_store2_till_##F_SFX                                               \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_uintp nlane, npyv_##F_SFX a)                           \\\n+    {                                                                                       \\\n+        npyv_store2_till_##T_SFX(                                                           \\\n+            (npyv_lanetype_##T_SFX *)ptr, nlane,                                            \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_storen2_till_##F_SFX                                              \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane, npyv_##F_SFX a)          \\\n+    {                                                                                       \\\n+        npyv_storen2_till_##T_SFX(                                                          \\\n+            (npyv_lanetype_##T_SFX *)ptr, stride, nlane,                                    \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }\n+\n+NPYV_IMPL_SSE_REST_PARTIAL_TYPES_PAIR(u32, s32)\n+NPYV_IMPL_SSE_REST_PARTIAL_TYPES_PAIR(f32, s32)\n+NPYV_IMPL_SSE_REST_PARTIAL_TYPES_PAIR(u64, s64)\n+NPYV_IMPL_SSE_REST_PARTIAL_TYPES_PAIR(f64, s64)\n+\n+/************************************************************\n+ *  de-interlave load / interleave contiguous store\n+ ************************************************************/\n+// two channels\n+#define NPYV_IMPL_SSE_MEM_INTERLEAVE(SFX, ZSFX)                              \\\n+    NPY_FINLINE npyv_##ZSFX##x2 npyv_zip_##ZSFX(npyv_##ZSFX, npyv_##ZSFX);   \\\n+    NPY_FINLINE npyv_##ZSFX##x2 npyv_unzip_##ZSFX(npyv_##ZSFX, npyv_##ZSFX); \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_load_##SFX##x2(                          \\\n+        const npyv_lanetype_##SFX *ptr                                       \\\n+    ) {                                                                      \\\n+        return npyv_unzip_##ZSFX(                                            \\\n+            npyv_load_##SFX(ptr), npyv_load_##SFX(ptr+npyv_nlanes_##SFX)     \\\n+        );                                                                   \\\n+    }                                                                        \\\n+    NPY_FINLINE void npyv_store_##SFX##x2(                                   \\\n+        npyv_lanetype_##SFX *ptr, npyv_##SFX##x2 v                           \\\n+    ) {                                                                      \\\n+        npyv_##SFX##x2 zip = npyv_zip_##ZSFX(v.val[0], v.val[1]);            \\\n+        npyv_store_##SFX(ptr, zip.val[0]);                                   \\\n+        npyv_store_##SFX(ptr + npyv_nlanes_##SFX, zip.val[1]);               \\\n+    }\n+\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(u8, u8)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(s8, u8)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(u16, u16)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(s16, u16)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(u32, u32)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(s32, u32)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(u64, u64)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(s64, u64)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(f32, f32)\n+NPYV_IMPL_SSE_MEM_INTERLEAVE(f64, f64)\n+\n /*********************************\n  * Lookup table\n  *********************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/reorder.h",
                "patch": "@@ -81,6 +81,75 @@ NPYV_IMPL_SSE_ZIP(npyv_s64, s64, epi64)\n NPYV_IMPL_SSE_ZIP(npyv_f32, f32, ps)\n NPYV_IMPL_SSE_ZIP(npyv_f64, f64, pd)\n \n+// deinterleave two vectors\n+NPY_FINLINE npyv_u8x2 npyv_unzip_u8(npyv_u8 ab0, npyv_u8 ab1)\n+{\n+#ifdef NPY_HAVE_SSSE3\n+    const __m128i idx = _mm_setr_epi8(\n+        0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15\n+    );\n+    __m128i abl = _mm_shuffle_epi8(ab0, idx);\n+    __m128i abh = _mm_shuffle_epi8(ab1, idx);\n+    return npyv_combine_u8(abl, abh);\n+#else\n+    __m128i ab_083b = _mm_unpacklo_epi8(ab0, ab1);\n+    __m128i ab_4c6e = _mm_unpackhi_epi8(ab0, ab1);\n+    __m128i ab_048c = _mm_unpacklo_epi8(ab_083b, ab_4c6e);\n+    __m128i ab_36be = _mm_unpackhi_epi8(ab_083b, ab_4c6e);\n+    __m128i ab_0346 = _mm_unpacklo_epi8(ab_048c, ab_36be);\n+    __m128i ab_8bc8 = _mm_unpackhi_epi8(ab_048c, ab_36be);\n+    npyv_u8x2 r;\n+    r.val[0] = _mm_unpacklo_epi8(ab_0346, ab_8bc8);\n+    r.val[1] = _mm_unpackhi_epi8(ab_0346, ab_8bc8);\n+    return r;\n+#endif\n+}\n+#define npyv_unzip_s8 npyv_unzip_u8\n+\n+NPY_FINLINE npyv_u16x2 npyv_unzip_u16(npyv_u16 ab0, npyv_u16 ab1)\n+{\n+#ifdef NPY_HAVE_SSSE3\n+    const __m128i idx = _mm_setr_epi8(\n+        0,1, 4,5, 8,9, 12,13, 2,3, 6,7, 10,11, 14,15\n+    );\n+    __m128i abl = _mm_shuffle_epi8(ab0, idx);\n+    __m128i abh = _mm_shuffle_epi8(ab1, idx);\n+    return npyv_combine_u16(abl, abh);\n+#else\n+    __m128i ab_0415 = _mm_unpacklo_epi16(ab0, ab1);\n+    __m128i ab_263f = _mm_unpackhi_epi16(ab0, ab1);\n+    __m128i ab_0246 = _mm_unpacklo_epi16(ab_0415, ab_263f);\n+    __m128i ab_135f = _mm_unpackhi_epi16(ab_0415, ab_263f);\n+    npyv_u16x2 r;\n+    r.val[0] = _mm_unpacklo_epi16(ab_0246, ab_135f);\n+    r.val[1] = _mm_unpackhi_epi16(ab_0246, ab_135f);\n+    return r;\n+#endif\n+}\n+#define npyv_unzip_s16 npyv_unzip_u16\n+\n+NPY_FINLINE npyv_u32x2 npyv_unzip_u32(npyv_u32 ab0, npyv_u32 ab1)\n+{\n+    __m128i abl = _mm_shuffle_epi32(ab0, _MM_SHUFFLE(3, 1, 2, 0));\n+    __m128i abh = _mm_shuffle_epi32(ab1, _MM_SHUFFLE(3, 1, 2, 0));\n+    return npyv_combine_u32(abl, abh);\n+}\n+#define npyv_unzip_s32 npyv_unzip_u32\n+\n+NPY_FINLINE npyv_u64x2 npyv_unzip_u64(npyv_u64 ab0, npyv_u64 ab1)\n+{ return npyv_combine_u64(ab0, ab1); }\n+#define npyv_unzip_s64 npyv_unzip_u64\n+\n+NPY_FINLINE npyv_f32x2 npyv_unzip_f32(npyv_f32 ab0, npyv_f32 ab1)\n+{\n+    npyv_f32x2 r;\n+    r.val[0] = _mm_shuffle_ps(ab0, ab1, _MM_SHUFFLE(2, 0, 2, 0));\n+    r.val[1] = _mm_shuffle_ps(ab0, ab1, _MM_SHUFFLE(3, 1, 3, 1));\n+    return r;\n+}\n+NPY_FINLINE npyv_f64x2 npyv_unzip_f64(npyv_f64 ab0, npyv_f64 ab1)\n+{ return npyv_combine_f64(ab0, ab1); }\n+\n // Reverse elements of each 64-bit lane\n NPY_FINLINE npyv_u16 npyv_rev64_u16(npyv_u16 a)\n {\n@@ -122,4 +191,22 @@ NPY_FINLINE npyv_f32 npyv_rev64_f32(npyv_f32 a)\n     return _mm_shuffle_ps(a, a, _MM_SHUFFLE(2, 3, 0, 1));\n }\n \n+// Permuting the elements of each 128-bit lane by immediate index for\n+// each element.\n+#define npyv_permi128_u32(A, E0, E1, E2, E3) \\\n+    _mm_shuffle_epi32(A, _MM_SHUFFLE(E3, E2, E1, E0))\n+\n+#define npyv_permi128_s32 npyv_permi128_u32\n+\n+#define npyv_permi128_u64(A, E0, E1) \\\n+    _mm_shuffle_epi32(A, _MM_SHUFFLE(((E1)<<1)+1, ((E1)<<1), ((E0)<<1)+1, ((E0)<<1)))\n+\n+#define npyv_permi128_s64 npyv_permi128_u64\n+\n+#define npyv_permi128_f32(A, E0, E1, E2, E3) \\\n+    _mm_shuffle_ps(A, A, _MM_SHUFFLE(E3, E2, E1, E0))\n+\n+#define npyv_permi128_f64(A, E0, E1) \\\n+    _mm_shuffle_pd(A, A, _MM_SHUFFLE2(E1, E0))\n+\n #endif // _NPY_SIMD_SSE_REORDER_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/arithmetic.h",
                "patch": "@@ -322,6 +322,20 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     NPY_FINLINE npyv_f64 npyv_nmulsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n     { return vec_neg(vec_madd(a, b, c)); }\n #endif\n+// multiply, add for odd elements and subtract even elements.\n+// (a * b) -+ c\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_f32 npyv_muladdsub_f32(npyv_f32 a, npyv_f32 b, npyv_f32 c)\n+{\n+    const npyv_f32 msign = npyv_set_f32(-0.0f, 0.0f, -0.0f, 0.0f);\n+    return npyv_muladd_f32(a, b, npyv_xor_f32(msign, c));\n+}\n+#endif\n+NPY_FINLINE npyv_f64 npyv_muladdsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+{\n+    const npyv_f64 msign = npyv_set_f64(-0.0, 0.0);\n+    return npyv_muladd_f64(a, b, npyv_xor_f64(msign, c));\n+}\n /***************************\n  * Summation\n  ***************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/memory.h",
                "patch": "@@ -136,6 +136,24 @@ NPY_FINLINE npyv_s64 npyv_loadn_s64(const npy_int64 *ptr, npy_intp stride)\n { return npyv_set_s64(ptr[0], ptr[stride]); }\n NPY_FINLINE npyv_f64 npyv_loadn_f64(const double *ptr, npy_intp stride)\n { return npyv_set_f64(ptr[0], ptr[stride]); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_u32 npyv_loadn2_u32(const npy_uint32 *ptr, npy_intp stride)\n+{ return (npyv_u32)npyv_set_u64(*(npy_uint64*)ptr, *(npy_uint64*)(ptr + stride)); }\n+NPY_FINLINE npyv_s32 npyv_loadn2_s32(const npy_int32 *ptr, npy_intp stride)\n+{ return (npyv_s32)npyv_set_u64(*(npy_uint64*)ptr, *(npy_uint64*)(ptr + stride)); }\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_f32 npyv_loadn2_f32(const float *ptr, npy_intp stride)\n+{ return (npyv_f32)npyv_set_u64(*(npy_uint64*)ptr, *(npy_uint64*)(ptr + stride)); }\n+#endif\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_u64 npyv_loadn2_u64(const npy_uint64 *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_u64(ptr); }\n+NPY_FINLINE npyv_s64 npyv_loadn2_s64(const npy_int64 *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_s64(ptr); }\n+NPY_FINLINE npyv_f64 npyv_loadn2_f64(const double *ptr, npy_intp stride)\n+{ (void)stride; return npyv_load_f64(ptr); }\n+\n /***************************\n  * Non-contiguous Store\n  ***************************/\n@@ -164,6 +182,26 @@ NPY_FINLINE void npyv_storen_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n NPY_FINLINE void npyv_storen_f64(double *ptr, npy_intp stride, npyv_f64 a)\n { npyv_storen_u64((npy_uint64*)ptr, stride, (npyv_u64)a); }\n \n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_u32(npy_uint32 *ptr, npy_intp stride, npyv_u32 a)\n+{\n+    *(npy_uint64*)ptr = vec_extract((npyv_u64)a, 0);\n+    *(npy_uint64*)(ptr + stride) = vec_extract((npyv_u64)a, 1);\n+}\n+NPY_FINLINE void npyv_storen2_s32(npy_int32 *ptr, npy_intp stride, npyv_s32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, (npyv_u32)a); }\n+#if NPY_SIMD_F32\n+NPY_FINLINE void npyv_storen2_f32(float *ptr, npy_intp stride, npyv_f32 a)\n+{ npyv_storen2_u32((npy_uint32*)ptr, stride, (npyv_u32)a); }\n+#endif\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n+{ (void)stride; npyv_store_u64(ptr, a); }\n+NPY_FINLINE void npyv_storen2_s64(npy_int64 *ptr, npy_intp stride, npyv_s64 a)\n+{ (void)stride; npyv_store_s64(ptr, a); }\n+NPY_FINLINE void npyv_storen2_f64(double *ptr, npy_intp stride, npyv_f64 a)\n+{ (void)stride; npyv_store_f64(ptr, a); }\n+\n /*********************************\n  * Partial Load\n  *********************************/\n@@ -226,6 +264,27 @@ NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n     return npyv_load_till_s64(ptr, nlane, 0);\n #endif\n }\n+//// 64-bit nlane\n+NPY_FINLINE npyv_s32 npyv_load2_till_s32(const npy_int32 *ptr, npy_uintp nlane,\n+                                          npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        return npyv_set_s32(ptr[0], ptr[1], fill_lo, fill_hi);\n+    }\n+    return npyv_load_s32(ptr);\n+}\n+// fill zero to rest lanes\n+NPY_FINLINE npyv_s32 npyv_load2_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n+{ return (npyv_s32)npyv_load_tillz_s64((const npy_int64*)ptr, nlane); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE npyv_s64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n+                                           npy_int64 fill_lo, npy_int64 fill_hi)\n+{ (void)nlane; (void)fill_lo; (void)fill_hi; return npyv_load_s64(ptr); }\n+\n+NPY_FINLINE npyv_s64 npyv_load2_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n+{ (void)nlane; return npyv_load_s64(ptr); }\n /*********************************\n  * Non-contiguous partial load\n  *********************************/\n@@ -265,6 +324,34 @@ npyv_loadn_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npy_\n // fill zero to rest lanes\n NPY_FINLINE npyv_s64 npyv_loadn_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n { return npyv_loadn_till_s64(ptr, stride, nlane, 0); }\n+\n+//// 64-bit load over 32-bit stride\n+NPY_FINLINE npyv_s32 npyv_loadn2_till_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                 npy_int32 fill_lo, npy_int32 fill_hi)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        return npyv_set_s32(ptr[0], ptr[1], fill_lo, fill_hi);\n+    }\n+    return npyv_loadn2_s32(ptr, stride);\n+}\n+NPY_FINLINE npyv_s32 npyv_loadn2_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n+{\n+    assert(nlane > 0);\n+    if (nlane == 1) {\n+        return (npyv_s32)npyv_set_s64(*(npy_int64*)ptr, 0);\n+    }\n+    return npyv_loadn2_s32(ptr, stride);\n+}\n+\n+//// 128-bit load over 64-bit stride\n+NPY_FINLINE npyv_s64 npyv_loadn2_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane,\n+                                                  npy_int64 fill_lo, npy_int64 fill_hi)\n+{ assert(nlane > 0); (void)stride; (void)nlane; (void)fill_lo; (void)fill_hi; return npyv_load_s64(ptr); }\n+\n+NPY_FINLINE npyv_s64 npyv_loadn2_tillz_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane)\n+{ assert(nlane > 0); (void)stride; (void)nlane; return npyv_load_s64(ptr); }\n+\n /*********************************\n  * Partial store\n  *********************************/\n@@ -307,23 +394,40 @@ NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a\n     npyv_store_s64(ptr, a);\n #endif\n }\n+\n+//// 64-bit nlane\n+NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n+{ npyv_store_till_s64((npy_int64*)ptr, nlane, (npyv_s64)a); }\n+\n+//// 128-bit nlane\n+NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n+{\n+    assert(nlane > 0); (void)nlane;\n+    npyv_store_s64(ptr, a);\n+}\n+\n /*********************************\n  * Non-contiguous partial store\n  *********************************/\n //// 32\n NPY_FINLINE void npyv_storen_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n {\n     assert(nlane > 0);\n+    ptr[stride*0] = vec_extract(a, 0);\n     switch(nlane) {\n-    default:\n-        ptr[stride*3] = vec_extract(a, 3);\n-    case 3:\n-        ptr[stride*2] = vec_extract(a, 2);\n+    case 1:\n+        return;\n     case 2:\n         ptr[stride*1] = vec_extract(a, 1);\n-    case 1:\n-        ptr[stride*0] = vec_extract(a, 0);\n-        break;\n+        return;\n+    case 3:\n+        ptr[stride*1] = vec_extract(a, 1);\n+        ptr[stride*2] = vec_extract(a, 2);\n+        return;\n+    default:\n+         ptr[stride*1] = vec_extract(a, 1);\n+         ptr[stride*2] = vec_extract(a, 2);\n+         ptr[stride*3] = vec_extract(a, 3);\n     }\n }\n //// 64\n@@ -336,6 +440,21 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n     }\n     npyv_storen_s64(ptr, stride, a);\n }\n+\n+//// 64-bit store over 32-bit stride\n+NPY_FINLINE void npyv_storen2_till_s32(npy_int32 *ptr, npy_intp stride, npy_uintp nlane, npyv_s32 a)\n+{\n+    assert(nlane > 0);\n+    npyv_storel_s32(ptr, a);\n+    if (nlane > 1) {\n+        npyv_storeh_s32(ptr + stride, a);\n+    }\n+}\n+\n+//// 128-bit store over 64-bit stride\n+NPY_FINLINE void npyv_storen2_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npyv_s64 a)\n+{ assert(nlane > 0); (void)stride; (void)nlane; npyv_store_s64(ptr, a); }\n+\n /*****************************************************************\n  * Implement partial load/store for u32/f32/u64/f64... via casting\n  *****************************************************************/\n@@ -346,7 +465,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load_till_##T_SFX(                   \\\n             (const npyv_lanetype_##T_SFX *)ptr, nlane, pun.to_##T_SFX                       \\\n         ));                                                                                 \\\n@@ -358,7 +478,8 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         union {                                                                             \\\n             npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n             npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n-        } pun = {.from_##F_SFX = fill};                                                     \\\n+        } pun;                                                                              \\\n+        pun.from_##F_SFX = fill;                                                            \\\n         return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn_till_##T_SFX(                  \\\n             (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun.to_##T_SFX               \\\n         ));                                                                                 \\\n@@ -401,6 +522,114 @@ NPYV_IMPL_VEC_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_VEC_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_VEC_REST_PARTIAL_TYPES(f64, s64)\n \n+// 128-bit/64-bit stride\n+#define NPYV_IMPL_VEC_REST_PARTIAL_TYPES_PAIR(F_SFX, T_SFX)                                 \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_till_##F_SFX                                        \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane,                                     \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_till_##T_SFX(                  \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane, pun_lo.to_##T_SFX, pun_hi.to_##T_SFX \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_till_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane,                    \\\n+     npyv_lanetype_##F_SFX fill_lo, npyv_lanetype_##F_SFX fill_hi)                          \\\n+    {                                                                                       \\\n+        union pun {                                                                         \\\n+            npyv_lanetype_##F_SFX from_##F_SFX;                                             \\\n+            npyv_lanetype_##T_SFX to_##T_SFX;                                               \\\n+        };                                                                                  \\\n+        union pun pun_lo;                                                                   \\\n+        union pun pun_hi;                                                                   \\\n+        pun_lo.from_##F_SFX = fill_lo;                                                      \\\n+        pun_hi.from_##F_SFX = fill_hi;                                                      \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_till_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane, pun_lo.to_##T_SFX,           \\\n+            pun_hi.to_##T_SFX                                                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_load2_tillz_##F_SFX                                       \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane)                                     \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_load2_tillz_##T_SFX(                 \\\n+            (const npyv_lanetype_##T_SFX *)ptr, nlane                                       \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE npyv_##F_SFX npyv_loadn2_tillz_##F_SFX                                      \\\n+    (const npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane)                    \\\n+    {                                                                                       \\\n+        return npyv_reinterpret_##F_SFX##_##T_SFX(npyv_loadn2_tillz_##T_SFX(                \\\n+            (const npyv_lanetype_##T_SFX *)ptr, stride, nlane                               \\\n+        ));                                                                                 \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_store2_till_##F_SFX                                               \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_uintp nlane, npyv_##F_SFX a)                           \\\n+    {                                                                                       \\\n+        npyv_store2_till_##T_SFX(                                                           \\\n+            (npyv_lanetype_##T_SFX *)ptr, nlane,                                            \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }                                                                                       \\\n+    NPY_FINLINE void npyv_storen2_till_##F_SFX                                              \\\n+    (npyv_lanetype_##F_SFX *ptr, npy_intp stride, npy_uintp nlane, npyv_##F_SFX a)          \\\n+    {                                                                                       \\\n+        npyv_storen2_till_##T_SFX(                                                          \\\n+            (npyv_lanetype_##T_SFX *)ptr, stride, nlane,                                    \\\n+            npyv_reinterpret_##T_SFX##_##F_SFX(a)                                           \\\n+        );                                                                                  \\\n+    }\n+\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES_PAIR(u32, s32)\n+#if NPY_SIMD_F32\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES_PAIR(f32, s32)\n+#endif\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES_PAIR(u64, s64)\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES_PAIR(f64, s64)\n+\n+/************************************************************\n+ *  de-interlave load / interleave contiguous store\n+ ************************************************************/\n+// two channels\n+#define NPYV_IMPL_VEC_MEM_INTERLEAVE(SFX)                                \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_zip_##SFX(npyv_##SFX, npyv_##SFX);   \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_unzip_##SFX(npyv_##SFX, npyv_##SFX); \\\n+    NPY_FINLINE npyv_##SFX##x2 npyv_load_##SFX##x2(                      \\\n+        const npyv_lanetype_##SFX *ptr                                   \\\n+    ) {                                                                  \\\n+        return npyv_unzip_##SFX(                                         \\\n+            npyv_load_##SFX(ptr), npyv_load_##SFX(ptr+npyv_nlanes_##SFX) \\\n+        );                                                               \\\n+    }                                                                    \\\n+    NPY_FINLINE void npyv_store_##SFX##x2(                               \\\n+        npyv_lanetype_##SFX *ptr, npyv_##SFX##x2 v                       \\\n+    ) {                                                                  \\\n+        npyv_##SFX##x2 zip = npyv_zip_##SFX(v.val[0], v.val[1]);         \\\n+        npyv_store_##SFX(ptr, zip.val[0]);                               \\\n+        npyv_store_##SFX(ptr + npyv_nlanes_##SFX, zip.val[1]);           \\\n+    }\n+\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(u8)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(s8)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(u16)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(s16)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(u32)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(s32)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(u64)\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(s64)\n+#if NPY_SIMD_F32\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(f32)\n+#endif\n+NPYV_IMPL_VEC_MEM_INTERLEAVE(f64)\n+\n /*********************************\n  * Lookup table\n  *********************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/reorder.h",
                "patch": "@@ -68,6 +68,85 @@ NPYV_IMPL_VEC_COMBINE_ZIP(npyv_s64, s64)\n #endif\n NPYV_IMPL_VEC_COMBINE_ZIP(npyv_f64, f64)\n \n+// deinterleave two vectors\n+NPY_FINLINE npyv_u8x2 npyv_unzip_u8(npyv_u8 ab0, npyv_u8 ab1)\n+{\n+    const npyv_u8 idx_even = npyv_set_u8(\n+        0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30\n+    );\n+    const npyv_u8 idx_odd = npyv_set_u8(\n+        1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31\n+    );\n+    npyv_u8x2 r;\n+    r.val[0] = vec_perm(ab0, ab1, idx_even);\n+    r.val[1] = vec_perm(ab0, ab1, idx_odd);\n+    return r;\n+}\n+NPY_FINLINE npyv_s8x2 npyv_unzip_s8(npyv_s8 ab0, npyv_s8 ab1)\n+{\n+    npyv_u8x2 ru = npyv_unzip_u8((npyv_u8)ab0, (npyv_u8)ab1);\n+    npyv_s8x2 r;\n+    r.val[0] = (npyv_s8)ru.val[0];\n+    r.val[1] = (npyv_s8)ru.val[1];\n+    return r;\n+}\n+NPY_FINLINE npyv_u16x2 npyv_unzip_u16(npyv_u16 ab0, npyv_u16 ab1)\n+{\n+    const npyv_u8 idx_even = npyv_set_u8(\n+        0, 1, 4, 5, 8, 9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29\n+    );\n+    const npyv_u8 idx_odd = npyv_set_u8(\n+        2, 3, 6, 7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31\n+    );\n+    npyv_u16x2 r;\n+    r.val[0] = vec_perm(ab0, ab1, idx_even);\n+    r.val[1] = vec_perm(ab0, ab1, idx_odd);\n+    return r;\n+}\n+NPY_FINLINE npyv_s16x2 npyv_unzip_s16(npyv_s16 ab0, npyv_s16 ab1)\n+{\n+    npyv_u16x2 ru = npyv_unzip_u16((npyv_u16)ab0, (npyv_u16)ab1);\n+    npyv_s16x2 r;\n+    r.val[0] = (npyv_s16)ru.val[0];\n+    r.val[1] = (npyv_s16)ru.val[1];\n+    return r;\n+}\n+NPY_FINLINE npyv_u32x2 npyv_unzip_u32(npyv_u32 ab0, npyv_u32 ab1)\n+{\n+    npyv_u32 m0 = vec_mergeh(ab0, ab1);\n+    npyv_u32 m1 = vec_mergel(ab0, ab1);\n+    npyv_u32 r0 = vec_mergeh(m0, m1);\n+    npyv_u32 r1 = vec_mergel(m0, m1);\n+    npyv_u32x2 r;\n+    r.val[0] = r0;\n+    r.val[1] = r1;\n+    return r;\n+}\n+NPY_FINLINE npyv_s32x2 npyv_unzip_s32(npyv_s32 ab0, npyv_s32 ab1)\n+{\n+    npyv_u32x2 ru = npyv_unzip_u32((npyv_u32)ab0, (npyv_u32)ab1);\n+    npyv_s32x2 r;\n+    r.val[0] = (npyv_s32)ru.val[0];\n+    r.val[1] = (npyv_s32)ru.val[1];\n+    return r;\n+}\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_f32x2 npyv_unzip_f32(npyv_f32 ab0, npyv_f32 ab1)\n+    {\n+        npyv_u32x2 ru = npyv_unzip_u32((npyv_u32)ab0, (npyv_u32)ab1);\n+        npyv_f32x2 r;\n+        r.val[0] = (npyv_f32)ru.val[0];\n+        r.val[1] = (npyv_f32)ru.val[1];\n+        return r;\n+    }\n+#endif\n+NPY_FINLINE npyv_u64x2 npyv_unzip_u64(npyv_u64 ab0, npyv_u64 ab1)\n+{ return npyv_combine_u64(ab0, ab1); }\n+NPY_FINLINE npyv_s64x2 npyv_unzip_s64(npyv_s64 ab0, npyv_s64 ab1)\n+{ return npyv_combine_s64(ab0, ab1); }\n+NPY_FINLINE npyv_f64x2 npyv_unzip_f64(npyv_f64 ab0, npyv_f64 ab1)\n+{ return npyv_combine_f64(ab0, ab1); }\n+\n // Reverse elements of each 64-bit lane\n NPY_FINLINE npyv_u8 npyv_rev64_u8(npyv_u8 a)\n {\n@@ -111,4 +190,24 @@ NPY_FINLINE npyv_s32 npyv_rev64_s32(npyv_s32 a)\n     { return (npyv_f32)npyv_rev64_u32((npyv_u32)a); }\n #endif\n \n+// Permuting the elements of each 128-bit lane by immediate index for\n+// each element.\n+#define npyv_permi128_u32(A, E0, E1, E2, E3)      \\\n+    vec_perm(A, A, npyv_set_u8(                   \\\n+        (E0<<2), (E0<<2)+1, (E0<<2)+2, (E0<<2)+3, \\\n+        (E1<<2), (E1<<2)+1, (E1<<2)+2, (E1<<2)+3, \\\n+        (E2<<2), (E2<<2)+1, (E2<<2)+2, (E2<<2)+3, \\\n+        (E3<<2), (E3<<2)+1, (E3<<2)+2, (E3<<2)+3  \\\n+    ))\n+#define npyv_permi128_s32 npyv_permi128_u32\n+#define npyv_permi128_f32 npyv_permi128_u32\n+\n+#if defined(__IBMC__) || defined(vec_permi)\n+    #define npyv_permi128_u64(A, E0, E1) vec_permi(A, A, ((E0)<<1) | (E1))\n+#else\n+    #define npyv_permi128_u64(A, E0, E1) vec_xxpermdi(A, A, ((E0)<<1) | (E1))\n+#endif\n+#define npyv_permi128_s64 npyv_permi128_u64\n+#define npyv_permi128_f64 npyv_permi128_u64\n+\n #endif // _NPY_SIMD_VEC_REORDER_H"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -13,6 +13,7 @@\n #include \"numpy/npy_math.h\"\n #include \"numpy/halffloat.h\"\n #include \"lowlevel_strided_loops.h\"\n+#include \"loops_utils.h\"\n \n #include \"npy_pycompat.h\"\n \n@@ -44,14 +45,6 @@\n /** Provides the various *_LOOP macros */\n #include \"fast_loop_macros.h\"\n \n-/*\n- * include vectorized functions and dispatchers\n- * this file is safe to include also for generic builds\n- * platform specific instructions are either masked via the proprocessor or\n- * runtime detected\n- */\n-#include \"simd.inc\"\n-\n /******************************************************************************\n  **                          GENERIC FLOAT LOOPS                             **\n  *****************************************************************************/\n@@ -2116,6 +2109,8 @@ NPY_NO_EXPORT void\n }\n /**end repeat1**/\n \n+#if !@SIMD@\n+// CFLOAT & CDOUBLE defined by 'loops_arithm_fp.dispatch.c.src'\n NPY_NO_EXPORT void\n @TYPE@_square(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {\n@@ -2126,6 +2121,7 @@ NPY_NO_EXPORT void\n         ((@ftype@ *)op1)[1] = in1r*in1i + in1i*in1r;\n     }\n }\n+#endif\n \n NPY_NO_EXPORT void\n @TYPE@_reciprocal(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n@@ -2156,6 +2152,8 @@ NPY_NO_EXPORT void\n     }\n }\n \n+#if !@SIMD@\n+// CFLOAT & CDOUBLE defined by 'loops_arithm_fp.dispatch.c.src'\n NPY_NO_EXPORT void\n @TYPE@_conjugate(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)) {\n     UNARY_LOOP {\n@@ -2175,20 +2173,6 @@ NPY_NO_EXPORT void\n         *((@ftype@ *)op1) = npy_hypot@c@(in1r, in1i);\n     }\n }\n-\n-#if @SIMD@ && defined(HAVE_ATTRIBUTE_TARGET_AVX512F)\n-/**begin repeat1\n- * arithmetic\n- * #kind = conjugate, square, absolute#\n- */\n-NPY_NO_EXPORT void\n-@TYPE@_@kind@_avx512f(char **args, const npy_intp *dimensions, const npy_intp *steps, void *func)\n-{\n-    if (!run_unary_avx512f_@kind@_@TYPE@(args, dimensions, steps)) {\n-        @TYPE@_@kind@(args, dimensions, steps, func);\n-    }\n-}\n-/**end repeat1**/\n #endif\n \n NPY_NO_EXPORT void"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -540,7 +540,21 @@ NPY_NO_EXPORT void\n  * #TYPE = CFLOAT, CDOUBLE#\n  */\n /**begin repeat1\n- * #kind = add, subtract, multiply#\n+ * #kind = add, subtract, multiply, conjugate, square#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+   (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data)))\n+/**end repeat1**/\n+/**end repeat**/\n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_unary_complex.dispatch.h\"\n+#endif\n+/**begin repeat\n+ * #TYPE = CFLOAT, CDOUBLE#\n+ */\n+/**begin repeat1\n+ * #kind = absolute#\n  */\n NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data)))\n@@ -609,19 +623,14 @@ C@TYPE@_reciprocal(char **args, npy_intp const *dimensions, npy_intp const *step\n NPY_NO_EXPORT void\n C@TYPE@__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n \n-/**begin repeat1\n- * #isa = , _avx512f#\n- */\n-\n NPY_NO_EXPORT void\n-C@TYPE@_conjugate@isa@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func));\n+C@TYPE@_conjugate(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func));\n \n NPY_NO_EXPORT void\n-C@TYPE@_absolute@isa@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func));\n+C@TYPE@_absolute(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func));\n \n NPY_NO_EXPORT void\n-C@TYPE@_square@isa@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data));\n-/**end repeat1**/\n+C@TYPE@_square(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data));\n \n NPY_NO_EXPORT void\n C@TYPE@__arg(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));"
            },
            {
                "filename": "numpy/core/src/umath/loops_arithm_fp.dispatch.c.src",
                "patch": "@@ -1,6 +1,8 @@\n /*@targets\n  ** $maxopt baseline\n- ** sse2 avx2 avx512f\n+ ** sse2 (avx2 fma3)\n+ ** neon asimd\n+ ** vsx2 vsx3\n  ** vx vxe\n  **/\n #define _UMATHMODULE\n@@ -14,708 +16,293 @@\n // Provides the various *_LOOP macros\n #include \"fast_loop_macros.h\"\n \n-// TODO: replace raw SIMD with NPYV\n+/**\n+ * TODO:\n+ *  - Improve the implementation of SIMD complex absolute,\n+ *    current one kinda slow and it can be optimized by\n+ *    at least avoiding the division and keep sqrt.\n+ *  - Vectorize reductions\n+ *  - Add support for ASIMD/VCMLA through universal intrinics.\n+ */\n+\n //###############################################################################\n //## Real Single/Double precision\n //###############################################################################\n /********************************************************************************\n- ** Defining the SIMD kernels\n+ ** Defining ufunc inner functions\n  ********************************************************************************/\n-#ifdef NPY_HAVE_SSE2\n+\n /**begin repeat\n+ * Float types\n  *  #type = npy_float, npy_double#\n  *  #TYPE = FLOAT, DOUBLE#\n- *  #scalarf = npy_sqrtf, npy_sqrt#\n+ *  #sfx  = f32, f64#\n  *  #c = f, #\n- *  #vtype = __m128, __m128d#\n- *  #vtype256 = __m256, __m256d#\n- *  #vtype512 = __m512, __m512d#\n- *  #vpre = _mm, _mm#\n- *  #vpre256 = _mm256, _mm256#\n- *  #vpre512 = _mm512, _mm512#\n- *  #vsuf = ps, pd#\n- *  #vsufs = ss, sd#\n- *  #nan = NPY_NANF, NPY_NAN#\n- *  #double = 0, 1#\n- *  #cast = _mm_castps_si128, _mm_castpd_si128#\n+ *  #C = F, #\n+ *  #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n /**begin repeat1\n-* Arithmetic\n-* # kind = add, subtract, multiply, divide#\n-* # OP = +, -, *, /#\n-* # VOP = add, sub, mul, div#\n-*/\n-static void\n-sse2_binary_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n+ * Arithmetic\n+ * # kind = add, subtract, multiply, divide#\n+ * # intrin = add, sub, mul, div#\n+ * # OP = +, -, *, /#\n+ * # PW = 1, 0, 0, 0#\n+ * # is_div = 0*3, 1#\n+ * # is_mul = 0*2, 1, 0#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-#ifdef NPY_HAVE_AVX512F\n-    const npy_intp vector_size_bytes = 64;\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[i] @OP@ ip2[i];\n-    /* lots of specializations, to squeeze out max performance */\n-    if (npy_is_aligned(&ip1[i], vector_size_bytes) && npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        if (ip1 == ip2) {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype512@ a = @vpre512@_load_@vsuf@(&ip1[i]);\n-                @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, a);\n-                @vpre512@_store_@vsuf@(&op[i], c);\n-            }\n-        }\n-        else {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype512@ a = @vpre512@_load_@vsuf@(&ip1[i]);\n-                @vtype512@ b = @vpre512@_load_@vsuf@(&ip2[i]);\n-                @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-                @vpre512@_store_@vsuf@(&op[i], c);\n+    npy_intp len = dimensions[0];\n+    char *src0 = args[0], *src1 = args[1], *dst = args[2];\n+    npy_intp ssrc0 = steps[0], ssrc1 = steps[1], sdst = steps[2];\n+    // reduce\n+    if (ssrc0 == 0 && ssrc0 == sdst && src0 == dst) {\n+    #if @PW@\n+        *((@type@*)src0) @OP@= @TYPE@_pairwise_sum(src1, len, ssrc1);\n+    #else\n+        @type@ acc = *((@type@*)src0);\n+        if (ssrc1 == sizeof(@type@)) {\n+            for (; len > 0; --len, src1 += sizeof(@type@)) {\n+                acc @OP@= *(@type@ *)src1;\n             }\n-        }\n-    }\n-    else if (npy_is_aligned(&ip1[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype512@ a = @vpre512@_load_@vsuf@(&ip1[i]);\n-            @vtype512@ b = @vpre512@_loadu_@vsuf@(&ip2[i]);\n-            @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-            @vpre512@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else if (npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype512@ a = @vpre512@_loadu_@vsuf@(&ip1[i]);\n-            @vtype512@ b = @vpre512@_load_@vsuf@(&ip2[i]);\n-            @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-            @vpre512@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        if (ip1 == ip2) {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype512@ a = @vpre512@_loadu_@vsuf@(&ip1[i]);\n-                @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, a);\n-                @vpre512@_store_@vsuf@(&op[i], c);\n-            }\n-        }\n-        else {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype512@ a = @vpre512@_loadu_@vsuf@(&ip1[i]);\n-                @vtype512@ b = @vpre512@_loadu_@vsuf@(&ip2[i]);\n-                @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-                @vpre512@_store_@vsuf@(&op[i], c);\n-            }\n-        }\n-    }\n-#elif defined NPY_HAVE_AVX2\n-    const npy_intp vector_size_bytes = 32;\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[i] @OP@ ip2[i];\n-    /* lots of specializations, to squeeze out max performance */\n-    if (npy_is_aligned(&ip1[i], vector_size_bytes) &&\n-            npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        if (ip1 == ip2) {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype256@ a = @vpre256@_load_@vsuf@(&ip1[i]);\n-                @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, a);\n-                @vpre256@_store_@vsuf@(&op[i], c);\n-            }\n-        }\n-        else {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype256@ a = @vpre256@_load_@vsuf@(&ip1[i]);\n-                @vtype256@ b = @vpre256@_load_@vsuf@(&ip2[i]);\n-                @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-                @vpre256@_store_@vsuf@(&op[i], c);\n+        } else {\n+            for (; len > 0; --len, src1 += ssrc1) {\n+                acc @OP@= *(@type@ *)src1;\n             }\n         }\n+        *((@type@*)src0) = acc;\n+    #endif\n+        return;\n     }\n-    else if (npy_is_aligned(&ip1[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype256@ a = @vpre256@_load_@vsuf@(&ip1[i]);\n-            @vtype256@ b = @vpre256@_loadu_@vsuf@(&ip2[i]);\n-            @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-            @vpre256@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else if (npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype256@ a = @vpre256@_loadu_@vsuf@(&ip1[i]);\n-            @vtype256@ b = @vpre256@_load_@vsuf@(&ip2[i]);\n-            @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-            @vpre256@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        if (ip1 == ip2) {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype256@ a = @vpre256@_loadu_@vsuf@(&ip1[i]);\n-                @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, a);\n-                @vpre256@_store_@vsuf@(&op[i], c);\n+#if @VECTOR@\n+    if (len > npyv_nlanes_@sfx@*2 &&\n+        !is_mem_overlap(src0, ssrc0, dst, sdst, len) &&\n+        !is_mem_overlap(src1, ssrc1, dst, sdst, len)\n+    ) {\n+        const int vstep = npyv_nlanes_u8;\n+        const int wstep = vstep * 2;\n+        const int hstep = npyv_nlanes_@sfx@;\n+        const int lstep = hstep * 2;\n+        // lots of specializations, to squeeze out max performance\n+        if (ssrc0 == sizeof(@type@) && ssrc0 == ssrc1 && ssrc0 == sdst) {\n+            for (; len >= lstep; len -= lstep, src0 += wstep, src1 += wstep, dst += wstep) {\n+                npyv_@sfx@ a0 = npyv_load_@sfx@((const @type@*)src0);\n+                npyv_@sfx@ a1 = npyv_load_@sfx@((const @type@*)(src0 + vstep));\n+                npyv_@sfx@ b0 = npyv_load_@sfx@((const @type@*)src1);\n+                npyv_@sfx@ b1 = npyv_load_@sfx@((const @type@*)(src1 + vstep));\n+                npyv_@sfx@ r0 = npyv_@intrin@_@sfx@(a0, b0);\n+                npyv_@sfx@ r1 = npyv_@intrin@_@sfx@(a1, b1);\n+                npyv_store_@sfx@((@type@*)dst, r0);\n+                npyv_store_@sfx@((@type@*)(dst + vstep), r1);\n             }\n-        }\n-        else {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype256@ a = @vpre256@_loadu_@vsuf@(&ip1[i]);\n-                @vtype256@ b = @vpre256@_loadu_@vsuf@(&ip2[i]);\n-                @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-                @vpre256@_store_@vsuf@(&op[i], c);\n+            for (; len > 0; len -= hstep, src0 += vstep, src1 += vstep, dst += vstep) {\n+            #if @is_div@\n+                npyv_@sfx@ a = npyv_load_till_@sfx@((const @type@*)src0, len, 1.0@c@);\n+                npyv_@sfx@ b = npyv_load_till_@sfx@((const @type@*)src1, len, 1.0@c@);\n+            #else\n+                npyv_@sfx@ a = npyv_load_tillz_@sfx@((const @type@*)src0, len);\n+                npyv_@sfx@ b = npyv_load_tillz_@sfx@((const @type@*)src1, len);\n+            #endif\n+                npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n+                npyv_store_till_@sfx@((@type@*)dst, len, r);\n             }\n         }\n-    }\n-#else\n-    const npy_intp vector_size_bytes = 16;\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[i] @OP@ ip2[i];\n-    /* lots of specializations, to squeeze out max performance */\n-    if (npy_is_aligned(&ip1[i], vector_size_bytes) &&\n-            npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        if (ip1 == ip2) {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype@ a = @vpre@_load_@vsuf@(&ip1[i]);\n-                @vtype@ c = @vpre@_@VOP@_@vsuf@(a, a);\n-                @vpre@_store_@vsuf@(&op[i], c);\n+        else if (ssrc0 == 0 && ssrc1 == sizeof(@type@) && sdst == ssrc1) {\n+            npyv_@sfx@ a = npyv_setall_@sfx@(*((@type@*)src0));\n+            for (; len >= lstep; len -= lstep, src1 += wstep, dst += wstep) {\n+                npyv_@sfx@ b0 = npyv_load_@sfx@((const @type@*)src1);\n+                npyv_@sfx@ b1 = npyv_load_@sfx@((const @type@*)(src1 + vstep));\n+                npyv_@sfx@ r0 = npyv_@intrin@_@sfx@(a, b0);\n+                npyv_@sfx@ r1 = npyv_@intrin@_@sfx@(a, b1);\n+                npyv_store_@sfx@((@type@*)dst, r0);\n+                npyv_store_@sfx@((@type@*)(dst + vstep), r1);\n             }\n-        }\n-        else {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype@ a = @vpre@_load_@vsuf@(&ip1[i]);\n-                @vtype@ b = @vpre@_load_@vsuf@(&ip2[i]);\n-                @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-                @vpre@_store_@vsuf@(&op[i], c);\n+            for (; len > 0; len -= hstep, src1 += vstep, dst += vstep) {\n+            #if @is_div@ || @is_mul@\n+                npyv_@sfx@ b = npyv_load_till_@sfx@((const @type@*)src1, len, 1.0@c@);\n+            #else\n+                npyv_@sfx@ b = npyv_load_tillz_@sfx@((const @type@*)src1, len);\n+            #endif\n+                npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n+                npyv_store_till_@sfx@((@type@*)dst, len, r);\n             }\n         }\n-    }\n-    else if (npy_is_aligned(&ip1[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype@ a = @vpre@_load_@vsuf@(&ip1[i]);\n-            @vtype@ b = @vpre@_loadu_@vsuf@(&ip2[i]);\n-            @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-            @vpre@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else if (npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype@ a = @vpre@_loadu_@vsuf@(&ip1[i]);\n-            @vtype@ b = @vpre@_load_@vsuf@(&ip2[i]);\n-            @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-            @vpre@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        if (ip1 == ip2) {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype@ a = @vpre@_loadu_@vsuf@(&ip1[i]);\n-                @vtype@ c = @vpre@_@VOP@_@vsuf@(a, a);\n-                @vpre@_store_@vsuf@(&op[i], c);\n+        else if (ssrc1 == 0 && ssrc0 == sizeof(@type@) && sdst == ssrc0) {\n+            npyv_@sfx@ b = npyv_setall_@sfx@(*((@type@*)src1));\n+            for (; len >= lstep; len -= lstep, src0 += wstep, dst += wstep) {\n+                npyv_@sfx@ a0 = npyv_load_@sfx@((const @type@*)src0);\n+                npyv_@sfx@ a1 = npyv_load_@sfx@((const @type@*)(src0 + vstep));\n+                npyv_@sfx@ r0 = npyv_@intrin@_@sfx@(a0, b);\n+                npyv_@sfx@ r1 = npyv_@intrin@_@sfx@(a1, b);\n+                npyv_store_@sfx@((@type@*)dst, r0);\n+                npyv_store_@sfx@((@type@*)(dst + vstep), r1);\n             }\n-        }\n-        else {\n-            LOOP_BLOCKED(@type@, vector_size_bytes) {\n-                @vtype@ a = @vpre@_loadu_@vsuf@(&ip1[i]);\n-                @vtype@ b = @vpre@_loadu_@vsuf@(&ip2[i]);\n-                @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-                @vpre@_store_@vsuf@(&op[i], c);\n+            for (; len > 0; len -= hstep, src0 += vstep, dst += vstep) {\n+            #if @is_div@ || @is_mul@\n+                npyv_@sfx@ a = npyv_load_till_@sfx@((const @type@*)src0, len, 1.0@c@);\n+            #else\n+                npyv_@sfx@ a = npyv_load_tillz_@sfx@((const @type@*)src0, len);\n+            #endif\n+                npyv_@sfx@ r = npyv_@intrin@_@sfx@(a, b);\n+                npyv_store_till_@sfx@((@type@*)dst, len, r);\n             }\n+        } else {\n+            goto loop_scalar;\n         }\n+        npyv_cleanup();\n+        return;\n     }\n+loop_scalar:\n #endif\n-    LOOP_BLOCKED_END {\n-        op[i] = ip1[i] @OP@ ip2[i];\n-    }\n-}\n-\n-static void\n-sse2_binary_scalar1_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-#ifdef NPY_HAVE_AVX512F\n-    const npy_intp vector_size_bytes = 64;\n-    const @vtype512@ a = @vpre512@_set1_@vsuf@(ip1[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[0] @OP@ ip2[i];\n-    if (npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype512@ b = @vpre512@_load_@vsuf@(&ip2[i]);\n-            @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-            @vpre512@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype512@ b = @vpre512@_loadu_@vsuf@(&ip2[i]);\n-            @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-            @vpre512@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-\n-\n-#elif defined NPY_HAVE_AVX2\n-    const npy_intp vector_size_bytes = 32;\n-    const @vtype256@ a = @vpre256@_set1_@vsuf@(ip1[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[0] @OP@ ip2[i];\n-    if (npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype256@ b = @vpre256@_load_@vsuf@(&ip2[i]);\n-            @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-            @vpre256@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype256@ b = @vpre256@_loadu_@vsuf@(&ip2[i]);\n-            @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-            @vpre256@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-#else\n-    const npy_intp vector_size_bytes = 16;\n-    const @vtype@ a = @vpre@_set1_@vsuf@(ip1[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[0] @OP@ ip2[i];\n-    if (npy_is_aligned(&ip2[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype@ b = @vpre@_load_@vsuf@(&ip2[i]);\n-            @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-            @vpre@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype@ b = @vpre@_loadu_@vsuf@(&ip2[i]);\n-            @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-            @vpre@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-#endif\n-    LOOP_BLOCKED_END {\n-        op[i] = ip1[0] @OP@ ip2[i];\n-    }\n-}\n-\n-static void\n-sse2_binary_scalar2_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-#ifdef NPY_HAVE_AVX512F\n-    const npy_intp vector_size_bytes = 64;\n-    const @vtype512@ b = @vpre512@_set1_@vsuf@(ip2[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[i] @OP@ ip2[0];\n-    if (npy_is_aligned(&ip1[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype512@ a = @vpre512@_load_@vsuf@(&ip1[i]);\n-            @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-            @vpre512@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype512@ a = @vpre512@_loadu_@vsuf@(&ip1[i]);\n-            @vtype512@ c = @vpre512@_@VOP@_@vsuf@(a, b);\n-            @vpre512@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-\n-#elif defined NPY_HAVE_AVX2\n-    const npy_intp vector_size_bytes = 32;\n-    const @vtype256@ b = @vpre256@_set1_@vsuf@(ip2[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[i] @OP@ ip2[0];\n-    if (npy_is_aligned(&ip1[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype256@ a = @vpre256@_load_@vsuf@(&ip1[i]);\n-            @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-            @vpre256@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype256@ a = @vpre256@_loadu_@vsuf@(&ip1[i]);\n-            @vtype256@ c = @vpre256@_@VOP@_@vsuf@(a, b);\n-            @vpre256@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-#else\n-    const npy_intp vector_size_bytes = 16;\n-    const @vtype@ b = @vpre@_set1_@vsuf@(ip2[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, vector_size_bytes)\n-        op[i] = ip1[i] @OP@ ip2[0];\n-    if (npy_is_aligned(&ip1[i], vector_size_bytes)) {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype@ a = @vpre@_load_@vsuf@(&ip1[i]);\n-            @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-            @vpre@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, vector_size_bytes) {\n-            @vtype@ a = @vpre@_loadu_@vsuf@(&ip1[i]);\n-            @vtype@ c = @vpre@_@VOP@_@vsuf@(a, b);\n-            @vpre@_store_@vsuf@(&op[i], c);\n-        }\n-    }\n-#endif\n-    LOOP_BLOCKED_END {\n-        op[i] = ip1[i] @OP@ ip2[0];\n+    for (; len > 0; --len, src0 += ssrc0, src1 += ssrc1, dst += sdst) {\n+        const @type@ a = *((@type@*)src0);\n+        const @type@ b = *((@type@*)src1);\n+        *((@type@*)dst) = a @OP@ b;\n     }\n }\n-\n /**end repeat1**/\n /**end repeat**/\n \n-#else // NPY_HAVE_SSE2\n-\n-/**begin repeat\n- *  #type = npy_float, npy_double#\n- *  #TYPE = FLOAT, DOUBLE#\n- *  #sfx = f32, f64#\n- *  #CHK = _F32, _F64#\n- */\n-#if NPY_SIMD@CHK@\n-/**begin repeat1\n-* Arithmetic\n-* # kind = add, subtract, multiply, divide#\n-* # OP = +, -, *, /#\n-* # VOP = add, sub, mul, div#\n-*/\n-\n-static void\n-simd_binary_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, NPY_SIMD_WIDTH) {\n-        op[i] = ip1[i] @OP@ ip2[i];\n-    }\n-    /* lots of specializations, to squeeze out max performance */\n-    if (ip1 == ip2) {\n-        LOOP_BLOCKED(@type@, NPY_SIMD_WIDTH) {\n-            npyv_@sfx@ a = npyv_load_@sfx@(&ip1[i]);\n-            npyv_@sfx@ c = npyv_@VOP@_@sfx@(a, a);\n-            npyv_store_@sfx@(&op[i], c);\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, NPY_SIMD_WIDTH) {\n-            npyv_@sfx@ a = npyv_load_@sfx@(&ip1[i]);\n-            npyv_@sfx@ b = npyv_load_@sfx@(&ip2[i]);\n-            npyv_@sfx@ c = npyv_@VOP@_@sfx@(a, b);\n-            npyv_store_@sfx@(&op[i], c);\n-        }\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = ip1[i] @OP@ ip2[i];\n-    }\n-}\n+//###############################################################################\n+//## Complex Single/Double precision\n+//###############################################################################\n \n-static void\n-simd_binary_scalar1_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-    const npyv_@sfx@ v1 = npyv_setall_@sfx@(ip1[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, NPY_SIMD_WIDTH) {\n-        op[i] = ip1[0] @OP@ ip2[i];\n-    }\n-    LOOP_BLOCKED(@type@, NPY_SIMD_WIDTH) {\n-        npyv_@sfx@ v2 = npyv_load_@sfx@(&ip2[i]);\n-        npyv_@sfx@ v3 = npyv_@VOP@_@sfx@(v1, v2);\n-        npyv_store_@sfx@(&op[i], v3);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = ip1[0] @OP@ ip2[i];\n-    }\n-}\n+/********************************************************************************\n+ ** op intrinics\n+ ********************************************************************************/\n \n-static void\n-simd_binary_scalar2_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_f32x2 simd_set2_f32(const float *a)\n {\n-    const npyv_@sfx@ v2 = npyv_setall_@sfx@(ip2[0]);\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, NPY_SIMD_WIDTH) {\n-        op[i] = ip1[i] @OP@ ip2[0];\n-    }\n-    LOOP_BLOCKED(@type@, NPY_SIMD_WIDTH) {\n-        npyv_@sfx@ v1 = npyv_load_@sfx@(&ip1[i]);\n-        npyv_@sfx@ v3 = npyv_@VOP@_@sfx@(v1, v2);\n-        npyv_store_@sfx@(&op[i], v3);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = ip1[i] @OP@ ip2[0];\n-    }\n+    npyv_f32 fill = npyv_reinterpret_f32_u64(npyv_setall_u64(*(npy_uint64*)a));\n+    npyv_f32x2 r;\n+    r.val[0] = fill;\n+    r.val[1] = fill;\n+    return r;\n }\n-/**end repeat1**/\n-#endif /* NPY_SIMD@CHK@ */\n-/**end repeat**/\n-#endif // NPY_HAVE_SSE2\n \n-/**begin repeat\n- * Float types\n- *  #type = npy_float, npy_double, npy_longdouble#\n- *  #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n- *  #vector = 1, 1, 0#\n- *  #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64, 0 #\n- */\n-/**begin repeat1\n- * Arithmetic\n- * # kind = add, subtract, multiply, divide#\n- */\n-static inline int\n-run_binary_simd_@kind@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+NPY_FINLINE npyv_f32\n+simd_cconjugate_f32(npyv_f32 x)\n {\n-#if @vector@ && defined NPY_HAVE_SSE2\n-    @type@ * ip1 = (@type@ *)args[0];\n-    @type@ * ip2 = (@type@ *)args[1];\n-    @type@ * op = (@type@ *)args[2];\n-    npy_intp n = dimensions[0];\n-#if defined NPY_HAVE_AVX512F\n-    const npy_uintp vector_size_bytes = 64;\n-#elif defined NPY_HAVE_AVX2\n-    const npy_uintp vector_size_bytes = 32;\n+#if NPY_SIMD_BIGENDIAN\n+    const npyv_f32 mask = npyv_reinterpret_f32_u64(npyv_setall_u64(0x80000000));\n #else\n-    const npy_uintp vector_size_bytes = 32;\n-#endif\n-    /* argument one scalar */\n-    if (IS_BLOCKABLE_BINARY_SCALAR1(sizeof(@type@), vector_size_bytes)) {\n-        sse2_binary_scalar1_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-    /* argument two scalar */\n-    else if (IS_BLOCKABLE_BINARY_SCALAR2(sizeof(@type@), vector_size_bytes)) {\n-        sse2_binary_scalar2_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-    else if (IS_BLOCKABLE_BINARY(sizeof(@type@), vector_size_bytes)) {\n-        sse2_binary_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-#elif @VECTOR@\n-    @type@ * ip1 = (@type@ *)args[0];\n-    @type@ * ip2 = (@type@ *)args[1];\n-    @type@ * op = (@type@ *)args[2];\n-    npy_intp n = dimensions[0];\n-    /* argument one scalar */\n-    if (IS_BLOCKABLE_BINARY_SCALAR1(sizeof(@type@), NPY_SIMD_WIDTH)) {\n-        simd_binary_scalar1_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-    /* argument two scalar */\n-    else if (IS_BLOCKABLE_BINARY_SCALAR2(sizeof(@type@), NPY_SIMD_WIDTH)) {\n-        simd_binary_scalar2_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-    else if (IS_BLOCKABLE_BINARY(sizeof(@type@), NPY_SIMD_WIDTH)) {\n-        simd_binary_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n+    const npyv_f32 mask = npyv_reinterpret_f32_u64(npyv_setall_u64(0x8000000000000000ULL));\n #endif\n-    return 0;\n+    return npyv_xor_f32(x, mask);\n }\n-/**end repeat1**/\n-/**end repeat**/\n \n-/********************************************************************************\n- ** Defining ufunc inner functions\n- ********************************************************************************/\n-/**begin repeat\n- * Float types\n- *  #type = npy_float, npy_double#\n- *  #TYPE = FLOAT, DOUBLE#\n- *  #c = f, #\n- *  #C = F, #\n- *  #count = 4,2#\n- */\n-/**begin repeat1\n- * Arithmetic\n- * # kind = add, subtract, multiply, divide#\n- * # OP = +, -, *, /#\n- * # PW = 1, 0, 0, 0#\n- */\n-NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n-(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+NPY_FINLINE npyv_f32\n+simd_cmul_f32(npyv_f32 a, npyv_f32 b)\n {\n-    if (IS_BINARY_REDUCE) {\n-#if @PW@\n-        @type@ * iop1 = (@type@ *)args[0];\n-        npy_intp n = dimensions[0];\n-\n-        *iop1 @OP@= @TYPE@_pairwise_sum(args[1], n, steps[1]);\n-#else\n-        BINARY_REDUCE_LOOP(@type@) {\n-            io1 @OP@= *(@type@ *)ip2;\n-        }\n-        *((@type@ *)iop1) = io1;\n-#endif\n-    }\n-    else if (dimensions[0] < @count@ || !run_binary_simd_@kind@_@TYPE@(args, dimensions, steps)) {\n-        BINARY_LOOP {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            const @type@ in2 = *(@type@ *)ip2;\n-            *((@type@ *)op1) = in1 @OP@ in2;\n-        }\n-    }\n+    npyv_f32 b_rev = npyv_permi128_f32(b, 1, 0, 3, 2);\n+    npyv_f32 a_re = npyv_permi128_f32(a, 0, 0, 2, 2);\n+    npyv_f32 a_im = npyv_permi128_f32(a, 1, 1, 3, 3);\n+    // a_im * b_im, a_im * b_re\n+    npyv_f32 ab_iiir = npyv_mul_f32(a_im, b_rev);\n+    return npyv_muladdsub_f32(a_re, b, ab_iiir);\n }\n-/**end repeat1**/\n-/**end repeat**/\n \n-//###############################################################################\n-//## Complex Single/Double precision\n-//###############################################################################\n-/********************************************************************************\n- ** Defining the SIMD kernels\n- ********************************************************************************/\n-#if !defined(_MSC_VER) && defined(NPY_HAVE_AVX512F)\n-    /**\n-     * For somehow MSVC commit aggressive optimization lead\n-     * to raises 'RuntimeWarning: invalid value encountered in multiply'\n-     *\n-     * the issue mainly caused by '_mm512_maskz_loadu_ps', we need to\n-     * investigate about it while moving to NPYV.\n-     */\n-    #define AVX512F_NOMSVC\n+NPY_FINLINE npyv_f32\n+simd_csquare_f32(npyv_f32 x)\n+{ return simd_cmul_f32(x, x); }\n #endif\n \n-#ifdef AVX512F_NOMSVC\n-NPY_FINLINE __mmask16\n-avx512_get_full_load_mask_ps(void)\n-{\n-    return 0xFFFF;\n-}\n+#if NPY_SIMD_F64\n \n-NPY_FINLINE __mmask8\n-avx512_get_full_load_mask_pd(void)\n-{\n-    return 0xFF;\n-}\n-NPY_FINLINE __m512\n-avx512_masked_load_ps(__mmask16 mask, npy_float* addr)\n+NPY_FINLINE npyv_f64x2 simd_set2_f64(const double *a)\n {\n-    return _mm512_maskz_loadu_ps(mask, (__m512 *)addr);\n+    npyv_f64 r = npyv_setall_f64(a[0]);\n+    npyv_f64 i = npyv_setall_f64(a[1]);\n+    return npyv_zip_f64(r, i);\n }\n \n-NPY_FINLINE __m512d\n-avx512_masked_load_pd(__mmask8 mask, npy_double* addr)\n+NPY_FINLINE npyv_f64\n+simd_cconjugate_f64(npyv_f64 x)\n {\n-    return _mm512_maskz_loadu_pd(mask, (__m512d *)addr);\n+    const npyv_f64 mask = npyv_reinterpret_f64_u64(npyv_set_u64(\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL,\n+       0, 0x8000000000000000ULL, 0, 0x8000000000000000ULL\n+    ));\n+    return npyv_xor_f64(x, mask);\n }\n \n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n-avx512_get_partial_load_mask_ps(const npy_int num_elem, const npy_int total_elem)\n+NPY_FINLINE npyv_f64\n+simd_cmul_f64(npyv_f64 a, npyv_f64 b)\n {\n-    return (0x0001 << num_elem) - 0x0001;\n+    npyv_f64 b_rev = npyv_permi128_f64(b, 1, 0);\n+    npyv_f64 a_re = npyv_permi128_f64(a, 0, 0);\n+    npyv_f64 a_im = npyv_permi128_f64(a, 1, 1);\n+    // a_im * b_im, a_im * b_re\n+    npyv_f64 ab_iiir = npyv_mul_f64(a_im, b_rev);\n+    return npyv_muladdsub_f64(a_re, b, ab_iiir);\n }\n \n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n-avx512_get_partial_load_mask_pd(const npy_int num_elem, const npy_int total_elem)\n-{\n-    return (0x01 << num_elem) - 0x01;\n-}\n-/**begin repeat\n- *  #vsub  = ps, pd#\n- *  #type= npy_float, npy_double#\n- *  #epi_vsub  = epi32, epi64#\n- *  #vtype = __m512, __m512d#\n- *  #mask = __mmask16, __mmask8#\n- *  #and_const = 0x7fffffff, 0x7fffffffffffffffLL#\n- *  #neg_mask = 0x80000000, 0x8000000000000000#\n- *  #perm_ = 0xb1, 0x55#\n- *  #cmpx_img_mask = 0xAAAA, 0xAA#\n- *  #cmpx_re_mask = 0x5555, 0x55#\n- *  #INF = NPY_INFINITYF, NPY_INFINITY#\n- *  #NAN = NPY_NANF, NPY_NAN#\n- */\n-NPY_FINLINE @vtype@\n-avx512_hadd_@vsub@(const @vtype@ x)\n-{\n-    return _mm512_add_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));\n-}\n-\n-NPY_FINLINE @vtype@\n-avx512_hsub_@vsub@(const @vtype@ x)\n-{\n-    return _mm512_sub_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));\n-}\n-NPY_FINLINE @vtype@\n-avx512_cmul_@vsub@(@vtype@ x1, @vtype@ x2)\n-{\n-    // x1 = r1, i1\n-    // x2 = r2, i2\n-    @vtype@ x3  = _mm512_permute_@vsub@(x2, @perm_@);   // i2, r2\n-    @vtype@ x12 = _mm512_mul_@vsub@(x1, x2);            // r1*r2, i1*i2\n-    @vtype@ x13 = _mm512_mul_@vsub@(x1, x3);            // r1*i2, r2*i1\n-    @vtype@ outreal = avx512_hsub_@vsub@(x12);          // r1*r2 - i1*i2, r1*r2 - i1*i2\n-    @vtype@ outimg  = avx512_hadd_@vsub@(x13);          // r1*i2 + i1*r2, r1*i2 + i1*r2\n-    return _mm512_mask_blend_@vsub@(@cmpx_img_mask@, outreal, outimg);\n-}\n-/**end repeat**/\n+NPY_FINLINE npyv_f64\n+simd_csquare_f64(npyv_f64 x)\n+{ return simd_cmul_f64(x, x); }\n #endif\n \n /**begin repeat\n- * #TYPE = CFLOAT, CDOUBLE#\n  * #type = npy_float, npy_double#\n- * #num_lanes = 16, 8#\n- * #vsuffix = ps, pd#\n- * #epi_vsub  = epi32, epi64#\n- * #mask = __mmask16, __mmask8#\n- * #vtype = __m512, __m512d#\n- * #scale = 4, 8#\n- * #vindextype = __m512i, __m256i#\n- * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#\n- * #storemask = 0xFF, 0xF#\n- * #IS_FLOAT = 1, 0#\n- */\n-/**begin repeat1\n- *  #func = add, subtract, multiply#\n- *  #vectorf = _mm512_add, _mm512_sub, avx512_cmul#\n- */\n-#if defined AVX512F_NOMSVC\n-static inline void\n-AVX512F_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)\n-{\n-    const npy_intp array_size = dimensions[0];\n-    npy_intp num_remaining_elements = 2*array_size;\n-    @type@* ip1 = (@type@*) args[0];\n-    @type@* ip2 = (@type@*) args[1];\n-    @type@* op  = (@type@*) args[2];\n-\n-    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();\n-\n-    while (num_remaining_elements > 0) {\n-        if (num_remaining_elements < @num_lanes@) {\n-            load_mask = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements, @num_lanes@);\n-        }\n-        @vtype@ x1, x2;\n-        x1 = avx512_masked_load_@vsuffix@(load_mask, ip1);\n-        x2 = avx512_masked_load_@vsuffix@(load_mask, ip2);\n-\n-        @vtype@ out = @vectorf@_@vsuffix@(x1, x2);\n-\n-        _mm512_mask_storeu_@vsuffix@(op, load_mask, out);\n-\n-        ip1 += @num_lanes@;\n-        ip2 += @num_lanes@;\n-        op += @num_lanes@;\n-        num_remaining_elements -= @num_lanes@;\n-    }\n-}\n-#endif // AVX512F_NOMSVC\n-/**end repeat1**/\n-/**end repeat**/\n-\n-/**begin repeat\n- * #TYPE = CFLOAT, CDOUBLE#\n- * #type= npy_float, npy_double#\n- * #esize = 8, 16#\n- */\n-/**begin repeat1\n- *  #func = add, subtract, multiply#\n+ * #sfx = f32, f64#\n+ * #bsfx = b32, b64#\n+ * #usfx = b32, u64#\n+ * #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #is_double = 0, 1#\n+ * #c = f, #\n+ * #INF = NPY_INFINITYF, NPY_INFINITY#\n+ * #NAN = NPY_NANF, NPY_NAN#\n  */\n-static inline int\n-run_binary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)\n+#if @VECTOR@\n+NPY_FINLINE npyv_@sfx@\n+simd_cabsolute_@sfx@(npyv_@sfx@ re, npyv_@sfx@ im)\n {\n-#if defined AVX512F_NOMSVC\n-    if (IS_BINARY_STRIDE_ONE(@esize@, 64)) {\n-        AVX512F_@func@_@TYPE@(args, dimensions, steps);\n-        return 1;\n-    }\n-    else\n-        return 0;\n-#endif\n-    return 0;\n+    const npyv_@sfx@ inf = npyv_setall_@sfx@(@INF@);\n+    const npyv_@sfx@ nan = npyv_setall_@sfx@(@NAN@);\n+\n+    re = npyv_abs_@sfx@(re);\n+    im = npyv_abs_@sfx@(im);\n+    /*\n+     * If real or imag = INF, then convert it to inf + j*inf\n+     * Handles: inf + j*nan, nan + j*inf\n+     */\n+    npyv_@bsfx@ re_infmask = npyv_cmpeq_@sfx@(re, inf);\n+    npyv_@bsfx@ im_infmask = npyv_cmpeq_@sfx@(im, inf);\n+    im = npyv_select_@sfx@(re_infmask, inf, im);\n+    re = npyv_select_@sfx@(im_infmask, inf, re);\n+    /*\n+     * If real or imag = NAN, then convert it to nan + j*nan\n+     * Handles: x + j*nan, nan + j*x\n+     */\n+    npyv_@bsfx@ re_nnanmask = npyv_notnan_@sfx@(re);\n+    npyv_@bsfx@ im_nnanmask = npyv_notnan_@sfx@(im);\n+    im = npyv_select_@sfx@(re_nnanmask, im, nan);\n+    re = npyv_select_@sfx@(im_nnanmask, re, nan);\n+\n+    npyv_@sfx@ larger  = npyv_max_@sfx@(re, im);\n+    npyv_@sfx@ smaller = npyv_min_@sfx@(im, re);\n+    /*\n+     * Calculate div_mask to prevent 0./0. and inf/inf operations in div\n+     */\n+    npyv_@bsfx@ zeromask = npyv_cmpeq_@sfx@(larger, npyv_zero_@sfx@());\n+    npyv_@bsfx@ infmask = npyv_cmpeq_@sfx@(smaller, inf);\n+    npyv_@bsfx@ div_mask = npyv_not_@bsfx@(npyv_or_@bsfx@(zeromask, infmask));\n+\n+    npyv_@sfx@ ratio = npyv_ifdivz_@sfx@(div_mask, smaller, larger);\n+    npyv_@sfx@ hypot = npyv_sqrt_@sfx@(\n+        npyv_muladd_@sfx@(ratio, ratio, npyv_setall_@sfx@(1.0@c@)\n+    ));\n+    return npyv_mul_@sfx@(hypot, larger);\n }\n-/**end repeat1**/\n+#endif // VECTOR\n /**end repeat**/\n \n /********************************************************************************\n@@ -725,55 +312,318 @@ run_binary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const\n  * complex types\n  * #TYPE = CFLOAT, CDOUBLE#\n  * #ftype = npy_float, npy_double#\n+ * #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #sfx = f32, f64#\n  * #c = f, #\n  * #C = F, #\n  */\n /**begin repeat1\n  * arithmetic\n- * #kind = add, subtract#\n- * #OP = +, -#\n- * #PW = 1, 0#\n+ * #kind = add, subtract, multiply#\n+ * #vectorf = npyv_add, npyv_sub, simd_cmul#\n+ * #OP = +, -, *#\n+ * #PW = 1, 0, 0#\n+ * #is_mul = 0*2, 1#\n  */\n NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    // Parenthesis around @PW@ tells clang dead code is intentional\n-    if (IS_BINARY_REDUCE && (@PW@)) {\n-        npy_intp n = dimensions[0];\n-        @ftype@ * or = ((@ftype@ *)args[0]);\n-        @ftype@ * oi = ((@ftype@ *)args[0]) + 1;\n+    npy_intp len = dimensions[0];\n+    char *b_src0 = args[0], *b_src1 = args[1], *b_dst = args[2];\n+    npy_intp b_ssrc0 = steps[0], b_ssrc1 = steps[1], b_sdst = steps[2];\n+#if @PW@\n+    // reduce\n+    if (b_ssrc0 == 0 && b_ssrc0 == b_sdst && b_src0 == b_dst &&\n+        b_ssrc1 % (sizeof(@ftype@)*2) == 0\n+    ) {\n+        @ftype@ *rl_im = (@ftype@ *)b_src0;\n         @ftype@ rr, ri;\n-\n-        @TYPE@_pairwise_sum(&rr, &ri, args[1], n * 2, steps[1] / 2);\n-        *or @OP@= rr;\n-        *oi @OP@= ri;\n+        @TYPE@_pairwise_sum(&rr, &ri, b_src1, len * 2, b_ssrc1 / 2);\n+        rl_im[0] @OP@= rr;\n+        rl_im[1] @OP@= ri;\n         return;\n     }\n-    if (!run_binary_avx512f_@kind@_@TYPE@(args, dimensions, steps)) {\n-        BINARY_LOOP {\n-            const @ftype@ in1r = ((@ftype@ *)ip1)[0];\n-            const @ftype@ in1i = ((@ftype@ *)ip1)[1];\n-            const @ftype@ in2r = ((@ftype@ *)ip2)[0];\n-            const @ftype@ in2i = ((@ftype@ *)ip2)[1];\n-            ((@ftype@ *)op1)[0] = in1r @OP@ in2r;\n-            ((@ftype@ *)op1)[1] = in1i @OP@ in2i;\n+#endif\n+#if @VECTOR@\n+    if (is_mem_overlap(b_src0, b_ssrc0, b_dst, b_sdst, len) ||\n+        is_mem_overlap(b_src1, b_ssrc1, b_dst, b_sdst, len) ||\n+        b_sdst  % sizeof(@ftype@) != 0 || b_sdst == 0 ||\n+        b_ssrc0 % sizeof(@ftype@) != 0 ||\n+        b_ssrc1 % sizeof(@ftype@) != 0\n+    ) {\n+        goto loop_scalar;\n+    }\n+    const @ftype@ *src0 = (@ftype@*)b_src0;\n+    const @ftype@ *src1 = (@ftype@*)b_src1;\n+          @ftype@ *dst  = (@ftype@*)b_dst;\n+\n+    const npy_intp ssrc0 = b_ssrc0 / sizeof(@ftype@);\n+    const npy_intp ssrc1 = b_ssrc1 / sizeof(@ftype@);\n+    const npy_intp sdst  = b_sdst / sizeof(@ftype@);\n+\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * 2;\n+    const int hstep = vstep / 2;\n+\n+    const int loadable0 = npyv_loadable_stride_s64(ssrc0);\n+    const int loadable1 = npyv_loadable_stride_s64(ssrc1);\n+    const int storeable = npyv_storable_stride_s64(sdst);\n+\n+    // lots**lots of specializations, to squeeze out max performance\n+    // contig\n+    if (ssrc0 == 2 && ssrc0 == ssrc1 && ssrc0 == sdst) {\n+        for (; len >= vstep; len -= vstep, src0 += wstep, src1 += wstep, dst += wstep) {\n+            npyv_@sfx@ a0 = npyv_load_@sfx@(src0);\n+            npyv_@sfx@ a1 = npyv_load_@sfx@(src0 + vstep);\n+            npyv_@sfx@ b0 = npyv_load_@sfx@(src1);\n+            npyv_@sfx@ b1 = npyv_load_@sfx@(src1 + vstep);\n+            npyv_@sfx@ r0 = @vectorf@_@sfx@(a0, b0);\n+            npyv_@sfx@ r1 = @vectorf@_@sfx@(a1, b1);\n+            npyv_store_@sfx@(dst, r0);\n+            npyv_store_@sfx@(dst + vstep, r1);\n+        }\n+        for (; len > 0; len -= hstep, src0 += vstep, src1 += vstep, dst += vstep) {\n+            npyv_@sfx@ a = npyv_load2_tillz_@sfx@(src0, len);\n+            npyv_@sfx@ b = npyv_load2_tillz_@sfx@(src1, len);\n+            npyv_@sfx@ r = @vectorf@_@sfx@(a, b);\n+            npyv_store2_till_@sfx@(dst, len, r);\n+        }\n+    }\n+    // scalar 0\n+    else if (ssrc0 == 0) {\n+        npyv_@sfx@x2 a = simd_set2_@sfx@(src0);\n+        // contig\n+        if (ssrc1 == 2 && sdst == ssrc1) {\n+            for (; len >= vstep; len -= vstep, src1 += wstep, dst += wstep) {\n+                npyv_@sfx@ b0 = npyv_load_@sfx@(src1);\n+                npyv_@sfx@ b1 = npyv_load_@sfx@(src1 + vstep);\n+                npyv_@sfx@ r0 = @vectorf@_@sfx@(a.val[0], b0);\n+                npyv_@sfx@ r1 = @vectorf@_@sfx@(a.val[1], b1);\n+                npyv_store_@sfx@(dst, r0);\n+                npyv_store_@sfx@(dst + vstep, r1);\n+            }\n+            for (; len > 0; len -= hstep, src1 += vstep, dst += vstep) {\n+            #if @is_mul@\n+                npyv_@sfx@ b = npyv_load2_till_@sfx@(src1, len, 1.0@c@, 1.0@c@);\n+            #else\n+                npyv_@sfx@ b = npyv_load2_tillz_@sfx@(src1, len);\n+            #endif\n+                npyv_@sfx@ r = @vectorf@_@sfx@(a.val[0], b);\n+                npyv_store2_till_@sfx@(dst, len, r);\n+            }\n+        }\n+        // non-contig\n+        else if (loadable1 && storeable) {\n+            for (; len >= vstep; len -= vstep, src1 += ssrc1*vstep, dst += sdst*vstep) {\n+                npyv_@sfx@ b0 = npyv_loadn2_@sfx@(src1, ssrc1);\n+                npyv_@sfx@ b1 = npyv_loadn2_@sfx@(src1 + ssrc1*hstep, ssrc1);\n+                npyv_@sfx@ r0 = @vectorf@_@sfx@(a.val[0], b0);\n+                npyv_@sfx@ r1 = @vectorf@_@sfx@(a.val[1], b1);\n+                npyv_storen2_@sfx@(dst, sdst, r0);\n+                npyv_storen2_@sfx@(dst + sdst*hstep, sdst, r1);\n+            }\n+            for (; len > 0; len -= hstep, src1 += ssrc1*hstep, dst += sdst*hstep) {\n+            #if @is_mul@\n+                npyv_@sfx@ b = npyv_loadn2_till_@sfx@(src1, ssrc1, len, 1.0@c@, 1.0@c@);\n+            #else\n+                npyv_@sfx@ b = npyv_loadn2_tillz_@sfx@(src1, ssrc1, len);\n+            #endif\n+                npyv_@sfx@ r = @vectorf@_@sfx@(a.val[0], b);\n+                npyv_storen2_till_@sfx@(dst, sdst, len, r);\n+            }\n+        }\n+        else {\n+            goto loop_scalar;\n+        }\n+    }\n+    // scalar 1\n+    else if (ssrc1 == 0) {\n+        npyv_@sfx@x2 b = simd_set2_@sfx@(src1);\n+        if (ssrc0 == 2 && sdst == ssrc0) {\n+            for (; len >= vstep; len -= vstep, src0 += wstep, dst += wstep) {\n+                npyv_@sfx@ a0 = npyv_load_@sfx@(src0);\n+                npyv_@sfx@ a1 = npyv_load_@sfx@(src0 + vstep);\n+                npyv_@sfx@ r0 = @vectorf@_@sfx@(a0, b.val[0]);\n+                npyv_@sfx@ r1 = @vectorf@_@sfx@(a1, b.val[1]);\n+                npyv_store_@sfx@(dst, r0);\n+                npyv_store_@sfx@(dst + vstep, r1);\n+            }\n+            for (; len > 0; len -= hstep, src0 += vstep, dst += vstep) {\n+            #if @is_mul@\n+                npyv_@sfx@ a = npyv_load2_till_@sfx@(src0, len, 1.0@c@, 1.0@c@);\n+            #else\n+                npyv_@sfx@ a = npyv_load2_tillz_@sfx@(src0, len);\n+            #endif\n+                npyv_@sfx@ r = @vectorf@_@sfx@(a, b.val[0]);\n+                npyv_store2_till_@sfx@(dst, len, r);\n+            }\n         }\n+        // non-contig\n+        else if (loadable0 && storeable) {\n+            for (; len >= vstep; len -= vstep, src0 += ssrc0*vstep, dst += sdst*vstep) {\n+                npyv_@sfx@ a0 = npyv_loadn2_@sfx@(src0, ssrc0);\n+                npyv_@sfx@ a1 = npyv_loadn2_@sfx@(src0 + ssrc0*hstep, ssrc0);\n+                npyv_@sfx@ r0 = @vectorf@_@sfx@(a0, b.val[0]);\n+                npyv_@sfx@ r1 = @vectorf@_@sfx@(a1, b.val[1]);\n+                npyv_storen2_@sfx@(dst, sdst, r0);\n+                npyv_storen2_@sfx@(dst + sdst*hstep, sdst, r1);\n+            }\n+            for (; len > 0; len -= hstep, src0 += ssrc0*hstep, dst += sdst*hstep) {\n+            #if @is_mul@\n+                npyv_@sfx@ a = npyv_loadn2_till_@sfx@(src0, ssrc0, len, 1.0@c@, 1.0@c@);\n+            #else\n+                npyv_@sfx@ a = npyv_loadn2_tillz_@sfx@(src0, ssrc0, len);\n+            #endif\n+                npyv_@sfx@ r = @vectorf@_@sfx@(a, b.val[0]);\n+                npyv_storen2_till_@sfx@(dst, sdst, len, r);\n+            }\n+        }\n+        else {\n+            goto loop_scalar;\n+        }\n+    }\n+    #if @is_mul@\n+    // non-contig\n+    else if (loadable0 && loadable1 && storeable) {\n+        for (; len >= vstep; len -= vstep, src0 += ssrc0*vstep,\n+                            src1 += ssrc1*vstep, dst += sdst*vstep\n+        ) {\n+            npyv_@sfx@ a0 = npyv_loadn2_@sfx@(src0, ssrc0);\n+            npyv_@sfx@ a1 = npyv_loadn2_@sfx@(src0 + ssrc0*hstep, ssrc0);\n+            npyv_@sfx@ b0 = npyv_loadn2_@sfx@(src1, ssrc1);\n+            npyv_@sfx@ b1 = npyv_loadn2_@sfx@(src1 + ssrc1*hstep, ssrc1);\n+            npyv_@sfx@ r0 = @vectorf@_@sfx@(a0, b0);\n+            npyv_@sfx@ r1 = @vectorf@_@sfx@(a1, b1);\n+            npyv_storen2_@sfx@(dst, sdst, r0);\n+            npyv_storen2_@sfx@(dst + sdst*hstep, sdst, r1);\n+        }\n+        for (; len > 0; len -= hstep, src0 += ssrc0*hstep,\n+                       src1 += ssrc1*hstep, dst += sdst*hstep\n+        ) {\n+        #if @is_mul@\n+            npyv_@sfx@ a = npyv_loadn2_till_@sfx@(src0, ssrc0, len, 1.0@c@, 1.0@c@);\n+            npyv_@sfx@ b = npyv_loadn2_till_@sfx@(src1, ssrc1, len, 1.0@c@, 1.0@c@);\n+        #else\n+            npyv_@sfx@ a = npyv_loadn2_tillz_@sfx@(src0, ssrc0, len);\n+            npyv_@sfx@ b = npyv_loadn2_tillz_@sfx@(src1, ssrc1, len);\n+        #endif\n+            npyv_@sfx@ r = @vectorf@_@sfx@(a, b);\n+            npyv_storen2_till_@sfx@(dst, sdst, len, r);\n+        }\n+    }\n+    #endif\n+    else {\n+        goto loop_scalar;\n+    }\n+    npyv_cleanup();\n+    return;\n+loop_scalar:\n+#endif\n+    for (; len > 0; --len, b_src0 += b_ssrc0, b_src1 += b_ssrc1, b_dst += b_sdst) {\n+        const @ftype@ a_r = ((@ftype@ *)b_src0)[0];\n+        const @ftype@ a_i = ((@ftype@ *)b_src0)[1];\n+        const @ftype@ b_r = ((@ftype@ *)b_src1)[0];\n+        const @ftype@ b_i = ((@ftype@ *)b_src1)[1];\n+    #if @is_mul@\n+        ((@ftype@ *)b_dst)[0] = a_r*b_r - a_i*b_i;\n+        ((@ftype@ *)b_dst)[1] = a_r*b_i + a_i*b_r;\n+    #else\n+        ((@ftype@ *)b_dst)[0] = a_r @OP@ b_r;\n+        ((@ftype@ *)b_dst)[1] = a_i @OP@ b_i;\n+    #endif\n     }\n }\n /**end repeat1**/\n \n-NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_multiply)\n+/**begin repeat1\n+ *  #kind = conjugate, square#\n+ *  #is_square = 0, 1#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    if (!run_binary_avx512f_multiply_@TYPE@(args, dimensions, steps)) {\n-        BINARY_LOOP {\n-            const @ftype@ in1r = ((@ftype@ *)ip1)[0];\n-            const @ftype@ in1i = ((@ftype@ *)ip1)[1];\n-            const @ftype@ in2r = ((@ftype@ *)ip2)[0];\n-            const @ftype@ in2i = ((@ftype@ *)ip2)[1];\n-            ((@ftype@ *)op1)[0] = in1r*in2r - in1i*in2i;\n-            ((@ftype@ *)op1)[1] = in1r*in2i + in1i*in2r;\n+    npy_intp len = dimensions[0];\n+    char *b_src = args[0], *b_dst = args[1];\n+    npy_intp b_ssrc = steps[0], b_sdst = steps[1];\n+#if @VECTOR@\n+    if (is_mem_overlap(b_src, b_ssrc, b_dst, b_sdst, len) ||\n+        b_sdst % sizeof(@ftype@) != 0 ||\n+        b_ssrc % sizeof(@ftype@) != 0\n+    ) {\n+        goto loop_scalar;\n+    }\n+    const @ftype@ *src  = (@ftype@*)b_src;\n+          @ftype@ *dst  = (@ftype@*)b_dst;\n+    const npy_intp ssrc = b_ssrc / sizeof(@ftype@);\n+    const npy_intp sdst = b_sdst / sizeof(@ftype@);\n+\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * 2;\n+    const int hstep = vstep / 2;\n+\n+    if (ssrc == 2 && ssrc == sdst) {\n+        for (; len >= vstep; len -= vstep, src += wstep, dst += wstep) {\n+            npyv_@sfx@ a0 = npyv_load_@sfx@(src);\n+            npyv_@sfx@ a1 = npyv_load_@sfx@(src + vstep);\n+            npyv_@sfx@ r0 = simd_c@kind@_@sfx@(a0);\n+            npyv_@sfx@ r1 = simd_c@kind@_@sfx@(a1);\n+            npyv_store_@sfx@(dst, r0);\n+            npyv_store_@sfx@(dst + vstep, r1);\n+        }\n+        for (; len > 0; len -= hstep, src += vstep, dst += vstep) {\n+            npyv_@sfx@ a = npyv_load2_tillz_@sfx@(src, len);\n+            npyv_@sfx@ r = simd_c@kind@_@sfx@(a);\n+            npyv_store2_till_@sfx@(dst, len, r);\n+        }\n+    }\n+    else if (ssrc == 2 && npyv_storable_stride_s64(sdst)) {\n+        for (; len >= vstep; len -= vstep, src += wstep, dst += sdst*vstep) {\n+            npyv_@sfx@ a0 = npyv_load_@sfx@(src);\n+            npyv_@sfx@ a1 = npyv_load_@sfx@(src + vstep);\n+            npyv_@sfx@ r0 = simd_c@kind@_@sfx@(a0);\n+            npyv_@sfx@ r1 = simd_c@kind@_@sfx@(a1);\n+            npyv_storen2_@sfx@(dst, sdst, r0);\n+            npyv_storen2_@sfx@(dst + sdst*hstep, sdst, r1);\n+        }\n+        for (; len > 0; len -= hstep, src += vstep, dst += sdst*hstep) {\n+            npyv_@sfx@ a = npyv_load2_tillz_@sfx@(src, len);\n+            npyv_@sfx@ r = simd_c@kind@_@sfx@(a);\n+            npyv_storen2_till_@sfx@(dst, sdst, len, r);\n+        }\n+    }\n+    else if (sdst == 2 && npyv_loadable_stride_s64(ssrc)) {\n+        for (; len >= vstep; len -= vstep, src += ssrc*vstep, dst += wstep) {\n+            npyv_@sfx@ a0 = npyv_loadn2_@sfx@(src, ssrc);\n+            npyv_@sfx@ a1 = npyv_loadn2_@sfx@(src + ssrc*hstep, ssrc);\n+            npyv_@sfx@ r0 = simd_c@kind@_@sfx@(a0);\n+            npyv_@sfx@ r1 = simd_c@kind@_@sfx@(a1);\n+            npyv_store_@sfx@(dst, r0);\n+            npyv_store_@sfx@(dst + vstep, r1);\n+        }\n+        for (; len > 0; len -= hstep, src += ssrc*hstep, dst += vstep) {\n+            npyv_@sfx@ a = npyv_loadn2_tillz_@sfx@((@ftype@*)src, ssrc, len);\n+            npyv_@sfx@ r = simd_c@kind@_@sfx@(a);\n+            npyv_store2_till_@sfx@(dst, len, r);\n         }\n     }\n+    else {\n+        goto loop_scalar;\n+    }\n+    npyv_cleanup();\n+    return;\n+loop_scalar:\n+#endif\n+    for (; len > 0; --len, b_src += b_ssrc, b_dst += b_sdst) {\n+        const @ftype@ rl = ((@ftype@ *)b_src)[0];\n+        const @ftype@ im = ((@ftype@ *)b_src)[1];\n+    #if @is_square@\n+        ((@ftype@ *)b_dst)[0] = rl*rl - im*im;\n+        ((@ftype@ *)b_dst)[1] = rl*im + im*rl;\n+    #else\n+        ((@ftype@ *)b_dst)[0] = rl;\n+        ((@ftype@ *)b_dst)[1] = -im;\n+    #endif\n+    }\n }\n+/**end repeat1**/\n /**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/loops_unary_complex.dispatch.c.src",
                "patch": "@@ -0,0 +1,139 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** sse2 (avx2 fma3) avx512f\n+ ** neon asimd\n+ ** vsx2 vsx3\n+ ** vx vxe\n+ **/\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"lowlevel_strided_loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+/**begin repeat\n+ * #type = npy_float, npy_double#\n+ * #sfx = f32, f64#\n+ * #bsfx = b32, b64#\n+ * #usfx = b32, u64#\n+ * #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #is_double = 0, 1#\n+ * #c = f, #\n+ * #INF = NPY_INFINITYF, NPY_INFINITY#\n+ * #NAN = NPY_NANF, NPY_NAN#\n+ */\n+#if @VECTOR@\n+NPY_FINLINE npyv_@sfx@\n+simd_cabsolute_@sfx@(npyv_@sfx@ re, npyv_@sfx@ im)\n+{\n+    const npyv_@sfx@ inf = npyv_setall_@sfx@(@INF@);\n+    const npyv_@sfx@ nan = npyv_setall_@sfx@(@NAN@);\n+\n+    re = npyv_abs_@sfx@(re);\n+    im = npyv_abs_@sfx@(im);\n+    /*\n+     * If real or imag = INF, then convert it to inf + j*inf\n+     * Handles: inf + j*nan, nan + j*inf\n+     */\n+    npyv_@bsfx@ re_infmask = npyv_cmpeq_@sfx@(re, inf);\n+    npyv_@bsfx@ im_infmask = npyv_cmpeq_@sfx@(im, inf);\n+    im = npyv_select_@sfx@(re_infmask, inf, im);\n+    re = npyv_select_@sfx@(im_infmask, inf, re);\n+    /*\n+     * If real or imag = NAN, then convert it to nan + j*nan\n+     * Handles: x + j*nan, nan + j*x\n+     */\n+    npyv_@bsfx@ re_nnanmask = npyv_notnan_@sfx@(re);\n+    npyv_@bsfx@ im_nnanmask = npyv_notnan_@sfx@(im);\n+    im = npyv_select_@sfx@(re_nnanmask, im, nan);\n+    re = npyv_select_@sfx@(im_nnanmask, re, nan);\n+\n+    npyv_@sfx@ larger  = npyv_max_@sfx@(re, im);\n+    npyv_@sfx@ smaller = npyv_min_@sfx@(im, re);\n+    /*\n+     * Calculate div_mask to prevent 0./0. and inf/inf operations in div\n+     */\n+    npyv_@bsfx@ zeromask = npyv_cmpeq_@sfx@(larger, npyv_zero_@sfx@());\n+    npyv_@bsfx@ infmask = npyv_cmpeq_@sfx@(smaller, inf);\n+    npyv_@bsfx@ div_mask = npyv_not_@bsfx@(npyv_or_@bsfx@(zeromask, infmask));\n+\n+    npyv_@sfx@ ratio = npyv_ifdivz_@sfx@(div_mask, smaller, larger);\n+    npyv_@sfx@ hypot = npyv_sqrt_@sfx@(\n+        npyv_muladd_@sfx@(ratio, ratio, npyv_setall_@sfx@(1.0@c@)\n+    ));\n+    return npyv_mul_@sfx@(hypot, larger);\n+}\n+#endif // VECTOR\n+/**end repeat**/\n+\n+/********************************************************************************\n+ ** Defining ufunc inner functions\n+ ********************************************************************************/\n+/**begin repeat\n+ * complex types\n+ * #TYPE = CFLOAT, CDOUBLE#\n+ * #ftype = npy_float, npy_double#\n+ * #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #sfx = f32, f64#\n+ * #c = f, #\n+ * #C = F, #\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_absolute)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+#if @VECTOR@\n+    npy_intp len = dimensions[0];\n+    npy_intp ssrc = steps[0] / sizeof(@ftype@);\n+    npy_intp sdst = steps[1] / sizeof(@ftype@);\n+\n+    if (!is_mem_overlap(args[0], steps[0], args[1], steps[1], len) &&\n+        npyv_loadable_stride_@sfx@(ssrc) && npyv_storable_stride_@sfx@(sdst)\n+        && steps[0] % sizeof(@ftype@) == 0\n+        && steps[1] % sizeof(@ftype@) == 0\n+    ) {\n+        const @ftype@ *src = (@ftype@*)args[0];\n+              @ftype@ *dst = (@ftype@*)args[1];\n+\n+        const int vstep = npyv_nlanes_@sfx@;\n+        const int wstep = vstep * 2;\n+        const int hstep = vstep / 2;\n+\n+        if (ssrc == 2 && sdst == 1) {\n+            for (; len >= vstep; len -= vstep, src += wstep, dst += vstep) {\n+                npyv_@sfx@x2 ab = npyv_load_@sfx@x2(src);\n+                npyv_@sfx@ r = simd_cabsolute_@sfx@(ab.val[0], ab.val[1]);\n+                npyv_store_@sfx@(dst, r);\n+            }\n+        }\n+        else {\n+            for (; len >= vstep; len -= vstep, src += ssrc*vstep, dst += sdst*vstep) {\n+                npyv_@sfx@ re_im0 = npyv_loadn2_@sfx@(src, ssrc);\n+                npyv_@sfx@ re_im1 = npyv_loadn2_@sfx@(src + ssrc*hstep, ssrc);\n+                npyv_@sfx@x2 ab = npyv_unzip_@sfx@(re_im0, re_im1);\n+                npyv_@sfx@ r = simd_cabsolute_@sfx@(ab.val[0], ab.val[1]);\n+                npyv_storen_@sfx@(dst, sdst, r);\n+            }\n+        }\n+        for (; len > 0; len -= vstep, src += ssrc*vstep, dst += sdst*vstep) {\n+            npyv_@sfx@ rl = npyv_loadn_tillz_@sfx@(src, ssrc, len);\n+            npyv_@sfx@ im = npyv_loadn_tillz_@sfx@(src + 1, ssrc, len);\n+            npyv_@sfx@ r = simd_cabsolute_@sfx@(rl, im);\n+            npyv_storen_till_@sfx@(dst, sdst, len, r);\n+        }\n+        npyv_cleanup();\n+        npy_clear_floatstatus_barrier((char*)&len);\n+        return;\n+    }\n+#endif\n+    UNARY_LOOP {\n+        const @ftype@ re = ((@ftype@ *)ip1)[0];\n+        const @ftype@ im = ((@ftype@ *)ip1)[1];\n+        *((@ftype@ *)op1) = npy_hypot@c@(re, im);\n+    }\n+}\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -1,653 +0,0 @@\n-\n-\n-/*\n- * This file is for the definitions of simd vectorized operations.\n- *\n- * Currently contains sse2 functions that are built on amd64, x32 or\n- * non-generic builds (CFLAGS=-march=...)\n- * In future it may contain other instruction sets like AVX or NEON detected\n- * at runtime in which case it needs to be included indirectly via a file\n- * compiled with special options (or use gcc target attributes) so the binary\n- * stays portable.\n- */\n-\n-\n-#ifndef __NPY_SIMD_INC\n-#define __NPY_SIMD_INC\n-\n-#include \"lowlevel_strided_loops.h\"\n-#include \"numpy/npy_common.h\"\n-#include \"numpy/npy_math.h\"\n-#include \"npy_simd_data.h\"\n-#ifdef NPY_HAVE_SSE2_INTRINSICS\n-#include <emmintrin.h>\n-#if !defined(_MSC_VER) || _MSC_VER >= 1600\n-#include <immintrin.h>\n-#else\n-#undef __AVX2__\n-#undef __AVX512F__\n-#endif\n-#endif\n-#include \"loops_utils.h\" // nomemoverlap\n-#include <assert.h>\n-#include <stdlib.h>\n-#include <float.h>\n-#include <string.h> /* for memcpy */\n-\n-#define VECTOR_SIZE_BYTES 16\n-\n-/*\n- * Dispatcher functions\n- * decide whether the operation can be vectorized and run it\n- * if it was run returns true and false if nothing was done\n- */\n-\n-/*\n- *****************************************************************************\n- **                           CMPLX DISPATCHERS\n- *****************************************************************************\n- */\n-\n-/**begin repeat\n- * #TYPE = CFLOAT, CDOUBLE#\n- * #type= npy_float, npy_double#\n- * #esize = 8, 16#\n- */\n-\n-/**begin repeat1\n- *  #func = square, absolute, conjugate#\n- *  #outsize = 1, 2, 1#\n- *  #max_stride = 2, 8, 8#\n- */\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n-static inline NPY_GCC_TARGET_AVX512F void\n-AVX512F_@func@_@TYPE@(@type@*, @type@*, const npy_intp n, const npy_intp stride);\n-#endif\n-\n-static inline int\n-run_unary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)\n-{\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n-    if ((IS_OUTPUT_BLOCKABLE_UNARY(@esize@, (npy_uint)(@esize@/@outsize@), 64)) && (labs(steps[0]) < 2*@max_stride@*@esize@)) {\n-        AVX512F_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0], steps[0]);\n-        return 1;\n-    }\n-    else\n-        return 0;\n-#endif\n-    return 0;\n-}\n-\n-/**end repeat1**/\n-/**end repeat**/\n-\n-#ifdef NPY_HAVE_SSE2_INTRINSICS\n-\n-/*\n- * Vectorized operations\n- */\n-/*\n- *****************************************************************************\n- **                           FLOAT LOOPS\n- *****************************************************************************\n- */\n-\n-/**begin repeat\n-* horizontal reductions on a vector\n-* # VOP = min, max#\n-*/\n-\n-NPY_FINLINE npy_float sse2_horizontal_@VOP@___m128(__m128 v)\n-{\n-    npy_float r;\n-    __m128 tmp = _mm_movehl_ps(v, v);                   /* c     d     ... */\n-    __m128 m = _mm_@VOP@_ps(v, tmp);                    /* m(ac) m(bd) ... */\n-    tmp = _mm_shuffle_ps(m, m, _MM_SHUFFLE(1, 1, 1, 1));/* m(bd) m(bd) ... */\n-    _mm_store_ss(&r, _mm_@VOP@_ps(tmp, m));             /* m(acbd) ... */\n-    return r;\n-}\n-\n-NPY_FINLINE npy_double sse2_horizontal_@VOP@___m128d(__m128d v)\n-{\n-    npy_double r;\n-    __m128d tmp = _mm_unpackhi_pd(v, v);    /* b     b */\n-    _mm_store_sd(&r, _mm_@VOP@_pd(tmp, v)); /* m(ab) m(bb) */\n-    return r;\n-}\n-/**end repeat**/\n-\n-/* bunch of helper functions used in ISA_exp/log_FLOAT*/\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_get_full_load_mask_ps(void)\n-{\n-    return _mm256_set1_ps(-1.0);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i\n-fma_get_full_load_mask_pd(void)\n-{\n-    return _mm256_castpd_si256(_mm256_set1_pd(-1.0));\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_get_partial_load_mask_ps(const npy_int num_elem, const npy_int num_lanes)\n-{\n-    float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,\n-                            1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};\n-    float* addr = maskint + num_lanes - num_elem;\n-    return _mm256_loadu_ps(addr);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i\n-fma_get_partial_load_mask_pd(const npy_int num_elem, const npy_int num_lanes)\n-{\n-    npy_int maskint[16] = {-1,-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1,1,1,1};\n-    npy_int* addr = maskint + 2*num_lanes - 2*num_elem;\n-    return _mm256_loadu_si256((__m256i*) addr);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_masked_gather_ps(__m256 src,\n-                     npy_float* addr,\n-                     __m256i vindex,\n-                     __m256 mask)\n-{\n-    return _mm256_mask_i32gather_ps(src, addr, vindex, mask, 4);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d\n-fma_masked_gather_pd(__m256d src,\n-                     npy_double* addr,\n-                     __m128i vindex,\n-                     __m256d mask)\n-{\n-    return _mm256_mask_i32gather_pd(src, addr, vindex, mask, 8);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_masked_load_ps(__m256 mask, npy_float* addr)\n-{\n-    return _mm256_maskload_ps(addr, _mm256_cvtps_epi32(mask));\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d\n-fma_masked_load_pd(__m256i mask, npy_double* addr)\n-{\n-    return _mm256_maskload_pd(addr, mask);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_set_masked_lanes_ps(__m256 x, __m256 val, __m256 mask)\n-{\n-    return _mm256_blendv_ps(x, val, mask);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d\n-fma_set_masked_lanes_pd(__m256d x, __m256d val, __m256d mask)\n-{\n-    return _mm256_blendv_pd(x, val, mask);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_blend(__m256 x, __m256 y, __m256 ymask)\n-{\n-    return _mm256_blendv_ps(x, y, ymask);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_invert_mask_ps(__m256 ymask)\n-{\n-    return _mm256_andnot_ps(ymask, _mm256_set1_ps(-1.0));\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i\n-fma_invert_mask_pd(__m256i ymask)\n-{\n-    return _mm256_andnot_si256(ymask, _mm256_set1_epi32(0xFFFFFFFF));\n-}\n-\n-/**begin repeat\n- *  #vsub = ps, pd#\n- *  #vtype = __m256, __m256d#\n- */\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n-fma_abs_@vsub@(@vtype@ x)\n-{\n-    return _mm256_andnot_@vsub@(_mm256_set1_@vsub@(-0.0), x);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n-fma_reciprocal_@vsub@(@vtype@ x)\n-{\n-    return _mm256_div_@vsub@(_mm256_set1_@vsub@(1.0f), x);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n-fma_rint_@vsub@(@vtype@ x)\n-{\n-    return _mm256_round_@vsub@(x, _MM_FROUND_TO_NEAREST_INT);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n-fma_floor_@vsub@(@vtype@ x)\n-{\n-    return _mm256_round_@vsub@(x, _MM_FROUND_TO_NEG_INF);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n-fma_trunc_@vsub@(@vtype@ x)\n-{\n-    return _mm256_round_@vsub@(x, _MM_FROUND_TO_ZERO);\n-}\n-/**end repeat**/\n-#endif\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n-avx512_get_full_load_mask_ps(void)\n-{\n-    return 0xFFFF;\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n-avx512_get_full_load_mask_pd(void)\n-{\n-    return 0xFF;\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n-avx512_get_partial_load_mask_ps(const npy_int num_elem, const npy_int total_elem)\n-{\n-    return (0x0001 << num_elem) - 0x0001;\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n-avx512_get_partial_load_mask_pd(const npy_int num_elem, const npy_int total_elem)\n-{\n-    return (0x01 << num_elem) - 0x01;\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_masked_gather_ps(__m512 src,\n-                        npy_float* addr,\n-                        __m512i vindex,\n-                        __mmask16 kmask)\n-{\n-    return _mm512_mask_i32gather_ps(src, kmask, vindex, addr, 4);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d\n-avx512_masked_gather_pd(__m512d src,\n-                        npy_double* addr,\n-                        __m256i vindex,\n-                        __mmask8 kmask)\n-{\n-    return _mm512_mask_i32gather_pd(src, kmask, vindex, addr, 8);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_masked_load_ps(__mmask16 mask, npy_float* addr)\n-{\n-    return _mm512_maskz_loadu_ps(mask, (__m512 *)addr);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d\n-avx512_masked_load_pd(__mmask8 mask, npy_double* addr)\n-{\n-    return _mm512_maskz_loadu_pd(mask, (__m512d *)addr);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_set_masked_lanes_ps(__m512 x, __m512 val, __mmask16 mask)\n-{\n-    return _mm512_mask_blend_ps(mask, x, val);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d\n-avx512_set_masked_lanes_pd(__m512d x, __m512d val, __mmask8 mask)\n-{\n-    return _mm512_mask_blend_pd(mask, x, val);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_blend(__m512 x, __m512 y, __mmask16 ymask)\n-{\n-    return _mm512_mask_mov_ps(x, ymask, y);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n-avx512_invert_mask_ps(__mmask16 ymask)\n-{\n-    return _mm512_knot(ymask);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n-avx512_invert_mask_pd(__mmask8 ymask)\n-{\n-    return _mm512_knot(ymask);\n-}\n-\n-/**begin repeat\n- *  #vsub  = ps, pd#\n- *  #type= npy_float, npy_double#\n- *  #epi_vsub  = epi32, epi64#\n- *  #vtype = __m512, __m512d#\n- *  #mask = __mmask16, __mmask8#\n- *  #and_const = 0x7fffffff, 0x7fffffffffffffffLL#\n- *  #neg_mask = 0x80000000, 0x8000000000000000#\n- *  #perm_ = 0xb1, 0x55#\n- *  #cmpx_img_mask = 0xAAAA, 0xAA#\n- *  #cmpx_re_mask = 0x5555, 0x55#\n- *  #INF = NPY_INFINITYF, NPY_INFINITY#\n- *  #NAN = NPY_NANF, NPY_NAN#\n- */\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_abs_@vsub@(@vtype@ x)\n-{\n-    return (@vtype@) _mm512_and_@epi_vsub@((__m512i) x,\n-\t\t\t\t    _mm512_set1_@epi_vsub@ (@and_const@));\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_reciprocal_@vsub@(@vtype@ x)\n-{\n-    return _mm512_div_@vsub@(_mm512_set1_@vsub@(1.0f), x);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_rint_@vsub@(@vtype@ x)\n-{\n-    return _mm512_roundscale_@vsub@(x, 0x08);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_floor_@vsub@(@vtype@ x)\n-{\n-    return _mm512_roundscale_@vsub@(x, 0x09);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_trunc_@vsub@(@vtype@ x)\n-{\n-    return _mm512_roundscale_@vsub@(x, 0x0B);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_hadd_@vsub@(const @vtype@ x)\n-{\n-    return _mm512_add_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_hsub_@vsub@(const @vtype@ x)\n-{\n-    return _mm512_sub_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_cabsolute_@vsub@(const @vtype@ x1,\n-                        const @vtype@ x2,\n-                        const __m512i re_indices,\n-                        const __m512i im_indices)\n-{\n-    @vtype@ inf = _mm512_set1_@vsub@(@INF@);\n-    @vtype@ nan = _mm512_set1_@vsub@(@NAN@);\n-    @vtype@ x1_abs = avx512_abs_@vsub@(x1);\n-    @vtype@ x2_abs = avx512_abs_@vsub@(x2);\n-    @vtype@ re = _mm512_permutex2var_@vsub@(x1_abs, re_indices, x2_abs);\n-    @vtype@ im = _mm512_permutex2var_@vsub@(x1_abs, im_indices , x2_abs);\n-    /*\n-     * If real or imag = INF, then convert it to inf + j*inf\n-     * Handles: inf + j*nan, nan + j*inf\n-     */\n-    @mask@ re_infmask = _mm512_cmp_@vsub@_mask(re, inf, _CMP_EQ_OQ);\n-    @mask@ im_infmask = _mm512_cmp_@vsub@_mask(im, inf, _CMP_EQ_OQ);\n-    im = _mm512_mask_mov_@vsub@(im, re_infmask, inf);\n-    re = _mm512_mask_mov_@vsub@(re, im_infmask, inf);\n-\n-    /*\n-     * If real or imag = NAN, then convert it to nan + j*nan\n-     * Handles: x + j*nan, nan + j*x\n-     */\n-    @mask@ re_nanmask = _mm512_cmp_@vsub@_mask(re, re, _CMP_NEQ_UQ);\n-    @mask@ im_nanmask = _mm512_cmp_@vsub@_mask(im, im, _CMP_NEQ_UQ);\n-    im = _mm512_mask_mov_@vsub@(im, re_nanmask, nan);\n-    re = _mm512_mask_mov_@vsub@(re, im_nanmask, nan);\n-\n-    @vtype@ larger  = _mm512_max_@vsub@(re, im);\n-    @vtype@ smaller = _mm512_min_@vsub@(im, re);\n-\n-    /*\n-     * Calculate div_mask to prevent 0./0. and inf/inf operations in div\n-     */\n-    @mask@ zeromask = _mm512_cmp_@vsub@_mask(larger, _mm512_setzero_@vsub@(), _CMP_EQ_OQ);\n-    @mask@ infmask = _mm512_cmp_@vsub@_mask(smaller, inf, _CMP_EQ_OQ);\n-    @mask@ div_mask = _mm512_knot(_mm512_kor(zeromask, infmask));\n-    @vtype@ ratio = _mm512_maskz_div_@vsub@(div_mask, smaller, larger);\n-    @vtype@ hypot = _mm512_sqrt_@vsub@(_mm512_fmadd_@vsub@(\n-                                        ratio, ratio, _mm512_set1_@vsub@(1.0f)));\n-    return _mm512_mul_@vsub@(hypot, larger);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_conjugate_@vsub@(const @vtype@ x)\n-{\n-    /*\n-     * __mm512_mask_xor_ps/pd requires AVX512DQ. We cast it to __m512i and\n-     * use the xor_epi32/64 uinstruction instead. Cast is a zero latency instruction\n-     */\n-    __m512i cast_x = _mm512_cast@vsub@_si512(x);\n-    __m512i res = _mm512_mask_xor_@epi_vsub@(cast_x, @cmpx_img_mask@,\n-                                        cast_x, _mm512_set1_@epi_vsub@(@neg_mask@));\n-    return _mm512_castsi512_@vsub@(res);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_cmul_@vsub@(@vtype@ x1, @vtype@ x2)\n-{\n-    // x1 = r1, i1\n-    // x2 = r2, i2\n-    @vtype@ x3  = _mm512_permute_@vsub@(x2, @perm_@);   // i2, r2\n-    @vtype@ x12 = _mm512_mul_@vsub@(x1, x2);            // r1*r2, i1*i2\n-    @vtype@ x13 = _mm512_mul_@vsub@(x1, x3);            // r1*i2, r2*i1\n-    @vtype@ outreal = avx512_hsub_@vsub@(x12);          // r1*r2 - i1*i2, r1*r2 - i1*i2\n-    @vtype@ outimg  = avx512_hadd_@vsub@(x13);          // r1*i2 + i1*r2, r1*i2 + i1*r2\n-    return _mm512_mask_blend_@vsub@(@cmpx_img_mask@, outreal, outimg);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n-avx512_csquare_@vsub@(@vtype@ x)\n-{\n-    return avx512_cmul_@vsub@(x, x);\n-}\n-\n-/**end repeat**/\n-#endif\n-\n-/**begin repeat\n- * #ISA = FMA, AVX512F#\n- * #isa = fma, avx512#\n- * #vtype = __m256, __m512#\n- * #vsize = 256, 512#\n- * #or = or_ps, kor#\n- * #vsub = , _mask#\n- * #mask = __m256, __mmask16#\n- * #fmadd = _mm256_fmadd_ps, _mm512_fmadd_ps#\n- * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n- **/\n-\n-#if defined @CHK@\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n-@isa@_sqrt_ps(@vtype@ x)\n-{\n-    return _mm@vsize@_sqrt_ps(x);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n-@isa@_sqrt_pd(@vtype@d x)\n-{\n-    return _mm@vsize@_sqrt_pd(x);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n-@isa@_square_ps(@vtype@ x)\n-{\n-    return _mm@vsize@_mul_ps(x,x);\n-}\n-\n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n-@isa@_square_pd(@vtype@d x)\n-{\n-    return _mm@vsize@_mul_pd(x,x);\n-}\n-\n-#endif\n-/**end repeat**/\n-\n-/**begin repeat\n- * #TYPE = CFLOAT, CDOUBLE#\n- * #type = npy_float, npy_double#\n- * #num_lanes = 16, 8#\n- * #vsuffix = ps, pd#\n- * #epi_vsub  = epi32, epi64#\n- * #mask = __mmask16, __mmask8#\n- * #vtype = __m512, __m512d#\n- * #scale = 4, 8#\n- * #vindextype = __m512i, __m256i#\n- * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#\n- * #storemask = 0xFF, 0xF#\n- * #IS_FLOAT = 1, 0#\n- */\n-\n-/**begin repeat1\n- *  #func = square, conjugate#\n- *  #vectorf = avx512_csquare, avx512_conjugate#\n- */\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n-static NPY_GCC_OPT_3 inline NPY_GCC_TARGET_AVX512F void\n-AVX512F_@func@_@TYPE@(@type@ * op,\n-                      @type@ * ip,\n-                      const npy_intp array_size,\n-                      const npy_intp steps)\n-{\n-    npy_intp num_remaining_elements = 2*array_size;\n-    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(@type@)/2;\n-\n-     /*\n-      * Note: while generally indices are npy_intp, we ensure that our maximum index\n-      * will fit in an int32 as a precondition for this function via max_stride\n-      */\n-    npy_int32 index_ip1[16];\n-    for (npy_int32 ii = 0; ii < @num_lanes@; ii=ii+2) {\n-        index_ip1[ii] = ii*stride_ip1;\n-        index_ip1[ii+1] = ii*stride_ip1 + 1;\n-    }\n-    @vindextype@ vindex = @vindexload@((@vindextype@*)index_ip1);\n-    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();\n-    @vtype@ zeros = _mm512_setzero_@vsuffix@();\n-\n-    while (num_remaining_elements > 0) {\n-        if (num_remaining_elements < @num_lanes@) {\n-            load_mask = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements, @num_lanes@);\n-        }\n-        @vtype@ x1;\n-        if (stride_ip1 == 1) {\n-            x1 = avx512_masked_load_@vsuffix@(load_mask, ip);\n-        }\n-        else {\n-            x1  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex, load_mask);\n-        }\n-\n-        @vtype@ out = @vectorf@_@vsuffix@(x1);\n-\n-        _mm512_mask_storeu_@vsuffix@(op, load_mask, out);\n-        op += @num_lanes@;\n-        ip += @num_lanes@*stride_ip1;\n-        num_remaining_elements -= @num_lanes@;\n-    }\n-}\n-#endif\n-/**end repeat1**/\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n-static NPY_GCC_OPT_3 inline NPY_GCC_TARGET_AVX512F void\n-AVX512F_absolute_@TYPE@(@type@ * op,\n-                        @type@ * ip,\n-                        const npy_intp array_size,\n-                        const npy_intp steps)\n-{\n-    npy_intp num_remaining_elements = 2*array_size;\n-    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(@type@)/2;\n-\n-    /*\n-     * Note: while generally indices are npy_intp, we ensure that our maximum index\n-     * will fit in an int32 as a precondition for this function via max_stride\n-     */\n-    npy_int32 index_ip[32];\n-    for (npy_int32 ii = 0; ii < 2*@num_lanes@; ii=ii+2) {\n-        index_ip[ii] = ii*stride_ip1;\n-        index_ip[ii+1] = ii*stride_ip1 + 1;\n-    }\n-    @vindextype@ vindex1 = @vindexload@((@vindextype@*)index_ip);\n-    @vindextype@ vindex2 = @vindexload@((@vindextype@*)(index_ip+@num_lanes@));\n-\n-    @mask@ load_mask1 = avx512_get_full_load_mask_@vsuffix@();\n-    @mask@ load_mask2 = avx512_get_full_load_mask_@vsuffix@();\n-    @mask@ store_mask = avx512_get_full_load_mask_@vsuffix@();\n-    @vtype@ zeros = _mm512_setzero_@vsuffix@();\n-\n-#if @IS_FLOAT@\n-    __m512i re_index = _mm512_set_epi32(30,28,26,24,22,20,18,16,14,12,10,8,6,4,2,0);\n-    __m512i im_index  = _mm512_set_epi32(31,29,27,25,23,21,19,17,15,13,11,9,7,5,3,1);\n-#else\n-    __m512i re_index = _mm512_set_epi64(14,12,10,8,6,4,2,0);\n-    __m512i im_index  = _mm512_set_epi64(15,13,11,9,7,5,3,1);\n-#endif\n-\n-    while (num_remaining_elements > 0) {\n-        if (num_remaining_elements < @num_lanes@) {\n-            load_mask1 = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements, @num_lanes@);\n-            load_mask2 = 0x0000;\n-            store_mask = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements/2, @num_lanes@);\n-        } else if (num_remaining_elements < 2*@num_lanes@) {\n-            load_mask1 = avx512_get_full_load_mask_@vsuffix@();\n-            load_mask2 = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements - @num_lanes@, @num_lanes@);\n-            store_mask = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements/2, @num_lanes@);\n-        }\n-        @vtype@ x1, x2;\n-        if (stride_ip1 == 1) {\n-            x1 = avx512_masked_load_@vsuffix@(load_mask1, ip);\n-            x2 = avx512_masked_load_@vsuffix@(load_mask2, ip+@num_lanes@);\n-        }\n-        else {\n-            x1  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex1, load_mask1);\n-            x2  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex2, load_mask2);\n-        }\n-\n-        @vtype@ out = avx512_cabsolute_@vsuffix@(x1, x2, re_index, im_index);\n-\n-        _mm512_mask_storeu_@vsuffix@(op, store_mask, out);\n-        op += @num_lanes@;\n-        ip += 2*@num_lanes@*stride_ip1;\n-        num_remaining_elements -= 2*@num_lanes@;\n-    }\n-    npy_clear_floatstatus_barrier((char*)&num_remaining_elements);\n-}\n-\n-#endif\n-/**end repeat**/\n-\n-#undef VECTOR_SIZE_BYTES\n-#endif  /* NPY_HAVE_SSE2_INTRINSICS */\n-#endif\n-"
            },
            {
                "filename": "numpy/core/tests/test_simd.py",
                "patch": "@@ -35,6 +35,9 @@ def __getattr__(self, attr):\n         \"\"\"\n         return getattr(self.npyv, attr + \"_\" + self.sfx)\n \n+    def _x2(self, intrin_name):\n+        return getattr(self.npyv, f\"{intrin_name}_{self.sfx}x2\")\n+\n     def _data(self, start=None, count=None, reverse=False):\n         \"\"\"\n         Create list of consecutive numbers according to number of vector's lanes.\n@@ -380,6 +383,11 @@ def test_arithmetic_fused(self):\n         nfms = self.nmulsub(vdata_a, vdata_b, vdata_c)\n         data_nfms = self.mul(data_fma, self.setall(-1))\n         assert nfms == data_nfms\n+        # multiply, add for odd elements and subtract even elements.\n+        # (a * b) -+ c\n+        fmas = list(self.muladdsub(vdata_a, vdata_b, vdata_c))\n+        assert fmas[0::2] == list(data_fms)[0::2]\n+        assert fmas[1::2] == list(data_fma)[1::2]\n \n     def test_abs(self):\n         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()\n@@ -678,131 +686,194 @@ def test_memory_store(self):\n         assert store_h[:self.nlanes//2] == data[self.nlanes//2:]\n         assert store_h != vdata  # detect overflow\n \n-    def test_memory_partial_load(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale, fill\", [\n+        (\"self.load_tillz, self.load_till\", (32, 64), 1, [0xffff]),\n+        (\"self.load2_tillz, self.load2_till\", (32, 64), 2, [0xffff, 0x7fff]),\n+    ])\n+    def test_memory_partial_load(self, intrin, elsizes, scale, fill):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_load_tillz, npyv_load_till = eval(intrin)\n         data = self._data()\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4] # test out of range\n         for n in lanes:\n-            load_till  = self.load_till(data, n, 15)\n-            data_till  = data[:n] + [15] * (self.nlanes-n)\n+            load_till = npyv_load_till(data, n, *fill)\n+            load_tillz = npyv_load_tillz(data, n)\n+            n *= scale\n+            data_till = data[:n] + fill * ((self.nlanes-n) // scale)\n             assert load_till == data_till\n-            load_tillz = self.load_tillz(data, n)\n             data_tillz = data[:n] + [0] * (self.nlanes-n)\n             assert load_tillz == data_tillz\n \n-    def test_memory_partial_store(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.store_till\", (32, 64), 1),\n+        (\"self.store2_till\", (32, 64), 2),\n+    ])\n+    def test_memory_partial_store(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_store_till = eval(intrin)\n         data = self._data()\n         data_rev = self._data(reverse=True)\n         vdata = self.load(data)\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4]\n         for n in lanes:\n             data_till = data_rev.copy()\n-            data_till[:n] = data[:n]\n+            data_till[:n*scale] = data[:n*scale]\n             store_till = self._data(reverse=True)\n-            self.store_till(store_till, n, vdata)\n+            npyv_store_till(store_till, n, vdata)\n             assert store_till == data_till\n \n-    def test_memory_noncont_load(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.loadn\", (32, 64), 1),\n+        (\"self.loadn2\", (32, 64), 2),\n+    ])\n+    def test_memory_noncont_load(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n-        for stride in range(1, 64):\n-            data = self._data(count=stride*self.nlanes)\n-            data_stride = data[::stride]\n-            loadn = self.loadn(data, stride)\n-            assert loadn == data_stride\n-\n-        for stride in range(-64, 0):\n-            data = self._data(stride, -stride*self.nlanes)\n-            data_stride = self.load(data[::stride]) # cast unsigned\n-            loadn = self.loadn(data, stride)\n+        npyv_loadn = eval(intrin)\n+        for stride in range(-64, 64):\n+            if stride < 0:\n+                data = self._data(stride, -stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[-i::stride] for i in range(scale, 0, -1)])\n+                ))\n+            elif stride == 0:\n+                data = self._data()\n+                data_stride = data[0:scale] * (self.nlanes//scale)\n+            else:\n+                data = self._data(count=stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[i::stride] for i in range(scale)]))\n+                )\n+            data_stride = self.load(data_stride)  # cast unsigned\n+            loadn = npyv_loadn(data, stride)\n             assert loadn == data_stride\n \n-    def test_memory_noncont_partial_load(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale, fill\", [\n+        (\"self.loadn_tillz, self.loadn_till\", (32, 64), 1, [0xffff]),\n+        (\"self.loadn2_tillz, self.loadn2_till\", (32, 64), 2, [0xffff, 0x7fff]),\n+    ])\n+    def test_memory_noncont_partial_load(self, intrin, elsizes, scale, fill):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_loadn_tillz, npyv_loadn_till = eval(intrin)\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4]\n-        for stride in range(1, 64):\n-            data = self._data(count=stride*self.nlanes)\n-            data_stride = data[::stride]\n-            for n in lanes:\n-                data_stride_till = data_stride[:n] + [15] * (self.nlanes-n)\n-                loadn_till = self.loadn_till(data, stride, n, 15)\n-                assert loadn_till == data_stride_till\n-                data_stride_tillz = data_stride[:n] + [0] * (self.nlanes-n)\n-                loadn_tillz = self.loadn_tillz(data, stride, n)\n-                assert loadn_tillz == data_stride_tillz\n-\n-        for stride in range(-64, 0):\n-            data = self._data(stride, -stride*self.nlanes)\n-            data_stride = list(self.load(data[::stride])) # cast unsigned\n+        for stride in range(-64, 64):\n+            if stride < 0:\n+                data = self._data(stride, -stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[-i::stride] for i in range(scale, 0, -1)])\n+                ))\n+            elif stride == 0:\n+                data = self._data()\n+                data_stride = data[0:scale] * (self.nlanes//scale)\n+            else:\n+                data = self._data(count=stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[i::stride] for i in range(scale)])\n+                ))\n+            data_stride = list(self.load(data_stride))  # cast unsigned\n             for n in lanes:\n-                data_stride_till = data_stride[:n] + [15] * (self.nlanes-n)\n-                loadn_till = self.loadn_till(data, stride, n, 15)\n+                nscale = n * scale\n+                llanes = self.nlanes - nscale\n+                data_stride_till = (\n+                    data_stride[:nscale] + fill * (llanes//scale)\n+                )\n+                loadn_till = npyv_loadn_till(data, stride, n, *fill)\n                 assert loadn_till == data_stride_till\n-                data_stride_tillz = data_stride[:n] + [0] * (self.nlanes-n)\n-                loadn_tillz = self.loadn_tillz(data, stride, n)\n+                data_stride_tillz = data_stride[:nscale] + [0] * llanes\n+                loadn_tillz = npyv_loadn_tillz(data, stride, n)\n                 assert loadn_tillz == data_stride_tillz\n \n-    def test_memory_noncont_store(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.storen\", (32, 64), 1),\n+        (\"self.storen2\", (32, 64), 2),\n+    ])\n+    def test_memory_noncont_store(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n-        vdata = self.load(self._data())\n+        npyv_storen = eval(intrin)\n+        data = self._data()\n+        vdata = self.load(data)\n+        hlanes = self.nlanes // scale\n         for stride in range(1, 64):\n-            data = [15] * stride * self.nlanes\n-            data[::stride] = vdata\n-            storen = [15] * stride * self.nlanes\n-            storen += [127]*64\n-            self.storen(storen, stride, vdata)\n-            assert storen[:-64] == data\n-            assert storen[-64:] == [127]*64 # detect overflow\n+            data_storen = [0xff] * stride * self.nlanes\n+            for s in range(0, hlanes*stride, stride):\n+                i = (s//stride)*scale\n+                data_storen[s:s+scale] = data[i:i+scale]\n+            storen = [0xff] * stride * self.nlanes\n+            storen += [0x7f]*64\n+            npyv_storen(storen, stride, vdata)\n+            assert storen[:-64] == data_storen\n+            assert storen[-64:] == [0x7f]*64  # detect overflow\n \n         for stride in range(-64, 0):\n-            data = [15] * -stride * self.nlanes\n-            data[::stride] = vdata\n-            storen = [127]*64\n-            storen += [15] * -stride * self.nlanes\n-            self.storen(storen, stride, vdata)\n-            assert storen[64:] == data\n-            assert storen[:64] == [127]*64 # detect overflow\n-\n-    def test_memory_noncont_partial_store(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+            data_storen = [0xff] * -stride * self.nlanes\n+            for s in range(0, hlanes*stride, stride):\n+                i = (s//stride)*scale\n+                data_storen[s-scale:s or None] = data[i:i+scale]\n+            storen = [0x7f]*64\n+            storen += [0xff] * -stride * self.nlanes\n+            npyv_storen(storen, stride, vdata)\n+            assert storen[64:] == data_storen\n+            assert storen[:64] == [0x7f]*64  # detect overflow\n+        # stride 0\n+        data_storen = [0x7f] * self.nlanes\n+        storen = data_storen.copy()\n+        data_storen[0:scale] = data[-scale:]\n+        npyv_storen(storen, 0, vdata)\n+        assert storen == data_storen\n+\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.storen_till\", (32, 64), 1),\n+        (\"self.storen2_till\", (32, 64), 2),\n+    ])\n+    def test_memory_noncont_partial_store(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_storen_till = eval(intrin)\n         data = self._data()\n         vdata = self.load(data)\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4]\n+        hlanes = self.nlanes // scale\n         for stride in range(1, 64):\n             for n in lanes:\n-                data_till = [15] * stride * self.nlanes\n-                data_till[::stride] = data[:n] + [15] * (self.nlanes-n)\n-                storen_till = [15] * stride * self.nlanes\n-                storen_till += [127]*64\n-                self.storen_till(storen_till, stride, n, vdata)\n+                data_till = [0xff] * stride * self.nlanes\n+                tdata = data[:n*scale] + [0xff] * (self.nlanes-n*scale)\n+                for s in range(0, hlanes*stride, stride)[:n]:\n+                    i = (s//stride)*scale\n+                    data_till[s:s+scale] = tdata[i:i+scale]\n+                storen_till = [0xff] * stride * self.nlanes\n+                storen_till += [0x7f]*64\n+                npyv_storen_till(storen_till, stride, n, vdata)\n                 assert storen_till[:-64] == data_till\n-                assert storen_till[-64:] == [127]*64 # detect overflow\n+                assert storen_till[-64:] == [0x7f]*64  # detect overflow\n \n         for stride in range(-64, 0):\n             for n in lanes:\n-                data_till = [15] * -stride * self.nlanes\n-                data_till[::stride] = data[:n] + [15] * (self.nlanes-n)\n-                storen_till = [127]*64\n-                storen_till += [15] * -stride * self.nlanes\n-                self.storen_till(storen_till, stride, n, vdata)\n+                data_till = [0xff] * -stride * self.nlanes\n+                tdata = data[:n*scale] + [0xff] * (self.nlanes-n*scale)\n+                for s in range(0, hlanes*stride, stride)[:n]:\n+                    i = (s//stride)*scale\n+                    data_till[s-scale:s or None] = tdata[i:i+scale]\n+                storen_till = [0x7f]*64\n+                storen_till += [0xff] * -stride * self.nlanes\n+                npyv_storen_till(storen_till, stride, n, vdata)\n                 assert storen_till[64:] == data_till\n-                assert storen_till[:64] == [127]*64 # detect overflow\n+                assert storen_till[:64] == [0x7f]*64  # detect overflow\n+\n+        # stride 0\n+        for n in lanes:\n+            data_till = [0x7f] * self.nlanes\n+            storen_till = data_till.copy()\n+            data_till[0:scale] = data[:n*scale][-scale:]\n+            npyv_storen_till(storen_till, 0, n, vdata)\n+            assert storen_till == data_till\n \n     @pytest.mark.parametrize(\"intrin, table_size, elsize\", [\n         (\"self.lut32\", 32, 32),\n@@ -886,13 +957,27 @@ def test_reorder(self):\n         combineh = self.combineh(vdata_a, vdata_b)\n         assert combineh == data_a_hi + data_b_hi\n         # combine x2\n-        combine  = self.combine(vdata_a, vdata_b)\n+        combine = self.combine(vdata_a, vdata_b)\n         assert combine == (data_a_lo + data_b_lo, data_a_hi + data_b_hi)\n+\n         # zip(interleave)\n-        data_zipl = [v for p in zip(data_a_lo, data_b_lo) for v in p]\n-        data_ziph = [v for p in zip(data_a_hi, data_b_hi) for v in p]\n-        vzip  = self.zip(vdata_a, vdata_b)\n+        data_zipl = self.load([\n+            v for p in zip(data_a_lo, data_b_lo) for v in p\n+        ])\n+        data_ziph = self.load([\n+            v for p in zip(data_a_hi, data_b_hi) for v in p\n+        ])\n+        vzip = self.zip(vdata_a, vdata_b)\n         assert vzip == (data_zipl, data_ziph)\n+        vzip = [0]*self.nlanes*2\n+        self._x2(\"store\")(vzip, (vdata_a, vdata_b))\n+        assert vzip == list(data_zipl) + list(data_ziph)\n+\n+        # unzip(deinterleave)\n+        unzip = self.unzip(data_zipl, data_ziph)\n+        assert unzip == (data_a, data_b)\n+        unzip = self._x2(\"load\")(list(data_zipl) + list(data_ziph))\n+        assert unzip == (data_a, data_b)\n \n     def test_reorder_rev64(self):\n         # Reverse elements of each 64-bit lane\n@@ -906,6 +991,28 @@ def test_reorder_rev64(self):\n         rev64 = self.rev64(self.load(range(self.nlanes)))\n         assert rev64 == data_rev64\n \n+    def test_reorder_permi128(self):\n+        \"\"\"\n+        Test permuting elements for each 128-bit lane.\n+        npyv_permi128_##sfx\n+        \"\"\"\n+        ssize = self._scalar_size()\n+        if ssize < 32:\n+            return\n+        data = self.load(self._data())\n+        permn = 128//ssize\n+        permd = permn-1\n+        nlane128 = self.nlanes//permn\n+        shfl = [0, 1] if ssize == 64 else [0, 2, 4, 6]\n+        for i in range(permn):\n+            indices = [(i >> shf) & permd for shf in shfl]\n+            vperm = self.permi128(data, *indices)\n+            data_vperm = [\n+                data[j + (e & -permn)]\n+                for e, j in enumerate(indices*nlane128)\n+            ]\n+            assert vperm == data_vperm\n+\n     @pytest.mark.parametrize('func, intrin', [\n         (operator.lt, \"cmplt\"),\n         (operator.le, \"cmple\"),\n@@ -1168,6 +1275,18 @@ def test_mask_conditional(self):\n         ifadd = self.ifadd(false_mask, vdata_a, vdata_b, vdata_b)\n         assert ifadd == vdata_b\n \n+        if not self._is_fp():\n+            return\n+        data_div = self.div(vdata_b, vdata_a)\n+        ifdiv = self.ifdiv(true_mask, vdata_b, vdata_a, vdata_b)\n+        assert ifdiv == data_div\n+        ifdivz = self.ifdivz(true_mask, vdata_b, vdata_a)\n+        assert ifdivz == data_div\n+        ifdiv = self.ifdiv(false_mask, vdata_a, vdata_b, vdata_b)\n+        assert ifdiv == vdata_b\n+        ifdivz = self.ifdivz(false_mask, vdata_a, vdata_b)\n+        assert ifdivz == self.zero()\n+\n bool_sfx = (\"b8\", \"b16\", \"b32\", \"b64\")\n int_sfx = (\"u8\", \"s8\", \"u16\", \"s16\", \"u32\", \"s32\", \"u64\", \"s64\")\n fp_sfx  = (\"f32\", \"f64\")"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22165,
        "body": "NumPy has SIMD versions of float / double `isnan`, `isinf`, `isfinite`, and `signbit` for SSE2 and AVX-512.  The changes here replace the SSE2 version with one that uses universal intrinsics.  This allows other architectures to have SIMD versions of the functions too.\r\n\r\nApple M1: **up to 3.4x faster**\r\n```\r\n-      93.5\u00b10.3\u03bcs       89.9\u00b10.3\u03bcs     0.96  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 4, 'd')\r\n-     65.9\u00b10.09\u03bcs       62.9\u00b10.1\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 2, 'f')\r\n-     66.7\u00b10.09\u03bcs       63.4\u00b10.1\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 1, 'd')\r\n-      43.4\u00b10.5\u03bcs      40.2\u00b10.06\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 1, 'f')\r\n-      73.3\u00b10.3\u03bcs       66.6\u00b10.4\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 1, 'd')\r\n-        85.0\u00b11\u03bcs       77.2\u00b10.4\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 4, 'd')\r\n-      69.9\u00b10.1\u03bcs       63.1\u00b10.1\u03bcs     0.90  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 1, 'd')\r\n-      69.7\u00b10.7\u03bcs      62.4\u00b10.08\u03bcs     0.90  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 2, 'd')\r\n-      72.1\u00b10.1\u03bcs       64.3\u00b10.2\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 4, 'f')\r\n-      75.9\u00b10.2\u03bcs       67.1\u00b10.2\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 1, 'd')\r\n-      81.8\u00b10.9\u03bcs       72.4\u00b10.3\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 2, 'd')\r\n-      96.8\u00b10.8\u03bcs       85.3\u00b10.5\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 4, 'd')\r\n-      72.6\u00b10.3\u03bcs       63.3\u00b10.3\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 1, 'd')\r\n-      75.2\u00b10.5\u03bcs      65.0\u00b10.03\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 2, 'd')\r\n-        89.2\u00b12\u03bcs       77.0\u00b10.5\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 4, 'd')\r\n-       101\u00b10.9\u03bcs       86.1\u00b10.3\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 4, 'd')\r\n-      79.0\u00b10.8\u03bcs       67.1\u00b10.1\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 1, 'd')\r\n-      75.5\u00b10.3\u03bcs       63.3\u00b10.2\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 1, 'd')\r\n-      85.6\u00b10.7\u03bcs       71.7\u00b10.4\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 2, 'd')\r\n-      79.5\u00b10.8\u03bcs      66.4\u00b10.07\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 2, 'd')\r\n-         102\u00b12\u03bcs       84.2\u00b10.8\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 4, 'd')\r\n-        94.4\u00b13\u03bcs       77.5\u00b10.4\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 4, 'd')\r\n-        87.0\u00b11\u03bcs      70.7\u00b10.08\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 2, 'd')\r\n-      71.3\u00b10.1\u03bcs      57.9\u00b10.05\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 2, 'f')\r\n-      96.0\u00b10.2\u03bcs       77.3\u00b10.5\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 4, 'd')\r\n-      76.1\u00b10.3\u03bcs       61.2\u00b10.7\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 4, 'f')\r\n-     73.9\u00b10.05\u03bcs       58.7\u00b10.1\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 2, 'f')\r\n-      82.7\u00b10.8\u03bcs      65.1\u00b10.03\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 2, 'd')\r\n-        79.4\u00b11\u03bcs      61.8\u00b10.06\u03bcs     0.78  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 4, 'f')\r\n-      65.9\u00b10.6\u03bcs      50.8\u00b10.03\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 2, 'f')\r\n-      69.2\u00b10.2\u03bcs       53.2\u00b10.1\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 4, 'f')\r\n-     81.9\u00b10.09\u03bcs      62.2\u00b10.05\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 4, 'f')\r\n-      77.7\u00b10.8\u03bcs       58.1\u00b10.6\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 2, 'f')\r\n-     64.6\u00b10.05\u03bcs      46.7\u00b10.04\u03bcs     0.72  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 2, 'f')\r\n-      89.8\u00b10.1\u03bcs       62.7\u00b10.3\u03bcs     0.70  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 4, 'd')\r\n-        92.3\u00b11\u03bcs       64.2\u00b10.4\u03bcs     0.70  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 4, 'd')\r\n-      69.9\u00b10.8\u03bcs       48.7\u00b10.2\u03bcs     0.70  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 4, 'f')\r\n-     74.1\u00b10.09\u03bcs       49.9\u00b10.1\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 1, 'd')\r\n-      50.5\u00b10.5\u03bcs       33.7\u00b10.1\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 1, 'f')\r\n-      42.2\u00b10.3\u03bcs      28.0\u00b10.01\u03bcs     0.66  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 1, 'f')\r\n-        96.5\u00b11\u03bcs       63.4\u00b10.2\u03bcs     0.66  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 4, 'd')\r\n-      74.8\u00b10.6\u03bcs       48.4\u00b10.5\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 4, 'f')\r\n-      49.1\u00b10.3\u03bcs       31.7\u00b10.1\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 1, 'f')\r\n-      76.5\u00b10.1\u03bcs       49.1\u00b10.6\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 4, 'f')\r\n-     78.0\u00b10.07\u03bcs       50.0\u00b10.1\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 2, 'd')\r\n-      74.9\u00b10.5\u03bcs       47.6\u00b10.4\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 2, 'd')\r\n-      71.7\u00b10.7\u03bcs      44.5\u00b10.03\u03bcs     0.62  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 2, 'f')\r\n-      73.2\u00b10.1\u03bcs      45.3\u00b10.08\u03bcs     0.62  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 1, 'd')\r\n-      73.3\u00b10.7\u03bcs      45.1\u00b10.02\u03bcs     0.61  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 2, 'f')\r\n-      80.3\u00b10.2\u03bcs      49.3\u00b10.05\u03bcs     0.61  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 4, 'f')\r\n-      77.2\u00b10.7\u03bcs       45.6\u00b10.4\u03bcs     0.59  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 2, 'f')\r\n-      82.0\u00b10.2\u03bcs       48.1\u00b10.6\u03bcs     0.59  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 2, 'd')\r\n-     42.0\u00b10.04\u03bcs      24.0\u00b10.01\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 1, 'f')\r\n-     55.0\u00b10.02\u03bcs      31.5\u00b10.08\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 1, 'f')\r\n-      79.6\u00b10.2\u03bcs      45.4\u00b10.09\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 1, 'd')\r\n-      72.2\u00b10.7\u03bcs      40.8\u00b10.02\u03bcs     0.56  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 2, 'f')\r\n-      76.3\u00b10.1\u03bcs      41.9\u00b10.03\u03bcs     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 4, 'f')\r\n-      71.5\u00b10.6\u03bcs       39.2\u00b10.4\u03bcs     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 2, 'f')\r\n-      77.9\u00b10.8\u03bcs       42.5\u00b10.1\u03bcs     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 4, 'f')\r\n-      76.4\u00b10.7\u03bcs       39.2\u00b10.4\u03bcs     0.51  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 2, 'f')\r\n-      82.5\u00b10.1\u03bcs       41.2\u00b10.2\u03bcs     0.50  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 4, 'f')\r\n-     50.4\u00b10.01\u03bcs       22.4\u00b10.2\u03bcs     0.44  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 1, 'f')\r\n-     48.6\u00b10.04\u03bcs      20.3\u00b10.04\u03bcs     0.42  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 1, 'f')\r\n-      54.0\u00b10.5\u03bcs       22.3\u00b10.2\u03bcs     0.41  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 1, 'f')\r\n-      49.5\u00b10.2\u03bcs      17.5\u00b10.01\u03bcs     0.35  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 1, 'f')\r\n-     48.5\u00b10.03\u03bcs      15.4\u00b10.04\u03bcs     0.32  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 1, 'f')\r\n-      53.9\u00b10.5\u03bcs       15.4\u00b10.1\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 1, 'f')\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\n\r\n\r\nApple M1 Rosetta: **up to 2.1x faster**\r\n```\r\n...\r\n-       102\u00b10.4\u03bcs       83.4\u00b10.7\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 2, 'd')\r\n-      97.1\u00b10.1\u03bcs       78.9\u00b10.4\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 1, 'd')\r\n-     98.1\u00b10.05\u03bcs      79.5\u00b10.02\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 1, 'f')\r\n-       119\u00b10.1\u03bcs       79.3\u00b10.3\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 4, 'd')\r\n-       106\u00b10.2\u03bcs      66.1\u00b10.08\u03bcs     0.62  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 2, 'd')\r\n-     98.7\u00b10.05\u03bcs      58.9\u00b10.01\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 1, 'd')\r\n-      118\u00b10.05\u03bcs       60.6\u00b10.3\u03bcs     0.52  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 4, 'f')\r\n-       113\u00b10.2\u03bcs       55.2\u00b10.2\u03bcs     0.49  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 2, 'f')\r\n-      111\u00b10.07\u03bcs      52.1\u00b10.03\u03bcs     0.47  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 1, 'f')\r\n```\r\n\r\n\r\niMacPro (AVX512): Similar.  A handful of benchmarks are 15% faster and another are 15% faster.  Which ones show up where changes depending on the run.  Averaging all gains / losses we're at ~4% faster.\r\n```\r\n+       130\u00b10.9\u03bcs         148\u00b110\u03bcs     1.14  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 2, 'd')\r\n+       160\u00b10.7\u03bcs         180\u00b110\u03bcs     1.13  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 1, 4, 'd')\r\n+         132\u00b11\u03bcs         146\u00b110\u03bcs     1.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 4, 'f')\r\n+        201\u00b110\u03bcs          216\u00b11\u03bcs     1.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 4, 'd')\r\n+         250\u00b12\u03bcs          266\u00b11\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 4, 'd')\r\n-         134\u00b16\u03bcs          127\u00b11\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 4, 2, 'f')\r\n-         127\u00b14\u03bcs          120\u00b16\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 1, 'd')\r\n-      67.1\u00b10.5\u03bcs       63.2\u00b10.4\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 1, 'f')\r\n-         111\u00b16\u03bcs          105\u00b17\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 2, 'd')\r\n-        95.1\u00b13\u03bcs         89.0\u00b12\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 1, 'f')\r\n-        99.6\u00b12\u03bcs         92.6\u00b11\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 1, 1, 'd')\r\n-         198\u00b12\u03bcs          183\u00b12\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 2, 'd')\r\n-      87.0\u00b10.9\u03bcs         80.4\u00b11\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 2, 'f')\r\n-        96.1\u00b12\u03bcs         88.7\u00b12\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 1, 'd')\r\n-         118\u00b11\u03bcs          108\u00b12\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 2, 4, 'f')\r\n-         140\u00b13\u03bcs          129\u00b12\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isinf'>, 4, 2, 'f')\r\n-      67.4\u00b10.7\u03bcs         61.4\u00b11\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 1, 'f')\r\n-         111\u00b12\u03bcs        101\u00b10.8\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 2, 'f')\r\n-         139\u00b18\u03bcs          126\u00b14\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 2, 'f')\r\n-         118\u00b11\u03bcs        107\u00b10.7\u03bcs     0.90  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 4, 'f')\r\n-         142\u00b15\u03bcs        128\u00b10.8\u03bcs     0.90  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isfinite'>, 2, 2, 'd')\r\n-         115\u00b17\u03bcs          101\u00b13\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 1, 2, 'd')\r\n-         130\u00b17\u03bcs        114\u00b10.8\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 4, 1, 'f')\r\n-       129\u00b10.7\u03bcs        114\u00b10.5\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 1, 'd')\r\n-         118\u00b16\u03bcs          104\u00b11\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'isnan'>, 2, 4, 'f')\r\n-         129\u00b15\u03bcs        113\u00b10.7\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 4, 1, 'f')\r\n-         145\u00b19\u03bcs        126\u00b10.6\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 2, 2, 'd')\r\n-         116\u00b12\u03bcs       98.1\u00b10.7\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'signbit'>, 1, 2, 'd')\r\n```",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -216,6 +216,7 @@ numpy/core/src/_simd/_simd_inc.h\n # umath module\n numpy/core/src/umath/loops_unary.dispatch.c\n numpy/core/src/umath/loops_unary_fp.dispatch.c\n+numpy/core/src/umath/loops_unary_fp_le.dispatch.c\n numpy/core/src/umath/loops_arithm_fp.dispatch.c\n numpy/core/src/umath/loops_arithmetic.dispatch.c\n numpy/core/src/umath/loops_logical.dispatch.c"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -921,7 +921,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.isnan'),\n           'PyUFunc_IsFiniteTypeResolver',\n-          TD(noobj, simd=[('avx512_skx', 'fd')], out='?'),\n+          TD(noobj, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n           ),\n 'isnat':\n     Ufunc(1, 1, None,\n@@ -933,19 +933,19 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.isinf'),\n           'PyUFunc_IsFiniteTypeResolver',\n-          TD(noobj, simd=[('avx512_skx', 'fd')], out='?'),\n+          TD(noobj, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n           ),\n 'isfinite':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.isfinite'),\n           'PyUFunc_IsFiniteTypeResolver',\n-          TD(noobj, simd=[('avx512_skx', 'fd')], out='?'),\n+          TD(noobj, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n           ),\n 'signbit':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.signbit'),\n           None,\n-          TD(flts, simd=[('avx512_skx', 'fd')], out='?'),\n+          TD(flts, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n           ),\n 'copysign':\n     Ufunc(2, 1, None,"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -758,6 +758,7 @@ src_umath = [\n   src_file.process('src/umath/loops_umath_fp.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary_fp.dispatch.c.src'),\n+  src_file.process('src/umath/loops_unary_fp_le.dispatch.c.src'),\n   src_file.process('src/umath/matmul.c.src'),\n   src_file.process('src/umath/matmul.h.src'),\n   src_file.process('src/umath/simd.inc.src'),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1007,6 +1007,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops.c.src'),\n             join('src', 'umath', 'loops_unary.dispatch.c.src'),\n             join('src', 'umath', 'loops_unary_fp.dispatch.c.src'),\n+            join('src', 'umath', 'loops_unary_fp_le.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),\n             join('src', 'umath', 'loops_logical.dispatch.c.src'),"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1306,6 +1306,8 @@ TIMEDELTA_mm_qm_divmod(char **args, npy_intp const *dimensions, npy_intp const *\n  *  #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n  *  #c = f, , l#\n  *  #C = F, , L#\n+ *  #fd = 1, 1, 0#\n+ *  #VCHK = 1, 1, 0#\n  */\n /**begin repeat1\n  * #kind = logical_and, logical_or#\n@@ -1342,32 +1344,22 @@ NPY_NO_EXPORT void\n     }\n }\n \n+#if !@fd@\n /**begin repeat1\n  * #kind = isnan, isinf, isfinite, signbit#\n  * #func = npy_isnan, npy_isinf, npy_isfinite, npy_signbit#\n  **/\n-\n-/**begin repeat2\n- * #ISA  = , _avx512_skx#\n- * #isa  = simd, avx512_skx#\n- * #CHK  = 1, defined(HAVE_ATTRIBUTE_TARGET_AVX512_SKX)#\n- **/\n-\n-#if @CHK@\n NPY_NO_EXPORT void\n-@TYPE@_@kind@@ISA@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    if (!run_@kind@_@isa@_@TYPE@(args, dimensions, steps)) {\n-        UNARY_LOOP {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            *((npy_bool *)op1) = @func@(in1) != 0;\n-        }\n+    UNARY_LOOP {\n+        const @type@ in1 = *(@type@ *)ip1;\n+        *((npy_bool *)op1) = @func@(in1) != 0;\n     }\n     npy_clear_floatstatus_barrier((char*)dimensions);\n }\n-#endif\n-/**end repeat2**/\n /**end repeat1**/\n+#endif\n \n NPY_NO_EXPORT void\n @TYPE@_spacing(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -248,6 +248,20 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n /**end repeat1**/\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_unary_fp_le.dispatch.h\"\n+#endif\n+/**begin repeat\n+ *  #TYPE = FLOAT, DOUBLE#\n+ */\n+/**begin repeat1\n+ * #kind = isnan, isinf, isfinite, signbit#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+   (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data)))\n+/**end repeat1**/\n+/**end repeat**/\n+\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_unary.dispatch.h\"\n #endif\n@@ -400,6 +414,7 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@, (\n  *  #c = f, f, , l#\n  *  #C = F, F, , L#\n  *  #half = 1, 0, 0, 0#\n+ *  #fd = 0, 1, 1, 0#\n  */\n \n /**begin repeat1\n@@ -428,13 +443,13 @@ NPY_NO_EXPORT void\n /**begin repeat1\n  * #kind = isnan, isinf, isfinite, signbit, copysign, nextafter, spacing#\n  * #func = npy_isnan, npy_isinf, npy_isfinite, npy_signbit, npy_copysign, nextafter, spacing#\n+ * #dispatched = 1, 1, 1, 1, 0, 0, 0#\n  **/\n \n-/**begin repeat2\n- * #ISA  = , _avx512_skx#\n- **/\n+#if !@fd@ || !@dispatched@\n NPY_NO_EXPORT void\n-@TYPE@_@kind@@ISA@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+#endif\n /**end repeat2**/\n /**end repeat1**/\n "
            },
            {
                "filename": "numpy/core/src/umath/loops_unary_fp_le.dispatch.c.src",
                "patch": "@@ -0,0 +1,565 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** sse2 sse41\n+ ** vsx2\n+ ** neon asimd\n+ **/\n+\n+/**\n+ * Force use SSE only on x86, even if AVX2 or AVX512F are enabled\n+ * through the baseline, since scatter(AVX512F) and gather very costly\n+ * to handle non-contiguous memory access comparing with SSE for\n+ * such small operations that this file covers.\n+ */\n+#define NPY_SIMD_FORCE_128\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#include <float.h>\n+#include \"numpy/npy_math.h\"\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"lowlevel_strided_loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+/**\n+ * This code should really be merged into loops_unary_fp.dispatch.c.src\n+ * However there is an issue with enabling the code here for VX and VXE\n+ * as the shifts don't behave as expected.\n+ * See the code below that references NPY__CPU_TARGET_VX and\n+ * NPY_BIG_ENDIAN. Suspect that this is a big endian vector issue.\n+ *\n+ * Splitting the files out allows us to keep loops_unary_fp.dispatch.c.src\n+ * building for VX and VXE so we don't regress performance while adding this\n+ * code for other platforms.\n+ */\n+// TODO(@seiko2plus): add support for big-endian\n+#if NPY_SIMD_BIGENDIAN\n+    #undef NPY_SIMD\n+    #undef NPY_SIMD_F32\n+    #undef NPY_SIMD_F64\n+    #define NPY_SIMD 0\n+    #define NPY_SIMD_F32 0\n+    #define NPY_SIMD_F64 0\n+#endif\n+\n+/*******************************************************************************\n+ ** extra SIMD intrinsics\n+ ******************************************************************************/\n+\n+#if NPY_SIMD\n+\n+/**\n+ * We define intrinsics for isnan, isinf, isfinite, and signbit below.  There's\n+ * a few flavors of each.  We'll use f32 as an example although f64 versions\n+ * are also defined.\n+ * \n+ * npyv_u32 npyv_KIND_f32(npyv_f32 v)\n+ *   These are mainly used for the single vector loops.  As such, result should\n+ *   be bool true / false, ready to write back.\n+ * \n+ * npyv_b32 _npyv_KIND_f32(npyv_f32 v)\n+ *   These are used by the geneal intrinsics above as well as the multi-vector\n+ *   packing intrinsics.  The multi-vector packing intrinsics are the ones\n+ *   utilized in the unrolled vector loops.  Results should be vector masks\n+ *   of 0x00/0xff.\n+ * \n+ * npyv_u8 npyv_pack_KIND_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)\n+ *   These are the multi-vector packing intrinsics utilized by unrolled vector\n+ *   loops.  They perform the operation on all input vectors and pack the\n+ *   results to a single npyv_u8.  Assuming NPY_SIMD == 128, that means we\n+ *   can pack results from 4x npyv_f32 or 8x npyv_64 in a single npyv_u8.\n+ *   Result should be bool true / false, ready to write back.\n+ */\n+\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_u32\n+npyv_isnan_f32(npyv_f32 v)\n+{\n+    const npyv_u8 truemask = npyv_reinterpret_u8_u32(npyv_setall_u32(1==1));\n+    npyv_u8 notnan = npyv_reinterpret_u8_u32(npyv_cvt_u32_b32(npyv_notnan_f32(v)));\n+    return npyv_reinterpret_u32_u8(npyv_andc_u8(truemask, notnan));\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_isnan_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)\n+{\n+    const npyv_u8 truemask = npyv_setall_u8(1==1);\n+    npyv_b32 b0 = npyv_notnan_f32(v0);\n+    npyv_b32 b1 = npyv_notnan_f32(v1);\n+    npyv_b32 b2 = npyv_notnan_f32(v2);\n+    npyv_b32 b3 = npyv_notnan_f32(v3);\n+    npyv_b8 notnan = npyv_pack_b8_b32(b0, b1, b2, b3);\n+    return npyv_andc_u8(truemask, npyv_cvt_u8_b8(notnan));\n+}\n+#endif\n+#if NPY_SIMD_F64\n+NPY_FINLINE npyv_u64\n+npyv_isnan_f64(npyv_f64 v)\n+{\n+    const npyv_u8 truemask = npyv_reinterpret_u8_u64(npyv_setall_u64(1==1));\n+    npyv_u8 notnan = npyv_reinterpret_u8_u64(npyv_cvt_u64_b64(npyv_notnan_f64(v)));\n+    return npyv_reinterpret_u64_u8(npyv_andc_u8(truemask, notnan));\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_isnan_f64(npyv_f64 v0, npyv_f64 v1, npyv_f64 v2, npyv_f64 v3,\n+                    npyv_f64 v4, npyv_f64 v5, npyv_f64 v6, npyv_f64 v7)\n+{\n+    const npyv_u8 truemask = npyv_setall_u8(1==1);\n+    npyv_b64 b0 = npyv_notnan_f64(v0);\n+    npyv_b64 b1 = npyv_notnan_f64(v1);\n+    npyv_b64 b2 = npyv_notnan_f64(v2);\n+    npyv_b64 b3 = npyv_notnan_f64(v3);\n+    npyv_b64 b4 = npyv_notnan_f64(v4);\n+    npyv_b64 b5 = npyv_notnan_f64(v5);\n+    npyv_b64 b6 = npyv_notnan_f64(v6);\n+    npyv_b64 b7 = npyv_notnan_f64(v7);\n+    npyv_b8 notnan = npyv_pack_b8_b64(b0, b1, b2, b3, b4, b5, b6, b7);\n+    return npyv_andc_u8(truemask, npyv_cvt_u8_b8(notnan));\n+}\n+#endif\n+\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_b32\n+_npyv_isinf_f32(npyv_f32 v)\n+{\n+#if defined(NPY_HAVE_NEON)\n+    // abs(v) > FLT_MAX\n+    const npyv_f32 fltmax = npyv_setall_f32(FLT_MAX);\n+    return vcagtq_f32(v, fltmax);\n+#else\n+    // cast out the sign and check if all exponent bits are set.\n+    const npyv_u32 exp_mask = npyv_setall_u32(0xff000000);\n+    npyv_u32 bits = npyv_shli_u32(npyv_reinterpret_u32_f32(v), 1);\n+    return npyv_cmpeq_u32(bits, exp_mask);\n+#endif\n+}\n+NPY_FINLINE npyv_u32\n+npyv_isinf_f32(npyv_f32 v)\n+{\n+    const npyv_u32 truemask = npyv_setall_u32(1==1);\n+    return npyv_and_u32(truemask, npyv_cvt_u32_b32(_npyv_isinf_f32(v)));\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_isinf_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)\n+{\n+    const npyv_u8 truemask = npyv_setall_u8(1==1);\n+    npyv_b32 b0 = _npyv_isinf_f32(v0);\n+    npyv_b32 b1 = _npyv_isinf_f32(v1);\n+    npyv_b32 b2 = _npyv_isinf_f32(v2);\n+    npyv_b32 b3 = _npyv_isinf_f32(v3);\n+    npyv_b8 isinf = npyv_pack_b8_b32(b0, b1, b2, b3);\n+    return npyv_and_u8(truemask, npyv_cvt_u8_b8(isinf));\n+}\n+#endif // NPY_SIMD_F32\n+#if NPY_SIMD_F64\n+NPY_FINLINE npyv_b64\n+_npyv_isinf_f64(npyv_f64 v)\n+{\n+#if defined(NPY_HAVE_NEON)\n+    // abs(v) > DBL_MAX\n+    const npyv_f64 fltmax = npyv_setall_f64(DBL_MAX);\n+    return vcagtq_f64(v, fltmax);\n+#else\n+    // cast out the sign and check if all exponent bits are set.\n+    const npyv_u64 exp_mask = npyv_setall_u64(0xffe0000000000000);\n+    npyv_u64 bits = npyv_shli_u64(npyv_reinterpret_u64_f64(v), 1);\n+    return npyv_cmpeq_u64(bits, exp_mask);\n+#endif\n+}\n+NPY_FINLINE npyv_u64\n+npyv_isinf_f64(npyv_f64 v)\n+{\n+    const npyv_u64 truemask = npyv_setall_u64(1==1);\n+    return npyv_and_u64(truemask, npyv_cvt_u64_b64(_npyv_isinf_f64(v)));\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_isinf_f64(npyv_f64 v0, npyv_f64 v1, npyv_f64 v2, npyv_f64 v3,\n+                    npyv_f64 v4, npyv_f64 v5, npyv_f64 v6, npyv_f64 v7)\n+{\n+    const npyv_u8 truemask = npyv_setall_u8(1==1);\n+    npyv_b64 b0 = _npyv_isinf_f64(v0);\n+    npyv_b64 b1 = _npyv_isinf_f64(v1);\n+    npyv_b64 b2 = _npyv_isinf_f64(v2);\n+    npyv_b64 b3 = _npyv_isinf_f64(v3);\n+    npyv_b64 b4 = _npyv_isinf_f64(v4);\n+    npyv_b64 b5 = _npyv_isinf_f64(v5);\n+    npyv_b64 b6 = _npyv_isinf_f64(v6);\n+    npyv_b64 b7 = _npyv_isinf_f64(v7);\n+    npyv_b8 isinf = npyv_pack_b8_b64(b0, b1, b2, b3, b4, b5, b6, b7);\n+    return npyv_and_u8(truemask, npyv_cvt_u8_b8(isinf));\n+}\n+#endif // NPY_SIMD_F64\n+\n+\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_b32\n+npyv_notfinite_f32(npyv_f32 v)\n+{\n+    // cast out the sign and check if all exponent bits are set\n+    // no matter the mentissa is.\n+    const npyv_u32 exp_mask = npyv_setall_u32(0x7f800000);\n+    npyv_u32 bits = npyv_reinterpret_u32_f32(v);\n+    bits = npyv_and_u32(bits, exp_mask);\n+    return npyv_cmpeq_u32(bits, exp_mask);\n+}\n+NPY_FINLINE npyv_u32\n+npyv_isfinite_f32(npyv_f32 v)\n+{\n+    const npyv_u8 truemask = npyv_reinterpret_u8_u32(npyv_setall_u32(1==1));\n+    npyv_u8 notfinite = npyv_reinterpret_u8_u32(npyv_cvt_u32_b32(npyv_notfinite_f32(v)));\n+    return npyv_reinterpret_u32_u8(npyv_andc_u8(truemask, notfinite));\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_isfinite_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)\n+{\n+#if defined(NPY_HAVE_NEON) && defined(__aarch64__)\n+    // F32 exponent is 8-bits, which means we can pack multiple into\n+    // a single vector.  We shift out sign bit so that we're left\n+    // with only exponent in high byte.  If not all bits are set,\n+    // then we've got a finite number.\n+    uint8x16x4_t tbl;\n+    tbl.val[0] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v0), 1));\n+    tbl.val[1] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v1), 1));\n+    tbl.val[2] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v2), 1));\n+    tbl.val[3] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v3), 1));\n+\n+    const npyv_u8 permute = {3,7,11,15,  19,23,27,31,  35,39,43,47,  51,55,59,63};\n+    npyv_u8 r = vqtbl4q_u8(tbl, permute);\n+\n+    const npyv_u8 expmask = npyv_setall_u8(0xff);\n+    r = npyv_cmpneq_u8(r, expmask);\n+    r = vshrq_n_u8(r, 7);\n+    return r;\n+#else\n+    const npyv_u8 truemask = npyv_setall_u8(1==1);\n+    npyv_b32 b0 = npyv_notfinite_f32(v0);\n+    npyv_b32 b1 = npyv_notfinite_f32(v1);\n+    npyv_b32 b2 = npyv_notfinite_f32(v2);\n+    npyv_b32 b3 = npyv_notfinite_f32(v3);\n+    npyv_b8 notfinite = npyv_pack_b8_b32(b0, b1, b2, b3);\n+    return npyv_andc_u8(truemask, npyv_cvt_u8_b8(notfinite));\n+#endif\n+}\n+#endif // NPY_SIMD_F32\n+#if NPY_SIMD_F64\n+NPY_FINLINE npyv_b64\n+npyv_notfinite_f64(npyv_f64 v)\n+{\n+    // cast out the sign and check if all exponent bits are set\n+    // no matter the mantissa is.\n+    const npyv_u64 exp_mask = npyv_setall_u64(0x7ff0000000000000);\n+    npyv_u64 bits = npyv_reinterpret_u64_f64(v);\n+    bits = npyv_and_u64(bits, exp_mask);\n+    return npyv_cmpeq_u64(bits, exp_mask);\n+}\n+NPY_FINLINE npyv_u64\n+npyv_isfinite_f64(npyv_f64 v)\n+{\n+    const npyv_u8 truemask = npyv_reinterpret_u8_u64(npyv_setall_u64(1==1));\n+    npyv_u8 notfinite = npyv_reinterpret_u8_u64(npyv_cvt_u64_b64(npyv_notfinite_f64(v)));\n+    return npyv_reinterpret_u64_u8(npyv_andc_u8(truemask, notfinite));\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_isfinite_f64(npyv_f64 v0, npyv_f64 v1, npyv_f64 v2, npyv_f64 v3,\n+                       npyv_f64 v4, npyv_f64 v5, npyv_f64 v6, npyv_f64 v7)\n+{\n+#if defined(NPY_HAVE_NEON) && defined(__aarch64__)\n+    // F64 exponent is 11-bits, which means we can pack multiple into\n+    // a single vector.  We'll need to use u16 to fit all exponent\n+    // bits.  If not all bits are set, then we've got a finite number.\n+    uint8x16x4_t t0123, t4567;\n+    t0123.val[0] = npyv_reinterpret_u8_f64(v0);\n+    t0123.val[1] = npyv_reinterpret_u8_f64(v1);\n+    t0123.val[2] = npyv_reinterpret_u8_f64(v2);\n+    t0123.val[3] = npyv_reinterpret_u8_f64(v3);\n+    t4567.val[0] = npyv_reinterpret_u8_f64(v4);\n+    t4567.val[1] = npyv_reinterpret_u8_f64(v5);\n+    t4567.val[2] = npyv_reinterpret_u8_f64(v6);\n+    t4567.val[3] = npyv_reinterpret_u8_f64(v7);\n+\n+    const npyv_u8 permute = {6,7,14,15,  22,23,30,31,  38,39,46,47,  54,55,62,63};\n+    npyv_u16 r0 = npyv_reinterpret_u16_u8(vqtbl4q_u8(t0123, permute));\n+    npyv_u16 r1 = npyv_reinterpret_u16_u8(vqtbl4q_u8(t4567, permute));\n+\n+    const npyv_u16 expmask = npyv_setall_u16(0x7ff0);\n+    r0 = npyv_and_u16(r0, expmask);\n+    r0 = npyv_cmpneq_u16(r0, expmask);\n+    r0 = npyv_shri_u16(r0, 15);\n+    r1 = npyv_and_u16(r1, expmask);\n+    r1 = npyv_cmpneq_u16(r1, expmask);\n+    r1 = npyv_shri_u16(r1, 15);\n+\n+    npyv_u8 r = npyv_pack_b8_b16(r0, r1);\n+    return r;\n+#else\n+    const npyv_u8 truemask = npyv_setall_u8(1==1);\n+    npyv_b64 b0 = npyv_notfinite_f64(v0);\n+    npyv_b64 b1 = npyv_notfinite_f64(v1);\n+    npyv_b64 b2 = npyv_notfinite_f64(v2);\n+    npyv_b64 b3 = npyv_notfinite_f64(v3);\n+    npyv_b64 b4 = npyv_notfinite_f64(v4);\n+    npyv_b64 b5 = npyv_notfinite_f64(v5);\n+    npyv_b64 b6 = npyv_notfinite_f64(v6);\n+    npyv_b64 b7 = npyv_notfinite_f64(v7);\n+    npyv_b8 notfinite = npyv_pack_b8_b64(b0, b1, b2, b3, b4, b5, b6, b7);\n+    return npyv_andc_u8(truemask, npyv_cvt_u8_b8(notfinite));\n+#endif\n+}\n+#endif // NPY_SIMD_F64\n+\n+#if NPY_SIMD_F32\n+NPY_FINLINE npyv_u32\n+npyv_signbit_f32(npyv_f32 v)\n+{\n+    return npyv_shri_u32(npyv_reinterpret_u32_f32(v), (sizeof(npyv_lanetype_f32)*8)-1);\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_signbit_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)\n+{\n+#if defined(NPY_HAVE_NEON) && defined(__aarch64__)\n+    // We only need high byte for signbit, which means we can pack\n+    // multiple inputs into a single vector.\n+    uint8x16x4_t tbl;\n+    tbl.val[0] = npyv_reinterpret_u8_f32(v0);\n+    tbl.val[1] = npyv_reinterpret_u8_f32(v1);\n+    tbl.val[2] = npyv_reinterpret_u8_f32(v2);\n+    tbl.val[3] = npyv_reinterpret_u8_f32(v3);\n+\n+    const npyv_u8 permute = {3,7,11,15,  19,23,27,31,  35,39,43,47,  51,55,59,63};\n+    npyv_u8 r = vqtbl4q_u8(tbl, permute);\n+            r = vshrq_n_u8(r, 7);\n+    return r;\n+#else\n+    npyv_b32 b0 = npyv_cvt_b32_u32(npyv_signbit_f32(v0));\n+    npyv_b32 b1 = npyv_cvt_b32_u32(npyv_signbit_f32(v1));\n+    npyv_b32 b2 = npyv_cvt_b32_u32(npyv_signbit_f32(v2));\n+    npyv_b32 b3 = npyv_cvt_b32_u32(npyv_signbit_f32(v3));\n+    npyv_b8 signbit = npyv_pack_b8_b32(b0, b1, b2, b3);\n+    return npyv_cvt_u8_b8(signbit);\n+#endif\n+}\n+#endif // NPY_SIMD_F32\n+#if NPY_SIMD_F64\n+NPY_FINLINE npyv_u64\n+npyv_signbit_f64(npyv_f64 v)\n+{\n+    return npyv_shri_u64(npyv_reinterpret_u64_f64(v), (sizeof(npyv_lanetype_f64)*8)-1);\n+}\n+NPY_FINLINE npyv_u8\n+npyv_pack_signbit_f64(npyv_f64 v0, npyv_f64 v1, npyv_f64 v2, npyv_f64 v3,\n+                      npyv_f64 v4, npyv_f64 v5, npyv_f64 v6, npyv_f64 v7)\n+{\n+#if defined(NPY_HAVE_NEON) && defined(__aarch64__)\n+    // We only need high byte for signbit, which means we can pack\n+    // multiple inputs into a single vector.\n+\n+    // vuzp2 faster than vtbl for f64\n+    npyv_u32 v01 = vuzp2q_u32(npyv_reinterpret_u32_f64(v0), npyv_reinterpret_u32_f64(v1));\n+    npyv_u32 v23 = vuzp2q_u32(npyv_reinterpret_u32_f64(v2), npyv_reinterpret_u32_f64(v3));\n+    npyv_u32 v45 = vuzp2q_u32(npyv_reinterpret_u32_f64(v4), npyv_reinterpret_u32_f64(v5));\n+    npyv_u32 v67 = vuzp2q_u32(npyv_reinterpret_u32_f64(v6), npyv_reinterpret_u32_f64(v7));\n+\n+    npyv_u16 v0123 = vuzp2q_u16(npyv_reinterpret_u16_u32(v01), npyv_reinterpret_u16_u32(v23));\n+    npyv_u16 v4567 = vuzp2q_u16(npyv_reinterpret_u16_u32(v45), npyv_reinterpret_u16_u32(v67));\n+\n+    npyv_u8 r = vuzp2q_u8(npyv_reinterpret_u8_u16(v0123), npyv_reinterpret_u8_u16(v4567));\n+            r = vshrq_n_u8(r, 7);\n+    return r;\n+#else\n+    npyv_b64 b0 = npyv_cvt_b64_u64(npyv_signbit_f64(v0));\n+    npyv_b64 b1 = npyv_cvt_b64_u64(npyv_signbit_f64(v1));\n+    npyv_b64 b2 = npyv_cvt_b64_u64(npyv_signbit_f64(v2));\n+    npyv_b64 b3 = npyv_cvt_b64_u64(npyv_signbit_f64(v3));\n+    npyv_b64 b4 = npyv_cvt_b64_u64(npyv_signbit_f64(v4));\n+    npyv_b64 b5 = npyv_cvt_b64_u64(npyv_signbit_f64(v5));\n+    npyv_b64 b6 = npyv_cvt_b64_u64(npyv_signbit_f64(v6));\n+    npyv_b64 b7 = npyv_cvt_b64_u64(npyv_signbit_f64(v7));\n+    npyv_b8 signbit = npyv_pack_b8_b64(b0, b1, b2, b3, b4, b5, b6, b7);\n+    return npyv_cvt_u8_b8(signbit);\n+#endif\n+}\n+#endif // NPY_SIMD_F64\n+\n+#endif // NPY_SIMD\n+\n+/********************************************************************************\n+ ** Defining the SIMD kernels\n+ ********************************************************************************/\n+/** Notes:\n+ * - avoid the use of libmath to unify fp/domain errors\n+ *   for both scalars and vectors among all compilers/architectures.\n+ * - use intrinsic npyv_load_till_* instead of npyv_load_tillz_\n+ *   to fill the remind lanes with 1.0 to avoid divide by zero fp\n+ *   exception in reciprocal.\n+ */\n+#define CONTIG  0\n+#define NCONTIG 1\n+\n+/**begin repeat\n+ * #TYPE = FLOAT, DOUBLE#\n+ * #sfx  = f32, f64#\n+ * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #ssfx = 32, 64#\n+ */\n+#if @VCHK@\n+/**begin repeat1\n+ * #kind = isnan, isinf, isfinite, signbit#\n+ */\n+/**begin repeat2\n+ * #STYPE  = CONTIG, NCONTIG, CONTIG,  NCONTIG#\n+ * #DTYPE  = CONTIG, CONTIG,  NCONTIG, NCONTIG#\n+ */\n+static void simd_unary_@kind@_@TYPE@_@STYPE@_@DTYPE@\n+(const void *src, npy_intp istride, void *dst, npy_intp ostride, npy_intp len)\n+{\n+    const npyv_lanetype_@sfx@ *ip = src;\n+    npy_bool *op = dst;\n+\n+    // How many vectors can be packed into a u8 / bool vector?\n+    #define PACK_FACTOR (NPY_SIMD_WIDTH / npyv_nlanes_@sfx@)\n+    assert(PACK_FACTOR == 4 || PACK_FACTOR == 8);\n+\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * PACK_FACTOR;\n+\n+    // unrolled iterations\n+    for (; len >= wstep; len -= wstep, ip += istride*wstep, op += ostride*wstep) {\n+        // Load vectors\n+        #if @STYPE@ == CONTIG\n+            // contiguous input\n+            npyv_@sfx@ v0 = npyv_load_@sfx@(ip + vstep * 0);\n+            npyv_@sfx@ v1 = npyv_load_@sfx@(ip + vstep * 1);\n+            npyv_@sfx@ v2 = npyv_load_@sfx@(ip + vstep * 2);\n+            npyv_@sfx@ v3 = npyv_load_@sfx@(ip + vstep * 3);\n+            #if PACK_FACTOR == 8\n+            npyv_@sfx@ v4 = npyv_load_@sfx@(ip + vstep * 4);\n+            npyv_@sfx@ v5 = npyv_load_@sfx@(ip + vstep * 5);\n+            npyv_@sfx@ v6 = npyv_load_@sfx@(ip + vstep * 6);\n+            npyv_@sfx@ v7 = npyv_load_@sfx@(ip + vstep * 7);\n+            #endif\n+        #else\n+            // non-contiguous input\n+            npyv_@sfx@ v0 = npyv_loadn_@sfx@(ip + istride * vstep * 0, istride);\n+            npyv_@sfx@ v1 = npyv_loadn_@sfx@(ip + istride * vstep * 1, istride);\n+            npyv_@sfx@ v2 = npyv_loadn_@sfx@(ip + istride * vstep * 2, istride);\n+            npyv_@sfx@ v3 = npyv_loadn_@sfx@(ip + istride * vstep * 3, istride);\n+            #if PACK_FACTOR == 8\n+            npyv_@sfx@ v4 = npyv_loadn_@sfx@(ip + istride * vstep * 4, istride);\n+            npyv_@sfx@ v5 = npyv_loadn_@sfx@(ip + istride * vstep * 5, istride);\n+            npyv_@sfx@ v6 = npyv_loadn_@sfx@(ip + istride * vstep * 6, istride);\n+            npyv_@sfx@ v7 = npyv_loadn_@sfx@(ip + istride * vstep * 7, istride);\n+            #endif\n+        #endif\n+\n+        #if PACK_FACTOR == 4\n+        npyv_u8 r = npyv_pack_@kind@_@sfx@(v0, v1, v2, v3);\n+        #elif PACK_FACTOR == 8\n+        npyv_u8 r = npyv_pack_@kind@_@sfx@(v0, v1, v2, v3, v4, v5, v6, v7);\n+        #endif\n+\n+        #if @DTYPE@ == CONTIG\n+            npyv_store_u8(op, r);\n+        #else // @DTYPE@ == CONTIG\n+            // Results are packed, so we can just loop over them\n+            npy_uint8 lane[npyv_nlanes_u8];\n+            npyv_store_u8(lane, r);\n+            for (int ln=0; (ln * sizeof(npyv_lanetype_@sfx@)) < npyv_nlanes_u8; ++ln){\n+                op[ln * ostride] = lane[ln * sizeof(npyv_lanetype_@sfx@)];\n+            }\n+        #endif // @DTYPE@ == CONTIG\n+    }\n+\n+    // vector-sized iterations\n+    for (; len >= vstep; len -= vstep, ip += istride*vstep, op += ostride*vstep) {\n+    #if @STYPE@ == CONTIG\n+        npyv_@sfx@ v = npyv_load_@sfx@(ip);\n+    #else\n+        npyv_@sfx@ v = npyv_loadn_@sfx@(ip, istride);\n+    #endif\n+\n+        npyv_u@ssfx@ r = npyv_@kind@_@sfx@(v);\n+\n+        npy_uint8 lane[npyv_nlanes_u8];\n+        npyv_store_u8(lane, npyv_reinterpret_u8_u@ssfx@(r));\n+\n+        op[0 * ostride] = lane[0 * sizeof(npyv_lanetype_@sfx@)];\n+        op[1 * ostride] = lane[1 * sizeof(npyv_lanetype_@sfx@)];\n+        #if npyv_nlanes_@sfx@ == 4\n+        op[2 * ostride] = lane[2 * sizeof(npyv_lanetype_@sfx@)];\n+        op[3 * ostride] = lane[3 * sizeof(npyv_lanetype_@sfx@)];\n+        #endif\n+    }\n+\n+    #undef PACK_FACTOR\n+\n+    // Scalar loop to finish off\n+    for (; len > 0; --len, ip += istride, op += ostride) {\n+        *op = (npy_@kind@(*ip) != 0);\n+    }\n+\n+    npyv_cleanup();\n+}\n+/**end repeat2**/\n+/**end repeat1**/\n+\n+#endif // @VCHK@\n+/**end repeat**/\n+\n+/********************************************************************************\n+ ** Defining ufunc inner functions\n+ ********************************************************************************/\n+/**begin repeat\n+ * #TYPE = FLOAT, DOUBLE#\n+ * #sfx  = f32, f64#\n+ * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#\n+ */\n+\n+/**begin repeat1\n+ * #kind = isnan, isinf, isfinite, signbit#\n+ **/\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+#if @VCHK@\n+    const char *ip = args[0];\n+    char *op = args[1];\n+    const npy_intp istep = steps[0];\n+    const npy_intp ostep = steps[1];\n+    npy_intp len = dimensions[0];\n+    const int ilsize = sizeof(npyv_lanetype_@sfx@);\n+    const int olsize = sizeof(npy_bool);\n+    const npy_intp istride = istep / ilsize;\n+    const npy_intp ostride = ostep / olsize;\n+    assert(len <= 1 || ostep % olsize == 0);\n+\n+    if ((istep % ilsize == 0) &&\n+        !is_mem_overlap(ip, istep, op, ostep, len) &&\n+        npyv_loadable_stride_@sfx@(istride) &&\n+        npyv_storable_stride_@sfx@(ostride))\n+    {\n+        if (istride == 1 && ostride == 1) {\n+            simd_unary_@kind@_@TYPE@_CONTIG_CONTIG(ip, 1, op, 1, len);\n+        }\n+        else if (ostride == 1) {\n+            simd_unary_@kind@_@TYPE@_NCONTIG_CONTIG(ip, istride, op, 1, len);\n+        }\n+        else if (istride == 1) {\n+            simd_unary_@kind@_@TYPE@_CONTIG_NCONTIG(ip, 1, op, ostride, len);\n+        } else {\n+            simd_unary_@kind@_@TYPE@_NCONTIG_NCONTIG(ip, istride, op, ostride, len);\n+        }\n+    } else\n+#endif // @VCHK@\n+    {\n+    UNARY_LOOP {\n+        const npyv_lanetype_@sfx@ in = *(npyv_lanetype_@sfx@ *)ip1;\n+        *((npy_bool *)op1) = (npy_@kind@(in) != 0);\n+    }\n+    }\n+\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+}\n+/**end repeat1**/\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -82,78 +82,6 @@ run_unary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const n\n /**end repeat1**/\n /**end repeat**/\n \n-/*\n- *****************************************************************************\n- **                           FLOAT DISPATCHERS\n- *****************************************************************************\n- */\n-\n-/**begin repeat\n- * #type = npy_float, npy_double, npy_longdouble#\n- * #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n- * #EXISTS = 1, 1, 0#\n- */\n-\n-/**begin repeat1\n- * #func = isnan, isfinite, isinf, signbit#\n- */\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@\n-static inline NPY_GCC_TARGET_AVX512_SKX void\n-AVX512_SKX_@func@_@TYPE@(npy_bool*, @type@*, const npy_intp n, const npy_intp stride);\n-#endif\n-\n-static inline int\n-run_@func@_avx512_skx_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@\n-    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(@type@), sizeof(npy_bool), 64)) {\n-        AVX512_SKX_@func@_@TYPE@((npy_bool*)args[1], (@type@*)args[0], dimensions[0], steps[0]);\n-        return 1;\n-    }\n-    else {\n-        return 0;\n-    }\n-#endif\n-    return 0;\n-}\n-\n-\n-/**end repeat1**/\n-/**end repeat**/\n-\n-/**begin repeat\n- * Float types\n- *  #type = npy_float, npy_double, npy_longdouble#\n- *  #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n- *  #vector = 1, 1, 0#\n- *  #VECTOR = NPY_SIMD, NPY_SIMD_F64, 0 #\n- */\n-/**begin repeat1\n- * #kind = isnan, isfinite, isinf, signbit#\n- */\n-#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS\n-\n-static void\n-sse2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n);\n-\n-#endif\n-\n-static inline int\n-run_@kind@_simd_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS\n-    if (steps[0] == sizeof(@type@) && steps[1] == 1 &&\n-        npy_is_aligned(args[0], sizeof(@type@))) {\n-        sse2_@kind@_@TYPE@((npy_bool*)args[1], (@type@*)args[0], dimensions[0]);\n-        return 1;\n-    }\n-#endif\n-    return 0;\n-}\n-/**end repeat1**/\n-/**end repeat**/\n-\n #ifdef NPY_HAVE_SSE2_INTRINSICS\n \n /*\n@@ -189,139 +117,6 @@ NPY_FINLINE npy_double sse2_horizontal_@VOP@___m128d(__m128d v)\n }\n /**end repeat**/\n \n-/**begin repeat\n- *  #type = npy_float, npy_double#\n- *  #TYPE = FLOAT, DOUBLE#\n- *  #scalarf = npy_sqrtf, npy_sqrt#\n- *  #c = f, #\n- *  #vtype = __m128, __m128d#\n- *  #vtype256 = __m256, __m256d#\n- *  #vtype512 = __m512, __m512d#\n- *  #vpre = _mm, _mm#\n- *  #vpre256 = _mm256, _mm256#\n- *  #vpre512 = _mm512, _mm512#\n- *  #vsuf = ps, pd#\n- *  #vsufs = ss, sd#\n- *  #nan = NPY_NANF, NPY_NAN#\n- *  #double = 0, 1#\n- *  #cast = _mm_castps_si128, _mm_castpd_si128#\n- */\n-/*\n- * compress 4 vectors to 4/8 bytes in op with filled with 0 or 1\n- * the last vector is passed as a pointer as MSVC 2010 is unable to ignore the\n- * calling convention leading to C2719 on 32 bit, see #4795\n- */\n-NPY_FINLINE void\n-sse2_compress4_to_byte_@TYPE@(@vtype@ r1, @vtype@ r2, @vtype@ r3, @vtype@ * r4,\n-                              npy_bool * op)\n-{\n-    const __m128i mask = @vpre@_set1_epi8(0x1);\n-    __m128i ir1 = @vpre@_packs_epi32(@cast@(r1), @cast@(r2));\n-    __m128i ir2 = @vpre@_packs_epi32(@cast@(r3), @cast@(*r4));\n-    __m128i rr = @vpre@_packs_epi16(ir1, ir2);\n-#if @double@\n-    rr = @vpre@_packs_epi16(rr, rr);\n-    rr = @vpre@_and_si128(rr, mask);\n-    @vpre@_storel_epi64((__m128i*)op, rr);\n-#else\n-    rr = @vpre@_and_si128(rr, mask);\n-    @vpre@_storeu_si128((__m128i*)op, rr);\n-#endif\n-}\n-\n-static void\n-sse2_signbit_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n)\n-{\n-    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {\n-        op[i] = npy_signbit(ip1[i]) != 0;\n-    }\n-    LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n-        @vtype@ a = @vpre@_load_@vsuf@(&ip1[i]);\n-        int r = @vpre@_movemask_@vsuf@(a);\n-        if (sizeof(@type@) == 8) {\n-            op[i] = r & 1;\n-            op[i + 1] = (r >> 1);\n-        }\n-        else {\n-            op[i] = r & 1;\n-            op[i + 1] = (r >> 1) & 1;\n-            op[i + 2] = (r >> 2) & 1;\n-            op[i + 3] = (r >> 3);\n-        }\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = npy_signbit(ip1[i]) != 0;\n-    }\n-}\n-\n-/**begin repeat1\n- * #kind = isnan, isfinite, isinf#\n- * #var = 0, 1, 2#\n- */\n-\n-static void\n-sse2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n)\n-{\n-#if @var@ != 0 /* isinf/isfinite */\n-    /* signbit mask 0x7FFFFFFF after andnot */\n-    const @vtype@ mask = @vpre@_set1_@vsuf@(-0.@c@);\n-    const @vtype@ ones = @vpre@_cmpeq_@vsuf@(@vpre@_setzero_@vsuf@(),\n-                                             @vpre@_setzero_@vsuf@());\n-#if @double@\n-    const @vtype@ fltmax = @vpre@_set1_@vsuf@(DBL_MAX);\n-#else\n-    const @vtype@ fltmax = @vpre@_set1_@vsuf@(FLT_MAX);\n-#endif\n-#endif\n-    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {\n-        op[i] = npy_@kind@(ip1[i]) != 0;\n-    }\n-    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {\n-        @vtype@ a = @vpre@_load_@vsuf@(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ b = @vpre@_load_@vsuf@(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ c = @vpre@_load_@vsuf@(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ d = @vpre@_load_@vsuf@(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ r1, r2, r3, r4;\n-#if @var@ != 0 /* isinf/isfinite */\n-        /* fabs via masking of sign bit */\n-        r1 = @vpre@_andnot_@vsuf@(mask, a);\n-        r2 = @vpre@_andnot_@vsuf@(mask, b);\n-        r3 = @vpre@_andnot_@vsuf@(mask, c);\n-        r4 = @vpre@_andnot_@vsuf@(mask, d);\n-#if @var@ == 1 /* isfinite */\n-        /* negative compare against max float, nan is always true */\n-        r1 = @vpre@_cmpnle_@vsuf@(r1, fltmax);\n-        r2 = @vpre@_cmpnle_@vsuf@(r2, fltmax);\n-        r3 = @vpre@_cmpnle_@vsuf@(r3, fltmax);\n-        r4 = @vpre@_cmpnle_@vsuf@(r4, fltmax);\n-#else /* isinf */\n-        r1 = @vpre@_cmpnlt_@vsuf@(fltmax, r1);\n-        r2 = @vpre@_cmpnlt_@vsuf@(fltmax, r2);\n-        r3 = @vpre@_cmpnlt_@vsuf@(fltmax, r3);\n-        r4 = @vpre@_cmpnlt_@vsuf@(fltmax, r4);\n-#endif\n-        /* flip results to what we want (andnot as there is no sse not) */\n-        r1 = @vpre@_andnot_@vsuf@(r1, ones);\n-        r2 = @vpre@_andnot_@vsuf@(r2, ones);\n-        r3 = @vpre@_andnot_@vsuf@(r3, ones);\n-        r4 = @vpre@_andnot_@vsuf@(r4, ones);\n-#endif\n-#if @var@ == 0 /* isnan */\n-        r1 = @vpre@_cmpneq_@vsuf@(a, a);\n-        r2 = @vpre@_cmpneq_@vsuf@(b, b);\n-        r3 = @vpre@_cmpneq_@vsuf@(c, c);\n-        r4 = @vpre@_cmpneq_@vsuf@(d, d);\n-#endif\n-        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = npy_@kind@(ip1[i]) != 0;\n-    }\n-}\n-\n-/**end repeat1**/\n-/**end repeat**/\n-\n /* bunch of helper functions used in ISA_exp/log_FLOAT*/\n \n #if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n@@ -713,85 +508,6 @@ NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n #endif\n /**end repeat**/\n \n-/**begin repeat\n- * #type = npy_float, npy_double#\n- * #TYPE = FLOAT, DOUBLE#\n- * #num_lanes = 16, 8#\n- * #vsuffix = ps, pd#\n- * #mask = __mmask16, __mmask8#\n- * #vtype = __m512, __m512d#\n- * #scale = 4, 8#\n- * #vindextype = __m512i, __m256i#\n- * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#\n- * #episize = epi32, epi64#\n- */\n-\n-/**begin repeat1\n- * #func = isnan, isfinite, isinf, signbit#\n- * #IMM8 = 0x81, 0x99, 0x18, 0x04#\n- * #is_finite = 0, 1, 0, 0#\n- * #is_signbit = 0, 0, 0, 1#\n- */\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n-static inline NPY_GCC_TARGET_AVX512_SKX void\n-AVX512_SKX_@func@_@TYPE@(npy_bool* op, @type@* ip, const npy_intp array_size, const npy_intp steps)\n-{\n-    const npy_intp stride_ip = steps/(npy_intp)sizeof(@type@);\n-    npy_intp num_remaining_elements = array_size;\n-\n-    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();\n-#if @is_signbit@\n-    @vtype@ signbit = _mm512_set1_@vsuffix@(-0.0);\n-#endif\n-\n-    /*\n-     * Note: while generally indices are npy_intp, we ensure that our maximum\n-     * index will fit in an int32 as a precondition for this function via\n-     * IS_OUTPUT_BLOCKABLE_UNARY\n-     */\n-\n-    npy_int32 index_ip[@num_lanes@];\n-    for (npy_int32 ii = 0; ii < @num_lanes@; ii++) {\n-        index_ip[ii] = ii*stride_ip;\n-    }\n-    @vindextype@ vindex_ip = @vindexload@((@vindextype@*)&index_ip[0]);\n-    @vtype@ zeros_f = _mm512_setzero_@vsuffix@();\n-    __m512i ones = _mm512_set1_@episize@(1);\n-\n-    while (num_remaining_elements > 0) {\n-        if (num_remaining_elements < @num_lanes@) {\n-            load_mask = avx512_get_partial_load_mask_@vsuffix@(\n-                                    num_remaining_elements, @num_lanes@);\n-        }\n-        @vtype@ x1;\n-        if (stride_ip == 1) {\n-            x1 = avx512_masked_load_@vsuffix@(load_mask, ip);\n-        }\n-        else {\n-            x1 = avx512_masked_gather_@vsuffix@(zeros_f, ip, vindex_ip, load_mask);\n-        }\n-#if @is_signbit@\n-        x1 = _mm512_and_@vsuffix@(x1,signbit);\n-#endif\n-\n-        @mask@ fpclassmask = _mm512_fpclass_@vsuffix@_mask(x1, @IMM8@);\n-#if @is_finite@\n-        fpclassmask = _mm512_knot(fpclassmask);\n-#endif\n-\n-        __m128i out =_mm512_maskz_cvts@episize@_epi8(fpclassmask, ones);\n-        _mm_mask_storeu_epi8(op, load_mask, out);\n-\n-        ip += @num_lanes@*stride_ip;\n-        op += @num_lanes@;\n-        num_remaining_elements -= @num_lanes@;\n-    }\n-}\n-#endif\n-/**end repeat1**/\n-/**end repeat**/\n-\n /**begin repeat\n  * #TYPE = CFLOAT, CDOUBLE#\n  * #type = npy_float, npy_double#"
            },
            {
                "filename": "numpy/core/tests/test_api.py",
                "patch": "@@ -102,6 +102,16 @@ def test_array_array():\n     assert_raises(ValueError, np.array, [nested], dtype=np.float64)\n \n     # Try with lists...\n+    # float32\n+    assert_equal(np.array([None] * 10, dtype=np.float32),\n+                 np.full((10,), np.nan, dtype=np.float32))\n+    assert_equal(np.array([[None]] * 10, dtype=np.float32),\n+                 np.full((10, 1), np.nan, dtype=np.float32))\n+    assert_equal(np.array([[None] * 10], dtype=np.float32),\n+                 np.full((1, 10), np.nan, dtype=np.float32))\n+    assert_equal(np.array([[None] * 10] * 10, dtype=np.float32),\n+                 np.full((10, 10), np.nan, dtype=np.float32))\n+    # float64\n     assert_equal(np.array([None] * 10, dtype=np.float64),\n                  np.full((10,), np.nan, dtype=np.float64))\n     assert_equal(np.array([[None]] * 10, dtype=np.float64),"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -1717,7 +1717,8 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n             ufunc(array)\n \n class TestFPClass:\n-    @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n+    @pytest.mark.parametrize(\"stride\", [-5, -4, -3, -2, -1, 1,\n+                                2, 4, 5, 6, 7, 8, 9, 10])\n     def test_fpclass(self, stride):\n         arr_f64 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 2.2251e-308, -2.2251e-308], dtype='d')\n         arr_f32 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 1.4013e-045, -1.4013e-045], dtype='f')\n@@ -1734,6 +1735,52 @@ def test_fpclass(self, stride):\n         assert_equal(np.isfinite(arr_f32[::stride]), finite[::stride])\n         assert_equal(np.isfinite(arr_f64[::stride]), finite[::stride])\n \n+    @pytest.mark.parametrize(\"dtype\", ['d', 'f'])\n+    def test_fp_noncontiguous(self, dtype):\n+        data = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0,\n+                            1.0, -0.0, 0.0, 2.2251e-308,\n+                            -2.2251e-308], dtype=dtype)\n+        nan = np.array([True, True, False, False, False, False,\n+                            False, False, False, False])\n+        inf = np.array([False, False, True, True, False, False,\n+                            False, False, False, False])\n+        sign = np.array([False, True, False, True, True, False,\n+                            True, False, False, True])\n+        finite = np.array([False, False, False, False, True, True,\n+                            True, True, True, True])\n+        out = np.ndarray(data.shape, dtype='bool')\n+        ncontig_in = data[1::3]\n+        ncontig_out = out[1::3]\n+        contig_in = np.array(ncontig_in)\n+        assert_equal(ncontig_in.flags.c_contiguous, False)\n+        assert_equal(ncontig_out.flags.c_contiguous, False)\n+        assert_equal(contig_in.flags.c_contiguous, True)\n+        # ncontig in, ncontig out\n+        assert_equal(np.isnan(ncontig_in, out=ncontig_out), nan[1::3])\n+        assert_equal(np.isinf(ncontig_in, out=ncontig_out), inf[1::3])\n+        assert_equal(np.signbit(ncontig_in, out=ncontig_out), sign[1::3])\n+        assert_equal(np.isfinite(ncontig_in, out=ncontig_out), finite[1::3])\n+        # contig in, ncontig out\n+        assert_equal(np.isnan(contig_in, out=ncontig_out), nan[1::3])\n+        assert_equal(np.isinf(contig_in, out=ncontig_out), inf[1::3])\n+        assert_equal(np.signbit(contig_in, out=ncontig_out), sign[1::3])\n+        assert_equal(np.isfinite(contig_in, out=ncontig_out), finite[1::3])\n+        # ncontig in, contig out\n+        assert_equal(np.isnan(ncontig_in), nan[1::3])\n+        assert_equal(np.isinf(ncontig_in), inf[1::3])\n+        assert_equal(np.signbit(ncontig_in), sign[1::3])\n+        assert_equal(np.isfinite(ncontig_in), finite[1::3])\n+        # contig in, contig out, nd stride\n+        data_split = np.array(np.array_split(data, 2))\n+        nan_split = np.array(np.array_split(nan, 2))\n+        inf_split = np.array(np.array_split(inf, 2))\n+        sign_split = np.array(np.array_split(sign, 2))\n+        finite_split = np.array(np.array_split(finite, 2))\n+        assert_equal(np.isnan(data_split), nan_split)\n+        assert_equal(np.isinf(data_split), inf_split)\n+        assert_equal(np.signbit(data_split), sign_split)\n+        assert_equal(np.isfinite(data_split), finite_split)\n+\n class TestLDExp:\n     @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n     @pytest.mark.parametrize(\"dtype\", ['f', 'd'])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 17032,
        "body": "This pull is to submit for consideration a pair of routines which automatically diagnose ill-conditioning in linear systems of equations and compute an appropriate Tihonov regularization parameter. The second of these routines allows the user to request that the solution be non-negative.\r\nThese methods are converted from a C++ library which has been available in various forms for years. See \r\nhttp://www.rejones7.net/ \r\nNote that the Python code summited here is much simplified from the full-featured code at that web sit.\r\n",
        "changed_files": [
            {
                "filename": "numpy/linalg/linalg.py",
                "patch": "@@ -2811,3 +2811,320 @@ def _multi_dot(arrays, order, i, j, out=None):\n         return dot(_multi_dot(arrays, order, i, order[i, j]),\n                    _multi_dot(arrays, order, order[i, j] + 1, j),\n                    out=out)\n+\n+\"\"\"\n+    Return an Automatically regularized least-squares solution to a \n+    linear matrix equation.\n+\n+    Computes the vector x that approximatively solves the equation\n+    ``a * x = b``. The equation may be under-, well-, or over-determined\n+    (i.e., the number of linearly independent rows of `a` can be less than,\n+    equal to, or greater than its number of linearly independent columns).\n+    If `a` is square and of full rank and NOT badly conditioned, then `x` \n+    (but for round-off error) is the \"exact\" solution of the equation. \n+    \n+    But if the system is badly conditioned and produces a solution of unreasonably\n+    large size or other diagnosed bad behavior then this routine automatically determines\n+    a \"usable rank\" for the problem. Then it uses that to estimate the average \n+    (Root-Mean-Square) error in the right hand side values.\n+    From that it computes the appropriate Tikhonov regularization parameter, lambda,\n+    adn computes a solution. \n+    \n+    In other words, when solving \n+       A*x=b, \n+    if the Singular Value Decomposition of A is \n+       A=U*S*Vt, \n+    then autoreg returns, except for coding details, the solution\n+       x = V*R*Ut*b \n+    where \n+       R[i]=(S[i]/(S[i]**2 + lambda**2)\n+    where lambda is as estimated by the just described process.\n+    \n+    Calls: \n+           x = autoreg(A,b)\n+           x, ur, sigma, lambda = autoreg2(A,b)     , for details\n+           x = autoregnn(A,b)    , to force the solution to be non-negative\n+    ----------\n+    A : (M, N) array_like \"Coefficient\" matrix.\n+    b : (M) 1-D array_like Ordinate or \"dependent variable\" values. \n+\n+    Returns\n+    -------\n+    x : (N) ndarray\n+        Least-squares solution, with minimal residual, if the system is well behaved.\n+        Regularized solution, with larger residual, if the system is badly conditioned.\n+        Regularized and non-negative solution if solvenn() is called, possibly with yet\n+        larger residual.\n+    ur: the \"usable rank\" : int\n+    sigma : estimate Right hand Side RMS error: float\n+    lambda : the estimated Tikhonov regularization parameter: float\n+\n+    Raises\n+    ------\n+    Autoreg is believed to be protected from virtually all calculation faults.\n+    However, Autoreg calls this package's SVD algorithm, which may raise this:\n+    \n+    LinAlgError:\n+        If computation does not converge.\n+\n+    Examples\n+    --------\n+    See documentation of lstsq in this package for a standard least-square example.\n+    These routines will behave the same as lstsq when the system is well behaved.\n+    Here is a tiny example of an ill-conditioned system as handled by autoreg.\n+\n+    x + y = 2\n+    x + 1.01 y =3\n+    \n+    Then A = array([[ 1.,  1.],\n+                    [ 1.,  1.01.]])\n+    and b = array([2.0, 3.0])\n+    \n+    Then standard solvers like lstsq will return:\n+    x = [-98. , 100.]\n+    \n+    But autoreg.solve(A,b) will recognize the violation of the Picard Condition and return\n+    x = [1.12216 , 1.12779]\n+\n+    Beware that the results of any equation solver, and especially this one, should always\n+    be checked for appropriateness for your problem. Especially check the residual by\n+    examining A*x - b.\n+    \n+    For more details and examples see http://www.rejones7.net/autorej/ .\n+    \n+    References\n+    \n+    Re Picard Condition: \"The discrete picard condition for discrete ill-posed problems\", \n+    Per Christian Chansen, 1990     https://link.springer.com/article/10.1007/BF01933214\n+    \n+    Re Nonnegative solutions: \"Solving Least Squares Problems\", by Charles L. Lawson \n+    and Richard J. Hanson. Prentice-Hall 1974\n+    \n+    Re autoreg algorithms: \"Solving Linear Algebraic Systems Arising in the Solution of \n+    Integral Equations of the First Kind\", Dissertation by Rondall E. Jones, 1985, U. of N.M.\n+    Advisor: Cleve B. Moler, creator of MatLab and co-founder of MathWorks.\n+\n+\"\"\"\n+from math import sqrt\n+import numpy as np\n+import sys\n+\n+# Compute two-norm of x[:] with no overflow even if values are very large.\n+# If x=[1.0 , 2.0 , 1.0E200, 5.0], then numpy.linalg.norm(x)\n+# incorrectly gives \"inf\" as the answer.\n+# Two_norm(x) correctly gives 1.0E200 as the answer.\n+def two_norm(x):\n+    ln = len(x)\n+    if ln == 0:\n+        return 0.0\n+    y=abs(x)\n+    mx=max(y)\n+    if (mx==0.0): return 0.0\n+    sm=0.0\n+    for i in range(0,ln):\n+        sm += (x[i]/mx)**2\n+    return mx*sqrt(sm)\n+\n+\n+# compute root-mean-square of values in x(:)\n+def rms(x):\n+    s = two_norm(x)\n+    return s / sqrt(len(x))\n+\n+\n+# compute all sums of width w in g[0] to g[m-1].\n+# if g=[0.5,1.0,4.0,9.0], then \"compute_mov_sums(g,2,4)\" returns\n+# an array of three sums: g[0]+g[1]=1.5, g[1]+g[2]=5.0, g[2]+g[3]=13.0\n+def compute_mov_sums(g, w, m):\n+    numsums = m - w + 1\n+    sums = np.zeros(numsums)\n+    for i in range(0, numsums):\n+        s = 0.0\n+        for j in range(i, i + w):\n+            s += g[j]\n+        sums[i] = s\n+        return sums\n+\n+\n+# sensitivity tracking to detect large instability\n+def splita(mg, g):\n+    # initialize\n+    sensitivity = g[0]\n+    small = sensitivity\n+    local = sensitivity\n+    urank = 1\n+    for i in range(1, mg):  # start with 2nd row; (i=1; i<mg; i++)\n+        sensitivity = g[i]\n+        if sensitivity > 15.0 * small and sensitivity > local:\n+            break\n+        if sensitivity < small:\n+            small = small + 0.40 * (sensitivity - small)\n+        else:\n+            small = small + 0.02 * (sensitivity - small)\n+        local = local + 0.40 * (sensitivity - local)\n+        urank = i + 1\n+    return urank\n+\n+\n+def splitb(mg, g):\n+    # look for usable rank via moving averages\n+    gg = np.zeros(mg)\n+    for i in range(0, mg):\n+        gg[i] = g[i] * g[i]\n+    w = min(int((mg + 3) / 4), 6)\n+    sums = compute_mov_sums(gg, w, mg)\n+    ilow = np.where(sums == min(sums))[0][0]\n+    # estimate a nominal value that should see a big rise later\n+    sum = 0.0\n+    for i in range(0, ilow):\n+        sum += abs(gg[i])\n+    gnom = sum / float(ilow+1)\n+    # see if the moving average ever gets much larger\n+    bad = 10.0 * gnom\n+    ibad = 0\n+    for i in range(ilow + 1, mg - w + 1):\n+        if sums[i] > bad:\n+            ibad = i\n+            break\n+    if ibad <= 0:\n+        urank = mg  # leave urank alone\n+    else:\n+        urank = ibad + w - 1\n+    return urank\n+\n+\n+# for a given ur, and Tikhonov lambdah, compute resulting RHS RMS error\n+def rmslambdah(A, b, U, S, Vt, ur, lamb):\n+    mn = S.shape[0]\n+    ps = np.zeros(mn)\n+    for i in range(0, ur):\n+        ps[i] = 1.0 / (S[i] + lamb ** 2 / S[i]) if S[i] > 0.0 else 0.\n+    for i in range(ur, mn):\n+        ps[i] = 0.0\n+    # best to do multiplies from right end....\n+    xa = np.transpose(Vt).dot(np.diag(ps).dot(np.transpose(U).dot(b)))\n+    res = b - A.dot(xa)\n+    r = rms(res)\n+    return xa, r\n+\n+\n+# Given 'my sigma' as the error estimate for the R.H.Side RMS error,\n+# discrep finds the Tikonov Regularization parameter 'lambdah'\n+# that fits this error estimate\n+def discrep(A, b, U, S, Vt, ur, mysigma):\n+    lo = 0.0  # for minimum achievable residual\n+    hi = 0.33 * float(S[0])  # for ridiculously large residual\n+    lamb = 0.0\n+    # bisect until hopefully we nail exactly the residual we want...but quit eventually\n+    for k in range(0, 50):\n+        lamb = (lo + hi) * 0.5\n+        xa, check = rmslambdah(A, b, U, S, Vt, ur, lamb)\n+        if abs(check - mysigma) < 0.0001 * mysigma:\n+            break  # close enough!\n+        if check > mysigma:\n+            hi = lamb\n+        else:\n+            lo = lamb\n+    return lamb\n+\n+# main solver---------------------\n+def autoreg2(A, b): \n+    m = A.shape[0]\n+    n = A.shape[1]\n+    mn = min(m, n)\n+    if (m<2) or (n<2): return 0.0 , 0, 0.0, 0.0\n+    if (np.count_nonzero(A)==0): return 0.0 , 0, 0.0, 0.0\n+    if (np.count_nonzero(b)==0): return 0.0 , 0, 0.0, 0.0\n+\n+    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n+    beta = np.matmul(np.transpose(U), b)\n+    # compute contributions to norm of solution\n+    k = 0  # rank of A so far\n+    g = np.zeros(mn)\n+    sense = 0.0\n+    si = 0.0\n+    for i in range(0, mn):\n+        si = S[i]\n+        if si == 0.0:\n+            break\n+        sense = beta[i] / si\n+        if sense < 0.0:\n+            sense = -sense\n+        g[i] = sense\n+        k = i + 1\n+    if k <= 0:\n+        return np.zeros(n)  # zero system\n+    # two-stage search for break in Picard Condition Vector\n+    ura = splita(k, g)\n+    ur = splitb(ura, g)\n+    if ur >= mn:\n+        # problem is not ill-conditioned\n+        x, check = rmslambdah(A, b, U, S, Vt, ur, 0.0)\n+        sigma = 0.0\n+        lambdah = 0.0\n+    else:\n+        # from urb, determine sigma, then lambda, then solution\n+        Utb = np.matmul(np.transpose(U), b)\n+        sigma = rms(Utb[ur:mn])\n+        lambdah = discrep(A, b, U, S, Vt, ur, sigma)\n+        x, check = rmslambdah(A, b, U, S, Vt, ur, lambdah)\n+    return x, ur, sigma, lambdah\n+\n+def autoreg(A, b): \n+  x, ur, sigma, lambdah = autoreg2(A, b)\n+  return x\n+\n+# call to solve with auto regularization and force nonnegativity\n+def autoregnn(A, b):\n+    m = A.shape[0]\n+    n = A.shape[1]\n+    mn = min(m, n)\n+    if (m<2) or (n<2): return 0.0 , 0, 0.0, 0.0\n+    if (np.count_nonzero(A)==0): return 0.0 , 0, 0.0, 0.0\n+    if (np.count_nonzero(b)==0): return 0.0 , 0, 0.0, 0.0\n+    \n+    x, ur, sigma, lambdah =autoreg2(A, b)\n+    # see if unconstrained solution is already non-negative\n+    if min(x) >= 0.0:\n+        return x\n+    # the approach here is to actually delete columns, for speed,\n+    # rather than just zero out columns and thereby complicate the SVD.\n+    C = A\n+    cols = [0] * n  # list of active column numbers\n+    for i in range(1, n):\n+        cols[i] = i\n+    xt = x\n+    nn = n\n+    for i in range(1, nn):\n+        # choose a column to zero\n+        p = -1\n+        worst = 0.0\n+        for j in range(0, nn):\n+            if xt[j] < worst:\n+                p = j\n+                worst = xt[p]\n+        if p < 0:\n+            break\n+        # remove column p and resolve\n+        C = np.delete(C, p, 1)\n+        cols.pop(p)\n+        nn -= 1\n+        U, S, Vt = np.linalg.svd(C, full_matrices=False)\n+        ms = len(S)\n+        ps = np.zeros(ms)\n+        if lambdah == 0.0:\n+            for i in range(0, ms):\n+                ps[i] = (1.0 / S[i]) if S[i] > 0.0 else 0.0\n+        else:\n+            for i in range(0, ms):\n+                #usual formula rewritten t0 avoid overflow\n+                ps[i] = 1.0 / (S[i] + lambdah ** 2 / S[i]) if S[i] > 0.0 else 0.0\n+        xt = np.transpose(Vt).dot(np.diag(ps).dot(np.transpose(U).dot(b)))\n+    if nn == n:  # if did nothing\n+        return xn\n+    # rebuild full solution vector\n+    xn = np.zeros(n)\n+    for j in range(0, nn):\n+        xn[int(cols[j])] = xt[j]\n+    return xn\n+"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21449,
        "body": "Hey all,\r\n\r\nThis is my followup to #21424. I added a pre-commit config that applies pep8 formatting ([Black](https://black.readthedocs.io/), via [Darker](https://pypi.org/project/darker/0.2.0/)), format checking ([flake8](https://flake8.pycqa.org/en/latest/) + [flake8-bugbear](https://github.com/PyCQA/flake8-bugbear), also via Darker) and pep7 C-code formatting (clang-format using the already-existing .clang-format in Numpy).\r\n\r\nIn the mailing list linked [in the comment here](https://github.com/numpy/numpy/issues/21424#issuecomment-1114489137) it was mentioned that there are goals of moving the codebase toward more formatting consistency, without blowing up diffs whenever a file gets formatted. This is meant to target that, overall simplifying the workflow for anyone who is otherwise hand-formatting code. My thought was that this would be an optional but encouraged tool, rather than something enforced via CI (though that is possible too).\r\n\r\nUsage is dead simple, as always with pre-commit:\r\n\r\n```bash\r\npip install pre-commit\r\n\r\n# Installing the git hook is optional, can also be invoked manually\r\npre-commit install\r\n\r\n# If changed lines of code are not formatted correctly, the commit\r\n# fails and the autoformatter runs.\r\ngit commit\r\n```\r\n\r\nI also included some simple checkers :\r\n- yaml and toml validators, ensure config files are structurally OK before sending to CI\r\n- end-of-file-fixer, just ensure there's a single newline\r\n- trailing-whitespace, remove whitespace from end of lines (this checks the entire file but shouldn't have changes often)\r\n- fix-byte-order-marker and mixed-line-ending, just to enforce file consistency\r\n\r\nThere is still some work to do configuring flake8 and some other formatting rules, but I wanted to judge interest before getting too much into it.\r\n\r\n\r\n### Demo\r\nIf I make the following poorly formatted changes:\r\n\r\n```bash\r\n--- a/numpy/core/src/multiarray/methods.c\r\n+++ b/numpy/core/src/multiarray/methods.c\r\n@@ -44,6 +44,7 @@ static int\r\n NpyArg_ParseKeywords(PyObject *keys, const char *format, char **kwlist, ...)\r\n {\r\n     PyObject *args = PyTuple_New(0);\r\n+    myobj* obj = newfunc(0, 1, 2   );\r\n     int ret;\r\n     va_list va;\r\n \r\ndiff --git a/setup.py b/setup.py\r\nindex e22349329..fa9b8a5f0 100755\r\n--- a/setup.py\r\n+++ b/setup.py\r\n@@ -94,8 +94,8 @@\r\n         os.environ['SETUPTOOLS_USE_DISTUTILS'] = \"stdlib\"\r\n     else:\r\n         if setuptools_use_distutils != \"stdlib\":\r\n-            raise RuntimeError(\"setuptools versions >= '60.0.0' require \"\r\n-                    \"SETUPTOOLS_USE_DISTUTILS=stdlib in the environment\")\r\n+            raise  RuntimeError(\"setuptools versions >= '60.1.0' require \"\r\n+                    \"SETUPTOOLS_USE_DISTUTILS=stdlib in the build environment\"  )\r\n # Initialize cmdclass from versioneer\r\n from numpy.distutils.core import numpy_cmdclass\r\n```\r\n\r\nThe formatter hooks will fix only the changed lines so the total diff becomes the following:\r\n\r\n```bash\r\n--- a/numpy/core/src/multiarray/methods.c\r\n+++ b/numpy/core/src/multiarray/methods.c\r\n@@ -44,6 +44,7 @@ static int\r\n NpyArg_ParseKeywords(PyObject *keys, const char *format, char **kwlist, ...)\r\n {\r\n     PyObject *args = PyTuple_New(0);\r\n+    myobj *obj = newfunc(0, 1, 2);\r\n     int ret;\r\n     va_list va;\r\n \r\ndiff --git a/setup.py b/setup.py\r\nindex e22349329..0e4072d25 100755\r\n--- a/setup.py\r\n+++ b/setup.py\r\n@@ -94,8 +94,10 @@\r\n         os.environ['SETUPTOOLS_USE_DISTUTILS'] = \"stdlib\"\r\n     else:\r\n         if setuptools_use_distutils != \"stdlib\":\r\n-            raise RuntimeError(\"setuptools versions >= '60.0.0' require \"\r\n-                    \"SETUPTOOLS_USE_DISTUTILS=stdlib in the environment\")\r\n+            raise RuntimeError(\r\n+                \"setuptools versions >= '60.1.0' require \"\r\n+                \"SETUPTOOLS_USE_DISTUTILS=stdlib in the build environment\"\r\n+            )\r\n \r\n # Initialize cmdclass from versioneer\r\n from numpy.distutils.core import numpy_cmdclass\r\n```\r\n\r\nAnd nothing else is changed. Additionally, for example, if I change `RuntimeError` to `runtimeerror`, flake8 catches it but nothing else in the file:\r\n\r\n```bash\r\n.../setup.py:97:19: F821 undefined name 'runtimeerror'\r\n```\r\n\r\nEdit: relevant mailing list here https://mail.python.org/archives/list/numpy-discussion@python.org/thread/WZTS3XZKKM3MKRPZ5XD6SHSREYLTQ3DC/\r\n\r\nExtra edit: depending on how this goes, in 1-2 years numpy could revisit formatting the entire project. GH now lets you ignore specific commits (e.g. formatting) in the blame - obviously wouldn't help with open PRs (unless each one was also autoformatted) but could make at least the blame cleaner https://docs.github.com/en/repositories/working-with-files/using-files/viewing-a-file#ignore-commits-in-the-blame-view",
        "changed_files": [
            {
                "filename": ".git-blame-ignore-revs",
                "patch": "@@ -0,0 +1,16 @@\n+# .git-blame-ignore-revs\n+#\n+# This file contains a list of SHAs that should be ignored by git blame Github\n+# follows this convention as of approximately 2022-03-24, and Gitlab appears to\n+# be adding support. The main use case is for large formatting commits as NumPy\n+# works on acheiving pre-commit compliance. More information at:\n+# https://docs.github.com/en/repositories/working-with-files/using-files\n+# /viewing-a-file#ignore-commits-in-the-blame-view\n+#\n+# Ensure commit hashes that appear here exist on main; this will not be the case\n+# if a pull request with multiple commits squashed before merge (squashing\n+# performs a rebase that changes commit hashes). In these cases, the commit hash\n+# will need to be added to this file after the merge.\n+#\n+# Note that because of this squashing, it is better to keep large formatting\n+# pull requests separate from code change pull requests."
            },
            {
                "filename": ".pre-commit-config.yaml",
                "patch": "@@ -0,0 +1,206 @@\n+---\n+# Pre-commit hooks definition file\n+#\n+# This file holds information for file formatting and validation. See\n+# documentation here:\n+# https://numpy.org/doc/stable/dev/development_workflow.html?highlight=development%20workflow\n+# Or the pre-commit docs here:\n+# https://numpy.org/doc/stable/dev/development_workflow.html\n+#\n+# If you add or adjust hooks, or want to update to the latest version, use\n+# `pre-commit autoupdate --freeze`. This pins the version to the commit hash\n+# rather than a (movable) tag, which adds some security benefit. This sets the\n+# \"rev:\" key and the adds the \"# frozen: vx.x.x\" comments that you see.\n+#\n+# Note that additional_dependencies pins need to be manually updated. Try to\n+# ensure versiong stay in sync when a dependency is used across multiple\n+# hooks.\n+\n+repos:\n+  ### Always-run hooks ###\n+  # This section includes hooks that run whenever relevant files change.\n+  # Basic checkers, fairly self-explanatory\n+  - repo: https://github.com/pre-commit/pre-commit-hooks\n+    rev: 3298ddab3c13dd77d6ce1fc0baf97691430d84b0  # frozen: v4.3.0\n+    hooks:\n+      # File validity checkers\n+      - id: check-yaml\n+      - id: check-toml\n+\n+      # Simple cleanliness things\n+      - id: fix-byte-order-marker\n+      - id: end-of-file-fixer\n+      - id: trailing-whitespace\n+      - id: mixed-line-ending\n+      # Defaults to 500 kB\n+      - id: check-added-large-files\n+\n+  # \"Darker\" runs the black formatter, isort import sorter, and\n+  # flake8 checker only on edited lines rather than entire file\n+  - repo: https://github.com/akaihola/darker\n+    rev: 05a0ce72765aa4ed7cab7b722fd8371a4f712423  # frozen: 1.5.0\n+    hooks:\n+      - id: darker\n+        args:\n+          - --isort\n+          - -L\n+          - flake8\n+        additional_dependencies:\n+          - isort==5.10.1\n+          - flake8==4.0.1\n+          - flake8-bugbear==16.4.0\n+          - flake8-import-order==0.18.1\n+          - flake8-builtins==1.5.3\n+          - flake8-future-annotations==0.0.5\n+          - flake8-docstrings==1.6.0\n+          - flake8-rst-docstrings==0.2.6\n+          - pep8-naming==0.13.0\n+\n+  # Format changed lines only of C/C++ files\n+  # We want to use git-clang-format which is provided by the clang-format wheel,\n+  # but not exposed in its precommit hook.\n+  # Do not specify args here; use the tools/precommit-clang-format.sh that this\n+  # calls.\n+  - repo: local\n+    hooks:\n+      - id: clang-format-diff\n+        name: clang-format-diff\n+        entry: python tools/precommit-clang-format.py\n+        language: python        # Script is bash but need python deps\n+        require_serial: true    # Lock errors if it tries to chunk the files\n+        types_or: [c++, c]\n+        verbose: true\n+        additional_dependencies:\n+          - clang-format==14.0.6\n+\n+  # YAML Formatter. Only enabled on one file for now, will add more eventually\n+  - repo: https://github.com/jumanjihouse/pre-commit-hook-yamlfmt\n+    rev: d0cf8a7a8e09127a5a61c0644a823544e32f492b  # frozen: 0.2.2\n+    hooks:\n+      - id: yamlfmt\n+        args: [--mapping, '2', --sequence, '4', --offset, '2', --width, '88']\n+        # Will eventually apply to all YAML files\n+        files: \\.pre-commit-config.yaml\n+\n+\n+  ### Manually-run hooks ###\n+  # These hooks are disabled by default because NumPy currently has one or more\n+  # files that are known to fail. You are encouraged to run these hooks if you\n+  # are changing some files, and aim to have them pass. However, the goal is\n+  # that somebody doing development is not required to work on anything outside\n+  # of their own section of interest, so do not worry if other parts of files fail.\n+  # (though in general, if you would _like_ to fix them, that would be amazing!)\n+  # A reason each hook is disabled accompanies each of them\n+\n+  # Allow black to be run on entire file manually\n+  # Disabled for now to avoid whole-file commit diffs\n+  # Run with: pre-commit run --hook-stage manual black\n+  - repo: https://github.com/psf/black\n+    rev: f6c139c5215ce04fd3e73a900f1372942d58eca0  # frozen: 22.6.0\n+    hooks:\n+      - id: black\n+        stages: [manual]\n+\n+  # Apply black formatting to docstrings and .rst files Disabled for now to\n+  # avoid whole-file commit diffs, diff-only formatter is applied by the\n+  # \"darker\" hook\n+  #\n+  # NOTE: This hook throws a ton of errors. Most of them are because `..\n+  # code-block:: python` is used where `.. code-block:: pycon` should be.\n+  # `python` is for code blocks as one might see in a source file. `pycon` is\n+  # the correct code block type for python console syntax, i.e., blocks with\n+  # `>>>`/`...`, like `doctest` might run\n+  #\n+  # Run with: pre-commit run --hook-stage manual blacken-docs\n+  - repo: https://github.com/asottile/blacken-docs\n+    rev: 7ae9389351f4090e3993de28015a05a18ca6b8a7  # frozen: v1.12.1\n+    hooks:\n+      - id: blacken-docs\n+        stages: [manual]\n+\n+  # Allow isort to be run on entire file manually\n+  # Disabled for now to avoid whole-file commit diffs\n+  # Run with: pre-commit run --hook-stage manual isort\n+  - repo: https://github.com/pycqa/isort\n+    rev: c5e8fa75dda5f764d20f66a215d71c21cfa198e1  # frozen: 5.10.1\n+    hooks:\n+      - id: isort\n+        stages: [manual]\n+\n+  # Allow flake8 to be run on entire file manually\n+  # Disabled for now because there are about 14k failures, and fixing them\n+  # requires whole-file changes\n+  # Run with: pre-commit run --hook-stage manual flake8\n+  - repo: https://github.com/pycqa/flake8\n+    rev: cbeb4c9c4137cff1568659fcc48e8b85cddd0c8d  # frozen: 4.0.1\n+    hooks:\n+      - id: flake8\n+        stages: [manual]\n+        additional_dependencies:\n+          - flake8-bugbear==16.4.0\n+          - flake8-import-order==0.18.1\n+          - flake8-builtins==1.5.3\n+          - flake8-future-annotations==0.0.5\n+          - flake8-docstrings==1.6.0\n+          - flake8-rst-docstrings==0.2.6\n+          - pep8-naming==0.13.0\n+\n+  # Allow clang-format to be run on entire file manually\n+  # Disabled for now to avoid large diffs. clang-format-diff runs this only\n+  # on changed lines.\n+  # Run with: pre-commit run --hook-stage manual clang-format\n+  - repo: https://github.com/pre-commit/mirrors-clang-format\n+    rev: 4c1506e7e6f7712eee075b58391eabe1192d3adb  # frozen: v14.0.6\n+    hooks:\n+      - id: clang-format\n+        stages: [manual]\n+        # By default, hook formats c/c++/c#/cuda/java/javascript/json/obj-c/proto\n+        # But it can't format `jsonc` (json+comments) so we exclude it here\n+        exclude_types: [json]\n+\n+  # Validate .rst files\n+  # Disabled for now because there are a few thousand failures\n+  # Run with: pre-commit run --hook-stage manual rstcheck\n+  - repo: https://github.com/myint/rstcheck\n+    rev: 2e2717d279fc149e1d0e063b42e2def3a08d37c1  # frozen: v6.0.0.post1\n+    hooks:\n+      - id: rstcheck\n+        stages: [manual]\n+\n+  # bash/sh checker\n+  # Disabled for now because of ~500 errors\n+  # Run with: pre-commit run --hook-stage manual shellcheck\n+  - repo: https://github.com/shellcheck-py/shellcheck-py\n+    rev: 4c7c3dd7161ef39e984cb295e93a968236dc8e8a  # frozen: v0.8.0.4\n+    hooks:\n+      - id: shellcheck\n+        stages: [manual]\n+\n+  # bash/sh formatter\n+  # Disabled for now because shellcheck fails\n+  # Run with: pre-commit run --hook-stage manual beautysh\n+  - repo: https://github.com/lovesegfault/beautysh\n+    rev: 386e46cf6e6e68e26e90a6c0e8c3d0f0d30c101c  # frozen: v6.2.1\n+    hooks:\n+      - id: beautysh\n+        stages: [manual]\n+\n+  # Fortran formatter\n+  # Disabled for now because files need mild cleanup for it to run\n+  # Run with: pre-commit run --hook-stage manual fprettify\n+  - repo: https://github.com/pseewald/fprettify\n+    rev: 71781aaaa0cd513e0e25093a3b69fbdd778e8409  # frozen: v0.3.7\n+    hooks:\n+      - id: fprettify\n+        stages: [manual]\n+\n+  # Dockerfile linter\n+  # Disabled for now because of failures\n+  # Run with: pre-commit run --hook-stage manual hadolint\n+  - repo: https://github.com/AleksaC/hadolint-py\n+    rev: ecf2d2c004e7a753f48e44952a81d7c9cf1f202c  # frozen: v2.10.0\n+    hooks:\n+      - id: hadolint\n+        # Ignore not pinning versions on `pip install``\n+        args: [--ignore, DL3013]\n+        stages: [manual]"
            },
            {
                "filename": "doc/source/dev/development_workflow.rst",
                "patch": "@@ -105,6 +105,9 @@ In more detail\n    related, complete changes. Leave files with unfinished changes for later\n    commits.\n \n+#. Format your code using pre-commit. See the section :ref:`Automatic Formatting\n+   with pre-commit<formatting-with-pre-commit>` for details.\n+\n #. To commit the staged files into the local copy of your repo, do ``git\n    commit``. At this point, a text editor will open up to allow you to write a\n    commit message. Read the :ref:`commit message\n@@ -150,6 +153,110 @@ been added to ``upstream`` that affect your work. In this case, follow the\n :ref:`rebasing-on-main` section of this document to apply those changes to\n your branch.\n \n+.. _formatting-with-pre-commit:\n+\n+Automatic code formatting with pre-commit\n+-----------------------------------------\n+\n+NumPy recommends that code follow PEP7/PEP8 guidelines, but does not currently\n+enforce any formatters. However, the repository does come with a an optional\n+pre-commit hook that performs automatic formatting and validation of changes,\n+and using it is highly recommended. Tasks and checks that it runs include:\n+\n+* Format changed lines only of Python files to pep8 standard using `Darker\n+  <darker_>`_, which wraps `black <black_>`_ and `isort <isort_>`_ and applies\n+  them to the diff (this hook is enabled by default). Additionally, check only\n+  these changed lines using `flake8 <flake8_>`_.\n+\n+* Format changed lines only of C and C++ source files to pep7 standard using a\n+  Python wrapper for `ClangFormat versioning <clang_format_>`_ (this hook is\n+  enabled by default).\n+\n+* Format and lint YAML and shell files, and perform linting on Fortran, rst,\n+  Docker, and markdown files (some enabled and some disabled by default)\n+\n+To use the hook, first make sure that pre-commit is installed:\n+\n+.. code-block:: shell\n+\n+   pip install pre-commit\n+\n+And then install the hook to your git repository:\n+\n+.. code-block:: shell\n+\n+   pre-commit install\n+\n+This will add a script to your ``.git/hooks`` folder. This script will run\n+pre-commit to check your changed files whenever you run ``git commit`` (the\n+Python environment does not need to be active for this to work). Note that the\n+first time pre-commit runs, it will take a while to download the necessary\n+files. Subsequent times will be very fast.\n+\n+If there are any failures or changes when the hook is run, the commit will fail.\n+Fix any linter issues (no manual action is needed if there were only\n+autoformatting changes) and re-run the commit (you will need to re-add any\n+changed files, either with ``git add`` or ``git commit -a``). If you would like\n+a quick way to retry a commit with the same message, an alias is a good option:\n+\n+.. code-block:: shell\n+\n+   # Add the below to ~/.bash_aliases, ~/.bashrc, or ~/.zshrc, as relevant,\n+   # then reload your terminal\n+\n+   # Git commit rerun alias; commit with the same message as the last commit\n+   # or commit attempt.\n+   alias gcr='git commit -am \"$(cat \"$(git rev-parse --git-dir)/COMMIT_EDITMSG\")\"'\n+\n+It is generally bad practice to commit files that do not pass tests, but it may\n+occationally be necessary (e.g., for a local commit that will be rebased later).\n+In these cases, running ``git commit --no-verify`` (``git commit -n``) will skip\n+pre-commit checks.\n+\n+Other useful commands includes:\n+\n+.. code-block:: shell\n+\n+   # Validate specific files\n+   pre-commit run  --files a.py b.py ...\n+\n+   # Run a specific hook. Use the ID given in .pre-commit-config.yaml\n+   pre-commit run <hook_id>  [--files a.py b.py ...]\n+\n+   # Run on all files. Note there will probably be a lot of failures\n+   pre-commit run --all-files\n+\n+   # Run hooks with the stage marked as \"manual\". These are hooks where we\n+   # expect lots of failures from before formatters were available\n+   # see .pre-commit-config.yaml for more possible manual hook names\n+   pre-commit run --hook-stage manual [hook-name]\n+\n+   # If you have a branch that you may soon want to merge to main and want to\n+   # ensure that all changes on your branch are linted and formatted, run the\n+   # following:\n+   pre-commit run --from-ref $(git merge-base --fork-point main) --to-ref HEAD\n+\n+More information is available `on the pre-commit website <pre_commit>`_. Do not\n+commit the changes of ``pre-comit run --all-files`` as that will likely generate\n+a diff much larger than your code.\n+\n+If a commit is done that includes formatting changes only, its hash should be\n+added to ``.git-blame-ignore-revs`` so that it does not appear in the blame -\n+more information is available in that file. For this reason, it is generally\n+good to separate large formatting commits from code changes. Since pull request\n+commits are usually squashed, this generally means that large formatting changes\n+should go in a separate PR and have its hash added to ``.git-blame-ignore-revs``\n+*after* merge separately.\n+\n+.. Section links (not displayed)\n+.. _black: https://black.readthedocs.io/en/stable/\n+.. _darker: https://pypi.org/project/darker/\n+.. _flake8: https://flake8.pycqa.org/en/latest/\n+.. _isort: https://pycqa.github.io/isort/index.html\n+.. _pre_commit: https://pre-commit.com/\n+.. _clang_format: https://clang.llvm.org/docs/ClangFormat.html\n+\n+\n .. _writing-the-commit-message:\n \n Writing the commit message"
            },
            {
                "filename": "environment.yml",
                "patch": "@@ -39,3 +39,5 @@ dependencies:\n   # Used in some tests\n   - cffi\n   - pytz\n+  # Used for automatic formatting and validation of code\n+  - pre-commit"
            },
            {
                "filename": "pyproject.toml",
                "patch": "@@ -105,3 +105,8 @@ environment = { OPENBLAS64_=\"openblas\", OPENBLAS=\"\", NPY_USE_BLAS_ILP64=\"1\", CFL\n [[tool.cibuildwheel.overrides]]\n select = \"*-win32\"\n environment = { OPENBLAS64_=\"\", OPENBLAS=\"openblas\", NPY_USE_BLAS_ILP64=\"0\", CFLAGS=\"-m32\", LDFLAGS=\"-m32\" }\n+\n+\n+[tool.isort]\n+profile = \"black\"\n+skip = [\"numpy/__init__.py\"]"
            },
            {
                "filename": "tools/lint_diff.ini",
                "patch": "@@ -1,5 +1,5 @@\n [pycodestyle]\n-max_line_length = 79\n+max_line_length = 88\n statistics = True\n-ignore = E121,E122,E123,E125,E126,E127,E128,E226,E241,E251,E265,E266,E302,E402,E704,E712,E721,E731,E741,W291,W293,W391,W503,W504\n+ignore = E121,E122,E123,E125,E126,E127,E128,E226,E241,E251,E265,E266,E302,E402,E704,E712,E721,E731,E741,W291,W293,W391,W503,W504,E203,E501\n exclude = numpy/__config__.py,numpy/typing/tests/data"
            },
            {
                "filename": "tools/precommit-clang-format.py",
                "patch": "@@ -0,0 +1,177 @@\n+#!/usr/bin/env python3\n+\"\"\"precommit-clang-format.py\n+\n+This script wraps git-clang-format (installed via a python wheel in pre-commit)\n+and runs it with our desired arguments, returning desired exit codes.\n+git-clang-format only formats the changed blocks between two commits.\n+\n+We need to provide the tool with the proper parameters to apply the diff if\n+possible (i.e. if it is comparing to HEAD), and if not, just display the diff\n+and exit error if it exists (when not pointing at HEAD, i.e. uneditable) This\n+script usage allows for comparing two diffs from arbritrary points in time\n+easily.\n+\n+It can also be fake run without pre-commit: PRE_COMMIT_FROM_REF=someref\n+PRE_COMMIT_TO_REF=someref ./tools/precommit-clang-format.py file1.c file2.c\n+PRE_COMMIT_TO_REF can be omitted to use HEAD.\n+\n+This script is known to work with clang-format==14.0.3 available on PyPi.\n+If their \"no modified files to format\" message or return code scheme changes,\n+this will need to be updated.\n+\"\"\"\n+\n+import os\n+import subprocess\n+import sys\n+from typing import Iterable, NoReturn, Tuple\n+\n+FORMAT_ARGS = [\"--style\", \"file\"]\n+\n+# Environment variables set by pre-commit\n+PRE_COMMIT_TO_REF = os.environ.get(\"PRE_COMMIT_TO_REF\")\n+PRE_COMMIT_FROM_REF = os.environ.get(\"PRE_COMMIT_FROM_REF\")\n+\n+\n+class TC:\n+    \"\"\"Terminal color escape sequences.\n+\n+    Print starting with any member and end with \"ENDC\". All colors are\n+    the \"light\" variants.\n+    \"\"\"\n+\n+    ENDC = \"\\033[0m\"\n+    BOLD = \"\\033[1m\"\n+    UNDERLINE = \"\\033[4m\"\n+\n+    BLACK = \"\\033[90m\"\n+    RED = \"\\033[91m\"\n+    GREEN = \"\\033[92m\"\n+    YELLOW = \"\\033[93m\"\n+    BLUE = \"\\033[94m\"\n+    MAGENTA = \"\\033[95m\"\n+    CYAN = \"\\033[96m\"\n+    WHITE = \"\\033[97m\"\n+\n+    YELLOW_BOLD = \"\\033[1;33m\"\n+    RED_BOLD = \"\\033[1;31m\"\n+    CYAN_BOLD = \"\\033[1;36m\"\n+    GREEN_BOLD = \"\\033[1;32m\"\n+\n+\n+def git_rev_parse(ref: str) -> str:\n+    \"\"\"Get the output of rev-parse\"\"\"\n+    return subprocess.check_output(\n+        (\"git\", \"rev-parse\", \"--short\", ref), encoding=sys.stdout.encoding\n+    ).strip()\n+\n+\n+def tee_cmd(args: Iterable[str]) -> Tuple[str, int]:\n+    \"\"\"Run a command with printing output, also return the output and\n+    return code.\n+    \"\"\"\n+    output = \"\"\n+\n+    proc = subprocess.Popen(\n+        args,\n+        stdout=subprocess.PIPE,\n+        stderr=subprocess.STDOUT,\n+        encoding=sys.stdout.encoding,\n+    )\n+\n+    while proc.poll() is None:\n+        text = proc.stdout.readline()\n+        output += text\n+        sys.stdout.write(text)\n+\n+    return (output.strip(), proc.returncode)\n+\n+\n+def compare_skip_formatting() -> NoReturn:\n+    \"\"\"Print the diff and exit; do not apply changes.\"\"\"\n+    run_cmd = (\n+        [\"git-clang-format\"]\n+        + FORMAT_ARGS\n+        + [\"--diff\", PRE_COMMIT_FROM_REF, PRE_COMMIT_TO_REF, \"--\"]\n+        + sys.argv\n+    )\n+\n+    from_ref = git_rev_parse(PRE_COMMIT_FROM_REF)\n+    to_ref = git_rev_parse(PRE_COMMIT_TO_REF)\n+\n+    print(\n+        f'\\n${TC.CYAN}Comparing \"{from_ref}\" to \"{to_ref}\"{TC.ENDC}\\n'\n+        f'{TC.YELLOW}Cannot safely apply changes; \"to\" ref \"{to_ref}\" is not HEAD.\\n'\n+        f\"I'll just show you the diff instead. Running...\\n\"\n+        f\"> {' '.join(run_cmd)}{TC.ENDC}\\n\"\n+    )\n+\n+    output, exitcode = tee_cmd(run_cmd)\n+\n+    # Success! No diff\n+    if output == \"no modified files to format\":\n+        print(f\"{TC.GREEN}Everything looks OK; no work for me here${TC.ENDC}\")\n+        sys.exit(0)\n+\n+    if exitcode == 0:\n+        # git-clang-format exits 0 even if a diff is created, so we need to\n+        # make sure this script fails\n+        print(\n+            f\"{TC.RED}Changes between the two revisions are not properely\"\n+            f\" formatted, but I'm too\\nscared to apply formatting to target\"\n+            f\" refs that are not HEAD. I quit.{TC.ENDC}\"\n+        )\n+        sys.exit(1)\n+\n+    # If we're here, git-clang-format exited nonzero which is some kind of error\n+    print(\n+        f\"{TC.RED}Call the doctor! Something went wrong with git-clang-format${TC.ENDC}\"\n+    )\n+    sys.exit(exitcode)\n+\n+\n+def main() -> NoReturn:\n+    print(\"Starting git-clang-format launcher\")\n+\n+    # Check if we have a TO_REF target and if so, enter this block\n+    if PRE_COMMIT_TO_REF is not None:\n+        # Get the hashes of the specified revs for easy comparison\n+        # Need a FROM_REF in this case so it's OK if the rev-parse fails\n+        to_ref = git_rev_parse(PRE_COMMIT_TO_REF)\n+        head_ref = git_rev_parse(\"HEAD\")\n+\n+        # If we are not comparing to HEAD, changes cannot be safely applied; all we\n+        # can do is print the diff and exit appropriately\n+        if to_ref != head_ref:\n+            # No return\n+            compare_skip_formatting()\n+\n+    run_cmd = [\"git-clang-format\"] + FORMAT_ARGS\n+\n+    if PRE_COMMIT_FROM_REF:\n+        run_cmd.append(PRE_COMMIT_FROM_REF)\n+\n+    run_cmd.append(\"--\")\n+    run_cmd.extend(sys.argv)\n+\n+    print(f\"{TC.CYAN}Target ref is HEAD, running...\\n> {' '.join(run_cmd)}{TC.ENDC}\")\n+\n+    output, exitcode = tee_cmd(run_cmd)\n+\n+    if (\n+        output == \"no modified files to format\"\n+        or output == \"clang-format did not modify any files\"\n+    ):\n+        # If clang-format doesn't have any changes, no problems. Expect to exit 0\n+        print(f\"{TC.GREEN}Everything looks OK; no work for me here{TC.ENDC}\")\n+        sys.exit(exitcode)\n+\n+    print(\n+        f\"{TC.YELLOW}OK, I formatted that really nice for you.\"\n+        f\" Retry your commit now.{TC.ENDC}\"\n+    )\n+    # Need to exit as error if files were changed\n+    sys.exit(1)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
            },
            {
                "filename": "tox.ini",
                "patch": "@@ -38,3 +38,43 @@ commands={envpython} -b {toxinidir}/runtests.py --mode=full {posargs:}\n [testenv:debug]\n basepython=python-dbg\n commands=gdb --args {envpython} {toxinidir}/runtests.py --mode=full {posargs:}\n+\n+\n+[flake8]\n+### Flake8 README ###\n+# If you are coming here because flake8 is flagging something that it should not\n+# (and the flagged problem is not reasonable, i.e. cannot be resolved and still\n+# accomplish the coding goal), please add \"# noqa\" to that line in code if it is\n+# only a few lines, or add a \"per-file-ignore\" in this file if there are\n+# multiple problems with one specific file.\n+#\n+# Adding an error code to the \"ignore\" directive makes it global, and adding a\n+# file to \"exclude\" (not shown at time of writing) means that the file will not\n+# be checked for any errors. Both of these should be avoided without good\n+# reason, as they immediately reduce the value provided by this checker.\n+#\n+# E: pycodestyle errors\n+# F: flake8 pyflakes\n+# N: pep8 naming\n+# W: pycodestyle warnings\n+# B: bugbear\n+# B9: bugbear opinions\n+# B950: bugbear line length, better than E501 since it allows long URLs\n+# and slight overages where needed\n+select = C, E, F, W, B, B9, B950\n+# E203: Whitespace before :, black is pep8 and flake8 is wrong here\n+# E501: Line too long. Handled by B950 and Black\n+# N801: Ignore rule that class names as UpperCamelCase\n+# W503: Line break before binary operator\n+ignore = E203, E501, N801, W503\n+# Creates C901: max-complexity, follows cyclomatic guidelines.\n+# May need to be disabled in some files\n+# https://en.wikipedia.org/wiki/Cyclomatic_complexity\n+max-complexity = 10\n+max-line-length = 88\n+per-file-ignores =\n+    # Ignore unused import error; most are for reexporting.\n+    **/__init__.py:F401\n+    # import not at top of file; docstring-related\n+    runtests.py:E402\n+    setup.py:E402"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22916,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nThis pull requests speeds up `numpy.load`. Since `_filter_header` is quite a bottleneck, we only run it if we must. Users will get a warning if they have a legacy Numpy file so that they can save it again for faster loading.\r\n\r\nMain discussion and benchmarks see https://github.com/numpy/numpy/issues/22898",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_io.py",
                "patch": "@@ -1,7 +1,7 @@\n-from .common import Benchmark, get_squares\n+from .common import Benchmark, get_squares, get_squares_\n \n import numpy as np\n-from io import StringIO\n+from io import SEEK_SET, StringIO, BytesIO\n \n \n class Copy(Benchmark):\n@@ -67,6 +67,15 @@ def time_vb_savez_squares(self):\n         np.savez('tmp.npz', **self.squares)\n \n \n+class LoadNpyOverhead(Benchmark):\n+    def setup(self):\n+        self.buffer = BytesIO()\n+        np.save(self.buffer, get_squares_()['float32'])\n+\n+    def time_loadnpy_overhead(self):\n+        self.buffer.seek(0, SEEK_SET)\n+        np.load(self.buffer)\n+\n class LoadtxtCSVComments(Benchmark):\n     # benchmarks for np.loadtxt comment handling\n     # when reading in CSV files"
            },
            {
                "filename": "numpy/lib/format.py",
                "patch": "@@ -623,13 +623,27 @@ def _read_array_header(fp, version, max_header_size=_MAX_HEADER_SIZE):\n     #   \"descr\" : dtype.descr\n     # Versions (2, 0) and (1, 0) could have been created by a Python 2\n     # implementation before header filtering was implemented.\n-    if version <= (2, 0):\n-        header = _filter_header(header)\n+    #\n+    # For performance reasons, we try without _filter_header first though\n     try:\n         d = safe_eval(header)\n     except SyntaxError as e:\n-        msg = \"Cannot parse header: {!r}\"\n-        raise ValueError(msg.format(header)) from e\n+        if version <= (2, 0):\n+            header = _filter_header(header)\n+            try:\n+                d = safe_eval(header)\n+            except SyntaxError as e2:\n+                msg = \"Cannot parse header: {!r}\"\n+                raise ValueError(msg.format(header)) from e2\n+            else:\n+                warnings.warn(\n+                    \"Reading `.npy` or `.npz` file required additional \"\n+                    \"header parsing as it was created on Python 2. Save the \"\n+                    \"file again to speed up loading and avoid this warning.\",\n+                    UserWarning, stacklevel=4)\n+        else:\n+            msg = \"Cannot parse header: {!r}\"\n+            raise ValueError(msg.format(header)) from e\n     if not isinstance(d, dict):\n         msg = \"Header is not a dictionary: {!r}\"\n         raise ValueError(msg.format(d))"
            },
            {
                "filename": "numpy/lib/tests/test_format.py",
                "patch": "@@ -531,7 +531,8 @@ def test_load_padded_dtype(tmpdir, dt):\n def test_python2_python3_interoperability():\n     fname = 'win64python2.npy'\n     path = os.path.join(os.path.dirname(__file__), 'data', fname)\n-    data = np.load(path)\n+    with pytest.warns(UserWarning, match=\"Reading.*this warning\\\\.\"):\n+        data = np.load(path)\n     assert_array_equal(data, np.ones(2))\n \n def test_pickle_python2_python3():"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22932,
        "body": "This is part of an attempt to have better regression detection for `numpy`. Not sure how useful it is to test all of the `array_api` space because they mostly call the underlying `numpy` function and then use `asarray`. This means a lot of them are covered already, but this might be a futureproofing exercise in case the implementation changes.",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_xp.py",
                "patch": "@@ -0,0 +1,34 @@\n+from .common import Benchmark\n+\n+import numpy as np\n+import numpy.array_api as xp\n+\n+# https://data-apis.org/array-api/latest/API_specification/data_types.html\n+XP_TYPES = {\n+    'bool' : xp.bool,\n+    'int8' : xp.int8,\n+    'int16' : xp.int16,\n+    'int32' : xp.int32,\n+    'int64' : xp.int64,\n+    'uint8' : xp.uint8,\n+    'uint16' : xp.uint16,\n+    'uint32' : xp.uint32,\n+    'uint64' : xp.uint64,\n+    'float32' : xp.float32,\n+    'float64' : xp.float64,\n+    # 'complex64' : xp.complex64,\n+    # 'complex128' : xp.complex128\n+}\n+\n+class XP_Creation(Benchmark):\n+    param_names=['xpdtype']\n+    params = XP_TYPES.keys()\n+\n+    def setup(self, args):\n+        self.d = xp.asarray(np.array([1, 2, 3]))\n+\n+    def time_xp_linspace_scalar(self, args):\n+        xp.linspace(0, 10, 2)\n+\n+    def time_xp_linspace_array(self, args):\n+        xp.linspace(self.d, 10, 10)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21154,
        "body": "The web assembly linker is strict about function types, it is\r\nunwilling to link a function defined with signature say double\r\nf(double, double) to an invocation with signature void f(void). This\r\ncauses trouble in numpy/core/setup.py in the functions\r\ncheck_math_capabilities and check_mathlib. \r\n\r\nThis patch fixes the problem by giving config.try_link the correct\r\nfunction signatures for these functions. In particular I added a\r\nseparate header file with all the math declarations. It would be be\r\npossible to just include 'math.h' but we also need to parse the\r\ndeclarations to figure out how to call the functions. If f has\r\narguments type1, type2, type3, we want to call it like f((type1)0,\r\n(type2)0, (type3)0). Anyways it is easier to parse the arguments out\r\nof our feature_detection_math.h than to worry about the possibility\r\nthat someone will have a differently formatted system math.h. We do a\r\ntest where we include both math.h and feature_detection_math.h to\r\nensure consistency between the signatures. \r\n\r\nI also separated out the fcntl functions, backtrace and madvise, and\r\nstrtold_l. This is because they require separate headers. strtold_l\r\nrequires testing both with the locale.h header and with the xlocale.h\r\nheader (this seems to vary even among different linuxes). All of the\r\nfunctions I moved out of OPTIONAL_STDFUNCS are absent on windows and\r\nsome are absent on OSX so separating them out of the\r\nOPTIONAL_STDFUNCS should mildly improve the speed of feature\r\ndetection on mac and windows (since they have all the functions\r\nremaining in OPTIONAL_STDFUNCS).",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/21154.improvement.rst",
                "patch": "@@ -0,0 +1,7 @@\n+Math C library feature detection now uses correct signatures\n+------------------------------------------------------------\n+Compiling is preceded by a detection phase to determine whether the\n+underlying libc supports certain math operations. Previously this code\n+did not respect the proper signatures. Fixing this enables compilation\n+for the ``wasm-ld`` backend (compilation for web assembly) and reduces\n+the number of warnings."
            },
            {
                "filename": "numpy/core/feature_detection_locale.h",
                "patch": "@@ -0,0 +1 @@\n+long double strtold_l(const char*, char**, locale_t);"
            },
            {
                "filename": "numpy/core/feature_detection_math.h",
                "patch": "@@ -0,0 +1,107 @@\n+double expm1(double);\n+double log1p(double);\n+double acosh(double);\n+double asinh(double);\n+double atanh(double);\n+double rint(double);\n+double trunc(double);\n+double exp2(double);\n+double log2(double);\n+double hypot(double, double);\n+double atan2(double, double);\n+double pow(double, double);\n+double copysign(double, double);\n+double nextafter(double, double);\n+long long strtoll(const char*, char**, int);\n+unsigned long long strtoull(const char*, char**, int);\n+double cbrt(double);\n+long double sinl(long double);\n+long double cosl(long double);\n+long double tanl(long double);\n+long double sinhl(long double);\n+long double coshl(long double);\n+long double tanhl(long double);\n+long double fabsl(long double);\n+long double floorl(long double);\n+long double ceill(long double);\n+long double rintl(long double);\n+long double truncl(long double);\n+long double sqrtl(long double);\n+long double log10l(long double);\n+long double logl(long double);\n+long double log1pl(long double);\n+long double expl(long double);\n+long double expm1l(long double);\n+long double asinl(long double);\n+long double acosl(long double);\n+long double atanl(long double);\n+long double asinhl(long double);\n+long double acoshl(long double);\n+long double atanhl(long double);\n+long double hypotl(long double, long double);\n+long double atan2l(long double, long double);\n+long double powl(long double, long double);\n+long double fmodl(long double, long double);\n+long double modfl(long double, long double*);\n+long double frexpl(long double, int*);\n+long double ldexpl(long double, int);\n+long double exp2l(long double);\n+long double log2l(long double);\n+long double copysignl(long double, long double);\n+long double nextafterl(long double, long double);\n+long double cbrtl(long double);\n+float sinf(float);\n+float cosf(float);\n+float tanf(float);\n+float sinhf(float);\n+float coshf(float);\n+float tanhf(float);\n+float fabsf(float);\n+float floorf(float);\n+float ceilf(float);\n+float rintf(float);\n+float truncf(float);\n+float sqrtf(float);\n+float log10f(float);\n+float logf(float);\n+float log1pf(float);\n+float expf(float);\n+float expm1f(float);\n+float asinf(float);\n+float acosf(float);\n+float atanf(float);\n+float asinhf(float);\n+float acoshf(float);\n+float atanhf(float);\n+float hypotf(float, float);\n+float atan2f(float, float);\n+float powf(float, float);\n+float fmodf(float, float);\n+float modff(float, float*);\n+float frexpf(float, int*);\n+float ldexpf(float, int);\n+float exp2f(float);\n+float log2f(float);\n+float copysignf(float, float);\n+float nextafterf(float, float);\n+float cbrtf(float);\n+double sin(double);\n+double cos(double);\n+double tan(double);\n+double sinh(double);\n+double cosh(double);\n+double tanh(double);\n+double fabs(double);\n+double floor(double);\n+double ceil(double);\n+double sqrt(double);\n+double log10(double);\n+double log(double);\n+double exp(double);\n+double asin(double);\n+double acos(double);\n+double atan(double);\n+double fmod(double, double);\n+double modf(double, double*);\n+double frexp(double, int*);\n+double ldexp(double, int);"
            },
            {
                "filename": "numpy/core/feature_detection_misc.h",
                "patch": "@@ -0,0 +1,4 @@\n+#include <stddef.h>\n+\n+int backtrace(void **, int);\n+int madvise(void *, size_t, int);"
            },
            {
                "filename": "numpy/core/feature_detection_stdio.h",
                "patch": "@@ -0,0 +1,6 @@\n+#include <stdio.h>\n+#include <fcntl.h>\n+\n+off_t ftello(FILE *stream);\n+int fseeko(FILE *stream, off_t offset, int whence);\n+int fallocate(int, int, off_t, off_t);"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -129,32 +129,48 @@ def win32_checks(deflist):\n         deflist.append('FORCE_NO_LONG_DOUBLE_FORMATTING')\n \n def check_math_capabilities(config, ext, moredefs, mathlibs):\n-    def check_func(func_name):\n-        return config.check_func(func_name, libraries=mathlibs,\n-                                 decl=True, call=True)\n-\n-    def check_funcs_once(funcs_name):\n-        decl = dict([(f, True) for f in funcs_name])\n-        st = config.check_funcs_once(funcs_name, libraries=mathlibs,\n-                                     decl=decl, call=decl)\n+    def check_func(\n+        func_name,\n+        decl=False,\n+        headers=[\"feature_detection_math.h\"],\n+    ):\n+        return config.check_func(\n+            func_name,\n+            libraries=mathlibs,\n+            decl=decl,\n+            call=True,\n+            call_args=FUNC_CALL_ARGS[func_name],\n+            headers=headers,\n+        )\n+\n+    def check_funcs_once(funcs_name, headers=[\"feature_detection_math.h\"]):\n+        call = dict([(f, True) for f in funcs_name])\n+        call_args = dict([(f, FUNC_CALL_ARGS[f]) for f in funcs_name])\n+        st = config.check_funcs_once(\n+            funcs_name,\n+            libraries=mathlibs,\n+            decl=False,\n+            call=call,\n+            call_args=call_args,\n+            headers=headers,\n+        )\n         if st:\n             moredefs.extend([(fname2def(f), 1) for f in funcs_name])\n         return st\n \n-    def check_funcs(funcs_name):\n+    def check_funcs(funcs_name, headers=[\"feature_detection_math.h\"]):\n         # Use check_funcs_once first, and if it does not work, test func per\n         # func. Return success only if all the functions are available\n-        if not check_funcs_once(funcs_name):\n+        if not check_funcs_once(funcs_name, headers=headers):\n             # Global check failed, check func per func\n             for f in funcs_name:\n-                if check_func(f):\n+                if check_func(f, headers=headers):\n                     moredefs.append((fname2def(f), 1))\n             return 0\n         else:\n             return 1\n \n     #use_msvc = config.check_decl(\"_MSC_VER\")\n-\n     if not check_funcs_once(MANDATORY_FUNCS):\n         raise SystemError(\"One of the required function to build numpy is not\"\n                 \" available (the list is %s).\" % str(MANDATORY_FUNCS))\n@@ -169,15 +185,34 @@ def check_funcs(funcs_name):\n     for f in OPTIONAL_STDFUNCS_MAYBE:\n         if config.check_decl(fname2def(f),\n                     headers=[\"Python.h\", \"math.h\"]):\n-            OPTIONAL_STDFUNCS.remove(f)\n+            if f in OPTIONAL_STDFUNCS:\n+                OPTIONAL_STDFUNCS.remove(f)\n+            else:\n+                OPTIONAL_FILE_FUNCS.remove(f)\n+\n \n     check_funcs(OPTIONAL_STDFUNCS)\n+    check_funcs(OPTIONAL_FILE_FUNCS, headers=[\"feature_detection_stdio.h\"])\n+    check_funcs(OPTIONAL_MISC_FUNCS, headers=[\"feature_detection_misc.h\"])\n+    \n+\n \n     for h in OPTIONAL_HEADERS:\n         if config.check_func(\"\", decl=False, call=False, headers=[h]):\n             h = h.replace(\".\", \"_\").replace(os.path.sep, \"_\")\n             moredefs.append((fname2def(h), 1))\n \n+    # Try with both \"locale.h\" and \"xlocale.h\"\n+    locale_headers = [\n+        \"stdlib.h\",\n+        \"xlocale.h\",\n+        \"feature_detection_locale.h\",\n+    ]\n+    if not check_funcs(OPTIONAL_LOCALE_FUNCS, headers=locale_headers):\n+        # It didn't work with xlocale.h, maybe it will work with locale.h?\n+        locale_headers[1] = \"locale.h\"\n+        check_funcs(OPTIONAL_LOCALE_FUNCS, headers=locale_headers)\n+\n     for tup in OPTIONAL_INTRINSICS:\n         headers = None\n         if len(tup) == 2:\n@@ -398,20 +433,28 @@ def check_types(config_cmd, ext, build_dir):\n def check_mathlib(config_cmd):\n     # Testing the C math library\n     mathlibs = []\n-    mathlibs_choices = [[], ['m'], ['cpml']]\n-    mathlib = os.environ.get('MATHLIB')\n+    mathlibs_choices = [[], [\"m\"], [\"cpml\"]]\n+    mathlib = os.environ.get(\"MATHLIB\")\n     if mathlib:\n-        mathlibs_choices.insert(0, mathlib.split(','))\n+        mathlibs_choices.insert(0, mathlib.split(\",\"))\n     for libs in mathlibs_choices:\n-        if config_cmd.check_func(\"exp\", libraries=libs, decl=True, call=True):\n+        if config_cmd.check_func(\n+            \"log\",\n+            libraries=libs,\n+            call_args=\"0\",\n+            decl=\"double log(double);\",\n+            call=True\n+        ):\n             mathlibs = libs\n             break\n     else:\n         raise RuntimeError(\n             \"math library missing; rerun setup.py after setting the \"\n-            \"MATHLIB env variable\")\n+            \"MATHLIB env variable\"\n+        )\n     return mathlibs\n \n+\n def visibility_define(config):\n     \"\"\"Return the define value to use for NPY_VISIBILITY_HIDDEN (may be empty\n     string).\"\"\""
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -1,8 +1,9 @@\n # Code common to build tools\n-import sys\n-import warnings\n import copy\n+import pathlib\n+import sys\n import textwrap\n+import warnings\n \n from numpy.distutils.misc_util import mingw32\n \n@@ -91,6 +92,32 @@ def check_api_version(apiversion, codegen_dir):\n         warnings.warn(msg % (apiversion, curapi_hash, apiversion, api_hash,\n                              __file__),\n                       MismatchCAPIWarning, stacklevel=2)\n+\n+\n+FUNC_CALL_ARGS = {}\n+\n+def set_sig(sig):\n+    prefix, _, args = sig.partition(\"(\")\n+    args = args.rpartition(\")\")[0]\n+    funcname = prefix.rpartition(\" \")[-1]\n+    args = [arg.strip() for arg in args.split(\",\")]\n+    FUNC_CALL_ARGS[funcname] = \", \".join(\"(%s) 0\" % arg for arg in args)\n+\n+\n+for file in [\n+    \"feature_detection_locale.h\",\n+    \"feature_detection_math.h\",\n+    \"feature_detection_misc.h\",\n+    \"feature_detection_stdio.h\",\n+]:\n+    with open(pathlib.Path(__file__).parent / file) as f:\n+        for line in f:\n+            if line.startswith(\"#\"):\n+                continue\n+            if not line.strip():\n+                continue\n+            set_sig(line)\n+\n # Mandatory functions: if not found, fail the build\n MANDATORY_FUNCS = [\"sin\", \"cos\", \"tan\", \"sinh\", \"cosh\", \"tanh\", \"fabs\",\n         \"floor\", \"ceil\", \"sqrt\", \"log10\", \"log\", \"exp\", \"asin\",\n@@ -100,9 +127,11 @@ def check_api_version(apiversion, codegen_dir):\n # replacement implementation. Note that some of these are C99 functions.\n OPTIONAL_STDFUNCS = [\"expm1\", \"log1p\", \"acosh\", \"asinh\", \"atanh\",\n         \"rint\", \"trunc\", \"exp2\", \"log2\", \"hypot\", \"atan2\", \"pow\",\n-        \"copysign\", \"nextafter\", \"ftello\", \"fseeko\",\n-        \"strtoll\", \"strtoull\", \"cbrt\", \"strtold_l\", \"fallocate\",\n-        \"backtrace\", \"madvise\"]\n+        \"copysign\", \"nextafter\", \"strtoll\", \"strtoull\", \"cbrt\"]\n+\n+OPTIONAL_LOCALE_FUNCS = [\"strtold_l\"]\n+OPTIONAL_FILE_FUNCS = [\"ftello\", \"fseeko\", \"fallocate\"]\n+OPTIONAL_MISC_FUNCS = [\"backtrace\", \"madvise\"]\n \n \n OPTIONAL_HEADERS = ["
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18368,
        "body": "This PR solves issue [11569](https://github.com/numpy/numpy/issues/11569). @tylerjereddy also requested optimizations for large boolean arrays. I have added optimizations to numpy.nonzero for contiguous and aligned numpy arrays. Generally, the speedup is at least 2 times and the maximum speed gain observed is about 8 times. Currently, the optimizations are exclusive to all int types, float types and bool type for arrays of dimension 1 to 3. If needed, optimizations for higher dimensional arrays can be added. Test cases have been added.\r\n\r\nThe following code illustrates some of the speed gains:\r\n```\r\nimport numpy as np\r\n\r\nx = np.random.rand(1000000) > 0.5\r\n%timeit np.nonzero(x)\r\n666 \u00b5s \u00b1 2.04 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n4.33 ms \u00b1 17.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.rand(1000,1000) > 0.5\r\n%timeit np.nonzero(x)\r\n1.18 ms \u00b1 2.68 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n9.55 ms \u00b1 32.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.rand(100,100,100) > 0.5\r\n%timeit np.nonzero(x)\r\n1.48 ms \u00b1 3.55 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n9.57 ms \u00b1 44.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.rand(1000000) > 0.9\r\n%timeit np.nonzero(x)\r\n655 \u00b5s \u00b1 2.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n1.77 ms \u00b1 7.87 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [Without optimization]\r\n\r\nx = np.random.rand(1000,1000) > 0.9\r\n%timeit np.nonzero(x)\r\n1.02 ms \u00b1 2.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n4.37 ms \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.rand(100,100,100) > 0.9\r\n%timeit np.nonzero(x)\r\n1.22 ms \u00b1 9.38 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n3.9 ms \u00b1 65.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.randint(0,9, size=(1000,1000), dtype=np.int32)\r\n%timeit np.nonzero(x)\r\n1.69 ms \u00b1 2.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n7.83 ms \u00b1 17.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.randint(0,2, size=(1000,1000), dtype=np.int32)\r\n%timeit np.nonzero(x)\r\n1.49 ms \u00b1 5.86 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n10.4 ms \u00b1 31.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.randint(0,9, size=(1000,1000), dtype=np.int64)\r\n%timeit np.nonzero(x)\r\n2.3 ms \u00b1 6.37 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n8.15 ms \u00b1 29.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n\r\nx = np.random.randint(0,2, size=(1000,1000), dtype=np.int64)\r\n%timeit np.nonzero(x)\r\n1.93 ms \u00b1 4.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) [With optimization]\r\n10.7 ms \u00b1 22.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) [Without optimization]\r\n```",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_core.py",
                "patch": "@@ -161,6 +161,23 @@ def time_count_nonzero_multi_axis(self, numaxes, size, dtype):\n                 self.x.ndim - 1, self.x.ndim - 2))\n \n \n+class Nonzero(Benchmark):\n+    params = [\n+    [bool, np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64, np.int64, np.float32, np.float64],\n+    [(1000000,), (1000,1000), (100,100,100)]\n+    ]\n+    param_names = [\"dtype\", \"shape\"]\n+\n+    def setup(self, dtype, size):\n+        self.x = np.random.randint(0,2,size=size).astype(dtype)\n+\n+    def time_nonzero(self, dtype, size):\n+        np.nonzero(self.x)\n+\n+    def time_nonzero_bool_comparison(self, dtype, size):\n+        np.nonzero(self.x > 0.5)\n+\n+\n class PackBits(Benchmark):\n     param_names = ['dtype']\n     params = [[bool, np.uintp]]"
            },
            {
                "filename": "numpy/core/src/common/lowlevel_strided_loops.h",
                "patch": "@@ -3,6 +3,7 @@\n #include \"common.h\"\n #include <npy_config.h>\n #include \"mem_overlap.h\"\n+#include \"stdbool.h\"\n \n /* For PyArray_ macros used below */\n #include \"numpy/ndarrayobject.h\"\n@@ -907,4 +908,11 @@ PyArray_EQUIVALENTLY_ITERABLE_OVERLAP_OK(PyArrayObject *arr1, PyArrayObject *arr\n                     stride3 = PyArray_TRIVIAL_PAIR_ITERATION_STRIDE(size3, arr3); \\\n                 }\n \n+bool nonzero_idxs_dispatcher1D_C(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count);\n+bool nonzero_idxs_dispatcher1D_F(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count); \n+bool nonzero_idxs_dispatcher2D_C(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count);\n+bool nonzero_idxs_dispatcher2D_F(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count); \n+bool nonzero_idxs_dispatcher3D_C(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count);\n+bool nonzero_idxs_dispatcher3D_F(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count); \n+\n #endif"
            },
            {
                "filename": "numpy/core/src/multiarray/item_selection.c",
                "patch": "@@ -30,6 +30,8 @@\n #include \"array_coercion.h\"\n #include \"simd/simd.h\"\n \n+#include \"stdbool.h\"\n+\n static NPY_GCC_OPT_3 NPY_INLINE int\n npy_fasttake_impl(\n         char *dest, char *src, const npy_intp *indices,\n@@ -2497,6 +2499,44 @@ PyArray_CountNonzero(PyArrayObject *self)\n     return nonzero_count;\n }\n \n+\n+bool nonzero_idxs_dispatcher_ND(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count, int ndims, bool is_C_layout) {\n+    bool executed = false;\n+    NPY_BEGIN_THREADS_DEF;\n+\n+    switch(ndims) {\n+        case 1:\n+        {\n+            executed = nonzero_idxs_dispatcher1D_C(data, idxs, shape, strides, dtype, nonzero_count);\n+            break;\n+        }\n+        case 2:\n+        {\n+            if (is_C_layout) {\n+                executed = nonzero_idxs_dispatcher2D_C(data, idxs, shape, strides, dtype, nonzero_count);\n+            }\n+            else {\n+                executed = nonzero_idxs_dispatcher2D_F(data, idxs, shape, strides, dtype, nonzero_count);                \n+            }\n+            break;\n+        }\n+        case 3:\n+        {\n+            if (is_C_layout) {\n+                executed = nonzero_idxs_dispatcher3D_C(data, idxs, shape, strides, dtype, nonzero_count);\n+            }\n+            else {\n+                executed = nonzero_idxs_dispatcher3D_F(data, idxs, shape, strides, dtype, nonzero_count);                \n+            }\n+            break;\n+        }\n+    }\n+\n+    NPY_END_THREADS;\n+    return executed;\n+}\n+\n+\n /*NUMPY_API\n  * Nonzero\n  *\n@@ -2584,18 +2624,64 @@ PyArray_Nonzero(PyArrayObject *self)\n         return NULL;\n     }\n \n+    /* nothing to do */\n+    if (nonzero_count == 0) {\n+        goto finish;\n+    }\n+\n+    npy_intp * multi_index = (npy_intp *)PyArray_DATA(ret);\n+    char * data = PyArray_BYTES(self);\n+    int flags = PyArray_FLAGS(self);\n+\n+    bool can_be_optimized = false;\n+    PyArrayObject* original_array = self;\n+\n+    if (PyArray_BASE(self) != NULL) {\n+        original_array = (PyArrayObject*) PyArray_BASE(self);\n+    } \n+\n+    if (dtype->kind == 'b' || dtype->kind == 'i' || dtype->kind == 'u' || dtype->kind == 'f')\n+        can_be_optimized = true;\n+\n+    if (can_be_optimized && (flags & NPY_ARRAY_ALIGNED) && PyArray_ISNOTSWAPPED(self) && PyArray_ISNOTSWAPPED(original_array)) { // Only apply the optimization if array is aligned and byteorder is not swapped.\n+        bool to_jmp = false;\n+        bool is_contiguous = true;\n+        bool is_C_layout = false;\n+\n+        if (!(flags & NPY_ARRAY_C_CONTIGUOUS) && !(flags & NPY_ARRAY_F_CONTIGUOUS)) {\n+            is_contiguous = false;\n+        }\n+        else if (flags & NPY_ARRAY_C_CONTIGUOUS) {\n+            is_C_layout = true;\n+        } \n+        /* No need to check for F_Contiguousness since if it is not C_contiguous and it is contiguous \n+        then it must be F_contiguous. Hence is_C_layout stays false to indicate F_contiguousness.*/\n+\n+        npy_intp* M_shape = PyArray_SHAPE(self);\n+        npy_intp* M_strides = PyArray_STRIDES(self);\n+        int M_type_num = dtype->type_num;\n+        int M_dim = ndim;\n+\n+        if (is_contiguous && is_C_layout) { \n+            to_jmp = nonzero_idxs_dispatcher_ND((void*)data, multi_index, M_shape, M_strides, M_type_num, nonzero_count, M_dim, true);\n+        }\n+        else if (is_contiguous && !is_C_layout) { \n+            to_jmp = nonzero_idxs_dispatcher_ND((void*)data, multi_index, M_shape, M_strides, M_type_num, nonzero_count, M_dim, false);\n+        }\n+\n+        if (to_jmp) {\n+            added_count = nonzero_count;\n+            goto finish;\n+        }\n+    }\n+\n+\n     /* If it's a one-dimensional result, don't use an iterator */\n-    if (ndim == 1) {\n-        npy_intp * multi_index = (npy_intp *)PyArray_DATA(ret);\n-        char * data = PyArray_BYTES(self);\n+    if (ndim == 1) {     \n         npy_intp stride = PyArray_STRIDE(self, 0);\n         npy_intp count = PyArray_DIM(self, 0);\n-        NPY_BEGIN_THREADS_DEF;\n \n-        /* nothing to do */\n-        if (nonzero_count == 0) {\n-            goto finish;\n-        }\n+        NPY_BEGIN_THREADS_DEF;\n \n         if (!needs_api) {\n             NPY_BEGIN_THREADS_THRESHOLDED(count);\n@@ -2650,6 +2736,7 @@ PyArray_Nonzero(PyArrayObject *self)\n         NPY_END_THREADS;\n \n         goto finish;\n+\n     }\n \n     /*\n@@ -2722,7 +2809,9 @@ PyArray_Nonzero(PyArrayObject *self)\n \n     NpyIter_Deallocate(iter);\n \n+\n finish:\n+\n     if (PyErr_Occurred()) {\n         Py_DECREF(ret);\n         return NULL;\n@@ -2765,6 +2854,7 @@ PyArray_Nonzero(PyArrayObject *self)\n     return ret_tuple;\n }\n \n+\n /*\n  * Gets a single item from the array, based on a single multi-index\n  * array of values, which must be of length PyArray_NDIM(self)."
            },
            {
                "filename": "numpy/core/src/multiarray/lowlevel_strided_loops.c.src",
                "patch": "@@ -22,6 +22,8 @@\n #include \"array_method.h\"\n #include \"usertypes.h\"\n \n+#include \"stdbool.h\"\n+\n \n /*\n  * x86 platform works with unaligned access but the compiler is allowed to\n@@ -1823,3 +1825,193 @@ mapiter_@name@(PyArrayMapIterObject *mit)\n }\n \n /**end repeat**/\n+\n+\n+\n+#define ptr_assignment1(ptr1, val1, stride1) *ptr1 = val1; ptr1 += stride1; \n+#define ptr_assignment2(ptr1, val1, stride1, ptr2, val2, stride2) ptr_assignment1(ptr1, val1, stride1) *ptr2 = val2; ptr2 += stride2; \n+\n+#define loop_ptr_assignment(start, end, ptr_assignment) \\\n+    for (npy_intp i=start; i<end; ++i) { \\\n+        ptr_assignment \\\n+    }\n+\n+/**begin repeat\n+ * \n+ * #dtype = npy_bool, npy_byte, npy_byte, npy_uint16, npy_int16, npy_uint32, npy_int32, npy_uint64, npy_int64, npy_float, npy_double#\n+ * #name = bool, u8, i8, u16, i16, u32, i32, u64, i64, f32, f64#\n+ */\n+\n+/**begin repeat1\n+ *\n+ * #layout=C,F#\n+ *\n+ */\n+\n+void nonzero_idxs_1D_@layout@_@name@(@dtype@ * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, npy_intp nonzero_count) {\n+    npy_intp stride = strides[0]; \n+    npy_intp added_count = 0; \n+    npy_intp a = 0; \n+    while (added_count < nonzero_count) { \n+        *idxs = a;  \n+        bool to_increment = ((bool) *data); \n+        idxs += ((int) to_increment); \n+        added_count += ((int) to_increment); \n+        data = (@dtype@ *) (((char*) data) + stride); \n+        ++a;\n+    }\n+}\n+\n+ /**end repeat1**/\n+\n+void nonzero_idxs_2D_C_@name@(@dtype@ * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, npy_intp nonzero_count) { \n+    npy_intp idxs_stride = 2; \n+    npy_intp size_1 = shape[1]; \n+    npy_intp stride_1 = strides[1]; \n+    npy_intp* idxs_0 = idxs;\n+    npy_intp* idxs_1 = idxs + 1;    \n+    npy_intp added_count = 0; \n+    npy_intp a = 0; \n+    while (added_count < nonzero_count) { \n+        npy_intp old_added_count = added_count; \n+        npy_intp b = 0;\n+        while (added_count < nonzero_count && b<size_1) { \n+            *idxs_1 = b; \n+            npy_uintp to_increment = (npy_uintp) ((bool) *data); \n+            idxs_1 += (idxs_stride & (-to_increment)); \n+            added_count += to_increment; \n+            data = (@dtype@ *) (((char *) data) + stride_1); \n+            ++b;\n+        } \n+        loop_ptr_assignment(0, added_count - old_added_count, ptr_assignment1(idxs_0, a, idxs_stride)) \n+        ++a;\n+    }\n+}\n+\n+\n+void nonzero_idxs_2D_F_@name@(@dtype@ * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, npy_intp nonzero_count) { \n+    npy_intp idxs_stride = 2; \n+    npy_intp size_0 = shape[0]; \n+    npy_intp stride_0 = strides[0]; \n+    npy_intp* idxs_0 = idxs;\n+    npy_intp* idxs_1 = idxs + 1;    \n+    npy_intp added_count = 0; \n+    npy_intp b = 0; \n+    while (added_count < nonzero_count) { \n+        npy_intp old_added_count = added_count; \n+        npy_intp a = 0;\n+        while (added_count < nonzero_count && a<size_0) { \n+            *idxs_0 = a; \n+            npy_uintp to_increment = (npy_uintp) ((bool) *data); \n+            idxs_0 += (idxs_stride & (-to_increment)); \n+            added_count += to_increment; \n+            data = (@dtype@ *) (((char *) data) + stride_0); \n+            ++a;\n+        } \n+        loop_ptr_assignment(0, added_count - old_added_count, ptr_assignment1(idxs_1, b, idxs_stride)) \n+        ++b;\n+    }\n+}\n+\n+\n+void nonzero_idxs_3D_C_@name@(@dtype@ * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, npy_intp nonzero_count) { \n+    npy_intp idxs_stride = 3; \n+    npy_intp size_1 = shape[1]; \n+    npy_intp size_2 = shape[2]; \n+    npy_intp stride_2 = strides[2]; \n+    npy_intp* idxs_0 = idxs;\n+    npy_intp* idxs_1 = idxs + 1;    \n+    npy_intp* idxs_2 = idxs + 2;    \n+    npy_intp added_count = 0; \n+    npy_intp a = 0; \n+    while (added_count < nonzero_count) { \n+        npy_intp b = 0;\n+        while (added_count < nonzero_count && b<size_1) { \n+            npy_intp old_added_count = added_count; \n+            npy_intp c = 0;\n+            while (added_count < nonzero_count && c<size_2) { \n+                *idxs_2 = c; \n+                npy_uintp to_increment = (npy_uintp) ((bool) *data); \n+                idxs_2 += (idxs_stride & (-to_increment)); \n+                added_count += to_increment; \n+                data = (@dtype@ *) (((char *) data) + stride_2); \n+                ++c;\n+            }\n+            loop_ptr_assignment(0, added_count - old_added_count, ptr_assignment2(idxs_0, a, idxs_stride, idxs_1, b, idxs_stride)) \n+            ++b;\n+        }\n+        ++a;\n+    }\n+}\n+\n+\n+void nonzero_idxs_3D_F_@name@(@dtype@ * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, npy_intp nonzero_count) { \n+    npy_intp idxs_stride = 3; \n+    npy_intp size_0 = shape[0]; \n+    npy_intp size_1 = shape[1]; \n+    npy_intp stride_0 = strides[0]; \n+    npy_intp* idxs_0 = idxs;\n+    npy_intp* idxs_1 = idxs + 1;    \n+    npy_intp* idxs_2 = idxs + 2;    \n+    npy_intp added_count = 0; \n+    npy_intp c = 0; \n+    while (added_count < nonzero_count) { \n+        npy_intp b = 0;\n+        while (added_count < nonzero_count && b<size_1) { \n+            npy_intp old_added_count = added_count; \n+            npy_intp a = 0;\n+            while (added_count < nonzero_count && a<size_0) { \n+                *idxs_0 = a; \n+                npy_uintp to_increment = (npy_uintp) ((bool) *data); \n+                idxs_0 += (idxs_stride & (-to_increment)); \n+                added_count += to_increment; \n+                data = (@dtype@ *) (((char *) data) + stride_0); \n+                ++a;\n+            }\n+            loop_ptr_assignment(0, added_count - old_added_count, ptr_assignment2(idxs_1, b, idxs_stride, idxs_2, c, idxs_stride)) \n+            ++b;\n+        }\n+        ++c;\n+    }\n+}\n+\n+/**end repeat**/\n+\n+\n+\n+ /**begin repeat\n+  *\n+  * #ndim = 1, 2, 3#\n+  */\n+\n+ /**begin repeat1\n+  *\n+  * #layout = C, F#\n+  */\n+\n+bool nonzero_idxs_dispatcher@ndim@D_@layout@(void * data, npy_intp* idxs, npy_intp* shape, npy_intp* strides, int dtype, npy_intp nonzero_count) { \n+\n+    switch(dtype) { \n+    \n+        /**begin repeat2\n+         *\n+         * #dtype = npy_bool, npy_byte, npy_byte, npy_uint16, npy_int16, npy_uint32, npy_int32, npy_uint64, npy_int64, npy_float, npy_double#\n+         * #dtypeID = NPY_BOOL, NPY_UINT8, NPY_INT8, NPY_UINT16, NPY_INT16, NPY_UINT32, NPY_INT32, NPY_UINT64, NPY_INT64, NPY_FLOAT32, NPY_FLOAT64#\n+         * #name = bool, u8, i8, u16, i16, u32, i32, u64, i64, f32, f64#\n+         */\n+\n+        case @dtypeID@: \n+        { \n+            @dtype@ * data_ptr = (@dtype@ *) data; \n+            nonzero_idxs_@ndim@D_@layout@_@name@(data_ptr, idxs, shape, strides, nonzero_count); \n+            return true; \n+        } \n+\n+        /**end repeat2**/\n+\n+    }\n+    return false; \n+}\n+\n+ /**end repeat1**/\n+ /**end repeat**/"
            },
            {
                "filename": "numpy/core/tests/test_numeric.py",
                "patch": "@@ -1277,6 +1277,27 @@ def test_nonzero_onedim(self):\n         assert_equal(np.nonzero(x['a']), ([0, 2, 3],))\n         assert_equal(np.nonzero(x['b']), ([0, 2, 3, 4],))\n \n+        types = [bool, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64, np.float32, np.float64]\n+        idxs = np.arange(100)\n+        iters = 10\n+        for _ in range(iters):\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(100)).astype(dtype)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 101)\n+                x[idxs[:num_zeros]] = 0\n+                idxs = np.nonzero(x)[0]\n+                assert_equal(np.array_equal(np.where(x != 0)[0], idxs), True)\n+\n+        for _ in range(iters): # Check for slices\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(100)).astype(dtype)\n+                x_view = x[:50]\n+                x_view[[0,2,8,22,24]] = 0\n+                idxs = np.nonzero(x_view)[0]\n+                assert_equal(np.array_equal(np.where(x_view != 0)[0], idxs), True)\n+\n+\n     def test_nonzero_twodim(self):\n         x = np.array([[0, 1, 0], [2, 0, 3]])\n         assert_equal(np.count_nonzero(x.astype('i1')), 3)\n@@ -1306,6 +1327,202 @@ def test_nonzero_twodim(self):\n         assert_equal(np.nonzero(x['a'].T), ([0, 1, 1, 2], [1, 1, 2, 0]))\n         assert_equal(np.nonzero(x['b'].T), ([0, 0, 1, 2, 2], [0, 1, 2, 0, 2]))\n \n+        types = [bool, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64, np.float32, np.float64]\n+        idxs = np.arange(100)\n+        iters = 10\n+        dim_permutation = np.arange(2) # Shuffle dimensions\n+\n+        for _ in range(iters):\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10)).astype(dtype)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 101)\n+                x.flat[idxs[:num_zeros]] = 0\n+                idxs_0, idxs_1 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j in zip(idxs_0, idxs_1):\n+                    if x[i,j] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+\n+        for _ in range(iters): # This time randomly permute dimensions\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10)).astype(dtype)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 101)\n+                x.flat[idxs[:num_zeros]] = 0\n+                np.random.shuffle(dim_permutation)\n+                x = np.transpose(x, dim_permutation)\n+                idxs_0, idxs_1 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j in zip(idxs_0, idxs_1):\n+                    if x[i,j] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+        # Repeat the above logic for fortran contiguous arrays\n+        for _ in range(iters):\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10)).astype(dtype)\n+                x = np.asfortranarray(x)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 101)\n+                x.flat[idxs[:num_zeros]] = 0\n+                idxs_0, idxs_1 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j in zip(idxs_0, idxs_1):\n+                    if x[i,j] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+\n+        for _ in range(iters): # This time randomly permute dimensions\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10)).astype(dtype)\n+                x = np.asfortranarray(x)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 101)\n+                x.flat[idxs[:num_zeros]] = 0\n+                np.random.shuffle(dim_permutation)\n+                x = np.transpose(x, dim_permutation)\n+                idxs_0, idxs_1 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j in zip(idxs_0, idxs_1):\n+                    if x[i,j] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+\n+        for i in range(iters):  # check for slices\n+            x = ((2**33)*np.random.randn(10, 10)).astype(np.int32)\n+            x.flat[[2,6,9,15,21]] = 0\n+            x_view = x[:5,:5]\n+\n+            idxs_0, idxs_1 = np.nonzero(x_view)\n+            assert_equal(np.count_nonzero(x_view), len(idxs_0))\n+            nzs = 0\n+            for i,j in zip(idxs_0, idxs_1):\n+                if x_view[i,j] == 0:\n+                    nzs += 1\n+            assert_equal(nzs, 0)\n+\n+            y = np.asfortranarray(x)\n+            y_view = y[:5,:5]\n+\n+            idxs_0, idxs_1 = np.nonzero(y_view)\n+            assert_equal(np.count_nonzero(y_view), len(idxs_0))\n+            nzs = 0\n+            for i,j in zip(idxs_0, idxs_1):\n+                if y_view[i,j] == 0:\n+                    nzs += 1\n+            assert_equal(nzs, 0)\n+\n+\n+\n+\n+    def test_nonzero_threedim(self):\n+        types = [bool, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64, np.float32, np.float64]\n+        idxs = np.arange(300)\n+        iters = 10\n+        dim_permutation = np.arange(3) # Shuffle dimensions\n+\n+        for _ in range(iters):\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10, 3)).astype(dtype)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 301)\n+                x.flat[idxs[:num_zeros]] = 0\n+                idxs_0, idxs_1, idxs_2 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j,k in zip(idxs_0, idxs_1, idxs_2):\n+                    if x[i,j,k] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+\n+        for _ in range(iters): # This time randomly permute dimensions\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10, 3)).astype(dtype)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 301)\n+                x.flat[idxs[:num_zeros]] = 0\n+                np.random.shuffle(dim_permutation)\n+                x = np.transpose(x, dim_permutation)\n+                idxs_0, idxs_1, idxs_2 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j,k in zip(idxs_0, idxs_1, idxs_2):\n+                    if x[i,j,k] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+        # Repeat the above for fortran contiguous arrays\n+        for _ in range(iters):\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10, 3)).astype(dtype)\n+                x = np.asfortranarray(x)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 301)\n+                x.flat[idxs[:num_zeros]] = 0\n+                idxs_0, idxs_1, idxs_2 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j,k in zip(idxs_0, idxs_1, idxs_2):\n+                    if x[i,j,k] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+\n+        for _ in range(iters): # This time randomly permute dimensions\n+            for dtype in types:\n+                x = ((2**33)*np.random.randn(10, 10, 3)).astype(dtype)\n+                x = np.asfortranarray(x)\n+                np.random.shuffle(idxs)\n+                num_zeros = np.random.randint(0, 301)\n+                x.flat[idxs[:num_zeros]] = 0\n+                np.random.shuffle(dim_permutation)\n+                x = np.transpose(x, dim_permutation)\n+                idxs_0, idxs_1, idxs_2 = np.nonzero(x)\n+                assert_equal(np.count_nonzero(x), len(idxs_0))\n+                nzs = 0\n+                for i,j,k in zip(idxs_0, idxs_1, idxs_2):\n+                    if x[i,j,k] == 0:\n+                        nzs += 1\n+                assert_equal(nzs, 0)\n+\n+\n+        for i in range(iters): # Check for slices\n+            x = ((2**33)*np.random.randn(10, 10)).astype(np.int32)\n+            x.flat[[2,6,9,15,21]] = 0\n+            x_view = x[:5,:5]\n+\n+            idxs_0, idxs_1 = np.nonzero(x_view)\n+            assert_equal(np.count_nonzero(x_view), len(idxs_0))\n+            nzs = 0\n+            for i,j in zip(idxs_0, idxs_1):\n+                if x_view[i,j] == 0:\n+                    nzs += 1\n+            assert_equal(nzs, 0)\n+\n+            y = np.asfortranarray(x)\n+            y_view = y[:5,:5]\n+\n+            idxs_0, idxs_1 = np.nonzero(y_view)\n+            assert_equal(np.count_nonzero(y_view), len(idxs_0))\n+            nzs = 0\n+            for i,j in zip(idxs_0, idxs_1):\n+                if y_view[i,j] == 0:\n+                    nzs += 1\n+            assert_equal(nzs, 0)\n+\n+\n+\n     def test_sparse(self):\n         # test special sparse condition boolean code path\n         for i in range(20):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22727,
        "body": "Add examples for getbufsize and setbufsize as requested by https://github.com/numpy/numpy/issues/22266.\r\n\r\nWritten as part of PyData 2022 sprint.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/_ufunc_config.py",
                "patch": "@@ -182,13 +182,25 @@ def geterr():\n @set_module('numpy')\n def setbufsize(size):\n     \"\"\"\n-    Set the size of the buffer used in ufuncs.\n+    Set the size of the buffer used in :ref:`ufuncs`.\n \n     Parameters\n     ----------\n     size : int\n-        Size of buffer.\n+        Size of buffer. Must be a multiple of 16.\n \n+    Returns\n+    -------\n+    setbufsize : int\n+        Original size of ufunc buffer in bytes.\n+\n+    Examples\n+    --------\n+    >>> original_size = np.getbufsize()\n+    >>> np.setbufsize(4096) == original_size\n+    True\n+    >>> np.getbufsize()\n+    4096\n     \"\"\"\n     if size > 10e6:\n         raise ValueError(\"Buffer size, %s, is too big.\" % size)\n@@ -207,13 +219,17 @@ def setbufsize(size):\n @set_module('numpy')\n def getbufsize():\n     \"\"\"\n-    Return the size of the buffer used in ufuncs.\n+    Return the size of the buffer used in :ref:`ufuncs`.\n \n     Returns\n     -------\n     getbufsize : int\n         Size of ufunc buffer in bytes.\n-\n+    \n+    Examples\n+    --------\n+    >>> np.getbufsize()  # doctest: +SKIP\n+    8192  # may vary\n     \"\"\"\n     return umath.geterrobj()[0]\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22166,
        "body": "NumPy already has SSE2 versions of `negative`.  Changes here convert that to universal intrinsics so other architectures can benefit.  Previously there was no unroll and SIMD was only used in contiguous cases.  We're now unrolling 4x/2x depending on whether destination is contiguous.  x86 doesn't perform as well for non-contiguous cases here, so we leave previous implementation / fall back to scalar.  Additionally, we've added SIMD versions for ints.\r\n\r\nApple M1: **up to 3.57x faster**\r\n```\r\n-     38.3\u00b10.04\u03bcs      35.4\u00b10.07\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'b')\r\n-      42.0\u00b10.4\u03bcs      37.3\u00b10.05\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'd')\r\n-      41.9\u00b10.4\u03bcs       37.1\u00b10.4\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 4, 'f')\r\n-        41.9\u00b11\u03bcs       36.9\u00b10.2\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'L')\r\n-      42.0\u00b10.7\u03bcs       36.9\u00b10.1\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 4, 'I')\r\n-     42.0\u00b10.03\u03bcs       36.7\u00b10.1\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'q')\r\n-     42.3\u00b10.09\u03bcs       36.9\u00b10.1\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 4, 'i')\r\n-     42.3\u00b10.04\u03bcs       36.8\u00b10.1\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'l')\r\n-      42.5\u00b10.4\u03bcs      36.6\u00b10.05\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'Q')\r\n-     53.5\u00b10.02\u03bcs       45.8\u00b10.1\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'l')\r\n-      53.6\u00b10.2\u03bcs      45.8\u00b10.08\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'Q')\r\n-      53.6\u00b10.1\u03bcs       45.7\u00b10.1\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'q')\r\n-      53.9\u00b10.4\u03bcs       45.9\u00b10.1\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'L')\r\n-      53.9\u00b10.2\u03bcs       45.8\u00b10.1\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'd')\r\n-      37.5\u00b10.3\u03bcs      31.7\u00b10.05\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 2, 'f')\r\n-      39.6\u00b10.2\u03bcs      33.0\u00b10.09\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 4, 'f')\r\n-      39.5\u00b10.3\u03bcs      32.6\u00b10.06\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'd')\r\n-      40.2\u00b10.4\u03bcs       33.1\u00b10.1\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 4, 'I')\r\n-     38.9\u00b10.04\u03bcs      31.8\u00b10.03\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 2, 'i')\r\n-      38.9\u00b10.1\u03bcs       31.5\u00b10.3\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 2, 'I')\r\n-     40.8\u00b10.01\u03bcs      33.1\u00b10.06\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 4, 'i')\r\n-      36.9\u00b10.2\u03bcs      29.6\u00b10.05\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'f')\r\n-      40.6\u00b10.4\u03bcs      32.6\u00b10.03\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'L')\r\n-     40.8\u00b10.07\u03bcs       32.6\u00b10.1\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'l')\r\n-      40.8\u00b10.1\u03bcs       32.6\u00b10.1\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'q')\r\n-     40.8\u00b10.05\u03bcs       32.6\u00b10.1\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'Q')\r\n-     38.9\u00b10.04\u03bcs      29.6\u00b10.03\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'I')\r\n-     38.9\u00b10.08\u03bcs      29.5\u00b10.04\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'i')\r\n-     36.8\u00b10.09\u03bcs      25.5\u00b10.06\u03bcs     0.69  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'd')\r\n-      32.3\u00b10.3\u03bcs       21.8\u00b10.2\u03bcs     0.68  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'f')\r\n-     38.9\u00b10.08\u03bcs      25.5\u00b10.05\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'L')\r\n-     39.0\u00b10.04\u03bcs      25.5\u00b10.04\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'Q')\r\n-     39.0\u00b10.08\u03bcs      25.5\u00b10.04\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'q')\r\n-     39.0\u00b10.05\u03bcs      25.5\u00b10.06\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'l')\r\n-      32.0\u00b10.2\u03bcs       20.7\u00b10.3\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'f')\r\n-     35.1\u00b10.01\u03bcs       21.5\u00b10.1\u03bcs     0.61  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 1, 'f')\r\n-     39.0\u00b10.02\u03bcs       21.8\u00b10.2\u03bcs     0.56  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'I')\r\n-     39.0\u00b10.02\u03bcs       21.7\u00b10.2\u03bcs     0.56  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'i')\r\n-     38.8\u00b10.02\u03bcs       21.5\u00b10.1\u03bcs     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 1, 'I')\r\n-     38.8\u00b10.02\u03bcs       21.2\u00b10.1\u03bcs     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 1, 'i')\r\n-      32.2\u00b10.3\u03bcs      17.5\u00b10.04\u03bcs     0.54  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'd')\r\n-     38.8\u00b10.02\u03bcs       20.3\u00b10.2\u03bcs     0.52  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'I')\r\n-      38.8\u00b10.2\u03bcs       20.3\u00b10.2\u03bcs     0.52  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'i')\r\n-     31.9\u00b10.02\u03bcs       13.3\u00b10.1\u03bcs     0.42  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'f')\r\n-     38.3\u00b10.04\u03bcs      13.2\u00b10.03\u03bcs     0.35  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'I')\r\n-     38.3\u00b10.05\u03bcs      13.2\u00b10.07\u03bcs     0.34  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'i')\r\n-     31.9\u00b10.01\u03bcs      9.01\u00b10.02\u03bcs     0.28  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'f')\r\n```\r\n\r\nApple M1 (Rosetta): **up to 1.75x faster**\r\n```\r\n-     37.4\u00b10.01\u03bcs      33.3\u00b10.03\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'f')\r\n-     41.8\u00b10.01\u03bcs      36.5\u00b10.08\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'i')\r\n-     41.8\u00b10.01\u03bcs       36.4\u00b10.1\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'I')\r\n-     54.0\u00b10.05\u03bcs       46.3\u00b10.6\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'd')\r\n-      54.4\u00b10.2\u03bcs      46.0\u00b10.08\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'l')\r\n-      54.4\u00b10.1\u03bcs       45.8\u00b10.1\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'Q')\r\n-      54.7\u00b10.1\u03bcs       46.0\u00b10.3\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'L')\r\n-      54.5\u00b10.2\u03bcs       45.8\u00b10.1\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'q')\r\n-     40.9\u00b10.05\u03bcs       33.5\u00b10.1\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'd')\r\n-     40.1\u00b10.08\u03bcs       31.6\u00b10.2\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'f')\r\n-     43.6\u00b10.04\u03bcs      33.4\u00b10.04\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'q')\r\n-     43.6\u00b10.03\u03bcs      33.4\u00b10.03\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'l')\r\n-      43.6\u00b10.1\u03bcs       33.4\u00b10.1\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'L')\r\n-      43.7\u00b10.1\u03bcs       33.4\u00b10.1\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'Q')\r\n-     38.1\u00b10.06\u03bcs       28.4\u00b10.1\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'd')\r\n-      42.7\u00b10.1\u03bcs      31.9\u00b10.01\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'i')\r\n-     42.8\u00b10.06\u03bcs      31.9\u00b10.02\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'I')\r\n-     41.7\u00b10.02\u03bcs       28.6\u00b10.1\u03bcs     0.69  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'L')\r\n-     41.7\u00b10.02\u03bcs      28.5\u00b10.08\u03bcs     0.68  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'q')\r\n-     41.7\u00b10.01\u03bcs       28.5\u00b10.1\u03bcs     0.68  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'Q')\r\n-     41.7\u00b10.07\u03bcs      28.5\u00b10.04\u03bcs     0.68  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'l')\r\n-     38.3\u00b10.01\u03bcs      24.5\u00b10.02\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'f')\r\n-     42.2\u00b10.01\u03bcs      24.5\u00b10.03\u03bcs     0.58  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'I')\r\n-     42.2\u00b10.01\u03bcs      24.5\u00b10.02\u03bcs     0.58  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'i')\r\n-     18.1\u00b10.06\u03bcs      10.4\u00b10.02\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'f')\r\n```\r\n\r\niMacPro (AVX512): About the same as before.  Some better, some worse.\r\n```\r\n+       149\u00b10.5\u03bcs          167\u00b17\u03bcs     1.12  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'Q')\r\n+        92.5\u00b12\u03bcs          103\u00b15\u03bcs     1.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'L')\r\n+      45.8\u00b10.4\u03bcs       50.5\u00b10.6\u03bcs     1.10  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 2, 'h')\r\n+      43.2\u00b10.3\u03bcs         47.1\u00b11\u03bcs     1.09  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 4, 'h')\r\n+       148\u00b10.7\u03bcs          161\u00b11\u03bcs     1.09  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'q')\r\n+      39.3\u00b10.3\u03bcs       42.6\u00b10.6\u03bcs     1.09  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 1, 'H')\r\n+         150\u00b12\u03bcs          162\u00b16\u03bcs     1.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'L')\r\n+     7.71\u00b10.05\u03bcs       8.31\u00b10.1\u03bcs     1.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'i')\r\n+      91.8\u00b10.5\u03bcs         98.6\u00b15\u03bcs     1.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 4, 'I')\r\n+      39.7\u00b10.4\u03bcs         42.5\u00b11\u03bcs     1.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 1, 'h')\r\n+        47.3\u00b12\u03bcs         50.6\u00b11\u03bcs     1.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 2, 'H')\r\n+      62.5\u00b10.3\u03bcs         66.7\u00b11\u03bcs     1.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'i')\r\n+        78.6\u00b11\u03bcs         83.7\u00b13\u03bcs     1.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'f')\r\n+     2.63\u00b10.03\u03bcs      2.79\u00b10.02\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'B')\r\n+     4.23\u00b10.02\u03bcs      4.49\u00b10.05\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'H')\r\n+     4.24\u00b10.03\u03bcs      4.48\u00b10.06\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'h')\r\n+       119\u00b10.7\u03bcs          126\u00b12\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 4, 'I')\r\n+       178\u00b10.8\u03bcs          187\u00b12\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 4, 'l')\r\n-      45.9\u00b10.5\u03bcs         43.4\u00b11\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'f')\r\n-        85.3\u00b13\u03bcs         80.4\u00b12\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 4, 'I')\r\n-         100\u00b14\u03bcs       93.7\u00b10.3\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 2, 'd')\r\n-       129\u00b10.4\u03bcs          118\u00b11\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 2, 'Q')\r\n-        64.9\u00b11\u03bcs         59.3\u00b12\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'd')\r\n-        84.3\u00b15\u03bcs       76.8\u00b10.4\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 4, 1, 'i')\r\n-      48.2\u00b10.3\u03bcs       37.9\u00b10.3\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'I')\r\n-        48.7\u00b11\u03bcs         37.3\u00b11\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'f')\r\n-     10.9\u00b10.05\u03bcs       8.18\u00b10.2\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 1, 1, 'f')\r\n-      46.0\u00b10.4\u03bcs       33.8\u00b10.5\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'negative'>, 2, 1, 'i')\r\n```",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -216,6 +216,7 @@ numpy/core/src/_simd/_simd.dispatch.c\n numpy/core/src/_simd/_simd_data.inc\n numpy/core/src/_simd/_simd_inc.h\n # umath module\n+numpy/core/src/umath/loops_unary.dispatch.c\n numpy/core/src/umath/loops_unary_fp.dispatch.c\n numpy/core/src/umath/loops_arithm_fp.dispatch.c\n numpy/core/src/umath/loops_arithmetic.dispatch.c"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -426,7 +426,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.negative'),\n           'PyUFunc_NegativeTypeResolver',\n-          TD(ints+flts+timedeltaonly, simd=[('avx2', ints)]),\n+          TD(ints+flts+timedeltaonly, dispatch=[('loops_unary', ints+'fdg')]),\n           TD(cmplx, f='neg'),\n           TD(O, f='PyNumber_Negative'),\n           ),"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -747,6 +747,7 @@ src_umath = [\n   src_file.process('src/umath/loops_modulo.dispatch.c.src'),\n   src_file.process('src/umath/loops_trigonometric.dispatch.c.src'),\n   src_file.process('src/umath/loops_umath_fp.dispatch.c.src'),\n+  src_file.process('src/umath/loops_unary.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary_fp.dispatch.c.src'),\n   src_file.process('src/umath/matmul.c.src'),\n   src_file.process('src/umath/matmul.h.src'),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1005,6 +1005,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops.h.src'),\n             join('src', 'umath', 'loops_utils.h.src'),\n             join('src', 'umath', 'loops.c.src'),\n+            join('src', 'umath', 'loops_unary.dispatch.c.src'),\n             join('src', 'umath', 'loops_unary_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -599,14 +599,6 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n }\n #endif\n \n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_negative@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = -in);\n-}\n-#endif\n-\n #if @CHK@\n NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n @TYPE@_logical_not@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n@@ -1545,17 +1537,6 @@ NPY_NO_EXPORT void\n     }\n }\n \n-NPY_NO_EXPORT void\n-@TYPE@_negative(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    if (!run_unary_simd_negative_@TYPE@(args, dimensions, steps)) {\n-        UNARY_LOOP {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            *((@type@ *)op1) = -in1;\n-        }\n-    }\n-}\n-\n NPY_NO_EXPORT void\n @TYPE@_positive(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -139,9 +139,6 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @S@@TYPE@_conjugate@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n-NPY_NO_EXPORT void\n-@S@@TYPE@_negative@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n NPY_NO_EXPORT void\n @S@@TYPE@_logical_not@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n@@ -206,6 +203,23 @@ NPY_NO_EXPORT void\n \n /**end repeat**/\n \n+ \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_unary.dispatch.h\"\n+#endif\n+/**begin repeat\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n+ *         BYTE,  SHORT,  INT,  LONG,  LONGLONG#\n+ */\n+/**begin repeat1\n+ * #kind = negative#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+   (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data)))\n+/**end repeat1**/\n+/**end repeat**/\n+\n+\n /*\n  *****************************************************************************\n  **                             FLOAT LOOPS                                 **\n@@ -225,6 +239,20 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n /**end repeat1**/\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_unary.dispatch.h\"\n+#endif\n+/**begin repeat\n+ *  #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n+ */\n+/**begin repeat1\n+ * #kind = negative#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+   (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data)))\n+/**end repeat1**/\n+/**end repeat**/\n+\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_arithm_fp.dispatch.h\"\n #endif\n@@ -362,6 +390,7 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@, (\n  *  #TYPE = HALF, FLOAT, DOUBLE, LONGDOUBLE#\n  *  #c = f, f, , l#\n  *  #C = F, F, , L#\n+ *  #half = 1, 0, 0, 0#\n  */\n \n /**begin repeat1\n@@ -440,8 +469,10 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @TYPE@_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n+#if @half@\n NPY_NO_EXPORT void\n @TYPE@_negative(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+#endif\n \n NPY_NO_EXPORT void\n @TYPE@_positive(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));"
            },
            {
                "filename": "numpy/core/src/umath/loops_unary.dispatch.c.src",
                "patch": "@@ -0,0 +1,364 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** neon asimd\n+ ** sse2 avx2 avx512_skx\n+ ** vsx2\n+ ** vx vxe\n+ **/\n+\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"numpy/npy_math.h\"\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"lowlevel_strided_loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+/*******************************************************************************\n+ ** Scalar ops\n+ ******************************************************************************/\n+#define scalar_negative(X) (-X)\n+\n+/*******************************************************************************\n+ ** extra SIMD intrinsics\n+ ******************************************************************************/\n+\n+#if NPY_SIMD\n+\n+/**begin repeat\n+ * #sfx  = s8, u8, s16, u16, s32, u32, s64, u64#\n+ * #ssfx =  8,  8,  16,  16,  32,  32,  64,  64#\n+ */\n+static NPY_INLINE npyv_@sfx@\n+npyv_negative_@sfx@(npyv_@sfx@ v)\n+{\n+#if defined(NPY_HAVE_NEON) && (defined(__aarch64__) || @ssfx@ < 64)\n+    return npyv_reinterpret_@sfx@_s@ssfx@(vnegq_s@ssfx@(npyv_reinterpret_s@ssfx@_@sfx@(v)));\n+#else\n+    // (x ^ -1) + 1\n+    const npyv_@sfx@ m1 = npyv_setall_@sfx@((npyv_lanetype_@sfx@)-1);\n+    return npyv_sub_@sfx@(npyv_xor_@sfx@(v, m1), m1);\n+#endif\n+}\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #sfx  = f32, f64#\n+ * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #fd = f, #\n+ */\n+#if @VCHK@\n+static NPY_INLINE npyv_@sfx@\n+npyv_negative_@sfx@(npyv_@sfx@ v)\n+{\n+#if defined(NPY_HAVE_NEON)\n+    return vnegq_@sfx@(v);\n+#else\n+    // (v ^ signmask)\n+    const npyv_@sfx@ signmask = npyv_setall_@sfx@(-0.@fd@);\n+    return npyv_xor_@sfx@(v, signmask);\n+#endif\n+}\n+#endif // @VCHK@\n+/**end repeat**/\n+\n+#endif // NPY_SIMD\n+\n+/********************************************************************************\n+ ** Defining the SIMD kernels\n+ ********************************************************************************/\n+/**begin repeat\n+ * #sfx = s8, u8, s16, u16, s32, u32, s64, u64, f32, f64#\n+ * #simd_chk = NPY_SIMD*8, NPY_SIMD_F32, NPY_SIMD_F64#\n+ * #is_fp = 0*8, 1*2#\n+ * #supports_ncontig = 0*4,1*6#\n+ */\n+/**begin repeat1\n+ * #kind   = negative#\n+ * #intrin = negative#\n+ * #unroll = 4#\n+ */\n+#if @simd_chk@\n+#if @unroll@ < 1\n+#error \"Unroll must be at least 1\"\n+#elif NPY_SIMD != 128 && @unroll@ > 2\n+// Avoid memory bandwidth bottleneck for larger SIMD\n+#define UNROLL 2\n+#else\n+#define UNROLL @unroll@\n+#endif\n+// contiguous inputs and output.\n+static NPY_INLINE void\n+simd_unary_cc_@intrin@_@sfx@(const npyv_lanetype_@sfx@ *ip,\n+                             npyv_lanetype_@sfx@ *op,\n+                             npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * UNROLL;\n+\n+    // unrolled vector loop\n+    for (; len >= wstep; len -= wstep, ip += wstep, op += wstep) {\n+    /**begin repeat2\n+     * #U = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+     */\n+    #if UNROLL > @U@\n+        npyv_@sfx@ v_@U@ = npyv_load_@sfx@(ip + @U@ * vstep);\n+        npyv_@sfx@ r_@U@ = npyv_@intrin@_@sfx@(v_@U@);\n+        npyv_store_@sfx@(op + @U@ * vstep, r_@U@);\n+    #endif\n+    /**end repeat2**/\n+    }\n+    // single vector loop\n+    for (; len >= vstep; len -= vstep, ip += vstep, op +=vstep) {\n+        npyv_@sfx@ v = npyv_load_@sfx@(ip);\n+        npyv_@sfx@ r = npyv_@intrin@_@sfx@(v);\n+        npyv_store_@sfx@(op, r);\n+    }\n+    // scalar finish up any remaining iterations\n+    for (; len > 0; --len, ++ip, ++op) {\n+        *op = scalar_@intrin@(*ip);\n+    }\n+}\n+\n+#if @supports_ncontig@\n+// contiguous input, non-contiguous output\n+static NPY_INLINE void\n+simd_unary_cn_@intrin@_@sfx@(const npyv_lanetype_@sfx@ *ip,\n+                             npyv_lanetype_@sfx@ *op, npy_intp ostride,\n+                             npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * UNROLL;\n+\n+    // unrolled vector loop\n+    for (; len >= wstep; len -= wstep, ip += wstep, op += ostride*wstep) {\n+    /**begin repeat2\n+     * #U = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+     */\n+    #if UNROLL > @U@\n+        npyv_@sfx@ v_@U@ = npyv_load_@sfx@(ip + @U@ * vstep);\n+        npyv_@sfx@ r_@U@ = npyv_@intrin@_@sfx@(v_@U@);\n+        npyv_storen_@sfx@(op + @U@ * vstep * ostride, ostride, r_@U@);\n+    #endif\n+    /**end repeat2**/\n+    }\n+    // single vector loop\n+    for (; len >= vstep; len -= vstep, ip += vstep, op += ostride*vstep) {\n+        npyv_@sfx@ v = npyv_load_@sfx@(ip);\n+        npyv_@sfx@ r = npyv_@intrin@_@sfx@(v);\n+        npyv_storen_@sfx@(op, ostride, r);\n+    }\n+    // scalar finish up any remaining iterations\n+    for (; len > 0; --len, ++ip, op += ostride) {\n+        *op = scalar_@intrin@(*ip);\n+    }\n+}\n+// non-contiguous input, contiguous output\n+static NPY_INLINE void\n+simd_unary_nc_@intrin@_@sfx@(const npyv_lanetype_@sfx@ *ip, npy_intp istride,\n+                             npyv_lanetype_@sfx@ *op,\n+                             npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * UNROLL;\n+\n+    // unrolled vector loop\n+    for (; len >= wstep; len -= wstep, ip += istride*wstep, op += wstep) {\n+    /**begin repeat2\n+     * #U = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+     */\n+    #if UNROLL > @U@\n+        npyv_@sfx@ v_@U@ = npyv_loadn_@sfx@(ip + @U@ * vstep * istride, istride);\n+        npyv_@sfx@ r_@U@ = npyv_@intrin@_@sfx@(v_@U@);\n+        npyv_store_@sfx@(op + @U@ * vstep, r_@U@);\n+    #endif\n+    /**end repeat2**/\n+    }\n+    // single vector loop\n+    for (; len >= vstep; len -= vstep, ip += istride*vstep, op += vstep) {\n+        npyv_@sfx@ v = npyv_loadn_@sfx@(ip, istride);\n+        npyv_@sfx@ r = npyv_@intrin@_@sfx@(v);\n+        npyv_store_@sfx@(op, r);\n+    }\n+    // scalar finish up any remaining iterations\n+    for (; len > 0; --len, ip += istride, ++op) {\n+        *op = scalar_@intrin@(*ip);\n+    }\n+}\n+// non-contiguous input and output\n+// limit unroll to 2x\n+#if UNROLL > 2\n+#undef UNROLL\n+#define UNROLL 2\n+#endif\n+static NPY_INLINE void\n+simd_unary_nn_@intrin@_@sfx@(const npyv_lanetype_@sfx@ *ip, npy_intp istride,\n+                             npyv_lanetype_@sfx@ *op, npy_intp ostride,\n+                             npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep * UNROLL;\n+\n+    // unrolled vector loop\n+    for (; len >= wstep; len -= wstep, ip += istride*wstep, op += ostride*wstep) {\n+    /**begin repeat2\n+     * #U = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+     */\n+    #if UNROLL > @U@\n+        npyv_@sfx@ v_@U@ = npyv_loadn_@sfx@(ip + @U@ * vstep * istride, istride);\n+        npyv_@sfx@ r_@U@ = npyv_@intrin@_@sfx@(v_@U@);\n+        npyv_storen_@sfx@(op + @U@ * vstep * ostride, ostride, r_@U@);\n+    #endif\n+    /**end repeat2**/\n+    }\n+    // single vector loop\n+    for (; len >= vstep; len -= vstep, ip += istride*vstep, op += ostride*vstep) {\n+        npyv_@sfx@ v = npyv_loadn_@sfx@(ip, istride);\n+        npyv_@sfx@ r = npyv_@intrin@_@sfx@(v);\n+        npyv_storen_@sfx@(op, ostride, r);\n+    }\n+    // scalar finish up any remaining iterations\n+    for (; len > 0; --len, ip += istride, op += ostride) {\n+        *op = scalar_@intrin@(*ip);\n+    }\n+}\n+#endif // @supports_ncontig@\n+#undef UNROLL\n+#endif // @simd_chk@\n+/*end repeat1**/\n+/**end repeat**/\n+\n+/********************************************************************************\n+ ** Defining ufunc inner functions\n+ ********************************************************************************/\n+/**begin repeat\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n+ *         BYTE,  SHORT,  INT,  LONG,  LONGLONG,\n+ *         FLOAT, DOUBLE, LONGDOUBLE#\n+ *\n+ * #BTYPE = BYTE, SHORT, INT,  LONG, LONGLONG,\n+ *          BYTE, SHORT, INT, LONG, LONGLONG,\n+ *          FLOAT, DOUBLE, LONGDOUBLE#\n+ * #type = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong,\n+ *         npy_byte, npy_short, npy_int, npy_long, npy_longlong,\n+ *         npy_float, npy_double, npy_longdouble#\n+ *\n+ * #is_fp = 0*10, 1*3#\n+ * #is_unsigned = 1*5, 0*5, 0*3#\n+ * #supports_ncontig = 0*2, 1*3, 0*2, 1*3, 1*3#\n+ */\n+#undef TO_SIMD_SFX\n+#if 0\n+/**begin repeat1\n+ * #len = 8, 16, 32, 64#\n+ */\n+#elif NPY_SIMD && NPY_BITSOF_@BTYPE@ == @len@\n+    #if @is_fp@\n+        #define TO_SIMD_SFX(X) X##_f@len@\n+        #if NPY_BITSOF_@BTYPE@ == 32 && !NPY_SIMD_F32\n+            #undef TO_SIMD_SFX\n+        #endif\n+        #if NPY_BITSOF_@BTYPE@ == 64 && !NPY_SIMD_F64\n+            #undef TO_SIMD_SFX\n+        #endif\n+    #elif @is_unsigned@\n+        #define TO_SIMD_SFX(X) X##_u@len@\n+    #else\n+        #define TO_SIMD_SFX(X) X##_s@len@\n+    #endif\n+/**end repeat1**/\n+#endif\n+\n+/**begin repeat1\n+ * #kind = negative#\n+ * #intrin = negative#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    char *ip = args[0], *op = args[1];\n+    npy_intp istep = steps[0], ostep = steps[1],\n+             len = dimensions[0];\n+#ifdef TO_SIMD_SFX\n+    #undef STYPE\n+    #define STYPE TO_SIMD_SFX(npyv_lanetype)\n+    if (!is_mem_overlap(ip, istep, op, ostep, len)) {\n+        if (IS_UNARY_CONT(@type@, @type@)) {\n+            // no overlap and operands are contiguous\n+            TO_SIMD_SFX(simd_unary_cc_@intrin@)(\n+                (STYPE*)ip, (STYPE*)op, len\n+            );\n+            goto clear;\n+        }\n+    #if @supports_ncontig@\n+        const npy_intp istride = istep / sizeof(STYPE);\n+        const npy_intp ostride = ostep / sizeof(STYPE);\n+        if (TO_SIMD_SFX(npyv_loadable_stride)(istride) &&\n+            TO_SIMD_SFX(npyv_storable_stride)(ostride))\n+        {\n+            if (istride == 1 && ostride != 1) {\n+                // contiguous input, non-contiguous output\n+                TO_SIMD_SFX(simd_unary_cn_@intrin@)(\n+                    (STYPE*)ip, (STYPE*)op, ostride, len\n+                );\n+                goto clear;\n+            }\n+            else if (istride != 1 && ostride == 1) {\n+                // non-contiguous input, contiguous output\n+                TO_SIMD_SFX(simd_unary_nc_@intrin@)(\n+                    (STYPE*)ip, istride, (STYPE*)op, len\n+                );\n+                goto clear;\n+            }\n+        // SSE2 does better with unrolled scalar for heavy non-contiguous\n+        #if !defined(NPY_HAVE_SSE2)\n+            else if (istride != 1 && ostride != 1) {\n+                // non-contiguous input and output\n+                TO_SIMD_SFX(simd_unary_nn_@intrin@)(\n+                    (STYPE*)ip, istride, (STYPE*)op, ostride, len\n+                );\n+                goto clear;\n+            }\n+        #endif\n+        }\n+    #endif // @supports_ncontig@\n+    }\n+#endif // TO_SIMD_SFX\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    /*\n+     * scalar unrolls\n+     * 8x unroll performed best on\n+     *  - Apple M1 Native / arm64\n+     *  - Apple M1 Rosetta / SSE42\n+     *  - iMacPro / AVX512\n+     */\n+    #define UNROLL 8\n+    for (; len >= UNROLL; len -= UNROLL, ip += istep*UNROLL, op += ostep*UNROLL) {\n+    /**begin repeat2\n+     * #U = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+     */\n+    #if UNROLL > @U@\n+        const @type@ in_@U@ = *((const @type@ *)(ip + @U@ * istep));\n+        *((@type@ *)(op + @U@ * ostep)) = scalar_@intrin@(in_@U@);\n+    #endif\n+    /**end repeat2**/\n+    }\n+#endif // NPY_DISABLE_OPTIMIZATION\n+    for (; len > 0; --len, ip += istep, op += ostep) {\n+        *((@type@ *)op) = scalar_@intrin@(*(const @type@ *)ip);\n+    }\n+#ifdef TO_SIMD_SFX\n+clear:\n+    npyv_cleanup();\n+#endif\n+#if @is_fp@\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+#endif\n+}\n+/**end repeat**/\n+\n+#undef NEGATIVE_CONTIG_ONLY"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -129,39 +129,9 @@ run_@func@_avx512_skx_@TYPE@(char **args, npy_intp const *dimensions, npy_intp c\n  *  #vector = 1, 1, 0#\n  *  #VECTOR = NPY_SIMD, NPY_SIMD_F64, 0 #\n  */\n-\n-/**begin repeat1\n- * #func = negative#\n- * #check = IS_BLOCKABLE_UNARY#\n- * #name = unary#\n- */\n-\n-#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS\n-\n-/* prototypes */\n-static void\n-sse2_@func@_@TYPE@(@type@ *, @type@ *, const npy_intp n);\n-\n-#endif\n-\n-static inline int\n-run_@name@_simd_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS\n-    if (@check@(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0]);\n-        return 1;\n-    }\n-#endif\n-    return 0;\n-}\n-\n-/**end repeat1**/\n-\n /**begin repeat1\n  * #kind = isnan, isfinite, isinf, signbit#\n  */\n-\n #if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS\n \n static void\n@@ -181,9 +151,7 @@ run_@kind@_simd_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *\n #endif\n     return 0;\n }\n-\n /**end repeat1**/\n-\n /**end repeat**/\n \n /*\n@@ -426,41 +394,6 @@ sse2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n)\n }\n \n /**end repeat1**/\n-\n-static void\n-sse2_negative_@TYPE@(@type@ * op, @type@ * ip, const npy_intp n)\n-{\n-    /*\n-     * get 0x7FFFFFFF mask (everything but signbit set)\n-     * float & ~mask will remove the sign, float ^ mask flips the sign\n-     * this is equivalent to how the compiler implements fabs on amd64\n-     */\n-    const @vtype@ mask = @vpre@_set1_@vsuf@(-0.@c@);\n-\n-    /* align output to VECTOR_SIZE_BYTES bytes */\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES) {\n-        op[i] = -ip[i];\n-    }\n-    assert((npy_uintp)n < (VECTOR_SIZE_BYTES / sizeof(@type@)) ||\n-           npy_is_aligned(&op[i], VECTOR_SIZE_BYTES));\n-    if (npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES)) {\n-        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n-            @vtype@ a = @vpre@_load_@vsuf@(&ip[i]);\n-            @vpre@_store_@vsuf@(&op[i], @vpre@_xor_@vsuf@(mask, a));\n-        }\n-    }\n-    else {\n-        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n-            @vtype@ a = @vpre@_loadu_@vsuf@(&ip[i]);\n-            @vpre@_store_@vsuf@(&op[i], @vpre@_xor_@vsuf@(mask, a));\n-        }\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = -ip[i];\n-    }\n-}\n-/**end repeat1**/\n-\n /**end repeat**/\n \n /* bunch of helper functions used in ISA_exp/log_FLOAT*/"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -2608,6 +2608,35 @@ def test_lower_align(self):\n         np.abs(d, out=d)\n         np.abs(np.ones_like(d), out=d)\n \n+    @pytest.mark.parametrize(\"dtype\", ['d', 'f', 'int32', 'int64'])\n+    @pytest.mark.parametrize(\"big\", [True, False])\n+    def test_noncontiguous(self, dtype, big):\n+        data = np.array([-1.0, 1.0, -0.0, 0.0, 2.2251e-308, -2.5, 2.5, -6,\n+                            6, -2.2251e-308, -8, 10], dtype=dtype)\n+        expect = np.array([1.0, -1.0, 0.0, -0.0, -2.2251e-308, 2.5, -2.5, 6,\n+                            -6, 2.2251e-308, 8, -10], dtype=dtype)\n+        if big:\n+            data = np.repeat(data, 10)\n+            expect = np.repeat(expect, 10)\n+        out = np.ndarray(data.shape, dtype=dtype)\n+        ncontig_in = data[1::2]\n+        ncontig_out = out[1::2]\n+        contig_in = np.array(ncontig_in)\n+        # contig in, contig out\n+        assert_array_equal(np.negative(contig_in), expect[1::2])\n+        # contig in, ncontig out\n+        assert_array_equal(np.negative(contig_in, out=ncontig_out),\n+                                expect[1::2])\n+        # ncontig in, contig out\n+        assert_array_equal(np.negative(ncontig_in), expect[1::2])\n+        # ncontig in, ncontig out\n+        assert_array_equal(np.negative(ncontig_in, out=ncontig_out),\n+                                expect[1::2])\n+        # contig in, contig out, nd stride\n+        data_split = np.array(np.array_split(data, 2))\n+        expect_split = np.array(np.array_split(expect, 2))\n+        assert_equal(np.negative(data_split), expect_split)\n+\n \n class TestPositive:\n     def test_valid(self):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21895,
        "body": "This is using a branch of Pyodide that finishes support for out of tree builds. This is blocked on Pyodide:\r\n- [x] Merge changes from my branch into main Pyodide\r\n- [x] Release Pyodide v0.21.0a3 (hopefully) with out of tree build support\r\n\r\nOther than that, I think it is nearly ready.",
        "changed_files": [
            {
                "filename": ".github/workflows/emscripten.yml",
                "patch": "@@ -0,0 +1,75 @@\n+# To enable this workflow on a fork, comment out:\n+#\n+# if: github.repository == 'numpy/numpy'\n+name: Test Emscripten/Pyodide build\n+\n+on:\n+  pull_request:\n+    branches:\n+      - main\n+      - maintenance/**\n+\n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n+  cancel-in-progress: true\n+\n+permissions:\n+  contents: read # to fetch code (actions/checkout)\n+\n+jobs:\n+  build-wasm-emscripten:\n+    runs-on: ubuntu-latest\n+    if: github.repository == 'numpy/numpy'\n+    env:\n+      PYODIDE_VERSION: 0.22.0a3\n+      # PYTHON_VERSION and EMSCRIPTEN_VERSION are determined by PYODIDE_VERSION.\n+      # The appropriate versions can be found in the Pyodide repodata.json\n+      # \"info\" field, or in Makefile.envs:\n+      # https://github.com/pyodide/pyodide/blob/main/Makefile.envs#L2\n+      PYTHON_VERSION: 3.10.2\n+      EMSCRIPTEN_VERSION: 3.1.24\n+      NODE_VERSION: 18\n+    steps:\n+      - name: Checkout numpy\n+        uses: actions/checkout@v3\n+        with:\n+          submodules: true\n+          # versioneer.py requires the latest tag to be reachable. Here we\n+          # fetch the complete history to get access to the tags.\n+          # A shallow clone can work when the following issue is resolved:\n+          # https://github.com/actions/checkout/issues/338\n+          fetch-depth: 0\n+\n+      - name: set up python\n+        id: setup-python\n+        uses: actions/setup-python@v4\n+        with:\n+          python-version: ${{ env.PYTHON_VERSION }}\n+\n+      - uses: mymindstorm/setup-emsdk@v11\n+        with:\n+          version: ${{ env.EMSCRIPTEN_VERSION }}\n+          actions-cache-folder: emsdk-cache\n+\n+      - name: Install pyodide-build\n+        run: pip install pyodide-build==$PYODIDE_VERSION\n+\n+      - name: Build\n+        run: CFLAGS=-g2 LDFLAGS=-g2 pyodide build\n+\n+      - name: set up node\n+        uses: actions/setup-node@v3\n+        with:\n+          node-version: ${{ env.NODE_VERSION }}\n+\n+      - name: Set up Pyodide virtual environment\n+        run: |\n+          pyodide venv .venv-pyodide\n+          source .venv-pyodide/bin/activate\n+          pip install dist/*.whl\n+          pip install -r test_requirements.txt\n+      - name: Test\n+        run: |\n+          source .venv-pyodide/bin/activate\n+          cd ..\n+          python numpy/runtests.py -n -vv"
            },
            {
                "filename": "numpy/core/feature_detection_cmath.h",
                "patch": "@@ -10,6 +10,18 @@\n #define cldouble complex long double\n #endif\n \n+// musl libc defines the following macros which breaks the declarations.\n+// See https://git.musl-libc.org/cgit/musl/tree/include/complex.h#n108\n+#undef crealf\n+#undef cimagf\n+#undef conjf\n+#undef creal\n+#undef cimag\n+#undef conj\n+#undef creall\n+#undef cimagl\n+#undef cconjl\n+\n cfloat csinf(cfloat);\n cfloat ccosf(cfloat);\n cfloat ctanf(cfloat);"
            },
            {
                "filename": "numpy/core/tests/test_casting_floatingpoint_errors.py",
                "patch": "@@ -1,6 +1,6 @@\n import pytest\n from pytest import param\n-\n+from numpy.testing import IS_WASM\n import numpy as np\n \n \n@@ -136,6 +136,7 @@ def flat_assignment():\n \n     yield flat_assignment\n \n+@pytest.mark.skipif(IS_WASM, reason=\"no wasm fp exception support\")\n @pytest.mark.parametrize([\"value\", \"dtype\"], values_and_dtypes())\n @pytest.mark.filterwarnings(\"ignore::numpy.ComplexWarning\")\n def test_floatingpoint_errors_casting(dtype, value):"
            },
            {
                "filename": "numpy/core/tests/test_cython.py",
                "patch": "@@ -5,6 +5,7 @@\n import pytest\n \n import numpy as np\n+from numpy.testing import IS_WASM\n \n # This import is copied from random.tests.test_extending\n try:\n@@ -30,6 +31,8 @@\n @pytest.fixture\n def install_temp(request, tmp_path):\n     # Based in part on test_cython from random.tests.test_extending\n+    if IS_WASM:\n+        pytest.skip(\"No subprocess\")\n \n     here = os.path.dirname(__file__)\n     ext_dir = os.path.join(here, \"examples\", \"cython\")"
            },
            {
                "filename": "numpy/core/tests/test_datetime.py",
                "patch": "@@ -4,6 +4,7 @@\n import datetime\n import pytest\n from numpy.testing import (\n+    IS_WASM,\n     assert_, assert_equal, assert_raises, assert_warns, suppress_warnings,\n     assert_raises_regex, assert_array_equal,\n     )\n@@ -1294,6 +1295,7 @@ def check(a, b, res):\n     def test_timedelta_floor_divide(self, op1, op2, exp):\n         assert_equal(op1 // op2, exp)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"op1, op2\", [\n         # div by 0\n         (np.timedelta64(10, 'us'),\n@@ -1368,6 +1370,7 @@ def test_timedelta_divmod(self, op1, op2):\n         expected = (op1 // op2, op1 % op2)\n         assert_equal(divmod(op1, op2), expected)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"does not work in wasm\")\n     @pytest.mark.parametrize(\"op1, op2\", [\n         # reuse cases from floordiv\n         # div by 0\n@@ -1993,6 +1996,7 @@ def test_timedelta_modulus_error(self, val1, val2):\n         with assert_raises_regex(TypeError, \"common metadata divisor\"):\n             val1 % val2\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_timedelta_modulus_div_by_zero(self):\n         with assert_warns(RuntimeWarning):\n             actual = np.timedelta64(10, 's') % np.timedelta64(0, 's')"
            },
            {
                "filename": "numpy/core/tests/test_errstate.py",
                "patch": "@@ -2,7 +2,7 @@\n import sysconfig\n \n import numpy as np\n-from numpy.testing import assert_, assert_raises\n+from numpy.testing import assert_, assert_raises, IS_WASM\n \n # The floating point emulation on ARM EABI systems lacking a hardware FPU is\n # known to be buggy. This is an attempt to identify these hosts. It may not\n@@ -12,6 +12,7 @@\n arm_softfloat = False if hosttype is None else hosttype.endswith('gnueabi')\n \n class TestErrstate:\n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.skipif(arm_softfloat,\n                         reason='platform/cpu issue with FPU (gh-413,-15562)')\n     def test_invalid(self):\n@@ -24,6 +25,7 @@ def test_invalid(self):\n             with assert_raises(FloatingPointError):\n                 np.sqrt(a)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.skipif(arm_softfloat,\n                         reason='platform/cpu issue with FPU (gh-15562)')\n     def test_divide(self):"
            },
            {
                "filename": "numpy/core/tests/test_half.py",
                "patch": "@@ -3,7 +3,7 @@\n \n import numpy as np\n from numpy import uint16, float16, float32, float64\n-from numpy.testing import assert_, assert_equal, _OLD_PROMOTION\n+from numpy.testing import assert_, assert_equal, _OLD_PROMOTION, IS_WASM\n \n \n def assert_raises_fpe(strmatch, callable, *args, **kwargs):\n@@ -483,6 +483,8 @@ def test_half_coercion(self, weak_promotion):\n \n     @pytest.mark.skipif(platform.machine() == \"armv5tel\",\n                         reason=\"See gh-413.\")\n+    @pytest.mark.skipif(IS_WASM,\n+                        reason=\"fp exceptions don't work in wasm.\")\n     def test_half_fpe(self):\n         with np.errstate(all='raise'):\n             sx16 = np.array((1e-4,), dtype=float16)"
            },
            {
                "filename": "numpy/core/tests/test_indexing.py",
                "patch": "@@ -10,7 +10,7 @@\n from itertools import product\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_raises_regex,\n-    assert_array_equal, assert_warns, HAS_REFCOUNT,\n+    assert_array_equal, assert_warns, HAS_REFCOUNT, IS_WASM\n     )\n \n \n@@ -563,6 +563,7 @@ def test_too_many_advanced_indices(self, index, num, original_ndim):\n         with pytest.raises(IndexError):\n             arr[(index,) * num] = 1.\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"no threading\")\n     def test_structured_advanced_indexing(self):\n         # Test that copyswap(n) used by integer array indexing is threadsafe\n         # for structured datatypes, see gh-15387. This test can behave randomly."
            },
            {
                "filename": "numpy/core/tests/test_limited_api.py",
                "patch": "@@ -5,7 +5,10 @@\n import sysconfig\n import pytest\n \n+from numpy.testing import IS_WASM\n \n+\n+@pytest.mark.skipif(IS_WASM, reason=\"Can't start subprocess\")\n @pytest.mark.xfail(\n     sysconfig.get_config_var(\"Py_DEBUG\"),\n     reason=("
            },
            {
                "filename": "numpy/core/tests/test_mem_policy.py",
                "patch": "@@ -5,7 +5,7 @@\n import numpy as np\n import threading\n import warnings\n-from numpy.testing import extbuild, assert_warns\n+from numpy.testing import extbuild, assert_warns, IS_WASM\n import sys\n \n \n@@ -18,6 +18,8 @@ def get_module(tmp_path):\n     \"\"\"\n     if sys.platform.startswith('cygwin'):\n         pytest.skip('link fails on cygwin')\n+    if IS_WASM:\n+        pytest.skip(\"Can't build module inside Wasm\")\n     functions = [\n         (\"get_default_policy\", \"METH_NOARGS\", \"\"\"\n              Py_INCREF(PyDataMem_DefaultHandler);"
            },
            {
                "filename": "numpy/core/tests/test_nditer.py",
                "patch": "@@ -9,7 +9,7 @@\n from numpy import array, arange, nditer, all\n from numpy.testing import (\n     assert_, assert_equal, assert_array_equal, assert_raises,\n-    HAS_REFCOUNT, suppress_warnings, break_cycles\n+    IS_WASM, HAS_REFCOUNT, suppress_warnings, break_cycles\n     )\n \n \n@@ -2025,6 +2025,7 @@ def test_buffered_cast_error_paths():\n             buf = next(it)\n             buf[...] = \"a\"  # cannot be converted to int.\n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n @pytest.mark.skipif(not HAS_REFCOUNT, reason=\"PyPy seems to not hit this.\")\n def test_buffered_cast_error_paths_unraisable():\n     # The following gives an unraisable error. Pytest sometimes captures that"
            },
            {
                "filename": "numpy/core/tests/test_nep50_promotions.py",
                "patch": "@@ -9,6 +9,7 @@\n import numpy as np\n \n import pytest\n+from numpy.testing import IS_WASM\n \n \n @pytest.fixture(scope=\"module\", autouse=True)\n@@ -19,6 +20,7 @@ def _weak_promotion_enabled():\n     np._set_promotion_state(state)\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"wasm doesn't have support for fp errors\")\n def test_nep50_examples():\n     with pytest.warns(UserWarning, match=\"result dtype changed\"):\n         res = np.uint8(1) + 2"
            },
            {
                "filename": "numpy/core/tests/test_numeric.py",
                "patch": "@@ -12,7 +12,7 @@\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_raises_regex,\n     assert_array_equal, assert_almost_equal, assert_array_almost_equal,\n-    assert_warns, assert_array_max_ulp, HAS_REFCOUNT\n+    assert_warns, assert_array_max_ulp, HAS_REFCOUNT, IS_WASM\n     )\n from numpy.core._rational_tests import rational\n \n@@ -556,6 +556,7 @@ def test_set(self):\n             np.seterr(**old)\n             assert_(np.geterr() == old)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"no wasm fp exception support\")\n     @pytest.mark.skipif(platform.machine() == \"armv5tel\", reason=\"See gh-413.\")\n     def test_divide_err(self):\n         with np.errstate(divide='raise'):\n@@ -565,6 +566,7 @@ def test_divide_err(self):\n             np.seterr(divide='ignore')\n             np.array([1.]) / np.array([0.])\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"no wasm fp exception support\")\n     def test_errobj(self):\n         olderrobj = np.geterrobj()\n         self.called = 0\n@@ -638,6 +640,7 @@ def assert_op_raises_fpe(self, fpeerr, flop, sc1, sc2):\n         self.assert_raises_fpe(fpeerr, flop, sc1[()], sc2[()])\n \n     # Test for all real and complex float types\n+    @pytest.mark.skipif(IS_WASM, reason=\"no wasm fp exception support\")\n     @pytest.mark.parametrize(\"typecode\", np.typecodes[\"AllFloat\"])\n     def test_floating_exceptions(self, typecode):\n         # Test basic arithmetic function errors\n@@ -697,6 +700,7 @@ def test_floating_exceptions(self, typecode):\n             self.assert_raises_fpe(invalid,\n                                    lambda a, b: a*b, ftype(0), ftype(np.inf))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"no wasm fp exception support\")\n     def test_warnings(self):\n         # test warning code path\n         with warnings.catch_warnings(record=True) as w:\n@@ -1584,6 +1588,7 @@ def __bool__(self):\n         a = np.array([[ThrowsAfter(15)]]*10)\n         assert_raises(ValueError, np.nonzero, a)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"wasm doesn't have threads\")\n     def test_structured_threadsafety(self):\n         # Nonzero (and some other functions) should be threadsafe for\n         # structured datatypes, see gh-15387. This test can behave randomly."
            },
            {
                "filename": "numpy/core/tests/test_regression.py",
                "patch": "@@ -12,7 +12,7 @@\n         assert_, assert_equal, IS_PYPY, assert_almost_equal,\n         assert_array_equal, assert_array_almost_equal, assert_raises,\n         assert_raises_regex, assert_warns, suppress_warnings,\n-        _assert_valid_refcount, HAS_REFCOUNT, IS_PYSTON\n+        _assert_valid_refcount, HAS_REFCOUNT, IS_PYSTON, IS_WASM\n         )\n from numpy.testing._private.utils import _no_tracing, requires_memory\n from numpy.compat import asbytes, asunicode, pickle\n@@ -326,6 +326,7 @@ def bfb():\n         assert_raises(ValueError, bfa)\n         assert_raises(ValueError, bfb)\n \n+    @pytest.mark.xfail(IS_WASM, reason=\"not sure why\")\n     @pytest.mark.parametrize(\"index\",\n             [np.ones(10, dtype=bool), np.arange(10)],\n             ids=[\"boolean-arr-index\", \"integer-arr-index\"])"
            },
            {
                "filename": "numpy/core/tests/test_ufunc.py",
                "patch": "@@ -13,7 +13,7 @@\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_array_equal,\n     assert_almost_equal, assert_array_almost_equal, assert_no_warnings,\n-    assert_allclose, HAS_REFCOUNT, suppress_warnings\n+    assert_allclose, HAS_REFCOUNT, suppress_warnings, IS_WASM\n     )\n from numpy.testing._private.utils import requires_memory\n from numpy.compat import pickle\n@@ -700,6 +700,7 @@ def test_sum_stability(self):\n         a = np.ones(500, dtype=np.float64)\n         assert_almost_equal((a / 10.).sum() - a.size / 10., 0, 13)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_sum(self):\n         for dt in (int, np.float16, np.float32, np.float64, np.longdouble):\n             for v in (0, 1, 2, 7, 8, 9, 15, 16, 19, 127,\n@@ -2538,6 +2539,7 @@ def test_ufunc_input_casterrors(bad_offset):\n         np.add(arr, arr, dtype=np.intp, casting=\"unsafe\")\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n @pytest.mark.parametrize(\"bad_offset\", [0, int(np.BUFSIZE * 1.5)])\n def test_ufunc_input_floatingpoint_error(bad_offset):\n     value = 123\n@@ -2584,6 +2586,7 @@ def test_reduce_casterrors(offset):\n     assert out[()] < value * offset\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n @pytest.mark.parametrize(\"method\",\n         [np.add.accumulate, np.add.reduce,\n          pytest.param(lambda x: np.add.reduceat(x, [0]), id=\"reduceat\"),"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -17,7 +17,7 @@\n     assert_, assert_equal, assert_raises, assert_raises_regex,\n     assert_array_equal, assert_almost_equal, assert_array_almost_equal,\n     assert_array_max_ulp, assert_allclose, assert_no_warnings, suppress_warnings,\n-    _gen_alignment_data, assert_array_almost_equal_nulp\n+    _gen_alignment_data, assert_array_almost_equal_nulp, IS_WASM\n     )\n from numpy.testing._private.utils import _glibc_older_than\n \n@@ -377,6 +377,7 @@ def test_division_int(self):\n         assert_equal(x // 100, [0, 0, 0, 1, -1, -1, -1, -1, -2])\n         assert_equal(x % 100, [5, 10, 90, 0, 95, 90, 10, 0, 80])\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"dtype,ex_val\", itertools.product(\n         np.sctypes['int'] + np.sctypes['uint'], (\n             (\n@@ -462,6 +463,7 @@ def test_division_int_boundary(self, dtype, ex_val):\n \n             np.array([], dtype=dtype) // 0\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"dtype,ex_val\", itertools.product(\n         np.sctypes['int'] + np.sctypes['uint'], (\n             \"np.array([fo.max, 1, 2, 1, 1, 2, 3], dtype=dtype)\",\n@@ -523,6 +525,8 @@ def test_division_int_timedelta(self, dividend, divisor, quotient):\n             quotient_array = np.array([quotient]*5)\n             assert all(dividend_array // divisor == quotient_array), msg\n         else:\n+            if IS_WASM:\n+                pytest.skip(\"fp errors don't work in wasm\")\n             with np.errstate(divide='raise', invalid='raise'):\n                 with pytest.raises(FloatingPointError):\n                     dividend // divisor\n@@ -569,6 +573,7 @@ def test_floor_division_signed_zero(self):\n         assert_equal(np.signbit(x//1), 0)\n         assert_equal(np.signbit((-x)//1), 1)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize('dtype', np.typecodes['Float'])\n     def test_floor_division_errors(self, dtype):\n         fnan = np.array(np.nan, dtype=dtype)\n@@ -683,6 +688,7 @@ def test_float_remainder_roundoff(self):\n                     else:\n                         assert_(b > rem >= 0, msg)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.xfail(sys.platform.startswith(\"darwin\"),\n             reason=\"MacOS seems to not give the correct 'invalid' warning for \"\n                    \"`fmod`.  Hopefully, others always do.\")\n@@ -709,6 +715,7 @@ def test_float_divmod_errors(self, dtype):\n             # inf / 0 does not set any flags, only the modulo creates a NaN\n             np.divmod(finf, fzero)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.xfail(sys.platform.startswith(\"darwin\"),\n            reason=\"MacOS seems to not give the correct 'invalid' warning for \"\n                   \"`fmod`.  Hopefully, others always do.\")\n@@ -730,6 +737,7 @@ def test_float_remainder_errors(self, dtype, fn):\n             fn(fone, fnan)\n             fn(fnan, fone)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_float_remainder_overflow(self):\n         a = np.finfo(np.float64).tiny\n         with np.errstate(over='ignore', invalid='ignore'):\n@@ -851,6 +859,7 @@ class TestDivisionIntegerOverflowsAndDivideByZero:\n             helper_lambdas['min-zero'], helper_lambdas['neg_min-zero'])\n     }\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"dtype\", np.typecodes[\"Integer\"])\n     def test_signed_division_overflow(self, dtype):\n         to_check = interesting_binop_operands(np.iinfo(dtype).min, -1, dtype)\n@@ -877,6 +886,7 @@ def test_signed_division_overflow(self, dtype):\n             assert extractor(res1) == np.iinfo(op1.dtype).min\n             assert extractor(res2) == 0\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n     def test_divide_by_zero(self, dtype):\n         # Note that the return value cannot be well defined here, but NumPy\n@@ -896,6 +906,7 @@ def test_divide_by_zero(self, dtype):\n             assert extractor(res1) == 0\n             assert extractor(res2) == 0\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"dividend_dtype\",\n             np.sctypes['int'])\n     @pytest.mark.parametrize(\"divisor_dtype\",\n@@ -1147,6 +1158,7 @@ def test_log2_ints(self, i):\n         v = np.log2(2.**i)\n         assert_equal(v, float(i), err_msg='at exponent %d' % i)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_log2_special(self):\n         assert_equal(np.log2(1.), 0.)\n         assert_equal(np.log2(np.inf), np.inf)\n@@ -1305,6 +1317,7 @@ def test_exp_exceptions(self):\n             assert_raises(FloatingPointError, np.exp, np.float64(-1000.))\n             assert_raises(FloatingPointError, np.exp, np.float64(-1E19))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_log_values(self):\n         with np.errstate(all='ignore'):\n             x = [np.nan, np.nan, np.inf, np.nan, -np.inf, np.nan]\n@@ -1354,6 +1367,7 @@ def test_log_values(self):\n             a = np.array(1e9, dtype='float32')\n             np.log(a)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_sincos_values(self):\n         with np.errstate(all='ignore'):\n             x = [np.nan, np.nan, np.nan, np.nan]\n@@ -1394,6 +1408,7 @@ def test_abs_values(self):\n             yf = np.array(y, dtype=dt)\n             assert_equal(np.abs(yf), xf)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_square_values(self):\n         x = [np.nan,  np.nan, np.inf, np.inf]\n         y = [np.nan, -np.nan, np.inf, -np.inf]\n@@ -1411,6 +1426,7 @@ def test_square_values(self):\n             assert_raises(FloatingPointError, np.square,\n                           np.array(1E200, dtype='d'))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_reciprocal_values(self):\n         with np.errstate(all='ignore'):\n             x = [np.nan,  np.nan, 0.0, -0.0, np.inf, -np.inf]\n@@ -1425,6 +1441,7 @@ def test_reciprocal_values(self):\n                 assert_raises(FloatingPointError, np.reciprocal,\n                               np.array(-0.0, dtype=dt))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_tan(self):\n         with np.errstate(all='ignore'):\n             in_ = [np.nan, -np.nan, 0.0, -0.0, np.inf, -np.inf]\n@@ -1441,6 +1458,7 @@ def test_tan(self):\n                 assert_raises(FloatingPointError, np.tan,\n                               np.array(-np.inf, dtype=dt))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_arcsincos(self):\n         with np.errstate(all='ignore'):\n             in_ = [np.nan, -np.nan, np.inf, -np.inf]\n@@ -1467,6 +1485,7 @@ def test_arctan(self):\n                 out_arr = np.array(out, dtype=dt)\n                 assert_equal(np.arctan(in_arr), out_arr)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_sinh(self):\n         in_ = [np.nan, -np.nan, np.inf, -np.inf]\n         out = [np.nan, np.nan, np.inf, -np.inf]\n@@ -1483,6 +1502,7 @@ def test_sinh(self):\n             assert_raises(FloatingPointError, np.sinh,\n                           np.array(1200.0, dtype='d'))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_cosh(self):\n         in_ = [np.nan, -np.nan, np.inf, -np.inf]\n         out = [np.nan, np.nan, np.inf, np.inf]\n@@ -1515,6 +1535,7 @@ def test_arcsinh(self):\n             out_arr = np.array(out, dtype=dt)\n             assert_equal(np.arcsinh(in_arr), out_arr)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_arccosh(self):\n         with np.errstate(all='ignore'):\n             in_ = [np.nan, -np.nan, np.inf, -np.inf, 1.0, 0.0]\n@@ -1530,6 +1551,7 @@ def test_arccosh(self):\n                     assert_raises(FloatingPointError, np.arccosh,\n                                   np.array(value, dtype=dt))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_arctanh(self):\n         with np.errstate(all='ignore'):\n             in_ = [np.nan, -np.nan, np.inf, -np.inf, 1.0, -1.0, 2.0]\n@@ -1565,6 +1587,7 @@ def test_exp2(self):\n                     assert_raises(FloatingPointError, np.exp2,\n                                   np.array(value, dtype=dt))\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_expm1(self):\n         with np.errstate(all='ignore'):\n             in_ = [np.nan, -np.nan, np.inf, -np.inf]\n@@ -3701,6 +3724,7 @@ def test_it(self):\n             assert_almost_equal(fz.real, fr, err_msg='real part %s' % f)\n             assert_almost_equal(fz.imag, 0., err_msg='imag part %s' % f)\n \n+    @pytest.mark.xfail(IS_WASM, reason=\"doesn't work\")\n     def test_precisions_consistent(self):\n         z = 1 + 1j\n         for f in self.funcs:\n@@ -3710,6 +3734,7 @@ def test_precisions_consistent(self):\n             assert_almost_equal(fcf, fcd, decimal=6, err_msg='fch-fcd %s' % f)\n             assert_almost_equal(fcl, fcd, decimal=15, err_msg='fch-fcl %s' % f)\n \n+    @pytest.mark.xfail(IS_WASM, reason=\"doesn't work\")\n     def test_branch_cuts(self):\n         # check branch cuts and continuity on them\n         _check_branch_cut(np.log,   -0.5, 1j, 1, -1, True)\n@@ -3735,6 +3760,7 @@ def test_branch_cuts(self):\n         _check_branch_cut(np.arccosh, [0-2j, 2j, 2], [1,  1,  1j], 1, 1)\n         _check_branch_cut(np.arctanh, [0-2j, 2j, 0], [1,  1,  1j], 1, 1)\n \n+    @pytest.mark.xfail(IS_WASM, reason=\"doesn't work\")\n     def test_branch_cuts_complex64(self):\n         # check branch cuts and continuity on them\n         _check_branch_cut(np.log,   -0.5, 1j, 1, -1, True, np.complex64)\n@@ -3779,6 +3805,7 @@ def test_against_cmath(self):\n                 b = cfunc(p)\n                 assert_(abs(a - b) < atol, \"%s %s: %s; cmath: %s\" % (fname, p, a, b))\n \n+    @pytest.mark.xfail(IS_WASM, reason=\"doesn't work\")\n     @pytest.mark.parametrize('dtype', [np.complex64, np.complex_, np.longcomplex])\n     def test_loss_of_precision(self, dtype):\n         \"\"\"Check loss of precision in complex arc* functions\"\"\""
            },
            {
                "filename": "numpy/distutils/tests/test_build_ext.py",
                "patch": "@@ -5,7 +5,9 @@\n import sys\n from textwrap import indent, dedent\n import pytest\n+from numpy.testing import IS_WASM\n \n+@pytest.mark.skipif(IS_WASM, reason=\"cannot start subprocess in wasm\")\n @pytest.mark.slow\n def test_multi_fortran_libs_link(tmp_path):\n     '''"
            },
            {
                "filename": "numpy/distutils/tests/test_exec_command.py",
                "patch": "@@ -1,10 +1,12 @@\n import os\n+import pytest\n import sys\n from tempfile import TemporaryFile\n \n from numpy.distutils import exec_command\n from numpy.distutils.exec_command import get_pythonexe\n-from numpy.testing import tempdir, assert_, assert_warns\n+from numpy.testing import tempdir, assert_, assert_warns, IS_WASM\n+\n \n # In python 3 stdout, stderr are text (unicode compliant) devices, so to\n # emulate them import StringIO from the io module.\n@@ -93,6 +95,7 @@ def test_exec_command_stderr():\n                         exec_command.exec_command(\"cd '.'\")\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n class TestExecCommand:\n     def setup_method(self):\n         self.pyexe = get_pythonexe()"
            },
            {
                "filename": "numpy/distutils/tests/test_shell_utils.py",
                "patch": "@@ -4,6 +4,7 @@\n import sys\n \n from numpy.distutils import _shell_utils\n+from numpy.testing import IS_WASM\n \n argv_cases = [\n     [r'exe'],\n@@ -49,6 +50,7 @@ def runner(Parser):\n         raise NotImplementedError\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n @pytest.mark.parametrize('argv', argv_cases)\n def test_join_matches_subprocess(Parser, runner, argv):\n     \"\"\"\n@@ -64,6 +66,7 @@ def test_join_matches_subprocess(Parser, runner, argv):\n     assert json.loads(json_out) == argv\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n @pytest.mark.parametrize('argv', argv_cases)\n def test_roundtrip(Parser, argv):\n     \"\"\""
            },
            {
                "filename": "numpy/f2py/tests/test_abstract_interface.py",
                "patch": "@@ -1,9 +1,12 @@\n from pathlib import Path\n+import pytest\n import textwrap\n from . import util\n from numpy.f2py import crackfortran\n+from numpy.testing import IS_WASM\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n class TestAbstractInterface(util.F2PyTest):\n     sources = [util.getpath(\"tests\", \"src\", \"abstract_interface\", \"foo.f90\")]\n "
            },
            {
                "filename": "numpy/f2py/tests/util.py",
                "patch": "@@ -20,7 +20,7 @@\n \n from pathlib import Path\n from numpy.compat import asbytes, asstr\n-from numpy.testing import temppath\n+from numpy.testing import temppath, IS_WASM\n from importlib import import_module\n \n #\n@@ -187,6 +187,9 @@ def _get_compiler_status():\n         return _compiler_status\n \n     _compiler_status = (False, False, False)\n+    if IS_WASM:\n+        # Can't run compiler from inside WASM.\n+        return _compiler_status\n \n     # XXX: this is really ugly. But I don't know how to invoke Distutils\n     #      in a safer way..."
            },
            {
                "filename": "numpy/fft/tests/test_pocketfft.py",
                "patch": "@@ -2,7 +2,7 @@\n import pytest\n from numpy.random import random\n from numpy.testing import (\n-        assert_array_equal, assert_raises, assert_allclose\n+        assert_array_equal, assert_raises, assert_allclose, IS_WASM\n         )\n import threading\n import queue\n@@ -268,6 +268,7 @@ def test_fft_with_order(dtype, order, fft):\n         raise ValueError()\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start thread\")\n class TestFFTThreadSafe:\n     threads = 16\n     input_shape = (800, 200)"
            },
            {
                "filename": "numpy/lib/tests/test_format.py",
                "patch": "@@ -283,7 +283,7 @@\n import numpy as np\n from numpy.testing import (\n     assert_, assert_array_equal, assert_raises, assert_raises_regex,\n-    assert_warns, IS_PYPY,\n+    assert_warns, IS_PYPY, IS_WASM\n     )\n from numpy.testing._private.utils import requires_memory\n from numpy.lib import format\n@@ -459,6 +459,7 @@ def test_long_str():\n     assert_array_equal(long_str_arr, long_str_arr2)\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"memmap doesn't work correctly\")\n @pytest.mark.slow\n def test_memmap_roundtrip(tmpdir):\n     for i, arr in enumerate(basic_arrays + record_arrays):\n@@ -526,6 +527,7 @@ def test_load_padded_dtype(tmpdir, dt):\n     assert_array_equal(arr, arr1)\n \n \n+@pytest.mark.xfail(IS_WASM, reason=\"Emscripten NODEFS has a buggy dup\")\n def test_python2_python3_interoperability():\n     fname = 'win64python2.npy'\n     path = os.path.join(os.path.dirname(__file__), 'data', fname)\n@@ -675,6 +677,7 @@ def test_version_2_0():\n     assert_raises(ValueError, format.write_array, f, d, (1, 0))\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"memmap doesn't work correctly\")\n def test_version_2_0_memmap(tmpdir):\n     # requires more than 2 byte for header\n     dt = [((\"%d\" % i) * 100, float) for i in range(500)]"
            },
            {
                "filename": "numpy/lib/tests/test_function_base.py",
                "patch": "@@ -15,7 +15,7 @@\n from numpy.testing import (\n     assert_, assert_equal, assert_array_equal, assert_almost_equal,\n     assert_array_almost_equal, assert_raises, assert_allclose, IS_PYPY,\n-    assert_warns, assert_raises_regex, suppress_warnings, HAS_REFCOUNT,\n+    assert_warns, assert_raises_regex, suppress_warnings, HAS_REFCOUNT, IS_WASM\n     )\n import numpy.lib.function_base as nfb\n from numpy.random import rand\n@@ -3754,6 +3754,7 @@ def test_nan_behavior(self):\n         b[2] = np.nan\n         assert_equal(np.median(a, (0, 2)), b)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work correctly\")\n     def test_empty(self):\n         # mean(empty array) emits two warnings: empty slice and divide by 0\n         a = np.array([], dtype=float)"
            },
            {
                "filename": "numpy/lib/tests/test_io.py",
                "patch": "@@ -25,7 +25,7 @@\n     assert_warns, assert_, assert_raises_regex, assert_raises,\n     assert_allclose, assert_array_equal, temppath, tempdir, IS_PYPY,\n     HAS_REFCOUNT, suppress_warnings, assert_no_gc_cycles, assert_no_warnings,\n-    break_cycles\n+    break_cycles, IS_WASM\n     )\n from numpy.testing._private.utils import requires_memory\n \n@@ -243,6 +243,7 @@ def test_BagObj(self):\n         assert_equal(a, l.f.file_a)\n         assert_equal(b, l.f.file_b)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"Cannot start thread\")\n     def test_savez_filename_clashes(self):\n         # Test that issue #852 is fixed\n         # and savez functions in multithreaded environment\n@@ -2539,6 +2540,7 @@ def test_save_load_memmap(self):\n                 break_cycles()\n                 break_cycles()\n \n+    @pytest.mark.xfail(IS_WASM, reason=\"memmap doesn't work correctly\")\n     def test_save_load_memmap_readwrite(self):\n         # Test that pathlib.Path instances can be written mem-mapped.\n         with temppath(suffix='.npy') as path:"
            },
            {
                "filename": "numpy/linalg/tests/test_linalg.py",
                "patch": "@@ -19,7 +19,7 @@\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_array_equal,\n     assert_almost_equal, assert_allclose, suppress_warnings,\n-    assert_raises_regex, HAS_LAPACK64,\n+    assert_raises_regex, HAS_LAPACK64, IS_WASM\n     )\n \n \n@@ -1063,6 +1063,7 @@ def test_exceptions_non_square(self, dt):\n         assert_raises(LinAlgError, matrix_power, np.array([[1], [2]], dt), 1)\n         assert_raises(LinAlgError, matrix_power, np.ones((4, 3, 2), dt), 1)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_exceptions_not_invertible(self, dt):\n         if dt in self.dtnoinv:\n             return\n@@ -1845,6 +1846,7 @@ def test_byteorder_check():\n             assert_array_equal(res, routine(sw_arr))\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n def test_generalized_raise_multiloop():\n     # It should raise an error even if the error doesn't occur in the\n     # last iteration of the ufunc inner loop\n@@ -1908,6 +1910,7 @@ def test_xerbla_override():\n             pytest.skip('Numpy xerbla not linked in.')\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n @pytest.mark.slow\n def test_sdot_bug_8577():\n     # Regression test that loading certain other libraries does not"
            },
            {
                "filename": "numpy/ma/tests/test_core.py",
                "patch": "@@ -21,7 +21,7 @@\n import numpy.core.fromnumeric as fromnumeric\n import numpy.core.umath as umath\n from numpy.testing import (\n-    assert_raises, assert_warns, suppress_warnings\n+    assert_raises, assert_warns, suppress_warnings, IS_WASM\n     )\n from numpy import ndarray\n from numpy.compat import asbytes\n@@ -4365,6 +4365,7 @@ def test_power_with_broadcasting(self):\n         assert_equal(test, ctrl)\n         assert_equal(test.mask, ctrl.mask)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_where(self):\n         # Test the where function\n         x = np.array([1., 1., 1., -2., pi/2.0, 4., 5., -10., 10., 1., 2., 3.])"
            },
            {
                "filename": "numpy/random/tests/test_extending.py",
                "patch": "@@ -6,6 +6,7 @@\n import warnings\n import numpy as np\n from numpy.distutils.misc_util import exec_mod_from_location\n+from numpy.testing import IS_WASM\n \n try:\n     import cffi\n@@ -41,6 +42,8 @@\n         # too old or wrong cython, skip the test\n         cython = None\n \n+\n+@pytest.mark.skipif(IS_WASM, reason=\"Can't start subprocess\")\n @pytest.mark.skipif(cython is None, reason=\"requires cython\")\n @pytest.mark.slow\n def test_cython(tmp_path):"
            },
            {
                "filename": "numpy/random/tests/test_generator_mt19937.py",
                "patch": "@@ -8,7 +8,7 @@\n from numpy.testing import (\n     assert_, assert_raises, assert_equal, assert_allclose,\n     assert_warns, assert_no_warnings, assert_array_equal,\n-    assert_array_almost_equal, suppress_warnings)\n+    assert_array_almost_equal, suppress_warnings, IS_WASM)\n \n from numpy.random import Generator, MT19937, SeedSequence, RandomState\n \n@@ -1391,6 +1391,7 @@ def test_multinomial(self):\n                              [5, 5, 3, 1, 2, 4]]])\n         assert_array_equal(actual, desired)\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     @pytest.mark.parametrize(\"method\", [\"svd\", \"eigh\", \"cholesky\"])\n     def test_multivariate_normal(self, method):\n         random = Generator(MT19937(self.seed))\n@@ -2452,6 +2453,7 @@ def test_empty_outputs(self):\n         assert actual.shape == (3, 0, 7, 4)\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"can't start thread\")\n class TestThread:\n     # make sure each state produces the same sequence even in threads\n     def setup_method(self):"
            },
            {
                "filename": "numpy/random/tests/test_random.py",
                "patch": "@@ -6,7 +6,7 @@\n from numpy.testing import (\n         assert_, assert_raises, assert_equal, assert_warns,\n         assert_no_warnings, assert_array_equal, assert_array_almost_equal,\n-        suppress_warnings\n+        suppress_warnings, IS_WASM\n         )\n from numpy import random\n import sys\n@@ -1615,6 +1615,7 @@ def test_logseries(self):\n         assert_raises(ValueError, logseries, bad_p_two * 3)\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"can't start thread\")\n class TestThread:\n     # make sure each state produces the same sequence even in threads\n     def setup_method(self):"
            },
            {
                "filename": "numpy/random/tests/test_randomstate.py",
                "patch": "@@ -8,7 +8,7 @@\n from numpy.testing import (\n         assert_, assert_raises, assert_equal, assert_warns,\n         assert_no_warnings, assert_array_equal, assert_array_almost_equal,\n-        suppress_warnings\n+        suppress_warnings, IS_WASM\n         )\n \n from numpy.random import MT19937, PCG64\n@@ -1894,6 +1894,7 @@ def test_logseries(self):\n         assert_raises(ValueError, logseries, bad_p_two * 3)\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"can't start thread\")\n class TestThread:\n     # make sure each state produces the same sequence even in threads\n     def setup_method(self):"
            },
            {
                "filename": "numpy/testing/_private/utils.py",
                "patch": "@@ -34,7 +34,7 @@\n         'assert_array_max_ulp', 'assert_warns', 'assert_no_warnings',\n         'assert_allclose', 'IgnoreException', 'clear_and_catch_warnings',\n         'SkipTest', 'KnownFailureException', 'temppath', 'tempdir', 'IS_PYPY',\n-        'HAS_REFCOUNT', 'suppress_warnings', 'assert_array_compare',\n+        'HAS_REFCOUNT', \"IS_WASM\", 'suppress_warnings', 'assert_array_compare',\n         'assert_no_gc_cycles', 'break_cycles', 'HAS_LAPACK64', 'IS_PYSTON',\n         '_OLD_PROMOTION'\n         ]\n@@ -48,6 +48,7 @@ class KnownFailureException(Exception):\n KnownFailureTest = KnownFailureException  # backwards compat\n verbose = 0\n \n+IS_WASM = platform.machine() in [\"wasm32\", \"wasm64\"]\n IS_PYPY = sys.implementation.name == 'pypy'\n IS_PYSTON = hasattr(sys, \"pyston_version_info\")\n HAS_REFCOUNT = getattr(sys, 'getrefcount', None) is not None and not IS_PYSTON"
            },
            {
                "filename": "numpy/tests/test_public_api.py",
                "patch": "@@ -9,6 +9,7 @@\n import numpy as np\n import numpy\n import pytest\n+from numpy.testing import IS_WASM\n \n try:\n     import ctypes\n@@ -62,6 +63,7 @@ def test_numpy_namespace():\n     assert bad_results == allowlist\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"can't start subprocess\")\n @pytest.mark.parametrize('name', ['testing', 'Tester'])\n def test_import_lazy_import(name):\n     \"\"\"Make sure we can actually use the modules we lazy load."
            },
            {
                "filename": "numpy/tests/test_reloading.py",
                "patch": "@@ -1,6 +1,13 @@\n-from numpy.testing import assert_raises, assert_warns, assert_, assert_equal\n+from numpy.testing import (\n+    assert_raises,\n+    assert_warns,\n+    assert_,\n+    assert_equal,\n+    IS_WASM,\n+)\n from numpy.compat import pickle\n \n+import pytest\n import sys\n import subprocess\n import textwrap\n@@ -37,6 +44,7 @@ def test_novalue():\n                                           protocol=proto)) is np._NoValue)\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"can't start subprocess\")\n def test_full_reimport():\n     \"\"\"At the time of writing this, it is *not* truly supported, but\n     apparently enough users rely on it, for it to be an annoying change"
            },
            {
                "filename": "numpy/tests/test_scripts.py",
                "patch": "@@ -9,7 +9,7 @@\n import subprocess\n \n import numpy as np\n-from numpy.testing import assert_equal\n+from numpy.testing import assert_equal, IS_WASM\n \n is_inplace = isfile(pathjoin(dirname(np.__file__),  '..', 'setup.py'))\n \n@@ -41,6 +41,7 @@ def test_f2py(f2py_cmd):\n     assert_equal(stdout.strip(), np.__version__.encode('ascii'))\n \n \n+@pytest.mark.skipif(IS_WASM, reason=\"Cannot start subprocess\")\n def test_pep338():\n     stdout = subprocess.check_output([sys.executable, '-mnumpy.f2py', '-v'])\n     assert_equal(stdout.strip(), np.__version__.encode('ascii'))"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20363,
        "body": "#### Replace SVML/ASM of tanh for both single and double precision with universal intrinsics\r\n\r\n   To bring the benefits of performance for all platforms\r\n   not just for `avx512` on Linux without performance/accuracy regression,\r\n   actually the other way around, better performance and\r\n   after all maintainable code.\r\n\r\nThe original code can be found in:\r\n  - https://github.com/numpy/SVML/blob/main/linux/avx512/svml_z0_tanh_d_la.s\r\n  - https://github.com/numpy/SVML/blob/main/linux/avx512/svml_z0_tanh_s_la.s\r\n\r\n## Benchmarks\r\n\r\n\r\n### X86\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nCPU(s):                          4\r\nOn-line CPU(s) list:             0-3\r\nThread(s) per core:              2\r\nCore(s) per socket:              2\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz\r\nStepping:                        4\r\nCPU MHz:                         3410.808\r\nBogoMIPS:                        5999.99\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       64 KiB\r\nL1i cache:                       64 KiB\r\nL2 cache:                        2 MiB\r\nL3 cache:                        24.8 MiB\r\nNUMA node0 CPU(s):               0-3\r\nVulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:              Mitigation; PTE Inversion\r\nVulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Full generic retpoline, STIBP disabled, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant\r\n                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt\r\n                                 tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep b\r\n                                 mi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat\r\n                                 pku ospke\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux ip-172-31-32-40 5.11.0-1020-aws #21~20.04.2-Ubuntu SMP Fri Oct 1 13:03:59 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\nPython 3.8.10\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n<details>\r\n <summary>SVML/AVX512_SKX(before) vs AVX512_SKX(after)</summary>\r\n\r\n```Bash\r\nunset NPY_DISABLE_CPU_FEATURES\r\npython3 runtests.py -n --bench-compare parent/main tanh -- --sort name\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [a1813504]       [b5bd8620]\r\n     <svml2npyv/tanh~3>       <svml2npyv/tanh>\r\n-       365\u00b10.5\u03bcs        218\u00b10.9\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'd')\r\n-      82.1\u00b10.3\u03bcs       70.9\u00b10.5\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-       375\u00b10.6\u03bcs        238\u00b10.2\u03bcs     0.63  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'd')\r\n-        379\u00b120\u03bcs         248\u00b120\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'd')\r\n-       366\u00b10.5\u03bcs          228\u00b13\u03bcs     0.62  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'd')\r\n-      88.2\u00b10.5\u03bcs       83.4\u00b10.3\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-       378\u00b10.8\u03bcs          254\u00b12\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'd')\r\n-         382\u00b12\u03bcs          272\u00b11\u03bcs     0.71  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'd')\r\n```\r\n</details>\r\n\r\n<details>\r\n <summary>AVX2_FMA3</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX512F AVX512_SKX\"\r\npython3 runtests.py -n --bench-compare parent/main tanh -- --sort name\r\n```\r\n```Bash\r\n      before           after         ratio\r\n     [a1813504]       [b5bd8620]\r\n     <svml2npyv/tanh~3>       <svml2npyv/tanh>\r\n-        1.96\u00b10ms          857\u00b11\u03bcs     0.44  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'd')\r\n-     1.86\u00b10.01ms          250\u00b12\u03bcs     0.13  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n-     1.97\u00b10.05ms         873\u00b130\u03bcs     0.44  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'd')\r\n-     1.86\u00b10.01ms          283\u00b12\u03bcs     0.15  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'f')\r\n-     2.14\u00b10.09ms         962\u00b150\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'd')\r\n-     1.93\u00b10.04ms          296\u00b18\u03bcs     0.15  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'f')\r\n-        1.96\u00b10ms          884\u00b11\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'd')\r\n-        1.86\u00b10ms        312\u00b10.9\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-        1.97\u00b10ms          885\u00b11\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'd')\r\n-     1.86\u00b10.02ms          329\u00b14\u03bcs     0.18  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'f')\r\n-      1.97\u00b10.1ms         888\u00b150\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'd')\r\n-        1.86\u00b10ms        329\u00b10.9\u03bcs     0.18  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'f')\r\n-        1.97\u00b10ms        888\u00b10.9\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'd')\r\n-        1.86\u00b10ms        315\u00b10.7\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-        1.97\u00b10ms          889\u00b12\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'd')\r\n-        1.86\u00b10ms        331\u00b10.3\u03bcs     0.18  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'f')\r\n-        1.97\u00b10ms          890\u00b13\u03bcs     0.45  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'd')\r\n-     1.87\u00b10.04ms          330\u00b17\u03bcs     0.18  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'f')\r\n```\r\n</details>\r\n\r\n\r\n-----\r\n\r\n### Power little-endian\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```\r\nArchitecture:                    ppc64le\r\nByte Order:                      Little Endian\r\nCPU(s):                          8\r\nOn-line CPU(s) list:             0-7\r\nThread(s) per core:              1\r\nCore(s) per socket:              1\r\nSocket(s):                       8\r\nNUMA node(s):                    1\r\nModel:                           2.2 (pvr 004e 1202)\r\nModel name:                      POWER9 (architected), altivec supported\r\nL1d cache:                       256 KiB\r\nL1i cache:                       256 KiB\r\nNUMA node0 CPU(s):               0-7\r\nVulnerability L1tf:              Not affected\r\nVulnerability Meltdown:          Mitigation; RFI Flush\r\nVulnerability Spec store bypass: Mitigation; Kernel entry/exit barrier (eieio)\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Vulnerable\r\n\r\nprocessor   : 7\r\ncpu     : POWER9 (architected), altivec supported\r\nclock       : 2200.000000MHz\r\nrevision    : 2.2 (pvr 004e 1202)\r\n\r\ntimebase    : 512000000\r\nplatform    : pSeries\r\nmodel       : IBM pSeries (emulated by qemu)\r\nmachine     : CHRP IBM pSeries (emulated by qemu)\r\nMMU     : Radix\r\n\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux e517009a912a 4.19.0-2-powerpc64le #1 SMP Debian 4.19.16-1 (2019-01-17) ppc64le ppc64le ppc64le GNU/Linux\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>VSX2</summary>\r\n\r\n```Bash\r\npython3 runtests.py -n --bench-compare parent/main tanh -- --sort name\r\n```\r\n```\r\n       before           after         ratio\r\n     [a1813504]       [feacd298]\r\n     <svml2npyv/tanh~3>       <svml2npyv/tanh>\r\n-     5.63\u00b10.01ms         2.46\u00b10ms     0.44  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'd')\r\n-     6.11\u00b10.01ms         1.71\u00b10ms     0.28  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n-     5.63\u00b10.03ms      2.48\u00b10.01ms     0.44  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'd')\r\n-     6.11\u00b10.03ms         1.77\u00b10ms     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'f')\r\n-     5.70\u00b10.05ms      2.51\u00b10.03ms     0.44  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'd')\r\n-     6.17\u00b10.04ms      1.78\u00b10.01ms     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'f')\r\n-     5.63\u00b10.01ms      2.67\u00b10.04ms     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'd')\r\n-     6.12\u00b10.01ms      1.81\u00b10.01ms     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-     5.62\u00b10.01ms      2.68\u00b10.01ms     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'd')\r\n-     6.13\u00b10.01ms         1.85\u00b10ms     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'f')\r\n-     5.63\u00b10.03ms      2.69\u00b10.02ms     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'd')\r\n-     6.13\u00b10.01ms         1.85\u00b10ms     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'f')\r\n-     5.62\u00b10.02ms      2.68\u00b10.01ms     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'd')\r\n-     6.10\u00b10.02ms      1.82\u00b10.01ms     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-     5.63\u00b10.01ms         2.70\u00b10ms     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'd')\r\n-     6.10\u00b10.02ms         1.86\u00b10ms     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'f')\r\n-        5.63\u00b10ms       2.70\u00b10.2ms     0.48  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'd')\r\n-     6.14\u00b10.02ms      1.85\u00b10.01ms     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'f')\r\n```\r\n</details>\r\n\r\n----\r\n\r\n### AArch64\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:                    aarch64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nCPU(s):                          2\r\nOn-line CPU(s) list:             0,1\r\nThread(s) per core:              1\r\nCore(s) per socket:              2\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       ARM\r\nModel:                           1\r\nModel name:                      Neoverse-N1\r\nStepping:                        r3p1\r\nBogoMIPS:                        243.75\r\nL1d cache:                       128 KiB\r\nL1i cache:                       128 KiB\r\nL2 cache:                        2 MiB\r\nL3 cache:                        32 MiB\r\nNUMA node0 CPU(s):               0,1\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp ssbs\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux ip-172-31-44-172 5.11.0-1020-aws #21~20.04.2-Ubuntu SMP Fri Oct 1 13:01:34 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>ASIMD</summary>\r\n\r\n```Bash\r\npython3 runtests.py --bench-compare parent/main tanh -- --sort name\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [a1813504]       [869eae28]\r\n     <svml2npyv/tanh~3>       <svml2npyv/tanh>\r\n-     2.69\u00b10.01ms         1.39\u00b10ms     0.52  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'd')\r\n-     2.71\u00b10.01ms          533\u00b12\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n-     2.69\u00b10.05ms      1.42\u00b10.02ms     0.53  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'd')\r\n-     2.70\u00b10.01ms          568\u00b15\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'f')\r\n-     2.85\u00b10.09ms      1.50\u00b10.04ms     0.52  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'd')\r\n-     2.78\u00b10.04ms         585\u00b110\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'f')\r\n-        2.70\u00b10ms         1.44\u00b10ms     0.53  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'd')\r\n-     2.70\u00b10.01ms          560\u00b13\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-        2.69\u00b10ms         1.47\u00b10ms     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'd')\r\n-     2.70\u00b10.02ms          598\u00b17\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'f')\r\n-     2.69\u00b10.09ms      1.47\u00b10.05ms     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'd')\r\n-        2.70\u00b10ms          600\u00b14\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'f')\r\n-     2.70\u00b10.01ms         1.45\u00b10ms     0.54  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'd')\r\n-     2.70\u00b10.01ms          593\u00b14\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-     2.69\u00b10.01ms         1.47\u00b10ms     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'd')\r\n-        2.71\u00b10ms          628\u00b14\u03bcs     0.23  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'f')\r\n-     2.69\u00b10.01ms         1.47\u00b10ms     0.55  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'd')\r\n-     2.72\u00b10.04ms         630\u00b110\u03bcs     0.23  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'f')\r\n```\r\n</details>\r\n\r\n----\r\n\r\n#### Binary size(striped)\r\n\r\n| LIB         | Before(KBytes) | After(KBytes) | Diff(KBytes) |\r\n| ----------- | ------------- | ------------ | ---------- |\r\n| _multiarray_umath.cpython-38-x86_64-linux-gnu.so | 4140  | 4144 | 4 |\r\n| _multiarray_umath.cpython-38-aarch64-linux-gnu.so | 3364  | 3384 | 20 |\r\n| _multiarray_umath.cpython-38-powerpc64le-linux-gnu.so | 4108  | 4116 | 8 |\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -745,7 +745,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.tanh'),\n           None,\n           TD('e', f='tanh', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('fd', dispatch=[('loops_hyperbolic', 'fd')]),\n           TD(inexact, f='tanh', astype={'e': 'f'}),\n           TD(P, f='tanh'),\n           ),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1003,6 +1003,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops_trigonometric.dispatch.c.src'),\n             join('src', 'umath', 'loops_umath_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_exponent_log.dispatch.c.src'),\n+            join('src', 'umath', 'loops_hyperbolic.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),\n             join('src', 'umath', 'matmul.c.src'),\n             join('src', 'umath', 'clip.h'),\n@@ -1033,8 +1034,17 @@ def generate_umath_doc_header(ext, build_dir):\n \n     svml_path = join('numpy', 'core', 'src', 'umath', 'svml')\n     svml_objs = []\n+    # we have converted the following into universal intrinsics\n+    # so we can bring the benefits of performance for all platforms\n+    # not just for avx512 on linux without performance/accuracy regression,\n+    # actually the other way around, better performance and\n+    # after all maintainable code.\n+    svml_filter = (\n+        'svml_z0_tanh_d_la.s', 'svml_z0_tanh_s_la.s'\n+    )\n     if can_link_svml() and check_svml_submodule(svml_path):\n         svml_objs = glob.glob(svml_path + '/**/*.s', recursive=True)\n+        svml_objs = [o for o in svml_objs if not o.endswith(svml_filter)]\n \n     config.add_extension('_multiarray_umath',\n                          # Forcing C language even though we have C++ sources."
            },
            {
                "filename": "numpy/core/src/_simd/_simd.dispatch.c.src",
                "patch": "@@ -15,7 +15,8 @@\n /**begin repeat\n  * #sfx       = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n  * #bsfx      = b8, b8, b16, b16, b32, b32, b64, b64, b32, b64#\n- * #esfx      = u16, s8, u32,s16, u32, s32, u64, s64, f32, f64#\n+ * #esfx      = u16,s8, u32, s16, u32, s32, u64, s64, f32, f64#\n+ * #size      = 8,  8,  16,  16,  32,  32,  64,  64,  32,  64#\n  * #expand_sup= 1,  0,  1,   0,   0,   0,   0,   0,   0,   0#\n  * #simd_sup  = 1,  1,  1,   1,   1,   1,   1,   1,   1,   NPY_SIMD_F64#\n  * #fp_only   = 0,  0,  0,   0,   0,   0,   0,   0,   1,   1#\n@@ -232,6 +233,15 @@ err:\n /**end repeat1**/\n #endif // @ncont_sup@\n \n+/****************************\n+ * Lookup tables\n+ ****************************/\n+#if @size@ == 32\n+SIMD_IMPL_INTRIN_2(lut32_@sfx@, v@sfx@, q@sfx@, vu@size@)\n+#endif\n+#if @size@ == 64\n+SIMD_IMPL_INTRIN_2(lut16_@sfx@, v@sfx@, q@sfx@, vu@size@)\n+#endif\n /***************************\n  * Misc\n  ***************************/\n@@ -470,8 +480,9 @@ static PyMethodDef simd__intrinsics_methods[] = {\n /**begin repeat\n  * #sfx       = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n  * #bsfx      = b8, b8, b16, b16, b32, b32, b64, b64, b32, b64#\n- * #esfx      = u16, s8, u32,s16, u32, s32, u64, s64, f32, f64#\n- * #expand_sup =1,  0,  1,   0,   0,   0,   0,   0,   0,   0#\n+ * #esfx      = u16,s8, u32, s16, u32, s32, u64, s64, f32, f64#\n+ * #size      = 8,  8,  16,  16,  32,  32,  64,  64,  32,  64#\n+ * #expand_sup= 1,  0,  1,   0,   0,   0,   0,   0,   0,   0#\n  * #simd_sup  = 1,  1,  1,   1,   1,   1,   1,   1,   1,   NPY_SIMD_F64#\n  * #fp_only   = 0,  0,  0,   0,   0,   0,   0,   0,   1,   1#\n  * #sat_sup   = 1,  1,  1,   1,   0,   0,   0,   0,   0,   0#\n@@ -509,6 +520,15 @@ SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n #endif // ncont_sup\n \n+/****************************\n+ * Lookup tables\n+ ****************************/\n+#if @size@ == 32\n+SIMD_INTRIN_DEF(lut32_@sfx@)\n+#endif\n+#if @size@ == 64\n+SIMD_INTRIN_DEF(lut16_@sfx@)\n+#endif\n /***************************\n  * Misc\n  ***************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/memory.h",
                "patch": "@@ -353,4 +353,25 @@ NPYV_IMPL_AVX2_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_AVX2_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_AVX2_REST_PARTIAL_TYPES(f64, s64)\n \n+/*********************************\n+ * Lookup tables\n+ *********************************/\n+// uses vector as indexes into a table\n+// that contains 32 elements of float32.\n+NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+{ return _mm256_i32gather_ps(table, idx, 4); }\n+NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_u32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+NPY_FINLINE npyv_s32 npyv_lut32_s32(const npy_int32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_s32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+\n+// uses vector as indexes into a table\n+// that contains 16 elements of float64.\n+NPY_FINLINE npyv_f64 npyv_lut16_f64(const double *table, npyv_u64 idx)\n+{ return _mm256_i64gather_pd(table, idx, 8); }\n+NPY_FINLINE npyv_u64 npyv_lut16_u64(const npy_uint64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_u64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+NPY_FINLINE npyv_s64 npyv_lut16_s64(const npy_int64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_s64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+\n #endif // _NPY_SIMD_AVX2_MEMORY_H"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/memory.h",
                "patch": "@@ -329,4 +329,33 @@ NPYV_IMPL_AVX512_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_AVX512_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_AVX512_REST_PARTIAL_TYPES(f64, s64)\n \n+/**************************************************\n+ * Lookup table\n+ *************************************************/\n+// uses vector as indexes into a table\n+// that contains 32 elements of float32.\n+NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+{\n+    const npyv_f32 table0 = npyv_load_f32(table);\n+    const npyv_f32 table1 = npyv_load_f32(table + 16);\n+    return _mm512_permutex2var_ps(table0, idx, table1);\n+}\n+NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_u32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+NPY_FINLINE npyv_s32 npyv_lut32_s32(const npy_int32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_s32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+\n+// uses vector as indexes into a table\n+// that contains 16 elements of float64.\n+NPY_FINLINE npyv_f64 npyv_lut16_f64(const double *table, npyv_u64 idx)\n+{\n+    const npyv_f64 table0 = npyv_load_f64(table);\n+    const npyv_f64 table1 = npyv_load_f64(table + 8);\n+    return _mm512_permutex2var_pd(table0, idx, table1);\n+}\n+NPY_FINLINE npyv_u64 npyv_lut16_u64(const npy_uint64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_u64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+NPY_FINLINE npyv_s64 npyv_lut16_s64(const npy_int64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_s64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+\n #endif // _NPY_SIMD_AVX512_MEMORY_H"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/memory.h",
                "patch": "@@ -332,5 +332,45 @@ NPYV_IMPL_NEON_REST_PARTIAL_TYPES(u64, s64)\n #if NPY_SIMD_F64\n NPYV_IMPL_NEON_REST_PARTIAL_TYPES(f64, s64)\n #endif\n+/*********************************\n+ * Lookup table\n+ *********************************/\n+// uses vector as indexes into a table\n+// that contains 32 elements of uint32.\n+NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n+{\n+    const unsigned i0 = vgetq_lane_u32(idx, 0);\n+    const unsigned i1 = vgetq_lane_u32(idx, 1);\n+    const unsigned i2 = vgetq_lane_u32(idx, 2);\n+    const unsigned i3 = vgetq_lane_u32(idx, 3);\n+\n+    uint32x2_t low = vcreate_u32(table[i0]);\n+               low = vld1_lane_u32((const uint32_t*)table + i1, low, 1);\n+    uint32x2_t high = vcreate_u32(table[i2]);\n+               high = vld1_lane_u32((const uint32_t*)table + i3, high, 1);\n+    return vcombine_u32(low, high);\n+}\n+NPY_FINLINE npyv_s32 npyv_lut32_s32(const npy_int32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_s32_u32(npyv_lut32_u32((const npy_uint32*)table, idx)); }\n+NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+{ return npyv_reinterpret_f32_u32(npyv_lut32_u32((const npy_uint32*)table, idx)); }\n+\n+// uses vector as indexes into a table\n+// that contains 16 elements of uint64.\n+NPY_FINLINE npyv_u64 npyv_lut16_u64(const npy_uint64 *table, npyv_u64 idx)\n+{\n+    const unsigned i0 = vgetq_lane_u32(vreinterpretq_u32_u64(idx), 0);\n+    const unsigned i1 = vgetq_lane_u32(vreinterpretq_u32_u64(idx), 2);\n+    return vcombine_u64(\n+        vld1_u64((const uint64_t*)table + i0),\n+        vld1_u64((const uint64_t*)table + i1)\n+    );\n+}\n+NPY_FINLINE npyv_s64 npyv_lut16_s64(const npy_int64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_s64_u64(npyv_lut16_u64((const npy_uint64*)table, idx)); }\n+#if NPY_SIMD_F64\n+NPY_FINLINE npyv_f64 npyv_lut16_f64(const double *table, npyv_u64 idx)\n+{ return npyv_reinterpret_f64_u64(npyv_lut16_u64((const npy_uint64*)table, idx)); }\n+#endif\n \n #endif // _NPY_SIMD_NEON_MEMORY_H"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/memory.h",
                "patch": "@@ -495,4 +495,45 @@ NPYV_IMPL_SSE_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_SSE_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_SSE_REST_PARTIAL_TYPES(f64, s64)\n \n+/*********************************\n+ * Lookup table\n+ *********************************/\n+// uses vector as indexes into a table\n+// that contains 32 elements of float32.\n+NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+{\n+    const int i0 = _mm_cvtsi128_si32(idx);\n+#ifdef NPY_HAVE_SSE41\n+    const int i1 = _mm_extract_epi32(idx, 1);\n+    const int i2 = _mm_extract_epi32(idx, 2);\n+    const int i3 = _mm_extract_epi32(idx, 3);\n+#else\n+    const int i1 = _mm_extract_epi16(idx, 2);\n+    const int i2 = _mm_extract_epi16(idx, 4);\n+    const int i3 = _mm_extract_epi16(idx, 6);\n+#endif\n+    return npyv_set_f32(table[i0], table[i1], table[i2], table[i3]);\n+}\n+NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_u32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+NPY_FINLINE npyv_s32 npyv_lut32_s32(const npy_int32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_s32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+\n+// uses vector as indexes into a table\n+// that contains 16 elements of float64.\n+NPY_FINLINE npyv_f64 npyv_lut16_f64(const double *table, npyv_u64 idx)\n+{\n+    const int i0 = _mm_cvtsi128_si32(idx);\n+#ifdef NPY_HAVE_SSE41\n+    const int i1 = _mm_extract_epi32(idx, 2);\n+#else\n+    const int i1 = _mm_extract_epi16(idx, 4);\n+#endif\n+    return npyv_set_f64(table[i0], table[i1]);\n+}\n+NPY_FINLINE npyv_u64 npyv_lut16_u64(const npy_uint64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_u64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+NPY_FINLINE npyv_s64 npyv_lut16_s64(const npy_int64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_s64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+\n #endif // _NPY_SIMD_SSE_MEMORY_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vsx/memory.h",
                "patch": "@@ -343,4 +343,41 @@ NPYV_IMPL_VSX_REST_PARTIAL_TYPES(f32, s32)\n NPYV_IMPL_VSX_REST_PARTIAL_TYPES(u64, s64)\n NPYV_IMPL_VSX_REST_PARTIAL_TYPES(f64, s64)\n \n+/*********************************\n+ * Lookup table\n+ *********************************/\n+// uses vector as indexes into a table\n+// that contains 32 elements of float32.\n+NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+{\n+    const unsigned i0 = vec_extract(idx, 0);\n+    const unsigned i1 = vec_extract(idx, 1);\n+    const unsigned i2 = vec_extract(idx, 2);\n+    const unsigned i3 = vec_extract(idx, 3);\n+    npyv_f32 r = vec_promote(table[i0], 0);\n+             r = vec_insert(table[i1], r, 1);\n+             r = vec_insert(table[i2], r, 2);\n+             r = vec_insert(table[i3], r, 3);\n+    return r;\n+}\n+NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_u32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+NPY_FINLINE npyv_s32 npyv_lut32_s32(const npy_int32 *table, npyv_u32 idx)\n+{ return npyv_reinterpret_s32_f32(npyv_lut32_f32((const float*)table, idx)); }\n+\n+// uses vector as indexes into a table\n+// that contains 16 elements of float64.\n+NPY_FINLINE npyv_f64 npyv_lut16_f64(const double *table, npyv_u64 idx)\n+{\n+    const unsigned i0 = vec_extract((npyv_u32)idx, 0);\n+    const unsigned i1 = vec_extract((npyv_u32)idx, 2);\n+    npyv_f64 r = vec_promote(table[i0], 0);\n+             r = vec_insert(table[i1], r, 1);\n+    return r;\n+}\n+NPY_FINLINE npyv_u64 npyv_lut16_u64(const npy_uint64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_u64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+NPY_FINLINE npyv_s64 npyv_lut16_s64(const npy_int64 *table, npyv_u64 idx)\n+{ return npyv_reinterpret_s64_f64(npyv_lut16_f64((const double*)table, idx)); }\n+\n #endif // _NPY_SIMD_VSX_MEMORY_H"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -209,6 +209,24 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n /**end repeat1**/\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_hyperbolic.dispatch.h\"\n+#endif\n+/**begin repeat\n+ *  #TYPE = FLOAT, DOUBLE#\n+ */\n+/**begin repeat1\n+ * #func = tanh#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@func@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n+// SVML\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_umath_fp.dispatch.h\"\n #endif"
            },
            {
                "filename": "numpy/core/src/umath/loops_hyperbolic.dispatch.c.src",
                "patch": "@@ -0,0 +1,384 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** (avx2 fma3) AVX512_SKX\n+ ** vsx2\n+ ** neon_vfpv4\n+ **/\n+#include \"numpy/npy_math.h\"\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+\n+#if NPY_SIMD_FMA3 // native support\n+/*\n+ * NOTE: The following implementation of tanh(f32, f64) have been converted from\n+ * Intel SVML to universal intrinsics, and the original code can be found in:\n+ *\n+ * - https://github.com/numpy/SVML/blob/main/linux/avx512/svml_z0_tanh_d_la.s\n+ * - https://github.com/numpy/SVML/blob/main/linux/avx512/svml_z0_tanh_s_la.s\n+ *\n+ * ALGORITHM DESCRIPTION:\n+ *\n+ *   NOTE: Since the hyperbolic tangent function is odd\n+ *         (tanh(x) = -tanh(-x)), below algorithm deals with the absolute\n+ *         value of the argument |x|: tanh(x) = sign(x) * tanh(|x|)\n+ *\n+ *   We use a table lookup method to compute tanh(|x|).\n+ *   The basic idea is to split the input range into a number of subintervals\n+ *   and to approximate tanh(.) with a polynomial on each of them.\n+ *\n+ *   IEEE SPECIAL CONDITIONS:\n+ *   x = [+,-]0, r = [+,-]0\n+ *   x = +Inf,   r = +1\n+ *   x = -Inf,   r = -1\n+ *   x = QNaN,   r = QNaN\n+ *   x = SNaN,   r = QNaN\n+ *\n+ *\n+ *  ALGORITHM DETAILS\n+ *\n+ *  SVML handel |x| > HUGE_THRESHOLD, INF and NaNs by scalar callout as following:\n+ *  1. check special cases\n+ *  2. return `+-1` for `|x| > HUGE_THRESHOLD`  otherwise return `x`\n+ *\n+ *  It wasn't clear to us the reason behind using callout instead of using\n+ *  AVX512 directly for single-precision.\n+ *  However, we saw it's better to use SIMD instead of following SVML.\n+ *\n+ *  Main path computations are organized as follows:\n+ *  Actually we split the interval [0, SATURATION_THRESHOLD)\n+ *  into a number of subintervals.  On each subinterval we approximate tanh(.)\n+ *   with a minimax polynomial of pre-defined degree. Polynomial coefficients\n+ *  are computed beforehand and stored in table. We also use\n+ *\n+ *       y := |x| + B,\n+ *\n+ *  here B depends on subinterval and is used to make argument\n+ *   closer to zero.\n+ *   We also add large fake interval [SATURATION_THRESHOLD, HUGE_THRESHOLD],\n+ *   where 1.0 + 0.0*y + 0.0*y^2 ... coefficients are stored - just to\n+ *   preserve main path computation logic but return 1.0 for all arguments.\n+ *\n+ *   Hence reconstruction looks as follows:\n+ *   we extract proper polynomial and range reduction coefficients\n+ *        (Pj and B), corresponding to subinterval, to which |x| belongs,\n+ *        and return\n+ *\n+ *       r := sign(x) * (P0 + P1 * y + ... + Pn * y^n)\n+ *\n+ *   NOTE: we use multiprecision technique to multiply and sum the first\n+ *         K terms of the polynomial. So Pj, j = 0..K are stored in\n+ *         table each as a pair of target precision numbers (Pj and PLj) to\n+ *         achieve wider than target precision.\n+ *\n+ */\n+#if NPY_SIMD_F64\n+static void\n+simd_tanh_f64(const double *src, npy_intp ssrc, double *dst, npy_intp sdst, npy_intp len)\n+{\n+    static const npy_uint64 NPY_DECL_ALIGNED(NPY_SIMD_WIDTH) lut16x18[] = {\n+        // 0\n+        0x0ull,                0x3fcc000000000000ull, 0x3fd4000000000000ull, 0x3fdc000000000000ull,\n+        0x3fe4000000000000ull, 0x3fec000000000000ull, 0x3ff4000000000000ull, 0x3ffc000000000000ull,\n+        0x4004000000000000ull, 0x400c000000000000ull, 0x4014000000000000ull, 0x401c000000000000ull,\n+        0x4024000000000000ull, 0x402c000000000000ull, 0x4034000000000000ull, 0x0ull,\n+        // 1\n+        0x0ull,                0x3fcb8fd0416a7c92ull, 0x3fd35f98a0ea650eull, 0x3fda5729ee488037ull,\n+        0x3fe1bf47eabb8f95ull, 0x3fe686650b8c2015ull, 0x3feb2523bb6b2deeull, 0x3fee1fbf97e33527ull,\n+        0x3fef9258260a71c2ull, 0x3feff112c63a9077ull, 0x3fefff419668df11ull, 0x3feffffc832750f2ull,\n+        0x3feffffffdc96f35ull, 0x3fefffffffffcf58ull, 0x3ff0000000000000ull, 0x3ff0000000000000ull,\n+        // 2\n+        0x3ff0000000000000ull, 0x3fee842ca3f08532ull, 0x3fed11574af58f1bull, 0x3fea945b9c24e4f9ull,\n+        0x3fe6284c3374f815ull, 0x3fe02500a09f8d6eull, 0x3fd1f25131e3a8c0ull, 0x3fbd22ca1c24a139ull,\n+        0x3f9b3afe1fba5c76ull, 0x3f6dd37d19b22b21ull, 0x3f27ccec13a9ef96ull, 0x3ecbe6c3f33250aeull,\n+        0x3e41b4865394f75full, 0x3d8853f01bda5f28ull, 0x3c73953c0197ef58ull, 0x0ull,\n+        // 3\n+        0xbbf0b3ea3fdfaa19ull, 0xbfca48aaeb53bc21ull, 0xbfd19921f4329916ull, 0xbfd5e0f09bef8011ull,\n+        0xbfd893b59c35c882ull, 0xbfd6ba7cb7576538ull, 0xbfce7291743d7555ull, 0xbfbb6d85a01efb80ull,\n+        0xbf9addae58c7141aull, 0xbf6dc59376c7aa19ull, 0xbf27cc5e74677410ull, 0xbecbe6c0e8b4cc87ull,\n+        0xbe41b486526b0565ull, 0xbd8853f01bef63a4ull, 0xbc73955be519be31ull, 0x0ull,\n+        // 4\n+        0xbfd5555555555555ull, 0xbfd183afc292ba11ull, 0xbfcc1a4b039c9bfaull, 0xbfc16e1e6d8d0be6ull,\n+        0xbf92426c751e48a2ull, 0x3fb4f152b2bad124ull, 0x3fbbba40cbef72beull, 0x3fb01ba038be6a3dull,\n+        0x3f916df44871efc8ull, 0x3f63c6869dfc8870ull, 0x3f1fb9aef915d828ull, 0x3ec299d1e27c6e11ull,\n+        0x3e379b5ddcca334cull, 0x3d8037f57bc62c9aull, 0x3c6a2d4b50a2cff7ull, 0x0ull,\n+        // 5\n+        0xbce6863ee44ed636ull, 0x3fc04dcd0476c75eull, 0x3fc43d3449a80f08ull, 0x3fc5c26f3699b7e7ull,\n+        0x3fc1a686f6ab2533ull, 0x3faf203c316ce730ull, 0xbf89c7a02788557cull, 0xbf98157e26e0d541ull,\n+        0xbf807b55c1c7d278ull, 0xbf53a18d5843190full, 0xbf0fb6bbc89b1a5bull, 0xbeb299c9c684a963ull,\n+        0xbe279b5dd4fb3d01ull, 0xbd7037f57ae72aa6ull, 0xbc5a2ca2bba78e86ull, 0x0ull,\n+        // 6\n+        0x3fc1111111112ab5ull, 0x3fb5c19efdfc08adull, 0x3fa74c98dc34fbacull, 0xbf790d6a8eff0a77ull,\n+        0xbfac3c021789a786ull, 0xbfae2196b7326859ull, 0xbf93a7a011ff8c2aull, 0x3f6e4709c7e8430eull,\n+        0x3f67682afa611151ull, 0x3f3ef2ee77717cbfull, 0x3ef95a4482f180b7ull, 0x3e9dc2c27da3b603ull,\n+        0x3e12e2afd9f7433eull, 0x3d59f320348679baull, 0x3c44b61d9bbcc940ull, 0x0ull,\n+        // 7\n+        0xbda1ea19ddddb3b4ull, 0xbfb0b8df995ce4dfull, 0xbfb2955cf41e8164ull, 0xbfaf9d05c309f7c6ull,\n+        0xbf987d27ccff4291ull, 0x3f8b2ca62572b098ull, 0x3f8f1cf6c7f5b00aull, 0x3f60379811e43dd5ull,\n+        0xbf4793826f78537eull, 0xbf2405695e36240full, 0xbee0e08de39ce756ull, 0xbe83d709ba5f714eull,\n+        0xbdf92e3fc5ee63e0ull, 0xbd414cc030f2110eull, 0xbc2ba022e8d82a87ull, 0x0ull,\n+        // 8\n+        0xbfaba1ba1990520bull, 0xbf96e37bba52f6fcull, 0x3ecff7df18455399ull, 0x3f97362834d33a4eull,\n+        0x3f9e7f8380184b45ull, 0x3f869543e7c420d4ull, 0xbf7326bd4914222aull, 0xbf5fc15b0a9d98faull,\n+        0x3f14cffcfa69fbb6ull, 0x3f057e48e5b79d10ull, 0x3ec33b66d7d77264ull, 0x3e66ac4e578b9b10ull,\n+        0x3ddcc74b8d3d5c42ull, 0x3d23c589137f92b4ull, 0x3c107f8e2c8707a1ull, 0x0ull,\n+        // 9\n+        0xbe351ca7f096011full, 0x3f9eaaf3320c3851ull, 0x3f9cf823fe761fc1ull, 0x3f9022271754ff1full,\n+        0xbf731fe77c9c60afull, 0xbf84a6046865ec7dull, 0xbf4ca3f1f2b9192bull, 0x3f4c77dee0afd227ull,\n+        0x3f04055bce68597aull, 0xbee2bf0cb4a71647ull, 0xbea31eaafe73efd5ull, 0xbe46abb02c4368edull,\n+        0xbdbcc749ca8079ddull, 0xbd03c5883836b9d2ull, 0xbbf07a5416264aecull, 0x0ull,\n+        // 10\n+        0x3f9664f94e6ac14eull, 0xbf94d3343bae39ddull, 0xbf7bc748e60df843ull, 0xbf8c89372b43ba85ull,\n+        0xbf8129a092de747aull, 0x3f60c85b4d538746ull, 0x3f5be9392199ec18ull, 0xbf2a0c68a4489f10ull,\n+        0xbf00462601dc2faaull, 0x3eb7b6a219dea9f4ull, 0x3e80cbcc8d4c5c8aull, 0x3e2425bb231a5e29ull,\n+        0x3d9992a4beac8662ull, 0x3ce191ba5ed3fb67ull, 0x3bc892450bad44c4ull, 0x0ull,\n+        // 11\n+        0xbea8c4c1fd7852feull, 0xbfccce16b1046f13ull, 0xbf81a16f224bb7b6ull, 0xbf62cbf00406bc09ull,\n+        0x3f75b29bb02cf69bull, 0x3f607df0f9f90c17ull, 0xbf4b852a6e0758d5ull, 0xbf0078c63d1b8445ull,\n+        0x3eec12eadd55be7aull, 0xbe6fa600f593181bull, 0xbe5a3c935dce3f7dull, 0xbe001c6d95e3ae96ull,\n+        0xbd74755a00ea1fd3ull, 0xbcbc1c6c063bb7acull, 0xbba3be9a4460fe00ull, 0x0ull,\n+        // 12\n+        0xbf822404577aa9ddull, 0x403d8b07f7a82aa3ull, 0xbf9f44ab92fbab0aull, 0x3fb2eac604473d6aull,\n+        0x3f45f87d903aaac8ull, 0xbf5e104671036300ull, 0x3f19bc98ddf0f340ull, 0x3f0d4304bc9246e8ull,\n+        0xbed13c415f7b9d41ull, 0xbe722b8d9720cdb0ull, 0x3e322666d739bec0ull, 0x3dd76a553d7e7918ull,\n+        0x3d4de0fa59416a39ull, 0x3c948716cf3681b4ull, 0x3b873f9f2d2fda99ull, 0x0ull,\n+        // 13\n+        0xbefdd99a221ed573ull, 0x4070593a3735bab4ull, 0xbfccab654e44835eull, 0x3fd13ed80037dbacull,\n+        0xbf6045b9076cc487ull, 0x3f2085ee7e8ac170ull, 0x3f23524622610430ull, 0xbeff12a6626911b4ull,\n+        0x3eab9008bca408afull, 0x3e634df71865f620ull, 0xbe05bb1bcf83ca73ull, 0xbdaf2ac143fb6762ull,\n+        0xbd23eae52a3dbf57ull, 0xbc6b5e3e9ca0955eull, 0xbb5eca68e2c1ba2eull, 0x0ull,\n+        // 14\n+        0x3f6e3be689423841ull, 0xc0d263511f5baac1ull, 0x40169f73b15ebe5cull, 0xc025c1dd41cd6cb5ull,\n+        0xbf58fd89fe05e0d1ull, 0x3f73f7af01d5af7aull, 0xbf1e40bdead17e6bull, 0x3ee224cd6c4513e5ull,\n+        0xbe24b645e68eeaa3ull, 0xbe4abfebfb72bc83ull, 0x3dd51c38f8695ed3ull, 0x3d8313ac38c6832bull,\n+        0x3cf7787935626685ull, 0x3c401ffc49c6bc29ull, 0xbabf0b21acfa52abull, 0x0ull,\n+        // 15\n+        0xbf2a1306713a4f3aull, 0xc1045e509116b066ull, 0x4041fab9250984ceull, 0xc0458d090ec3de95ull,\n+        0xbf74949d60113d63ull, 0x3f7c9fd6200d0adeull, 0x3f02cd40e0ad0a9full, 0xbe858ab8e019f311ull,\n+        0xbe792fa6323b7cf8ull, 0x3e2df04d67876402ull, 0xbd95c72be95e4d2cull, 0xbd55a89c30203106ull,\n+        0xbccad6b3bb9eff65ull, 0xbc12705ccd3dd884ull, 0xba8e0a4c47ae75f5ull, 0x0ull,\n+        // 16\n+        0xbf55d7e76dc56871ull, 0x41528c38809c90c7ull, 0xc076d57fb5190b02ull, 0x4085f09f888f8adaull,\n+        0x3fa246332a2fcba5ull, 0xbfb29d851a896fcdull, 0x3ed9065ae369b212ull, 0xbeb8e1ba4c98a030ull,\n+        0x3e6ffd0766ad4016ull, 0xbe0c63c29f505f5bull, 0xbd7fab216b9e0e49ull, 0x3d2826b62056aa27ull,\n+        0x3ca313e31762f523ull, 0x3bea37aa21895319ull, 0x3ae5c7f1fd871496ull, 0x0ull,\n+        // 17\n+        0x3f35e67ab76a26e7ull, 0x41848ee0627d8206ull, 0xc0a216d618b489ecull, 0x40a5b89107c8af4full,\n+        0x3fb69d8374520edaull, 0xbfbded519f981716ull, 0xbef02d288b5b3371ull, 0x3eb290981209c1a6ull,\n+        0xbe567e924bf5ff6eull, 0x3de3f7f7de6b0eb6ull, 0x3d69ed18bae3ebbcull, 0xbcf7534c4f3dfa71ull,\n+        0xbc730b73f1eaff20ull, 0xbbba2cff8135d462ull, 0xbab5a71b5f7d9035ull, 0x0ull\n+    };\n+    const int nlanes = npyv_nlanes_f64;\n+    const npyv_f64 qnan = npyv_setall_f64(NPY_NAN);\n+    for (; len > 0; len -= nlanes, src += ssrc*nlanes, dst += sdst*nlanes) {\n+        npyv_f64 x;\n+        if (ssrc == 1) {\n+            x = npyv_load_tillz_f64(src, len);\n+        } else {\n+            x = npyv_loadn_tillz_f64(src, ssrc, len);\n+        }\n+        npyv_s64 ndnan = npyv_and_s64(npyv_reinterpret_s64_f64(x), npyv_setall_s64(0x7ff8000000000000ll));\n+        // |x| > HUGE_THRESHOLD, INF and NaNs.\n+        npyv_b64 special_m = npyv_cmple_s64(ndnan, npyv_setall_s64(0x7fe0000000000000ll));\n+        npyv_b64 nnan_m = npyv_notnan_f64(x);\n+        npyv_s64 idxs = npyv_sub_s64(ndnan, npyv_setall_s64(0x3fc0000000000000ll));\n+        // no native 64-bit for max/min and its fine to use 32-bit max/min\n+        // since we're not crossing 32-bit edge\n+        npyv_s32 idxl = npyv_max_s32(npyv_reinterpret_s32_s64(idxs), npyv_zero_s32());\n+                 idxl = npyv_min_s32(idxl, npyv_setall_s32(0x780000));\n+        npyv_u64 idx  = npyv_shri_u64(npyv_reinterpret_u64_s32(idxl), 51);\n+\n+        npyv_f64 b = npyv_lut16_f64((const double*)lut16x18 + 16*0, idx);\n+        npyv_f64 c0 = npyv_lut16_f64((const double*)lut16x18 + 1*16, idx);\n+        npyv_f64 c1 = npyv_lut16_f64((const double*)lut16x18 + 2*16, idx);\n+        npyv_f64 c2 = npyv_lut16_f64((const double*)lut16x18 + 3*16, idx);\n+        npyv_f64 c3 = npyv_lut16_f64((const double*)lut16x18 + 4*16, idx);\n+        npyv_f64 c4 = npyv_lut16_f64((const double*)lut16x18 + 5*16, idx);\n+        npyv_f64 c5 = npyv_lut16_f64((const double*)lut16x18 + 6*16, idx);\n+        npyv_f64 c6 = npyv_lut16_f64((const double*)lut16x18 + 7*16, idx);\n+        npyv_f64 c7 = npyv_lut16_f64((const double*)lut16x18 + 8*16, idx);\n+        npyv_f64 c8 = npyv_lut16_f64((const double*)lut16x18 + 9*16, idx);\n+        npyv_f64 c9 = npyv_lut16_f64((const double*)lut16x18 + 10*16, idx);\n+        npyv_f64 c10 = npyv_lut16_f64((const double*)lut16x18 + 11*16, idx);\n+        npyv_f64 c11 = npyv_lut16_f64((const double*)lut16x18 + 12*16, idx);\n+        npyv_f64 c12 = npyv_lut16_f64((const double*)lut16x18 + 13*16, idx);\n+        npyv_f64 c13 = npyv_lut16_f64((const double*)lut16x18 + 14*16, idx);\n+        npyv_f64 c14 = npyv_lut16_f64((const double*)lut16x18 + 15*16, idx);\n+        npyv_f64 c15 = npyv_lut16_f64((const double*)lut16x18 + 16*16, idx);\n+        npyv_f64 c16 = npyv_lut16_f64((const double*)lut16x18 + 17*16, idx);\n+\n+        // no need to zerofy nans or avoid FP exceptions by NO_EXC like SVML does\n+        // since we're clearing the FP status anyway.\n+        npyv_f64 sign = npyv_and_f64(x, npyv_reinterpret_f64_s64(npyv_setall_s64(0x8000000000000000ull)));\n+        npyv_f64 y = npyv_sub_f64(npyv_abs_f64(x), b);\n+        npyv_f64 r = npyv_muladd_f64(c16, y, c15);\n+        r = npyv_muladd_f64(r, y, c14);\n+        r = npyv_muladd_f64(r, y, c13);\n+        r = npyv_muladd_f64(r, y, c12);\n+        r = npyv_muladd_f64(r, y, c11);\n+        r = npyv_muladd_f64(r, y, c10);\n+        r = npyv_muladd_f64(r, y, c9);\n+        r = npyv_muladd_f64(r, y, c8);\n+        r = npyv_muladd_f64(r, y, c7);\n+        r = npyv_muladd_f64(r, y, c6);\n+        r = npyv_muladd_f64(r, y, c5);\n+        r = npyv_muladd_f64(r, y, c4);\n+        r = npyv_muladd_f64(r, y, c3);\n+        r = npyv_muladd_f64(r, y, c2);\n+        r = npyv_muladd_f64(r, y, c1);\n+        r = npyv_muladd_f64(r, y, c0);\n+        // 1.0 if |x| > HUGE_THRESHOLD || INF\n+        r = npyv_select_f64(special_m, r, npyv_setall_f64(1.0));\n+        r = npyv_or_f64(r, sign);\n+        // qnan if nan\n+        r = npyv_select_f64(nnan_m, r, qnan);\n+        if (sdst == 1) {\n+            npyv_store_till_f64(dst, len, r);\n+        } else {\n+            npyv_storen_till_f64(dst, sdst, len, r);\n+        }\n+    }\n+}\n+#endif // NPY_SIMD_F64\n+static void\n+simd_tanh_f32(const float *src, npy_intp ssrc, float *dst, npy_intp sdst, npy_intp len)\n+{\n+    static const npy_uint32 NPY_DECL_ALIGNED(NPY_SIMD_WIDTH) lut32x8[] = {\n+        // 0\n+        0x0,        0x3d700000, 0x3d900000, 0x3db00000, 0x3dd00000, 0x3df00000, 0x3e100000, 0x3e300000,\n+        0x3e500000, 0x3e700000, 0x3e900000, 0x3eb00000, 0x3ed00000, 0x3ef00000, 0x3f100000, 0x3f300000,\n+        0x3f500000, 0x3f700000, 0x3f900000, 0x3fb00000, 0x3fd00000, 0x3ff00000, 0x40100000, 0x40300000,\n+        0x40500000, 0x40700000, 0x40900000, 0x40b00000, 0x40d00000, 0x40f00000, 0x41100000, 0x0,\n+        // 1\n+        0x0,        0x3d6fb9c9, 0x3d8fc35f, 0x3daf9169, 0x3dcf49ab, 0x3deee849, 0x3e0f0ee8, 0x3e2e4984,\n+        0x3e4d2f8e, 0x3e6bb32e, 0x3e8c51cd, 0x3ea96163, 0x3ec543f1, 0x3edfd735, 0x3f028438, 0x3f18abf0,\n+        0x3f2bc480, 0x3f3bec1c, 0x3f4f2e5b, 0x3f613c53, 0x3f6ce37d, 0x3f743c4f, 0x3f7a5feb, 0x3f7dea85,\n+        0x3f7f3b3d, 0x3f7fb78c, 0x3f7fefd4, 0x3f7ffdd0, 0x3f7fffb4, 0x3f7ffff6, 0x3f7fffff, 0x3f800000,\n+        // 2\n+        0x3f800000, 0x3f7f1f84, 0x3f7ebd11, 0x3f7e1e5f, 0x3f7d609f, 0x3f7c842d, 0x3f7b00e5, 0x3f789580,\n+        0x3f75b8ad, 0x3f726fd9, 0x3f6cc59b, 0x3f63fb92, 0x3f59ff97, 0x3f4f11d7, 0x3f3d7573, 0x3f24f360,\n+        0x3f0cbfe7, 0x3eec1a69, 0x3eb0a801, 0x3e6753a2, 0x3e132f1a, 0x3db7e7d3, 0x3d320845, 0x3c84d3d4,\n+        0x3bc477b7, 0x3b10d3da, 0x3a01601e, 0x388c1a3b, 0x3717b0da, 0x35a43bce, 0x338306c6, 0x0,\n+        // 3\n+        0xb0343c7b, 0xbd6ee69d, 0xbd8f0da7, 0xbdae477d, 0xbdcd2a1f, 0xbdeba80d, 0xbe0c443b, 0xbe293cf3,\n+        0xbe44f282, 0xbe5f3651, 0xbe81c7c0, 0xbe96d7ca, 0xbea7fb8e, 0xbeb50e9e, 0xbec12efe, 0xbec4be92,\n+        0xbebce070, 0xbead510e, 0xbe8ef7d6, 0xbe4b8704, 0xbe083237, 0xbdaf7449, 0xbd2e1ec4, 0xbc83bf06,\n+        0xbbc3e0b5, 0xbb10aadc, 0xba0157db, 0xb88c18f2, 0xb717b096, 0xb5a43bae, 0xb383012c, 0x0,\n+        // 4\n+        0xbeaaaaa5, 0xbeab0612, 0xbea7f01f, 0xbea4e120, 0xbea387b7, 0xbea15962, 0xbe9d57f7, 0xbe976b5a,\n+        0xbe90230d, 0xbe880dff, 0xbe7479b3, 0xbe4c3d88, 0xbe212482, 0xbdeb8cba, 0xbd5e78ad, 0x3c6b5e6e,\n+        0x3d839143, 0x3dc21ee1, 0x3de347af, 0x3dcbec96, 0x3d99ef2d, 0x3d542ea1, 0x3cdde701, 0x3c2cca67,\n+        0x3b81cb27, 0x3ac073a1, 0x39ac3032, 0x383a94d9, 0x36ca081d, 0x355abd4c, 0x332b3cb6, 0x0,\n+        // 5\n+        0xb76dd6b9, 0xbe1c276d, 0x3c1dcf2f, 0x3dc1a78d, 0x3d96f985, 0x3da2b61b, 0x3dc13397, 0x3dd2f670,\n+        0x3df48a0a, 0x3e06c5a8, 0x3e1a3aba, 0x3e27c405, 0x3e2e78d0, 0x3e2c3e44, 0x3e1d3097, 0x3df4a8f4,\n+        0x3da38508, 0x3d31416a, 0x3b562657, 0xbcaeeac9, 0xbcce9419, 0xbcaaeac4, 0xbc49e7d0, 0xbba71ddd,\n+        0xbb003b0e, 0xba3f9a05, 0xb92c08a7, 0xb7ba9232, 0xb64a0b0f, 0xb4dac169, 0xb2ab78ac, 0x0,\n+        // 6\n+        0x3e0910e9, 0x43761143, 0x4165ecdc, 0xc190f756, 0xc08c097d, 0xc02ba813, 0xbf7f6bda, 0x3f2b1dc0,\n+        0x3ece105d, 0x3f426a94, 0xbadb0dc4, 0x3da43b17, 0xbd51ab88, 0xbcaea23d, 0xbd3b6d8d, 0xbd6caaad,\n+        0xbd795bed, 0xbd5fddda, 0xbd038f3b, 0xbc1cad63, 0x3abb4766, 0x3b95f10b, 0x3b825873, 0x3afaea66,\n+        0x3a49f878, 0x39996bf3, 0x388f3e6c, 0x371bb0e3, 0x35a8a5e6, 0x34369b17, 0x322487b0, 0x0,\n+        // 7\n+        0xbc0e2f66, 0x460bda12, 0x43d638ef, 0xc3e11c3e, 0xc2baa4e9, 0xc249da2d, 0xc1859b82, 0x40dd5b57,\n+        0x40494640, 0x40c730a8, 0xbf0f160e, 0x3e30e76f, 0xbea81387, 0xbdb26a1c, 0xbd351e57, 0xbb4c01a0,\n+        0x3c1d7bfb, 0x3c722cd1, 0x3c973f1c, 0x3c33a31b, 0x3b862ef4, 0x3a27b3d0, 0xba3b5907, 0xba0efc22,\n+        0xb97f9f0f, 0xb8c8af50, 0xb7bdddfb, 0xb64f2950, 0xb4e085b1, 0xb3731dfa, 0xb15a1f04, 0x0\n+    };\n+\n+    const int nlanes = npyv_nlanes_f32;\n+    const npyv_f32 qnan = npyv_setall_f32(NPY_NANF);\n+    for (; len > 0; len -= nlanes, src += ssrc*nlanes, dst += sdst*nlanes) {\n+        npyv_f32 x;\n+        if (ssrc == 1) {\n+            x = npyv_load_tillz_f32(src, len);\n+        } else {\n+            x = npyv_loadn_tillz_f32(src, ssrc, len);\n+        }\n+        npyv_s32 ndnan = npyv_and_s32(npyv_reinterpret_s32_f32(x), npyv_setall_s32(0x7fe00000));\n+        // check |x| > HUGE_THRESHOLD, INF and NaNs.\n+        npyv_b32 special_m = npyv_cmple_s32(ndnan, npyv_setall_s32(0x7f000000));\n+        npyv_b32 nnan_m = npyv_notnan_f32(x);\n+        npyv_s32 idxs = npyv_sub_s32(ndnan, npyv_setall_s32(0x3d400000));\n+                 idxs = npyv_max_s32(idxs, npyv_zero_s32());\n+                 idxs = npyv_min_s32(idxs, npyv_setall_s32(0x3e00000));\n+        npyv_u32 idx  = npyv_shri_u32(npyv_reinterpret_u32_s32(idxs), 21);\n+\n+        npyv_f32 b  = npyv_lut32_f32((const float*)lut32x8 + 32*0, idx);\n+        npyv_f32 c0 = npyv_lut32_f32((const float*)lut32x8 + 32*1, idx);\n+        npyv_f32 c1 = npyv_lut32_f32((const float*)lut32x8 + 32*2, idx);\n+        npyv_f32 c2 = npyv_lut32_f32((const float*)lut32x8 + 32*3, idx);\n+        npyv_f32 c3 = npyv_lut32_f32((const float*)lut32x8 + 32*4, idx);\n+        npyv_f32 c4 = npyv_lut32_f32((const float*)lut32x8 + 32*5, idx);\n+        npyv_f32 c5 = npyv_lut32_f32((const float*)lut32x8 + 32*6, idx);\n+        npyv_f32 c6 = npyv_lut32_f32((const float*)lut32x8 + 32*7, idx);\n+\n+        // no need to zerofy nans or avoid FP exceptions by NO_EXC like SVML does\n+        // since we're clearing the FP status anyway.\n+        npyv_f32 sign = npyv_and_f32(x, npyv_reinterpret_f32_u32(npyv_setall_u32(0x80000000)));\n+        npyv_f32 y = npyv_sub_f32(npyv_abs_f32(x), b);\n+        npyv_f32 r = npyv_muladd_f32(c6, y, c5);\n+        r = npyv_muladd_f32(r, y, c4);\n+        r = npyv_muladd_f32(r, y, c3);\n+        r = npyv_muladd_f32(r, y, c2);\n+        r = npyv_muladd_f32(r, y, c1);\n+        r = npyv_muladd_f32(r, y, c0);\n+        // 1.0 if |x| > HUGE_THRESHOLD || INF\n+        r = npyv_select_f32(special_m, r, npyv_setall_f32(1.0f));\n+        r = npyv_or_f32(r, sign);\n+        // qnan if nan\n+        r = npyv_select_f32(nnan_m, r, qnan);\n+        if (sdst == 1) {\n+            npyv_store_till_f32(dst, len, r);\n+        } else {\n+            npyv_storen_till_f32(dst, sdst, len, r);\n+        }\n+    }\n+}\n+#endif // NPY_SIMD_FMA3\n+\n+/**begin repeat\n+ * #TYPE = FLOAT, DOUBLE#\n+ * #type = float, double#\n+ * #sfx  = f32,   f64#\n+ * #ssfx = f,     #\n+ * #simd = NPY_SIMD_FMA3, NPY_SIMD_FMA3 && NPY_SIMD_F64#\n+ */\n+/**begin repeat1\n+ *  #func = tanh#\n+ *  #simd_req_clear = 1#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+    const @type@ *src = (@type@*)args[0];\n+          @type@ *dst = (@type@*)args[1];\n+\n+    const int lsize = sizeof(src[0]);\n+    const npy_intp ssrc = steps[0] / lsize;\n+    const npy_intp sdst = steps[1] / lsize;\n+    npy_intp len = dimensions[0];\n+    assert(len <= 1 || (steps[0] % lsize == 0 && steps[1] % lsize == 0));\n+#if @simd@\n+    if (is_mem_overlap(src, steps[0], dst, steps[1], len) ||\n+        !npyv_loadable_stride_@sfx@(ssrc) || !npyv_storable_stride_@sfx@(sdst)\n+    ) {\n+        for (; len > 0; --len, src += ssrc, dst += sdst) {\n+            simd_@func@_@sfx@(src, 1, dst, 1, 1);\n+        }\n+    } else {\n+        simd_@func@_@sfx@(src, ssrc, dst, sdst, len);\n+    }\n+    npyv_cleanup();\n+    #if @simd_req_clear@\n+        npy_clear_floatstatus_barrier((char*)dimensions);\n+    #endif\n+#else\n+    for (; len > 0; --len, src += ssrc, dst += sdst) {\n+        const @type@ src0 = *src;\n+        *dst = npy_@func@@ssfx@(src0);\n+    }\n+#endif\n+}\n+/**end repeat1**/\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/loops_umath_fp.dispatch.c.src",
                "patch": "@@ -14,8 +14,8 @@\n  * #func_suffix = f16, 8#\n  */\n /**begin repeat1\n- * #func = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, asin, acos, atan, sinh, cosh, asinh, acosh, atanh#\n- * #default_val = 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0#\n+ * #func = exp2, log2, log10, expm1, log1p, cbrt, tan, asin, acos, atan, sinh, cosh, asinh, acosh, atanh#\n+ * #default_val = 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0#\n  */\n static void\n simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src, npy_intp ssrc,\n@@ -83,8 +83,8 @@ simd_@func@_f64(const double *src, npy_intp ssrc,\n  *  #sfx  = f64, f32#\n  */\n /**begin repeat1\n- *  #func = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, arcsin, arccos, arctan, sinh, cosh, arcsinh, arccosh, arctanh#\n- *  #intrin = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, asin, acos, atan, sinh, cosh, asinh, acosh, atanh#\n+ *  #func = exp2, log2, log10, expm1, log1p, cbrt, tan, arcsin, arccos, arctan, sinh, cosh, arcsinh, arccosh, arctanh#\n+ *  #intrin = exp2, log2, log10, expm1, log1p, cbrt, tan, asin, acos, atan, sinh, cosh, asinh, acosh, atanh#\n  */\n NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))"
            },
            {
                "filename": "numpy/core/tests/test_simd.py",
                "patch": "@@ -627,6 +627,27 @@ def test_memory_noncont_partial_store(self):\n                 assert storen_till[64:] == data_till\n                 assert storen_till[:64] == [127]*64 # detect overflow\n \n+    @pytest.mark.parametrize(\"intrin, table_size, elsize\", [\n+        (\"self.lut32\", 32, 32),\n+        (\"self.lut16\", 16, 64)\n+    ])\n+    def test_lut(self, intrin, table_size, elsize):\n+        \"\"\"\n+        Test lookup table intrinsics:\n+            npyv_lut32_##sfx\n+            npyv_lut16_##sfx\n+        \"\"\"\n+        if elsize != self._scalar_size():\n+            return\n+        intrin = eval(intrin)\n+        idx_itrin = getattr(self.npyv, f\"setall_u{elsize}\")\n+        table = range(0, table_size)\n+        for i in table:\n+            broadi = self.setall(i)\n+            idx = idx_itrin(i)\n+            lut = intrin(table, idx)\n+            assert lut == broadi\n+\n     def test_misc(self):\n         broadcast_zero = self.zero()\n         assert broadcast_zero == [0] * self.nlanes"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21103,
        "body": "Note, this really is a very early draft.  For myself, I have not\r\neven 100% made up my mind that this is actually the best approach.\r\n\r\nHowever, to solicitate feedback and input, I would like to put it\r\nout there (and have a nice rendered version).\r\n\r\nFor those mainly interested in the possible different approaches\r\n(i.e. making decisions), the \"Alternatives\" section is probably most\r\ninteresting.\r\n\r\nPlease make a note of blatend errors or unnecessary repitition, but\r\ndon't be too surprised if you see them, I will not be surprised if\r\nthis needs to be redone from scratch eventually.\r\n\r\n---\r\n\r\nArtifact link: https://25381-908607-gh.circle-artifacts.com/0/doc/neps/_build/html/nep-0050-scalar-promotion.html   (Updated 2022-03-16)\r\n\r\n(early) branch for testing: https://github.com/seberg/numpy/tree/weak-scalars  (Note that this branch currently does not attempt to give warnings, or implement the `uint8_arr + 1000 -> error` behavior, which is more complicated.  It should implement the general \"weak\" logic though.",
        "changed_files": [
            {
                "filename": "doc/neps/_static/nep-0050-promotion-no-fonts.svg",
                "patch": "No changes"
            },
            {
                "filename": "doc/neps/_static/nep-0050-promotion.svg",
                "patch": "@@ -0,0 +1,685 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n+<svg\n+   xmlns:dc=\"http://purl.org/dc/elements/1.1/\"\n+   xmlns:cc=\"http://creativecommons.org/ns#\"\n+   xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n+   xmlns:svg=\"http://www.w3.org/2000/svg\"\n+   xmlns=\"http://www.w3.org/2000/svg\"\n+   xmlns:sodipodi=\"http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd\"\n+   xmlns:inkscape=\"http://www.inkscape.org/namespaces/inkscape\"\n+   width=\"346.21313mm\"\n+   height=\"138.47121mm\"\n+   viewBox=\"0 0 346.21313 138.47116\"\n+   version=\"1.1\"\n+   id=\"svg5\"\n+   sodipodi:docname=\"nep-0050-promotion.svg\"\n+   inkscape:version=\"1.0.2 (e86c870879, 2021-01-15)\">\n+  <metadata\n+     id=\"metadata125\">\n+    <rdf:RDF>\n+      <cc:Work\n+         rdf:about=\"\">\n+        <dc:format>image/svg+xml</dc:format>\n+        <dc:type\n+           rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n+      </cc:Work>\n+    </rdf:RDF>\n+  </metadata>\n+  <sodipodi:namedview\n+     id=\"namedview109\"\n+     pagecolor=\"#ffffff\"\n+     bordercolor=\"#666666\"\n+     borderopacity=\"1.0\"\n+     inkscape:pageshadow=\"2\"\n+     inkscape:pageopacity=\"0.0\"\n+     inkscape:pagecheckerboard=\"0\"\n+     inkscape:document-units=\"mm\"\n+     showgrid=\"false\"\n+     fit-margin-right=\"1\"\n+     lock-margins=\"true\"\n+     fit-margin-top=\"1\"\n+     fit-margin-left=\"1\"\n+     fit-margin-bottom=\"1\"\n+     inkscape:zoom=\"1.1024096\"\n+     inkscape:cx=\"643.13663\"\n+     inkscape:cy=\"60.322406\"\n+     inkscape:window-width=\"2560\"\n+     inkscape:window-height=\"1376\"\n+     inkscape:window-x=\"0\"\n+     inkscape:window-y=\"0\"\n+     inkscape:window-maximized=\"1\"\n+     inkscape:current-layer=\"layer1\"\n+     inkscape:document-rotation=\"0\" />\n+  <defs\n+     id=\"defs2\">\n+    <inkscape:path-effect\n+       effect=\"powerclip\"\n+       id=\"path-effect4411\"\n+       is_visible=\"true\"\n+       lpeversion=\"1\"\n+       inverse=\"true\"\n+       flatten=\"false\"\n+       hide_clip=\"false\"\n+       message=\"Use fill-rule evenodd on &lt;b&gt;fill and stroke&lt;/b&gt; dialog if no flatten result after convert clip to paths.\" />\n+    <marker\n+       style=\"overflow:visible\"\n+       id=\"Arrow2Lend\"\n+       refX=\"0\"\n+       refY=\"0\"\n+       orient=\"auto\">\n+      <path\n+         transform=\"matrix(-1.1,0,0,-1.1,-1.1,0)\"\n+         d=\"M 8.7185878,4.0337352 -2.2072895,0.01601326 8.7185884,-4.0017078 c -1.7454984,2.3720609 -1.7354408,5.6174519 -6e-7,8.035443 z\"\n+         style=\"fill:context-stroke;fill-rule:evenodd;stroke:context-stroke;stroke-width:0.625;stroke-linejoin:round\"\n+         id=\"path55505\" />\n+    </marker>\n+    <marker\n+       style=\"overflow:visible\"\n+       id=\"Arrow1Lend\"\n+       refX=\"0\"\n+       refY=\"0\"\n+       orient=\"auto\">\n+      <path\n+         transform=\"matrix(-0.8,0,0,-0.8,-10,0)\"\n+         style=\"fill:context-stroke;fill-rule:evenodd;stroke:context-stroke;stroke-width:1pt\"\n+         d=\"M 0,0 5,-5 -12.5,0 5,5 Z\"\n+         id=\"path55487\" />\n+    </marker>\n+    <clipPath\n+       clipPathUnits=\"userSpaceOnUse\"\n+       id=\"clipPath3010\">\n+      <ellipse\n+         style=\"display:none;fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:0.999998;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+         id=\"ellipse3012\"\n+         cx=\"48.04174\"\n+         cy=\"138.30666\"\n+         rx=\"21.353086\"\n+         ry=\"7.1176949\" />\n+      <path\n+         id=\"lpe_path-effect3014\"\n+         style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:0.999998;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+         class=\"powerclip\"\n+         d=\"M 42.339369,84.989221 H 105.46443 V 167.47381 H 42.339369 Z m 27.937395,53.317439 a 22.235023,7.4619589 0 0 0 -22.235024,-7.46196 22.235023,7.4619589 0 0 0 -22.235023,7.46196 22.235023,7.4619589 0 0 0 22.235023,7.46195 22.235023,7.4619589 0 0 0 22.235024,-7.46195 z\" />\n+    </clipPath>\n+    <clipPath\n+       clipPathUnits=\"userSpaceOnUse\"\n+       id=\"clipPath5608\">\n+      <ellipse\n+         style=\"display:none;fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:0.749999;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+         id=\"ellipse5610\"\n+         cx=\"48.04174\"\n+         cy=\"138.30666\"\n+         rx=\"22.235023\"\n+         ry=\"7.4619589\" />\n+      <path\n+         id=\"lpe_path-effect5612\"\n+         style=\"fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:0.749999;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+         class=\"powerclip\"\n+         d=\"M 42.339369,84.989221 H 105.46443 V 167.47381 H 42.339369 Z m 27.937395,53.317439 a 22.235023,7.4619589 0 0 0 -22.235024,-7.46196 22.235023,7.4619589 0 0 0 -22.235023,7.46196 22.235023,7.4619589 0 0 0 22.235023,7.46195 22.235023,7.4619589 0 0 0 22.235024,-7.46195 z\" />\n+    </clipPath>\n+    <clipPath\n+       clipPathUnits=\"userSpaceOnUse\"\n+       id=\"clipPath4407\">\n+      <ellipse\n+         style=\"display:none;fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:0.999998;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+         id=\"ellipse4409\"\n+         cx=\"48.04174\"\n+         cy=\"138.30666\"\n+         rx=\"24.871597\"\n+         ry=\"8.4035168\"\n+         d=\"m 72.913338,138.30666 a 24.871597,8.4035168 0 0 1 -24.871598,8.40351 24.871597,8.4035168 0 0 1 -24.871597,-8.40351 24.871597,8.4035168 0 0 1 24.871597,-8.40352 24.871597,8.4035168 0 0 1 24.871598,8.40352 z\" />\n+      <path\n+         id=\"lpe_path-effect4411\"\n+         style=\"fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:0.999998;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+         class=\"powerclip\"\n+         d=\"m 54.183708,85.237775 h 11 v 81.654565 h -11 z m 18.72963,53.068885 a 24.871597,8.4035168 0 0 0 -24.871598,-8.40352 24.871597,8.4035168 0 0 0 -24.871597,8.40352 24.871597,8.4035168 0 0 0 24.871597,8.40351 24.871597,8.4035168 0 0 0 24.871598,-8.40351 z\" />\n+    </clipPath>\n+  </defs>\n+  <g\n+     id=\"layer1\"\n+     transform=\"translate(215.26911,-51.42495)\">\n+    <path\n+       style=\"fill:none;fill-opacity:0.482353;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:0.795245\"\n+       d=\"M 59.683708,161.89234 V 90.237775\"\n+       id=\"path1886\"\n+       sodipodi:nodetypes=\"cc\"\n+       clip-path=\"url(#clipPath4407)\"\n+       inkscape:path-effect=\"#path-effect4411\"\n+       inkscape:original-d=\"M 59.683708,161.89234 V 90.237775\" />\n+    <rect\n+       style=\"fill:#ececec;fill-opacity:1;stroke:none;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:128.863;stroke-opacity:1\"\n+       id=\"rect27474\"\n+       width=\"316.13\"\n+       height=\"26.332268\"\n+       x=\"-186.18567\"\n+       y=\"52.42495\"\n+       ry=\"4.4001865\"\n+       rx=\"4.4001865\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:0.741176\"\n+       d=\"M 59.683708,114.4133 48.607336,138.30666 8.429198,162.2584 m 51.25451,-47.8451 -63.4414082,24.21428 -39.9012598,23.1813 28.100481,-46.5597 -78.491938,47.71915 27.310068,-47.71915 -39.201961,22.46163\"\n+       id=\"path51595\"\n+       sodipodi:nodetypes=\"cccccccccc\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1\"\n+       d=\"M 48.607336,162.14732 H -153.07677\"\n+       id=\"path98273\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1\"\n+       d=\"M 48.607336,138.30666 H -153.07677\"\n+       id=\"path98096\" />\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse18346\"\n+       cx=\"-94.47477\"\n+       cy=\"162.14732\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse11160\"\n+       cx=\"-106.04179\"\n+       cy=\"138.30666\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1\"\n+       d=\"M 99.851673,114.4661 H -153.07677\"\n+       id=\"path98094\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:0.741176\"\n+       d=\"M -15.558479,90.754771 V 115.24918\"\n+       id=\"path53983\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1\"\n+       d=\"M 99.851673,90.625336 H -153.07677\"\n+       id=\"path97397\" />\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse1674\"\n+       cx=\"-14.886699\"\n+       cy=\"90.625336\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <path\n+       style=\"fill:none;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:0.741176\"\n+       d=\"M -153.07677,91.853869 V 153.30979\"\n+       id=\"path54445\" />\n+    <g\n+       id=\"g50892\"\n+       transform=\"translate(-33.400505,2.4455707)\"\n+       style=\"stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\" />\n+    <g\n+       id=\"g49473\"\n+       transform=\"translate(17.781365,2.4455707)\"\n+       style=\"stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-29.107145\"\n+       y=\"91.994759\"\n+       id=\"text1678\"><tspan\n+         id=\"tspan1676\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-29.107145\"\n+         y=\"91.994759\">complex64</tspan></text>\n+    <g\n+       id=\"g4360\">\n+      <g\n+         id=\"g4342\">\n+        <rect\n+           style=\"fill:#ffdc86;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+           id=\"rect2223\"\n+           width=\"46.223087\"\n+           height=\"12.299\"\n+           x=\"-176.18831\"\n+           y=\"84.475945\"\n+           rx=\"4.6271939\"\n+           ry=\"4.6271939\" />\n+        <text\n+           xml:space=\"preserve\"\n+           style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+           x=\"-174.05717\"\n+           y=\"91.994759\"\n+           id=\"text9259\"><tspan\n+             id=\"tspan9257\"\n+             style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+             x=\"-174.05717\"\n+             y=\"91.994759\"><tspan\n+   style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-family:Lato;-inkscape-font-specification:Lato\"\n+   id=\"tspan24730\">Python</tspan> complex</tspan></text>\n+      </g>\n+      <g\n+         id=\"g4348\">\n+        <rect\n+           style=\"fill:#ffdc86;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+           id=\"rect12259\"\n+           width=\"46.223087\"\n+           height=\"12.299\"\n+           x=\"-176.18831\"\n+           y=\"108.3166\"\n+           rx=\"4.6271939\"\n+           ry=\"4.6271939\" />\n+        <text\n+           xml:space=\"preserve\"\n+           style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+           x=\"-174.05717\"\n+           y=\"115.77239\"\n+           id=\"text12263\"><tspan\n+             id=\"tspan12261\"\n+             style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+             x=\"-174.05717\"\n+             y=\"115.77239\"><tspan\n+   style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-family:Lato;-inkscape-font-specification:Lato\"\n+   id=\"tspan29428\">Python</tspan> float</tspan></text>\n+      </g>\n+    </g>\n+    <rect\n+       style=\"fill:#ffdc86;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"rect13393\"\n+       width=\"46.223087\"\n+       height=\"36.139999\"\n+       x=\"-176.18831\"\n+       y=\"132.15694\"\n+       rx=\"4.6271939\"\n+       ry=\"4.6271935\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-174.05717\"\n+       y=\"151.85147\"\n+       id=\"text13397\"><tspan\n+         id=\"tspan13395\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-174.05717\"\n+         y=\"151.85147\"><tspan\n+   style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-family:Lato;-inkscape-font-specification:Lato\"\n+   id=\"tspan31512\">Python</tspan> int</tspan></text>\n+    <path\n+       style=\"fill:none;fill-opacity:0.482353;stroke:#00b200;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:0.795245\"\n+       d=\"M 112.03144,112.64983 V 89.989221\"\n+       id=\"path1279\" />\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"path866\"\n+       cx=\"111.46992\"\n+       cy=\"90.625336\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"94.110794\"\n+       y=\"91.994759\"\n+       id=\"text1588\"><tspan\n+         id=\"tspan1586\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"94.110794\"\n+         y=\"91.994759\">clongdouble</tspan></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+       id=\"ellipse1668\"\n+       cx=\"60.108749\"\n+       cy=\"90.625336\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"44.301254\"\n+       y=\"91.994759\"\n+       id=\"text1672\"><tspan\n+         id=\"tspan1670\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"44.301254\"\n+         y=\"91.994759\">complex128</tspan><tspan\n+         style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';stroke-width:0.264583\"\n+         x=\"44.301254\"\n+         y=\"97.617149\"\n+         id=\"tspan1708\" /></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse6006\"\n+       cx=\"111.46992\"\n+       cy=\"114.46599\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"95.772598\"\n+       y=\"115.8354\"\n+       id=\"text6010\"><tspan\n+         id=\"tspan6008\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"95.772598\"\n+         y=\"115.8354\">longdouble</tspan></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+       id=\"ellipse6012\"\n+       cx=\"60.108749\"\n+       cy=\"114.46599\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"49.024353\"\n+       y=\"116.17561\"\n+       id=\"text6018\"><tspan\n+         id=\"tspan6014\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"49.024353\"\n+         y=\"116.17561\">float64</tspan><tspan\n+         style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';stroke-width:0.264583\"\n+         x=\"49.024353\"\n+         y=\"121.798\"\n+         id=\"tspan6016\" /></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse6020\"\n+       cx=\"-14.886699\"\n+       cy=\"114.46599\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-25.891085\"\n+       y=\"116.17561\"\n+       id=\"text6024\"><tspan\n+         id=\"tspan6022\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-25.891085\"\n+         y=\"116.17561\">float32</tspan></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse9494\"\n+       cx=\"-66.247871\"\n+       cy=\"114.46599\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <g\n+       id=\"g27273\"\n+       transform=\"translate(4.583686,-3.6978503)\">\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+         x=\"-81.910706\"\n+         y=\"119.87347\"\n+         id=\"text9498\"><tspan\n+           id=\"tspan9496\"\n+           style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+           x=\"-81.910706\"\n+           y=\"119.87347\">float16</tspan></text>\n+    </g>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:#ffdc86;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:3.75;stroke-opacity:1\"\n+       id=\"ellipse11140\"\n+       cx=\"48.04174\"\n+       cy=\"138.30666\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"40.052734\"\n+       y=\"139.67607\"\n+       id=\"text11146\"><tspan\n+         id=\"tspan11142\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"40.052734\"\n+         y=\"139.67607\">int64</tspan><tspan\n+         style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';stroke-width:0.264583\"\n+         x=\"40.052734\"\n+         y=\"145.29846\"\n+         id=\"tspan11144\" /></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse11148\"\n+       cx=\"-3.3194337\"\n+       cy=\"138.30666\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <g\n+       id=\"g27258\"\n+       transform=\"translate(19.576118,-7.1774646)\">\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+         x=\"-30.804546\"\n+         y=\"146.85353\"\n+         id=\"text11152\"><tspan\n+           id=\"tspan11150\"\n+           style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+           x=\"-30.804546\"\n+           y=\"146.85353\">int32</tspan></text>\n+    </g>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse11154\"\n+       cx=\"-54.680611\"\n+       cy=\"138.30666\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-62.664368\"\n+       y=\"139.67607\"\n+       id=\"text11158\"><tspan\n+         id=\"tspan11156\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-62.664368\"\n+         y=\"139.67607\">int16</tspan></text>\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-112.46998\"\n+       y=\"139.67607\"\n+       id=\"text11164\"><tspan\n+         id=\"tspan11162\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-112.46998\"\n+         y=\"139.67607\">int8</tspan></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse18326\"\n+       cx=\"59.608749\"\n+       cy=\"162.14732\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"50.094345\"\n+       y=\"163.51674\"\n+       id=\"text18332\"><tspan\n+         id=\"tspan18328\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"50.094345\"\n+         y=\"163.51674\">uint64</tspan><tspan\n+         style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.23333px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';stroke-width:0.264583\"\n+         x=\"50.094345\"\n+         y=\"169.13913\"\n+         id=\"tspan18330\" /></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse18334\"\n+       cx=\"8.2475758\"\n+       cy=\"162.14732\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-1.1868176\"\n+       y=\"163.51674\"\n+       id=\"text18338\"><tspan\n+         id=\"tspan18336\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-1.1868176\"\n+         y=\"163.51674\">uint32</tspan></text>\n+    <ellipse\n+       style=\"fill:#8cc9e1;fill-opacity:1;stroke:none;stroke-width:6.819;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:13.638, 13.638;stroke-dashoffset:34.095\"\n+       id=\"ellipse18340\"\n+       cx=\"-43.113602\"\n+       cy=\"162.14732\"\n+       rx=\"18.448826\"\n+       ry=\"6.1496081\" />\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-52.622757\"\n+       y=\"163.51674\"\n+       id=\"text18344\"><tspan\n+         id=\"tspan18342\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-52.622757\"\n+         y=\"163.51674\">uint16</tspan></text>\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:5.11528px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code';letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-102.42836\"\n+       y=\"163.51674\"\n+       id=\"text18350\"><tspan\n+         id=\"tspan18348\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:5.11528px;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';stroke-width:0.264583\"\n+         x=\"-102.42836\"\n+         y=\"163.51674\">uint8</tspan></text>\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-size:10.5833px;line-height:1.25;font-family:Lato;-inkscape-font-specification:Lato;letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-156.69156\"\n+       y=\"-206.17818\"\n+       id=\"text30437\"\n+       transform=\"rotate(-90)\"><tspan\n+         id=\"tspan30435\"\n+         style=\"font-size:10.5833px;stroke-width:0.264583\"\n+         x=\"-156.69156\"\n+         y=\"-206.17818\">DType \u201ckind\u201d</tspan></text>\n+    <text\n+       xml:space=\"preserve\"\n+       style=\"font-size:10.5833px;line-height:1.25;font-family:Lato;-inkscape-font-specification:Lato;letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+       x=\"-66.553741\"\n+       y=\"187.08112\"\n+       id=\"text35573\"><tspan\n+         id=\"tspan35571\"\n+         style=\"font-size:10.5833px;stroke-width:0.264583\"\n+         x=\"-66.553741\"\n+         y=\"187.08112\">DType precision</tspan></text>\n+    <path\n+       style=\"fill:none;stroke:#000000;stroke-width:0.75;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-end:url(#Arrow2Lend)\"\n+       d=\"M -89.46727,176.53412 H 29.674668\"\n+       id=\"path55402\" />\n+    <path\n+       style=\"fill:none;stroke:#000000;stroke-width:0.75;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-end:url(#Arrow2Lend)\"\n+       d=\"M -202.54791,166.48316 V 87.745517\"\n+       id=\"path55752\" />\n+    <g\n+       id=\"g955\"\n+       transform=\"translate(11.46753,-1.7989611)\">\n+      <rect\n+         style=\"opacity:0.779541;fill:#00b200;fill-opacity:0.255768;stroke:#00b200;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:128.863;stroke-opacity:1\"\n+         id=\"rect80903\"\n+         width=\"35.016891\"\n+         height=\"20\"\n+         x=\"-97.191238\"\n+         y=\"57.390038\" />\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:6.35px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';text-align:center;letter-spacing:0px;word-spacing:0px;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+         x=\"-79.76535\"\n+         y=\"65.029427\"\n+         id=\"text65310\"><tspan\n+           style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:6.35px;font-family:Lato;-inkscape-font-specification:Lato;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+           x=\"-79.76535\"\n+           y=\"65.029427\"\n+           id=\"tspan70805\">Promotion</tspan><tspan\n+           style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:6.35px;font-family:Lato;-inkscape-font-specification:Lato;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+           x=\"-79.76535\"\n+           y=\"73.316177\"\n+           id=\"tspan100709\">paths</tspan></text>\n+    </g>\n+    <g\n+       id=\"g2471\"\n+       transform=\"translate(-59.573056,-101.10613)\">\n+      <rect\n+         style=\"fill:#8cc9e1;fill-opacity:1;stroke:#ffd876;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:128.863;stroke-opacity:1\"\n+         id=\"rect14959\"\n+         width=\"120.41356\"\n+         height=\"20\"\n+         x=\"53.441502\"\n+         y=\"156.69722\" />\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:6.35px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';text-align:center;letter-spacing:0px;word-spacing:0px;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+         x=\"113.56572\"\n+         y=\"164.29057\"\n+         id=\"text20753\"><tspan\n+           style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:6.35px;font-family:Lato;-inkscape-font-specification:Lato;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+           x=\"113.56572\"\n+           y=\"164.29057\"\n+           id=\"tspan20751\">Minimum Python scalar</tspan><tspan\n+           style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:6.35px;font-family:Lato;-inkscape-font-specification:Lato;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+           x=\"113.56572\"\n+           y=\"172.57732\"\n+           id=\"tspan22734\">precision when other is integral/boolean</tspan></text>\n+    </g>\n+    <g\n+       id=\"g961\"\n+       transform=\"translate(-3.7473145e-6,-1.7989611)\">\n+      <rect\n+         style=\"fill:#ffdc86;fill-opacity:1;stroke:#ffdc86;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:128.863;stroke-opacity:1\"\n+         id=\"rect10573\"\n+         width=\"45.555595\"\n+         height=\"20\"\n+         x=\"-175.85457\"\n+         y=\"57.390038\" />\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-style:normal;font-variant:normal;font-weight:500;font-stretch:normal;font-size:6.35px;line-height:1.25;font-family:'Fira Code';-inkscape-font-specification:'Fira Code Medium';text-align:center;letter-spacing:0px;word-spacing:0px;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+         x=\"-153.24345\"\n+         y=\"69.183914\"\n+         id=\"text10579\"><tspan\n+           style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:6.35px;font-family:Lato;-inkscape-font-specification:Lato;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+           x=\"-153.24345\"\n+           y=\"69.183914\"\n+           id=\"tspan10575\">Python scalars</tspan><tspan\n+           style=\"font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:6.35px;font-family:Lato;-inkscape-font-specification:Lato;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;stroke-width:1;stroke-miterlimit:4;stroke-dasharray:none\"\n+           x=\"-153.24345\"\n+           y=\"77.121414\"\n+           id=\"tspan10577\" /></text>\n+    </g>\n+    <g\n+       id=\"g4482\">\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-size:10.5833px;line-height:1.25;font-family:Lato;-inkscape-font-specification:Lato;letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+         x=\"-167.88512\"\n+         y=\"-189.26865\"\n+         id=\"text1272\"\n+         transform=\"rotate(-90)\"><tspan\n+           id=\"tspan1270\"\n+           style=\"font-size:10.5833px;stroke-width:0.264583\"\n+           x=\"-167.88512\"\n+           y=\"-189.26865\">integral</tspan></text>\n+      <text\n+         xml:space=\"preserve\"\n+         style=\"font-size:10.5833px;line-height:1.25;font-family:Lato;-inkscape-font-specification:Lato;letter-spacing:0px;word-spacing:0px;stroke-width:0.264583\"\n+         x=\"-119.82033\"\n+         y=\"-189.48032\"\n+         id=\"text1654\"\n+         transform=\"rotate(-90)\"><tspan\n+           id=\"tspan1652\"\n+           style=\"font-size:10.5833px;stroke-width:0.264583\"\n+           x=\"-119.82033\"\n+           y=\"-189.48032\">inexact</tspan></text>\n+    </g>\n+    <path\n+       style=\"fill:#666666;stroke:#000000;stroke-width:1.5;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:12, 6;stroke-dashoffset:0;stroke-opacity:1\"\n+       d=\"M -195.08166,125.8553 H 119.44831\"\n+       id=\"path937\"\n+       sodipodi:nodetypes=\"cc\" />\n+  </g>\n+</svg>"
            },
            {
                "filename": "doc/neps/nep-0050-scalar-promotion.rst",
                "patch": "@@ -0,0 +1,763 @@\n+.. _NEP50:\n+\n+===========================================\n+NEP 50 \u2014 Promotion rules for Python scalars\n+===========================================\n+:Author: Sebastian Berg\n+:Status: Draft\n+:Type: Standards Track\n+:Created: 2021-05-25\n+\n+\n+Abstract\n+========\n+\n+Since NumPy 1.7, promotion rules use so-called \"safe casting\"\n+which relies on inspection of the values involved.\n+This helped identify a number of edge cases for users, but was\n+complex to implement and also made behavior hard to predict.\n+\n+There are two kinds of confusing results:\n+\n+1. Value-based promotion means that the value, for example of a Python integer,\n+   can determine output type as found by ``np.result_type``::\n+\n+     np.result_type(np.int8, 1) == np.int8\n+     np.result_type(np.int8, 255) == np.int16\n+\n+   This logic arises because ``1`` can be represented by a ``uint8`` or\n+   ``int8`` while ``255`` cannot be represented by an ``int8`` but only by\n+   by a ``uint8`` or ``int16``.\n+\n+   This also holds when working with 0-D arrays (so-called \"scalar arrays\")::\n+\n+     int64_0d_array = np.array(1, dtype=np.int64)\n+     np.result_type(np.int8, int64_0d_array) == np.int8\n+\n+   Where the fact that ``int64_0d_array`` has an ``int64`` dtype has no\n+   influence on the resulting dtype.  The ``dtype=np.int64`` is effectively\n+   ignored in this example since only its value matters.\n+\n+2. For a Python ``int``, ``float``, or ``complex`` the value is inspected as\n+   previously shown.  But surprisingly *not* when the NumPy object is a 0-D array\n+   or NumPy scalar::\n+\n+     np.result_type(np.array(1, dtype=np.uint8), 1) == np.int64\n+     np.result_type(np.int8(1), 1) == np.int64\n+\n+   The reason is that value-based promotion is disabled when all\n+   objects are scalars or 0-D arrays.\n+   NumPy thus returns the same type as ``np.array(1)``, which is usually\n+   an ``int64`` (this depends on the system).\n+\n+Note that the examples apply also to operations like multiplication,\n+addition, comparisons, and their corresponding functions like ``np.multiply``.\n+\n+This NEP proposes to refactor the behaviour around two guiding principles:\n+\n+1. Values must never influence result type.\n+2. NumPy scalars and 0-D arrays should behave consistently with their\n+   N-D counterparts.\n+\n+We propose to remove all value-based logic and add special handling for\n+Python scalars to preserve some convenient behaviors.\n+Python scalars will be considered \"weakly\" typed.\n+When a NumPy array/scalar is combined with a Python scalar, it will\n+be converted to the NumPy dtype, such that::\n+\n+    np.array([1, 2, 3], dtype=np.uint8) + 1  # returns a uint8 array\n+    np.array([1, 2, 3], dtype=np.float32) + 2.  # returns a float32 array\n+\n+There will be no dependence on the Python value itself.\n+\n+The proposed changes also apply to ``np.can_cast(100, np.int8)``, however,\n+we expect that the behaviour in functions (promotion) will, in practice, be far\n+more important than the casting change itself.\n+\n+\n+Schema of the new proposed promotion rules\n+------------------------------------------\n+\n+After the change, the promotions in NumPy will follow the schema below.\n+Promotion always occurs along the green lines:\n+from left to right within their kind and to a higher kind only when\n+necessary.\n+The result kind is always the largest kind of the inputs.\n+Note that ``float32`` has a lower precision than ``int32`` or ``uint32`` and\n+is thus sorted slightly to the left in the schematic.  This is because\n+``float32`` cannot represent all ``int32`` values exactly.\n+However, for practical reasons, NumPy allows promoting ``int64`` to ``float64``\n+effectively considering them to have the same precision.\n+\n+The Python scalars are inserted at the very left of each \"kind\" and the\n+Python integer does not distinguish signed and unsigned.  NumPy promotion\n+thus uses the following, ordered, kind categories:\n+\n+* `boolean`\n+* `integral`: signed or unsigned integers\n+* `inexact`: floating point numbers and complex floating point numbers\n+\n+When promoting a Python scalar with a dtype of lower kind\n+category (`boolean < integral < inexact`) with a higher one, we  use the\n+minimum/default precision: that is ``float64``, ``complex128`` or ``int64``\n+(``int32`` is used on some systems, e.g. windows).\n+\n+.. figure:: _static/nep-0050-promotion-no-fonts.svg\n+    :figclass: align-center\n+\n+See the next section for examples which clarify the proposed behavior.\n+Further examples with a comparison to the current behavior can be found\n+in the table below.\n+\n+Examples of new behaviour\n+-------------------------\n+\n+To make interpretation of above text and figure easier, we provide a few examples of the new behaviour.  Below, the Python integer has no influence on the result type::\n+\n+    np.uint8(1) + 1 == np.uint8(2)\n+    np.int16(2) + 2 == np.int16(4)\n+\n+In the following the Python ``float`` and ``complex`` are \"inexact\", but the\n+NumPy value is integral, so we use at least ``float64``/``complex128``::\n+\n+    np.uint16(3) + 3.0 == np.float64(6.0)\n+    np.int16(4) + 4j == np.complex128(4+4j)\n+\n+But this does not happen for ``float`` to ``complex`` promotions, where\n+``float32`` and ``complex64`` have the same precision::\n+\n+    np.float32(5) + 5j == np.complex64(5+5j)\n+\n+Note that the schematic omits ``bool``.  It is set below \"integral\", so that the\n+following hold::\n+\n+    np.bool_(True) + 1 == np.int64(2)\n+    True + np.uint8(2) == np.uint8(3)\n+\n+\n+Table comparing new and old behaviour\n+-------------------------------------\n+\n+The following table lists relevant changes and unchanged behaviours.\n+Please see the `Old implementation`_ for a detailed explanation of the rules\n+that lead to the \"Old result\", and the following sections for the rules\n+detailing the new.\n+The backwards compatibility section discusses how these changes are likely\n+to impact users.\n+\n+Note the important distinction between a 0-D array like ``array(2)`` and\n+arrays that are not 0-D, such as ``array([2])``.\n+\n+.. list-table:: Table of changed behaviours\n+   :widths: 20 12 12\n+   :header-rows: 1\n+\n+   * - Expression\n+     - Old result\n+     - New result\n+   * - ``uint8(1) + 2``\n+     - ``int64(3)``\n+     - ``uint8(3)`` [T1]_\n+   * - ``array([1], uint8) + int64(1)`` or\n+\n+       ``array([1], uint8) + array(1, int64)``\n+     - ``array([2], unit8)``\n+     - ``array([2], int64)`` [T2]_\n+   * - ``array([1.], float32) + float64(1.)`` or\n+\n+       ``array([1.], float32) + array(1., float64)``\n+     - ``array([2.], float32)``\n+     - ``array([2.], float64)``\n+   * - ``array([1], uint8) + 1``\n+     - ``array([2], uint8)``\n+     - *unchanged*\n+   * - ``array([1], uint8) + 200``\n+     - ``array([201], np.uint8)``\n+     - *unchanged*\n+   * - ``array([100], uint8) + 200``\n+     - ``array([ 44], uint8)``\n+     - *unchanged* [T3]_\n+   * - ``array([1], uint8) + 300``\n+     - ``array([301], uint16)``\n+     - *Exception* [T4]_\n+   * - ``uint8(1) + 300``\n+     - ``int64(301)``\n+     - *Exception* [T5]_\n+   * - ``float32(1) + 3e100``\n+     - ``float64(3e100)``\n+     - ``float32(Inf)`` *and* ``OverflowWarning`` [T6]_\n+   * - ``array([0.1], float32) == 0.1``\n+     - ``array([False])``\n+     - *unchanged*\n+   * - ``array([0.1], float32) == float64(0.1)``\n+     - ``array([ True])``\n+     - ``array([False])``  [T7]_\n+   * - ``array([1.], float32) + 3``\n+     - ``array([4.], float32)``\n+     - *unchanged*\n+   * - ``array([1.], float32) + int64(3)``\n+     - ``array([4.], float32)``\n+     - ``array([4.], float64)``  [T8]_\n+\n+.. [T1] New behaviour honours the dtype of the ``uint8`` scalar.\n+.. [T2] Current NumPy ignores the precision of 0-D arrays or NumPy scalars\n+        when combined with arrays.\n+.. [T3] Current NumPy ignores the precision of 0-D arrays or NumPy scalars\n+        when combined with arrays.\n+.. [T4] Old behaviour uses ``uint16`` because ``300`` does not fit ``uint8``,\n+        new behaviour raises an error for the same reason.\n+.. [T5] ``300`` cannot be converted to ``uint8``.\n+.. [T6] ``np.float32(3e100)`` overflows to infinity.\n+.. [T7] ``0.1`` loses precision when cast to ``float32``, but old behaviour\n+        casts the ``float64(0.1)`` and then matches.\n+.. [T8] NumPy promotes ``float32`` and ``int64`` to ``float64``.  The old\n+        behaviour ignored the ``int64`` here.\n+\n+\n+Motivation and Scope\n+====================\n+\n+The motivation for changing the behaviour with respect to inspecting the value\n+of Python scalars and NumPy scalars/0-D arrays is three-fold:\n+\n+1. The special handling of NumPy scalars/0-D arrays as well as the value\n+   inspection can be very surprising to users,\n+2. The value-inspection logic is much harder to explain and implement.\n+   It is further harder to make it available to user-defined DTypes through\n+   :ref:`NEP 42 <NEP42>`.\n+   Currently, this leads to a dual implementation of a new and an old (value\n+   sensitive) system.  Fixing this will greatly simplify the internal logic\n+   and make results more consistent.\n+3. It largely aligns with the choice of other projects like `JAX` and\n+   `data-apis.org` (see also `Related Work`).\n+\n+We believe that the proposal of \"weak\" Python scalars will help users by\n+providing a clear mental model for which datatype an operation will\n+result in.\n+This model fits well with the preservation of array precisions that NumPy\n+currently often follows, and also uses for in-place operations::\n+\n+    arr += value\n+\n+Preserves precision as long as \"kind\" boundaries are not crossed (otherwise\n+an error is raised).\n+\n+While some users will potentially miss the value inspecting behavior, even for\n+those cases where it seems useful it quickly leads to surprises.  This may be\n+expected::\n+\n+    np.array([100], dtype=np.uint8) + 1000 == np.array([1100], dtype=np.uint16)\n+\n+But the following will then be a surprise::\n+\n+    np.array([100], dtype=np.uint8) + 200 == np.array([44], dtype=np.uint8)\n+\n+Considering that the proposal aligns with the behavior of in-place operands\n+and avoids the surprising switch in behavior that only sometimes avoids\n+overflow in the result,\n+we believe that the proposal follows the \"principle of least surprise\".\n+\n+\n+Usage and Impact\n+================\n+\n+This NEP is expected to be implemented with **no** transition period that warns\n+for all changes.  Such a transition period would create many (often harmless)\n+warnings which would be difficult to silence.\n+We expect that most users will benefit long term from the clearer promotion\n+rules and that few are directly (negatively) impacted by the change.\n+However, certain usage patterns may lead to problematic changes, these are\n+detailed in the backwards compatibility section.\n+\n+The solution to this will be an *optional* warning mode capable of notifying\n+users of potential changes in behavior.\n+This mode is expected to generate many harmless warnings, but provide a way\n+to systematically vet code and track down changes if problems are observed.\n+\n+\n+Impact on ``can_cast``\n+----------------------\n+\n+`can_cast` will never inspect the value anymore.  So that the following results\n+are expected to change from ``True`` to ``False``::\n+\n+  np.can_cast(np.int64(100), np.uint8)\n+  np.can_cast(np.array(100, dtype=np.int64), np.uint8)\n+  np.can_cast(100, np.uint8)\n+\n+We expect that the impact of this change will be small compared to that of\n+the following changes.\n+\n+.. note::\n+\n+    The last example where the input is a Python scalar _may_ be preserved\n+    since ``100`` can be represented by a ``uint8``.\n+\n+\n+Impact on operators and functions involving NumPy arrays or scalars\n+-------------------------------------------------------------------\n+\n+The main impact on operations not involving Python scalars (``float``, ``int``,\n+``complex``) will be that operations on 0-D arrays and NumPy scalars will never\n+depend on their values.\n+This removes currently surprising cases.  For example::\n+\n+  np.arange(10, dtype=np.uint8) + np.int64(1)\n+  # and:\n+  np.add(np.arange(10, dtype=np.uint8), np.int64(1))\n+\n+Will return an ``int64`` array in the future because the type of\n+``np.int64(1)`` is strictly honoured.\n+Currently a ``uint8`` array is returned.\n+\n+\n+Impact on operators involving Python ``int``, ``float``, and ``complex``\n+------------------------------------------------------------------------\n+\n+This NEP attempts to preserve the convenience of the old behaviour\n+when working with literal values.\n+The current value-based logic had some nice properties when \"untyped\",\n+literal Python scalars are involved::\n+\n+  np.arange(10, dtype=np.int8) + 1  # returns an int8 array\n+  np.array([1., 2.], dtype=np.float32) * 3.5  # returns a float32 array\n+\n+But led to surprises when it came to \"unrepresentable\" values::\n+\n+  np.arange(10, dtype=np.int8) + 256  # returns int16\n+  np.array([1., 2.], dtype=np.float32) * 1e200  # returns float64\n+\n+The proposal is to preserve this behaviour for the most part.  This is achieved\n+by considering Python ``int``, ``float``, and ``complex`` to be \"weakly\" typed\n+in operations.\n+However, to avoid surprises, we plan to make conversion to the new type\n+more strict:  The results will be unchanged in the first two examples,\n+but in the second one, it will change the following way::\n+\n+  np.arange(10, dtype=np.int8) + 256  # raises a TypeError\n+  np.array([1., 2.], dtype=np.float32) * 1e200  # warning and returns infinity\n+\n+The second one warns because ``np.float32(1e200)`` overflows to infinity.\n+It will then continue to do the calculation with ``inf`` as usual.\n+\n+\n+.. admonition:: Behaviour in other libraries\n+\n+   Overflowing in the conversion rather than raising an error is a choice;\n+   it is one that is the default in most C setups (similar to NumPy C can be\n+   set up to raise an error due to the overflow, however).\n+   It is also for example the behaviour of ``pytorch`` 1.10.\n+\n+\n+\n+Backward compatibility\n+======================\n+\n+In general, code which only uses the default dtypes float64, or int32/int64\n+or more precise ones should not be affected.\n+\n+However, the proposed changes will modify results in quite a few cases where\n+0-D or scalar values (with non-default dtypes) are mixed.\n+In many cases, these will be bug-fixes, however, there are certain changes\n+which may be problematic to the end-user.\n+\n+The most important possible failure is probably the following example::\n+\n+  arr = np.arange(100, dtype=np.uint8)  # storage array with low precision\n+  value = arr[10]\n+\n+  # calculation continues with \"value\" without considering where it came from\n+  value * 100\n+\n+Where previously the ``value * 100`` would cause an up-cast to\n+``int32``/``int64`` (because value is a scalar).\n+The new behaviour will preserve the lower precision unless explicitly\n+dealt with (just as if ``value`` was an array).\n+This can lead to integer overflows and thus incorrect results beyond precision.\n+In many cases this may be silent, although NumPy usually gives warnings for the\n+scalar operators.\n+\n+Similarly, if the storage array is ``float32`` a calculation may retain the\n+lower ``float32`` precision rather than use the default ``float64``.\n+\n+Further issues can occur.  For example:\n+\n+* Floating point comparisons, especially equality, may change when mixing\n+  precisions::\n+\n+     np.float32(1/3) == 1/3  # was False, will be True.\n+\n+* Certain operations are expected to start failing::\n+\n+     np.array([1], np.uint8) * 1000\n+     np.array([1], np.uint8) == 1000  # possibly also\n+\n+  to protect users in cases where previous value-based casting led to an\n+  upcast.  (Failures occur when converting ``1000`` to a ``uint8``.)\n+\n+* Floating point overflow may occur in odder cases::\n+\n+     np.float32(1e-30) * 1e50  # will return ``inf`` and a warning\n+\n+  Because ``np.float32(1e50)`` returns ``inf``.  Previously, this would return\n+  a double precision result even if the ``1e50`` was not a 0-D array\n+\n+In other cases, increased precision may occur.  For example::\n+\n+  np.multiple(float32_arr, 2.)\n+  float32_arr * np.float64(2.)\n+\n+Will both return a float64 rather than ``float32``.  This improves precision but\n+slightly changes results and uses double the memory.\n+\n+\n+Changes due to the integer \"ladder of precision\"\n+------------------------------------------------\n+\n+When creating an array from a Python integer, NumPy will try the following\n+types in order, with the result depending on the value::\n+\n+    long (usually int64) \u2192 int64 \u2192 uint64 -> object\n+\n+which is subtly different from the promotion described above.\n+\n+This NEP currently does not include changing this ladder (although it may be\n+suggested in a separate document).\n+However, in mixed operations, this ladder will be ignored, since the value\n+will be ignored.  This means, that operations will never silently use the\n+``object`` dtype::\n+\n+    np.array([3]) + 2**100  # Will error\n+\n+The user will have to write one of::\n+\n+    np.array([3]) + np.array(2**100)\n+    np.array([3]) + np.array(2**100, dtype=object)\n+\n+As such implicit conversion to ``object`` should be rare and the work-around\n+is clear, we expect that the backwards compatibility concerns are fairly small.\n+\n+\n+Detailed description\n+====================\n+\n+The following provides some additional details on the current \"value based\"\n+promotion logic, and then on the \"weak scalar\" promotion and how it is handled\n+internally.\n+\n+.. _Old implementation:\n+\n+Old implementation of \"values based\" promotion\n+----------------------------------------------\n+\n+This section reviews how the current value-based logic works in practice,\n+please see the following section for examples on how it can be useful.\n+\n+When NumPy sees a \"scalar\" value, which can be a Python int, float, complex,\n+a NumPy scalar or an array::\n+\n+    1000  # Python scalar\n+    int32(1000)  # NumPy scalar\n+    np.array(1000, dtype=int64)  # zero dimensional\n+\n+Or the float/complex equivalents, NumPy will ignore the precision of the dtype\n+and find the smallest possible dtype that can hold the value.\n+That is, it will try the following dtypes:\n+\n+* Integral: ``uint8``, ``int8``, ``uint16``, ``int16``, ``uint32``, ``int32``,\n+  ``uint64``, ``int64``.\n+* Floating: ``float16``, ``float32``, ``float64``, ``longdouble``.\n+* Complex: ``complex64``, ``complex128``, ``clongdouble``.\n+\n+Note that e.g. for the integer value of ``10``, the smallest dtype can be\n+*either* ``uint8`` or ``int8``.\n+\n+NumPy never applied this rule when all arguments are scalar values:\n+\n+    np.int64(1) + np.int32(2) == np.int64(3)\n+\n+For integers, whether a value fits is decided precisely by whether it can\n+be represented by the dtype.\n+For float and complex, the a dtype is considered sufficient if:\n+\n+* ``float16``: ``-65000 < value < 65000``  (or NaN/Inf)\n+* ``float32``: ``-3.4e38 < value < 3.4e38``  (or NaN/Inf)\n+* ``float64``: ``-1.7e308 < value < 1.7e308``  (or Nan/Inf)\n+* ``longdouble``:  (largest range, so no limit)\n+\n+for complex these bounds were applied to the real and imaginary component.\n+These values roughly correspond to ``np.finfo(np.float32).max``.\n+(NumPy did never force the use of ``float64`` for a value of\n+``float32(3.402e38)`` though, but it will for a Python value of ``3.402e38``.)\n+\n+\n+State of the current \"value based\" promotion\n+---------------------------------------------\n+\n+Before we can propose alternatives to the current datatype system,\n+it is helpful to review how \"value based promotion\" is used and can be useful.\n+Value based promotion allows for the following code to work::\n+\n+    # Create uint8 array, as this is sufficient:\n+    uint8_arr = np.array([1, 2, 3], dtype=np.uint8)\n+    result = uint8_arr + 4\n+    result.dtype == np.uint8\n+\n+    result = uint8_arr * (-1)\n+    result.dtype == np.int16  # upcast as little as possible.\n+\n+Where especially the first part can be useful: The user knows that the input\n+is an integer array with a specific precision. Considering that plain ``+ 4``\n+retaining the previous datatype is intuitive.\n+Replacing this example with ``np.float32`` is maybe even more clear,\n+as float will rarely have overflows.\n+Without this behaviour, the above example would require writing ``np.uint8(4)``\n+and lack of the behaviour would make the following suprising::\n+\n+    result = np.array([1, 2, 3], dtype=np.float32) * 2.\n+    result.dtype == np.float32\n+\n+where lack of a special case would cause ``float64`` to be returned.\n+\n+It is important to note that the behaviour also applies to universal functions\n+and zero dimensional arrays::\n+\n+    # This logic is also used for ufuncs:\n+    np.add(uint8_arr, 4).dtype == np.uint8\n+    # And even if the other array is explicitly typed:\n+    np.add(uint8_arr, np.array(4, dtype=np.int64)).dtype == np.uint8 \n+\n+To review, if we replace ``4`` with ``[4]`` to make it one dimensional, the\n+result will be different::\n+\n+    # This logic is also used for ufuncs:\n+    np.add(uint8_arr, [4]).dtype == np.int64  # platform dependent\n+    # And even if the other array is explicitly typed:\n+    np.add(uint8_arr, np.array([4], dtype=np.int64)).dtype == np.int64\n+\n+\n+Proposed Weak Promotion\n+-----------------------\n+\n+This proposal uses a \"weak scalar\" logic.  This means that Python ``int``, ``float``,\n+and ``complex`` are not assigned one of the typical dtypes, such as float64 or int64.\n+Rather, they are assigned a special abstract DType, similar to the \"scalar\" hierarchy\n+names: Integral, Floating, ComplexFloating.\n+\n+When promotion occurs (as it does for ufuncs if no exact loop matches),\n+the other DType is able to decide how to regard the Python\n+scalar.  E.g. a ``UInt16`` promoting with an ``Integral`` will give ``UInt16``.\n+\n+.. note::\n+\n+    A default will most likely be provided in the future for user-defined DTypes.\n+    Most likely this will end up being the default integer/float, but in principle\n+    more complex schemes could be implemented.\n+\n+At no time is the value used to decide the result of this promotion.  The value is only\n+considered when it is converted to the new dtype; this may raise an error.\n+\n+\n+Related Work\n+============\n+\n+Different Python projects that fill a similar space to NumPy prefer the weakly\n+typed Python scalars as proposed in this NEP.  Details of these may differ\n+or be unspecified though:\n+\n+* `JAX promotion`_ also uses the weak-scalar concept.  However, it makes use\n+  of it also for most functions.  JAX further stores the \"weak-type\" information\n+  on the array: ``jnp.array(1)`` remains weakly typed.\n+\n+* `data-apis.org`_ also suggests this weak-scalar logic for the Python scalars.\n+\n+\n+Implementation\n+==============\n+\n+Implemeting this NEP requires some additional machinery to be added to all\n+binary operators (or ufuncs), so that they attempt to use the \"weak\" logic\n+if possible.\n+There are two possible approaches to this:\n+\n+1. The binary operator simply tries to call ``np.result_type()`` if this\n+   situation arises and converts the Python scalar to the result-type (if\n+   defined).\n+2. The binary operator indicates that an input was a Python scalar, and the\n+   ufunc dispatching/promotion machinery is used for the rest (see\n+   :ref:`NEP 42 <NEP42>`).  This allows more flexibility, but requires some\n+   additional logic in the ufunc machinery.\n+\n+.. note::\n+   As of now, it is not quite clear which approach is better, either will\n+   give fairly equivalent results and 1. could be extended by 2. in the future\n+   if necessary.\n+\n+It further requires removing all current special value-based code paths.\n+\n+Unintuitively, a larger step in the implementation may be to implement a\n+solution to allow an error to be raised in the following example::\n+\n+   np.arange(10, dtype=np.uint8) + 1000\n+\n+Even though ``np.uint8(1000)`` returns the same value as ``np.uint8(232)``.\n+\n+.. note::\n+\n+    See alternatives, we may yet decide that this silent overflow is acceptable\n+    or at least a separate issue.\n+\n+\n+Alternatives\n+============\n+\n+There are several design axes where different choices are possible.\n+The below sections outline these.\n+\n+Use strongly-typed scalars or a mix of both\n+-------------------------------------------\n+\n+The simplest solution to the value-based promotion/casting issue would be to use\n+strongly typed Python scalars, i.e. Python floats are considered double precision\n+and Python integers are always considered the same as the default integer dtype.\n+\n+This would be the simplest solution, however, it would lead to many upcasts when\n+working with arrays of ``float32`` or ``int16``, etc.  The solution for these cases\n+would be to rely on in-place operations.\n+We currently believe that while less dangerous, this change would affect many users\n+and would be surprising more often than not (although expectations differ widely).\n+\n+In principle, the weak vs. strong behaviour need not be uniform.  It would also\n+be possible to make Python floats use the weak behaviour, but Python integers use the\n+strong one, since integer overflows are far more surprising.\n+\n+\n+Do not use weak scalar logic in functions\n+-----------------------------------------\n+\n+One alternative to this NEPs proposal is to narrow the use of weak types\n+to Python operators.\n+\n+This has advantages and disadvantages:\n+\n+* The main advantage is that limiting it to Python operators means that these\n+  \"weak\" types/dtypes are clearly ephemeral to short Python statements.\n+* A disadvantage is that ``np.multiply`` and ``*`` are less interchangable.\n+* Using \"weak\" promotion only for operators means that libraries do not have\n+  to worry about whether they want to \"remember\" that an input was a Python\n+  scalar initially.  On the other hand, it would add a the need for slightly\n+  different (or additional) logic for Python operators.\n+  (Technically, probably as a flag to the ufunc dispatching mechanism to toggle\n+  the weak logic.)\n+* ``__array_ufunc__`` is often used on its own to provide Python operator\n+  support for array-likes implementing it.  If operators are special, these\n+  array-likes may need a mechanism to match NumPy (e.g. a kwarg to ufuncs to\n+  enable weak promotion.)\n+\n+\n+NumPy scalars could be special\n+------------------------------\n+\n+Many users expect that NumPy scalars should be different from NumPy\n+arrays, in that ``np.uint8(3) + 3`` should return an ``int64`` (or Python\n+integer), when ``uint8_arr + 3`` preserves the ``uint8`` dtype.\n+\n+This alternative would be very close to the current behaviour for NumPy scalars\n+but it would cement a distinction between arrays and scalars (NumPy arrays\n+are \"stronger\" than Python scalars, but NumPy scalars are not).\n+\n+Such a distinction is very much possible, however, at this time NumPy will\n+often (and silently) convert 0-D arrays to scalars.\n+It may thus make sense, to only consider this alternative if we also\n+change this silent conversion (sometimes refered to as \"decay\") behaviour.\n+\n+\n+Handling conversion of scalars when unsafe\n+------------------------------------------\n+\n+Cases such as::\n+\n+  np.arange(10, dtype=np.uint8) + 1000\n+\n+should raise an error as per this NEP.  This could be relaxed to give a warning\n+or even ignore the \"unsafe\" conversion which (on all relevant hardware) would\n+lead to ``np.uint8(1000) == np.uint8(232)`` being used.\n+\n+\n+Allowing weakly typed arrays\n+----------------------------\n+\n+One problem with having weakly typed Python scalars, but not weakly typed\n+arrays is that in many cases ``np.asarray()`` is called indiscriminately on\n+inputs.  To solve this issue JAX will consider the result of ``np.asarray(1)``\n+also to be weakly typed.\n+There are, however, two difficulties with this:\n+\n+1. JAX noticed that it can be confusing that::\n+\n+     np.broadcast_to(np.asarray(1), (100, 100))\n+\n+   is a non 0-D array that \"inherits\" the weak typing. [2]_\n+2. Unlike JAX tensors, NumPy arrays are mutable, so assignment may need to\n+   cause it to be strongly typed?\n+\n+A flag will likely be useful as an implementation detail (e.g. in ufuncs),\n+however, as of now we do not expect to have this as user API.\n+The main reason is that such a flag may be surprising for users if it is\n+passed out as a result from a function, rather than used only very localized.\n+\n+\n+.. admonition:: TODO\n+\n+    Before accepting the NEP it may be good to discuss this issue further.\n+    Libraries may need clearer patterns to \"propagate\" the \"weak\" type, this\n+    could just be an ``np.asarray_or_literal()`` to preserve Python scalars,\n+    or a pattern of calling ``np.result_type()`` before ``np.asarray()``.\n+\n+\n+Keep using value-based logic for Python scalars\n+-----------------------------------------------\n+\n+Some of the main issues with the current logic arise, because we apply it\n+to NumPy scalars and 0-D arrays, rather than the application to Python scalars.\n+We could thus consider to keep inspecting the value for Python scalars.\n+\n+We reject this idea on the grounds that it will not remove the surprises\n+given earlier::\n+\n+    np.uint8(100) + 1000 == np.uint16(1100)\n+    np.uint8(100) + 200 == np.uint8(44)\n+\n+And adapting the precision based on the result value rather than the input\n+value might be possible for scalar operations, but is not feasible for array\n+operations.\n+This is because array operations need to allocate the result array before\n+performing the calculation.\n+\n+\n+Discussion\n+==========\n+\n+* https://github.com/numpy/numpy/issues/2878\n+* https://mail.python.org/archives/list/numpy-discussion@python.org/thread/R7D65SNGJW4PD6V7N3CEI4NJUHU6QP2I/#RB3JLIYJITVO3BWUPGLN4FJUUIKWKZIW\n+* https://mail.python.org/archives/list/numpy-discussion@python.org/thread/NA3UBE3XAUTXFYBX6HPIOCNCTNF3PWSZ/#T5WAYQPRMI5UCK7PKPCE3LGK7AQ5WNGH\n+* Poll about the desired future behavior: https://discuss.scientific-python.org/t/poll-future-numpy-behavior-when-mixing-arrays-numpy-scalars-and-python-scalars/202\n+\n+References and Footnotes\n+========================\n+\n+.. [1] Each NEP must either be explicitly labeled as placed in the public domain (see\n+   this NEP as an example) or licensed under the `Open Publication License`_.\n+\n+.. _Open Publication License: https://www.opencontent.org/openpub/\n+\n+.. _JAX promotion: https://jax.readthedocs.io/en/latest/type_promotion.html\n+\n+.. _data-apis.org: https://data-apis.org/array-api/latest/API_specification/type_promotion.html\n+\n+.. [2] https://github.com/numpy/numpy/pull/21103/files#r814188019\n+\n+Copyright\n+=========\n+\n+This document has been placed in the public domain. [1]_"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 17970,
        "body": "## Changes:\r\n1. Added check to see if RHS is a python number and prevent the creation of array.\r\n2. Added bench\r\n\r\n## Benchmarks:\r\n<details>\r\n<summary>Benchmarks:</summary>\r\n\r\n<pre>                                                                                                                   \r\n********************************************************************************                                                                          \r\nWARNING: you have uncommitted changes --- these will NOT be benchmarked!                                                                                  \r\n********************************************************************************                                                                          \r\n\u00b7 Creating environments                                                      \r\n\u00b7 Discovering benchmarks                                                     \r\n\u00b7\u00b7 Uninstalling from virtualenv-py3.8-Cython                                 \r\n\u00b7\u00b7 Building 933620b4 <enh_14415_fast_compare> for virtualenv-py3.8-Cython..................................                                                                                                                                                                                                          \r\n\u00b7\u00b7 Installing 933620b4 <enh_14415_fast_compare> into virtualenv-py3.8-Cython.                                                                             \r\n\u00b7 Running 2 total benchmarks (2 commits * 1 environments * 1 benchmarks)                                                                                  \r\n[  0.00%] \u00b7 For numpy commit 4d290795 <master> (round 1/2):                                                                                               \r\n[  0.00%] \u00b7\u00b7 Building for virtualenv-py3.8-Cython.                                                                                                                                                                                                                                                                   \r\n[  0.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython                                                                                                         \r\n[ 25.00%] \u00b7\u00b7\u00b7 Running (bench_ufunc.CustomScalarEqual.time_scalar_equality--).                                                                             \r\n[ 25.00%] \u00b7 For numpy commit 933620b4 <enh_14415_fast_compare> (round 1/2):                                                                               \r\n[ 25.00%] \u00b7\u00b7 Building for virtualenv-py3.8-Cython.                                                                                                        \r\n[ 25.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython                                                                                                         \r\n[ 50.00%] \u00b7\u00b7\u00b7 Running (bench_ufunc.CustomScalarEqual.time_scalar_equality--).                                                                             \r\n[ 50.00%] \u00b7 For numpy commit 933620b4 <enh_14415_fast_compare> (round 2/2):                                                                               \r\n[ 50.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython                                                                                                         \r\n[ 75.00%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalarEqual.time_scalar_equality                                                                                                              ok                                                                                                                                     \r\n[ 75.00%] \u00b7\u00b7\u00b7 =============== ==========                                     \r\n                   dtype                                                                                                                                  \r\n              --------------- ----------                                     \r\n                 numpy.int8    439\u00b16ms                                       \r\n                numpy.int16    445\u00b18ms                                       \r\n                numpy.int32    435\u00b110ms                                      \r\n                numpy.int64    147\u00b12ms                                       \r\n                numpy.uint8    427\u00b110ms                                      \r\n                numpy.uint16   444\u00b18ms                                       \r\n                numpy.uint32   437\u00b110ms                                      \r\n                numpy.uint64   447\u00b14ms                                       \r\n               numpy.float32   471\u00b110ms                                      \r\n               numpy.float64   180\u00b14ms                                                                                                                    \r\n              =============== ==========                                                                                                                  \r\n                                                                             \r\n[ 75.00%] \u00b7 For numpy commit 4d290795 <master> (round 2/2):                                                                                               \r\n[ 75.00%] \u00b7\u00b7 Building for virtualenv-py3.8-Cython.                                                                                                        \r\n[ 75.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython                                                                                                         \r\n[100.00%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalarEqual.time_scalar_equality                                                                                                              ok                                                                                                                                     \r\n[100.00%] \u00b7\u00b7\u00b7 =============== ============                                   \r\n                   dtype                                                     \r\n              --------------- ------------                                   \r\n                 numpy.int8     1.35\u00b10s                                      \r\n                numpy.int16    1.34\u00b10.01s                                    \r\n                numpy.int32    1.34\u00b10.02s                                    \r\n                numpy.int64     147\u00b13ms                                      \r\n                numpy.uint8     1.34\u00b10s                                      \r\n                numpy.uint16    1.36\u00b10s                                      \r\n                numpy.uint32    1.35\u00b10s                                      \r\n                numpy.uint64   1.54\u00b10.01s                                    \r\n               numpy.float32    1.54\u00b10s                                      \r\n               numpy.float64    184\u00b14ms                                      \r\n              =============== ============                                   \r\n\r\n       before           after         ratio                                  \r\n     [4d290795]       [933620b4]                                             \r\n     <master>         <enh_14415_fast_compare>                                                                                                            \r\n-      1.34\u00b10.01s          445\u00b18ms     0.33  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.int16'>)                                                                                                                                                                                               \r\n-         1.36\u00b10s          444\u00b18ms     0.33  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.uint16'>)                                                                                                                                                                                              \r\n-      1.34\u00b10.02s         435\u00b110ms     0.33  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.int32'>)                                                                                                                                                                                               \r\n-         1.35\u00b10s          439\u00b16ms     0.33  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.int8'>)                                                                                                                                                                                                \r\n-         1.35\u00b10s         437\u00b110ms     0.32  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.uint32'>)                                                                                                                                                                                              \r\n-         1.34\u00b10s         427\u00b110ms     0.32  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.uint8'>)                                                                                                                                                                                               \r\n-         1.54\u00b10s         471\u00b110ms     0.31  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.float32'>)                                                                                                                                                                                             \r\n-      1.54\u00b10.01s          447\u00b14ms     0.29  bench_ufunc.CustomScalarEqual.time_scalar_equality(<class 'numpy.uint64'>)                                                                                                                                                                                              \r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.                                  \r\nPERFORMANCE INCREASED.                                                       \r\n\r\n</pre>\r\n</details>\r\n\r\nresolves: #14415\r\n\r\ncc: @seberg , @eric-wieser , @mattip ",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_scalar.py",
                "patch": "@@ -1,3 +1,4 @@\n+from random import randint\n from .common import Benchmark, TYPES1\n \n import numpy as np\n@@ -31,3 +32,12 @@ def time_abs(self, typename):\n         n = self.num\n         res = abs(abs(abs(abs(abs(abs(abs(abs(abs(abs(n))))))))))\n \n+    def time_compare(self, typename):\n+        n = self.num\n+        res = [n == randint(-128, 127) for _ in range(10)]\n+\n+    def time_compare_types(self, typename):\n+        n1 = self.num\n+        for type_lhs in TYPES1:\n+            n2 = np.dtype(type_lhs).type(randint(-128, 127))\n+            res = [n1 == n2 for _ in range(10)]"
            },
            {
                "filename": "numpy/core/src/multiarray/convert_datatype.c",
                "patch": "@@ -717,8 +717,8 @@ PyArray_CanCastScalar(PyTypeObject *from, PyTypeObject *to)\n     int fromtype;\n     int totype;\n \n-    fromtype = _typenum_fromtypeobj((PyObject *)from, 0);\n-    totype = _typenum_fromtypeobj((PyObject *)to, 0);\n+    fromtype = PyArray_TypeNumFromNumPyScalarType((PyObject *)from, 0);\n+    totype = PyArray_TypeNumFromNumPyScalarType((PyObject *)to, 0);\n     if (fromtype == NPY_NOTYPE || totype == NPY_NOTYPE) {\n         return NPY_FALSE;\n     }"
            },
            {
                "filename": "numpy/core/src/multiarray/scalarapi.c",
                "patch": "@@ -443,7 +443,7 @@ NPY_NO_EXPORT PyArray_Descr *\n PyArray_DescrFromTypeObject(PyObject *type)\n {\n     /* if it's a builtin type, then use the typenumber */\n-    int typenum = _typenum_fromtypeobj(type,1);\n+    int typenum = PyArray_TypeNumFromNumPyScalarType(type,1);\n     if (typenum != NPY_NOTYPE) {\n         return PyArray_DescrFromType(typenum);\n     }"
            },
            {
                "filename": "numpy/core/src/multiarray/scalartypes.c.src",
                "patch": "@@ -4144,7 +4144,7 @@ is_anyscalar_exact(PyObject *obj)\n }\n \n NPY_NO_EXPORT int\n-_typenum_fromtypeobj(PyObject *type, int user)\n+PyArray_TypeNumFromNumPyScalarType(PyObject *type, int user)\n {\n     int typenum, i;\n "
            },
            {
                "filename": "numpy/core/src/multiarray/scalartypes.h",
                "patch": "@@ -26,7 +26,7 @@ NPY_NO_EXPORT int\n is_anyscalar_exact(PyObject *obj);\n \n NPY_NO_EXPORT int\n-_typenum_fromtypeobj(PyObject *type, int user);\n+PyArray_TypeNumFromNumPyScalarType(PyObject *type, int user);\n \n NPY_NO_EXPORT void *\n scalar_value(PyObject *scalar, PyArray_Descr *descr);"
            },
            {
                "filename": "numpy/core/src/umath/scalarmath.c.src",
                "patch": "@@ -16,6 +16,8 @@\n #include \"numpy/ufuncobject.h\"\n #include \"numpy/arrayscalars.h\"\n \n+#include \"scalartypes.h\"\n+\n #include \"npy_import.h\"\n #include \"npy_pycompat.h\"\n \n@@ -569,6 +571,117 @@ static void\n \n /*** END OF BASIC CODE **/\n \n+/**\n+ * Find the descriptor for a builtin or Python numerical scalar.\n+ * This function should only be used for scalar math fast paths.\n+ *\n+ * @param obj The object for which to find the descriptor\n+ * @param descr The descriptor that was found.\n+ * @returns 1 if the other object is a Python type and thus must not\n+ *          handle the operation, otherwise 0. -1 _without_ an error set\n+ *          if no descriptor was found.\n+ */\n+static int\n+descr_from_basic_scalar(PyObject *obj, PyArray_Descr **descr)\n+{\n+    /* TODO: We could try giving defined scalars a chance... */\n+    int type_num = PyArray_TypeNumFromNumPyScalarType(\n+            (PyObject *)Py_TYPE(obj), 0);\n+    if (type_num != NPY_NOTYPE) {\n+        *descr = PyArray_DescrFromType(type_num);\n+        return 0;\n+    }\n+    else if (PyFloat_CheckExact(obj)) {\n+        *descr = PyArray_DescrFromType(NPY_DOUBLE);\n+        return 1;\n+    }\n+    else if (PyBool_Check(obj)) {\n+        *descr = PyArray_DescrFromType(NPY_BOOL);\n+        return 1;\n+    }\n+    else if (PyLong_CheckExact(obj)) {\n+        if (PyLong_AsLong(obj) == -1 && PyErr_Occurred()) {\n+            PyErr_Clear();\n+            return -1;\n+        }\n+        *descr = PyArray_DescrFromType(NPY_LONG);\n+        return 1;\n+    }\n+    else if (PyComplex_CheckExact(obj)) {\n+        *descr = PyArray_DescrFromType(NPY_CDOUBLE);\n+        return 1;\n+    }\n+    return -1;\n+}\n+\n+#define PyDescr_PythonRepresentable(d) (d->type_num < NPY_LONGDOUBLE || \\\n+        d->type_num == NPY_CFLOAT || d->type_num == NPY_CDOUBLE)\n+\n+/**\n+ * This function attempts to compare NumPy or Python scalars\n+ * The operation is done by getting value of both scalars\n+ * and calling richcompare on them.\n+ *\n+ * @param self The first object which will be converted to scalar item\n+ * @param other The second object to compare to\n+ * @param cmp_op The value of comparison operator\n+ * @returns Py_False or Py_True if richcompare is successfull\n+ *          , otherwise NULL.\n+ */\n+static PyObject*\n+do_richcompare_on_scalars(PyObject *self, PyObject *other, int cmp_op) {\n+    PyObject *cmp_item_self, *cmp_item_other, *ret=NULL;\n+    PyArray_Descr *self_descr=NULL, *other_descr=NULL;\n+    void *data_self, *data_other;\n+    int pyscalar_other, pyscalar_self;\n+    int is_complex_operands, is_equality_operator, is_python_representable;\n+\n+    pyscalar_self = descr_from_basic_scalar(self, &self_descr);\n+    pyscalar_other = descr_from_basic_scalar(other, &other_descr);\n+\n+    if (pyscalar_self >= 0 && pyscalar_other >= 0) {\n+        /*\n+         * If either of the operands are complex and operator is not equality,\n+         * or the operands are not representable as a native type,\n+         * python's built-in richcompare cannot be used as it is not supported.\n+         */\n+        is_complex_operands = (PyTypeNum_ISCOMPLEX(self_descr->type_num) ||\n+            PyTypeNum_ISCOMPLEX(other_descr->type_num));\n+        is_equality_operator = (cmp_op != Py_EQ && cmp_op != Py_NE);\n+        is_python_representable = PyDescr_PythonRepresentable(self_descr) &&\n+            PyDescr_PythonRepresentable(other_descr);\n+\n+        if (!(is_complex_operands && is_equality_operator) && is_python_representable) {\n+            /*\n+             * If the scalar is a python built-in, we can use the object as is.\n+             * Else we need to obtain the value from the operand.\n+             * Note: If we reach this point, one of the scalars must be built-in.\n+             */\n+            data_self = scalar_value(self, NULL);\n+            data_other = scalar_value(other, NULL);\n+            cmp_item_self = pyscalar_self == 0 ? self_descr->f->getitem(data_self, NULL):\n+                self;\n+            cmp_item_other = pyscalar_other == 0 ? other_descr->f->getitem(data_other, NULL):\n+                other;\n+\n+            if (cmp_item_self != NULL && cmp_item_other != NULL) {\n+                ret = PyObject_RichCompare(cmp_item_self, cmp_item_other, cmp_op);\n+            }\n+\n+            if (pyscalar_self == 0) {\n+                Py_XDECREF(cmp_item_self);\n+            }\n+            if (pyscalar_other == 0) {\n+                Py_XDECREF(cmp_item_other);\n+            }\n+        }\n+    }\n+    Py_XDECREF(self_descr);\n+    Py_XDECREF(other_descr);\n+\n+    return ret;\n+}\n+\n \n /* The general strategy for commutative binary operators is to\n  *\n@@ -587,86 +700,38 @@ static void\n /**begin repeat\n  * #name = byte, ubyte, short, ushort, int, uint,\n  *         long, ulong, longlong, ulonglong,\n- *         half, float, longdouble,\n+ *         half, float, double, longdouble,\n  *         cfloat, cdouble, clongdouble#\n  * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n  *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n- *         npy_half, npy_float, npy_longdouble,\n+ *         npy_half, npy_float, npy_double, npy_longdouble,\n  *         npy_cfloat, npy_cdouble, npy_clongdouble#\n  * #Name = Byte, UByte, Short, UShort, Int, UInt,\n  *         Long, ULong, LongLong, ULongLong,\n- *         Half, Float, LongDouble,\n+ *         Half, Float, Double, LongDouble,\n  *         CFloat, CDouble, CLongDouble#\n  * #TYPE = NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT,\n  *         NPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG,\n- *         NPY_HALF, NPY_FLOAT, NPY_LONGDOUBLE,\n+ *         NPY_HALF, NPY_FLOAT, NPY_DOUBLE, NPY_LONGDOUBLE,\n  *         NPY_CFLOAT, NPY_CDOUBLE, NPY_CLONGDOUBLE#\n  */\n \n-static int\n-_@name@_convert_to_ctype(PyObject *a, @type@ *arg1)\n-{\n-    PyObject *temp;\n-\n-    if (PyArray_IsScalar(a, @Name@)) {\n-        *arg1 = PyArrayScalar_VAL(a, @Name@);\n-        return 0;\n-    }\n-    else if (PyArray_IsScalar(a, Generic)) {\n-        PyArray_Descr *descr1;\n-\n-        if (!PyArray_IsScalar(a, Number)) {\n-            return -1;\n-        }\n-        descr1 = PyArray_DescrFromTypeObject((PyObject *)Py_TYPE(a));\n-        if (PyArray_CanCastSafely(descr1->type_num, @TYPE@)) {\n-            PyArray_CastScalarDirect(a, descr1, arg1, @TYPE@);\n-            Py_DECREF(descr1);\n-            return 0;\n-        }\n-        else {\n-            Py_DECREF(descr1);\n-            return -1;\n-        }\n-    }\n-    else if (PyArray_GetPriority(a, NPY_PRIORITY) > NPY_PRIORITY) {\n-        return -2;\n-    }\n-    else if ((temp = PyArray_ScalarFromObject(a)) != NULL) {\n-        int retval = _@name@_convert_to_ctype(temp, arg1);\n-\n-        Py_DECREF(temp);\n-        return retval;\n-    }\n-    return -2;\n-}\n-\n-/**end repeat**/\n-\n-\n-/* Same as above but added exact checks against known python types for speed */\n-\n-/**begin repeat\n- * #name = double#\n- * #type = npy_double#\n- * #Name = Double#\n- * #TYPE = NPY_DOUBLE#\n- * #PYCHECKEXACT = PyFloat_CheckExact#\n- * #PYEXTRACTCTYPE = PyFloat_AS_DOUBLE#\n- */\n+#define _IS_@Name@\n \n static int\n-_@name@_convert_to_ctype(PyObject *a, @type@ *arg1)\n+_@name@_convert_to_ctype(PyObject *a, @type@ *arg)\n {\n     PyObject *temp;\n \n-    if (@PYCHECKEXACT@(a)){\n-        *arg1 = @PYEXTRACTCTYPE@(a);\n+#if defined(_IS_Double) || defined(_IS_LongDouble)\n+    if (PyFloat_CheckExact(a)) {\n+        *arg = (@type@)PyFloat_AS_DOUBLE(a);\n         return 0;\n     }\n+#endif\n \n     if (PyArray_IsScalar(a, @Name@)) {\n-        *arg1 = PyArrayScalar_VAL(a, @Name@);\n+        *arg = PyArrayScalar_VAL(a, @Name@);\n         return 0;\n     }\n     else if (PyArray_IsScalar(a, Generic)) {\n@@ -677,7 +742,7 @@ _@name@_convert_to_ctype(PyObject *a, @type@ *arg1)\n         }\n         descr1 = PyArray_DescrFromTypeObject((PyObject *)Py_TYPE(a));\n         if (PyArray_CanCastSafely(descr1->type_num, @TYPE@)) {\n-            PyArray_CastScalarDirect(a, descr1, arg1, @TYPE@);\n+            PyArray_CastScalarDirect(a, descr1, arg, @TYPE@);\n             Py_DECREF(descr1);\n             return 0;\n         }\n@@ -690,14 +755,16 @@ _@name@_convert_to_ctype(PyObject *a, @type@ *arg1)\n         return -2;\n     }\n     else if ((temp = PyArray_ScalarFromObject(a)) != NULL) {\n-        int retval = _@name@_convert_to_ctype(temp, arg1);\n+        int retval = _@name@_convert_to_ctype(temp, arg);\n \n         Py_DECREF(temp);\n         return retval;\n     }\n     return -2;\n }\n \n+#undef _IS_@Name@\n+\n /**end repeat**/\n \n \n@@ -1308,14 +1375,20 @@ static PyObject*\n {\n     npy_@name@ arg1, arg2;\n     int out=0;\n+    PyObject *ret;\n \n     RICHCMP_GIVE_UP_IF_NEEDED(self, other);\n \n     switch(_@name@_convert2_to_ctypes(self, &arg1, other, &arg2)) {\n     case 0:\n         break;\n     case -1:\n-        /* can't cast both safely use different add function */\n+        /* can't cast both safely to same type.\n+         * Try fastpath else use ufuncs */\n+        ret = do_richcompare_on_scalars(self, other, cmp_op);\n+        if (ret != NULL) {\n+            return ret;\n+        }\n     case -2:\n         /* use ufunc */\n         if (PyErr_Occurred()) {"
            },
            {
                "filename": "numpy/core/tests/test_scalarmath.py",
                "patch": "@@ -16,6 +16,8 @@\n          np.int_, np.uint, np.longlong, np.ulonglong,\n          np.single, np.double, np.longdouble, np.csingle,\n          np.cdouble, np.clongdouble]\n+all_numbers_dtypes = np.typecodes['AllInteger'] + np.typecodes['AllFloat'] +\\\n+        np.typecodes['Complex']\n \n floating_types = np.floating.__subclasses__()\n complex_floating_types = np.complexfloating.__subclasses__()\n@@ -707,3 +709,13 @@ def test_shift_all_bits(self, type_code, op):\n                 shift_arr = np.array([shift]*32, dtype=dt)\n                 res_arr = op(val_arr, shift_arr)\n                 assert_equal(res_arr, res_scl)\n+\n+class TestComparison:\n+    @pytest.mark.parametrize('type_code_rhs', all_numbers_dtypes)\n+    @pytest.mark.parametrize('type_code_lhs', all_numbers_dtypes)\n+    def test_numbers_compare(self, type_code_rhs, type_code_lhs):\n+        rand_num = np.random.randint(0, 127)\n+        a = np.dtype(type_code_rhs).type(rand_num)\n+        b = np.dtype(type_code_lhs).type(rand_num)\n+\n+        assert_almost_equal(a, b)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21955,
        "body": "This patch leverages the` vcvtps2ph`, `vcvtpd2ps `instructions and float32 SVML functions to accelerate float16 umath functions. Max ULP error < 1 for all the math functions. \r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_ufunc_strides.py",
                "patch": "@@ -9,7 +9,7 @@\n \n stride = [1, 2, 4]\n stride_out = [1, 2, 4]\n-dtype  = ['f', 'd']\n+dtype = ['e', 'f', 'd']\n \n class Unary(Benchmark):\n     params = [UNARY_OBJECT_UFUNCS, stride, stride_out, dtype]"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -645,108 +645,97 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arccos'),\n           None,\n-          TD('e', f='acos', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='acos', astype={'e': 'f'}),\n           TD(P, f='arccos'),\n           ),\n 'arccosh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arccosh'),\n           None,\n-          TD('e', f='acosh', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='acosh', astype={'e': 'f'}),\n           TD(P, f='arccosh'),\n           ),\n 'arcsin':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arcsin'),\n           None,\n-          TD('e', f='asin', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='asin', astype={'e': 'f'}),\n           TD(P, f='arcsin'),\n           ),\n 'arcsinh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arcsinh'),\n           None,\n-          TD('e', f='asinh', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='asinh', astype={'e': 'f'}),\n           TD(P, f='arcsinh'),\n           ),\n 'arctan':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arctan'),\n           None,\n-          TD('e', f='atan', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='atan', astype={'e': 'f'}),\n           TD(P, f='arctan'),\n           ),\n 'arctanh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arctanh'),\n           None,\n-          TD('e', f='atanh', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='atanh', astype={'e': 'f'}),\n           TD(P, f='arctanh'),\n           ),\n 'cos':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cos'),\n           None,\n-          TD('e', f='cos', astype={'e': 'f'}),\n           TD('f', dispatch=[('loops_trigonometric', 'f')]),\n-          TD('d', dispatch=[('loops_umath_fp', 'd')]),\n+          TD('ed', dispatch=[('loops_umath_fp', 'ed')]),\n           TD('fdg' + cmplx, f='cos'),\n           TD(P, f='cos'),\n           ),\n 'sin':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sin'),\n           None,\n-          TD('e', f='sin', astype={'e': 'f'}),\n           TD('f', dispatch=[('loops_trigonometric', 'f')]),\n-          TD('d', dispatch=[('loops_umath_fp', 'd')]),\n+          TD('ed', dispatch=[('loops_umath_fp', 'ed')]),\n           TD('fdg' + cmplx, f='sin'),\n           TD(P, f='sin'),\n           ),\n 'tan':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.tan'),\n           None,\n-          TD('e', f='tan', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='tan', astype={'e': 'f'}),\n           TD(P, f='tan'),\n           ),\n 'cosh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cosh'),\n           None,\n-          TD('e', f='cosh', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='cosh', astype={'e': 'f'}),\n           TD(P, f='cosh'),\n           ),\n 'sinh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sinh'),\n           None,\n-          TD('e', f='sinh', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='sinh', astype={'e': 'f'}),\n           TD(P, f='sinh'),\n           ),\n 'tanh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.tanh'),\n           None,\n-          TD('e', f='tanh', astype={'e': 'f'}),\n+          TD('e', dispatch=[('loops_umath_fp', 'e')]),\n           TD('fd', dispatch=[('loops_hyperbolic', 'fd')]),\n           TD(inexact, f='tanh', astype={'e': 'f'}),\n           TD(P, f='tanh'),\n@@ -755,7 +744,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.exp'),\n           None,\n-          TD('e', f='exp', astype={'e': 'f'}),\n+          TD('e', dispatch=[('loops_umath_fp', 'e')]),\n           TD('fd', dispatch=[('loops_exponent_log', 'fd')]),\n           TD('fdg' + cmplx, f='exp'),\n           TD(P, f='exp'),\n@@ -764,25 +753,23 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.exp2'),\n           None,\n-          TD('e', f='exp2', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='exp2', astype={'e': 'f'}),\n           TD(P, f='exp2'),\n           ),\n 'expm1':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.expm1'),\n           None,\n-          TD('e', f='expm1', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='expm1', astype={'e': 'f'}),\n           TD(P, f='expm1'),\n           ),\n 'log':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log'),\n           None,\n-          TD('e', f='log', astype={'e': 'f'}),\n+          TD('e', dispatch=[('loops_umath_fp', 'e')]),\n           TD('fd', dispatch=[('loops_exponent_log', 'fd')]),\n           TD('fdg' + cmplx, f='log'),\n           TD(P, f='log'),\n@@ -791,26 +778,23 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log2'),\n           None,\n-          TD('e', f='log2', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='log2', astype={'e': 'f'}),\n           TD(P, f='log2'),\n           ),\n 'log10':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log10'),\n           None,\n-          TD('e', f='log10', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='log10', astype={'e': 'f'}),\n           TD(P, f='log10'),\n           ),\n 'log1p':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log1p'),\n           None,\n-          TD('e', f='log1p', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(inexact, f='log1p', astype={'e': 'f'}),\n           TD(P, f='log1p'),\n           ),\n@@ -827,8 +811,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cbrt'),\n           None,\n-          TD('e', f='cbrt', astype={'e': 'f'}),\n-          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD('efd', dispatch=[('loops_umath_fp', 'efd')]),\n           TD(flts, f='cbrt', astype={'e': 'f'}),\n           TD(P, f='cbrt'),\n           ),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1077,7 +1077,6 @@ def generate_umath_doc_header(ext, build_dir):\n     # actually the other way around, better performance and\n     # after all maintainable code.\n     svml_filter = (\n-        'svml_z0_tanh_d_la.s', 'svml_z0_tanh_s_la.s'\n     )\n     if can_link_svml() and check_svml_submodule(svml_path):\n         svml_objs = glob.glob(svml_path + '/**/*.s', recursive=True)"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -273,6 +273,15 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@func@,\n /**end repeat1**/\n /**end repeat**/\n \n+/**begin repeat\n+ * #func = sin, cos, tan, exp, exp2, log, log2, log10, expm1, log1p, cbrt, arcsin, arccos, arctan, sinh, cosh, tanh, arcsinh, arccosh, arctanh#\n+ */\n+\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void HALF_@func@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+\n+/**end repeat**/\n+\n /**begin repeat\n  * #func = sin, cos#\n  */"
            },
            {
                "filename": "numpy/core/src/umath/loops_umath_fp.dispatch.c.src",
                "patch": "@@ -82,7 +82,7 @@ simd_@func@_f64(const double *src, npy_intp ssrc,\n /**begin repeat1\n  * #func = pow, atan2#\n  */\n- \n+\n static void\n simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src1, npy_intp ssrc1,\n                   const npyv_lanetype_@sfx@ *src2, npy_intp ssrc2,\n@@ -96,14 +96,14 @@ simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src1, npy_intp ssrc1,\n         } else {\n             x1 = npyv_loadn_till_@sfx@(src1, ssrc1, len, 1);\n         }\n-        \n+\n         npyv_@sfx@ x2;\n         if (ssrc2 == 1) {\n             x2 = npyv_load_till_@sfx@(src2, len, 1);\n         } else {\n             x2 = npyv_loadn_till_@sfx@(src2, ssrc2, len, 1);\n         }\n-        \n+\n         npyv_@sfx@ out = __svml_@func@@func_suffix@(x1, x2);\n         if (sdst == 1) {\n             npyv_store_till_@sfx@(dst, len, out);\n@@ -115,7 +115,83 @@ simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src1, npy_intp ssrc1,\n /**end repeat1**/\n /**end repeat**/\n \n+typedef __m256i npyvh_f16;\n+#define npyv_cvt_f16_f32 _mm512_cvtph_ps\n+#define npyv_cvt_f32_f16 _mm512_cvtps_ph\n+#define npyvh_load_f16(PTR) _mm256_loadu_si256((const __m256i*)(PTR))\n+#define npyvh_store_f16(PTR, data) _mm256_storeu_si256((__m256i*)PTR, data)\n+NPY_FINLINE npyvh_f16 npyvh_load_till_f16(const npy_half *ptr, npy_uintp nlane, npy_half fill)\n+{\n+    assert(nlane > 0);\n+    const __m256i vfill = _mm256_set1_epi16(fill);\n+    const __mmask16 mask = (0x0001 << nlane) - 0x0001;\n+    return _mm256_mask_loadu_epi16(vfill, mask, ptr);\n+}\n+NPY_FINLINE void npyvh_store_till_f16(npy_half *ptr, npy_uintp nlane, npyvh_f16 data)\n+{\n+    assert(nlane > 0);\n+    const __mmask16 mask = (0x0001 << nlane) - 0x0001;\n+    _mm256_mask_storeu_epi16(ptr, mask, data);\n+}\n+\n+/**begin repeat\n+ * #func = sin, cos, tan, exp, exp2, expm1, log, log2, log10, log1p, cbrt, asin, acos, atan, sinh, cosh, tanh, asinh, acosh, atanh#\n+ * #default_val = 0, 0, 0, 0, 0, 0x3c00, 0x3c00, 0x3c00, 0x3c00, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x3c00, 0#\n+ */\n+static void\n+avx512_@func@_f16(const npy_half *src, npy_half *dst, npy_intp len)\n+{\n+    const int num_lanes = npyv_nlanes_f32;\n+    npyvh_f16 x, out;\n+    npyv_f32 x_ps, out_ps;\n+    for (; len > 0; len -= num_lanes, src += num_lanes, dst += num_lanes) {\n+        if (len >= num_lanes) {\n+            x       = npyvh_load_f16(src);\n+            x_ps    = npyv_cvt_f16_f32(x);\n+            out_ps  = __svml_@func@f16(x_ps);\n+            out     = npyv_cvt_f32_f16(out_ps, 0);\n+            npyvh_store_f16(dst, out);\n+        }\n+        else {\n+            x       = npyvh_load_till_f16(src, len, @default_val@);\n+            x_ps    = npyv_cvt_f16_f32(x);\n+            out_ps  = __svml_@func@f16(x_ps);\n+            out     = npyv_cvt_f32_f16(out_ps, 0);\n+            npyvh_store_till_f16(dst, len, out);\n+        }\n+    }\n+    npyv_cleanup();\n+}\n+/**end repeat**/\n+#endif\n+\n+/**begin repeat\n+ *  #func = sin, cos, tan, exp, exp2, expm1, log, log2, log10, log1p, cbrt, arcsin, arccos, arctan, sinh, cosh, tanh, arcsinh, arccosh, arctanh#\n+ *  #intrin = sin, cos, tan, exp, exp2, expm1, log, log2, log10, log1p, cbrt, asin, acos, atan, sinh, cosh, tanh, asinh, acosh, atanh#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(HALF_@func@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+    const npy_half *src = (npy_half*)args[0];\n+          npy_half *dst = (npy_half*)args[1];\n+    const int lsize = sizeof(src[0]);\n+    const npy_intp ssrc = steps[0] / lsize;\n+    const npy_intp sdst = steps[1] / lsize;\n+    const npy_intp len = dimensions[0];\n+    if (!is_mem_overlap(src, steps[0], dst, steps[1], len) &&\n+        (ssrc == 1) &&\n+        (sdst == 1)) {\n+        avx512_@intrin@_f16(src, dst, len);\n+        return;\n+    }\n #endif\n+    UNARY_LOOP {\n+        const npy_float in1 = npy_half_to_float(*(npy_half *)ip1);\n+        *((npy_half *)op1) = npy_float_to_half(npy_@intrin@f(in1));\n+    }\n+}\n+/**end repeat**/\n \n /**begin repeat\n  *  #TYPE = DOUBLE, FLOAT#"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21507,
        "body": "### Changes\r\n\r\n- Handle overflow cases\r\n- Testcases for same\r\n\r\nRelated: https://github.com/numpy/numpy/issues/21506\r\nFinishes: https://github.com/numpy/numpy/pull/19260",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/loops_modulo.dispatch.c.src",
                "patch": "@@ -12,6 +12,21 @@\n // Provides the various *_LOOP macros\n #include \"fast_loop_macros.h\"\n \n+\n+#define DIVIDEBYZERO_OVERFLOW_CHECK(x, y, min_val, signed) \\\n+    (NPY_UNLIKELY(                                         \\\n+        (signed)                                    ?      \\\n+        ((y == 0) || ((x == min_val) && (y == -1))) :      \\\n+        (y == 0))                                          \\\n+    )\n+\n+#define FLAG_IF_DIVIDEBYZERO(x) do {     \\\n+    if (NPY_UNLIKELY(x == 0)) {          \\\n+        npy_set_floatstatus_divbyzero(); \\\n+    }                                    \\\n+} while (0)\n+\n+\n #if NPY_SIMD && defined(NPY_HAVE_VSX4)\n typedef struct {\n     npyv_u32x2 hi;\n@@ -166,7 +181,6 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n     const int vstep           = npyv_nlanes_@sfx@;\n #if @id@ == 2 /* divmod */\n     npyv_lanetype_@sfx@ *dst2 = (npyv_lanetype_@sfx@ *) args[3];\n-    const npyv_@sfx@ vneg_one = npyv_setall_@sfx@(-1);\n     npyv_b@len@ warn          = npyv_cvt_b@len@_@sfx@(npyv_zero_@sfx@());\n \n     for (; len >= vstep; len -= vstep, src1 += vstep, src2 += vstep,\n@@ -176,11 +190,11 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n         npyv_@sfx@ quo      = vsx4_div_@sfx@(a, b);\n         npyv_@sfx@ rem      = npyv_sub_@sfx@(a, vec_mul(b, quo));\n         npyv_b@len@ bzero   = npyv_cmpeq_@sfx@(b, vzero);\n-        // when b is 0, 'cvtozero' forces the modulo to be 0 too\n-        npyv_@sfx@ cvtozero = npyv_select_@sfx@(bzero, vzero, vneg_one);\n+        // when b is 0, forces the remainder to be 0 too\n+                        rem = npyv_select_@sfx@(bzero, vzero, rem);\n                        warn = npyv_or_@sfx@(bzero, warn);\n         npyv_store_@sfx@(dst1, quo);\n-        npyv_store_@sfx@(dst2, npyv_and_@sfx@(cvtozero, rem));\n+        npyv_store_@sfx@(dst2, rem);\n     }\n \n     if (!vec_all_eq(warn, vzero)) {\n@@ -290,7 +304,8 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n     npyv_lanetype_@sfx@ *dst2 = (npyv_lanetype_@sfx@ *) args[3];\n     const npyv_@sfx@ vneg_one = npyv_setall_@sfx@(-1);\n     const npyv_@sfx@ vmin     = npyv_setall_@sfx@(NPY_MIN_INT@len@);\n-    npyv_b@len@ warn          = npyv_cvt_b@len@_@sfx@(npyv_zero_@sfx@());\n+    npyv_b@len@ warn_zero     = npyv_cvt_b@len@_@sfx@(npyv_zero_@sfx@());\n+    npyv_b@len@ warn_overflow = npyv_cvt_b@len@_@sfx@(npyv_zero_@sfx@());\n \n     for (; len >= vstep; len -= vstep, src1 += vstep, src2 += vstep,\n          dst1 += vstep, dst2 += vstep) {\n@@ -310,10 +325,8 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n         npyv_b@len@ amin     = npyv_cmpeq_@sfx@(a, vmin);\n         npyv_b@len@ bneg_one = npyv_cmpeq_@sfx@(b, vneg_one);\n         npyv_b@len@ overflow = npyv_and_@sfx@(bneg_one, amin);\n-        npyv_b@len@ error    = npyv_or_@sfx@(bzero, overflow);\n-        // in case of overflow or b = 0, 'cvtozero' forces quo/rem to be 0\n-        npyv_@sfx@ cvtozero  = npyv_select_@sfx@(error, vzero, vneg_one);\n-                        warn = npyv_or_@sfx@(error, warn);\n+                warn_zero = npyv_or_@sfx@(bzero, warn_zero);\n+               warn_overflow = npyv_or_@sfx@(overflow, warn_overflow);\n #endif\n #if @id@ >= 1 /* remainder and divmod */\n         // handle mixed case the way Python does\n@@ -329,8 +342,14 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n #if @id@ == 2 /* divmod */\n         npyv_@sfx@ to_sub = npyv_select_@sfx@(or, vzero, vneg_one);\n                       quo = npyv_add_@sfx@(quo, to_sub);\n-        npyv_store_@sfx@(dst1, npyv_and_@sfx@(cvtozero, quo));\n-        npyv_store_@sfx@(dst2, npyv_and_@sfx@(cvtozero, rem));\n+                      // Divide by zero\n+                      quo = npyv_select_@sfx@(bzero, vzero, quo);\n+                      rem = npyv_select_@sfx@(bzero, vzero, rem);\n+                      // Overflow\n+                      quo = npyv_select_@sfx@(overflow, vmin, quo);\n+                      rem = npyv_select_@sfx@(overflow, vzero, rem);\n+        npyv_store_@sfx@(dst1, quo);\n+        npyv_store_@sfx@(dst2, rem);\n #else /* fmod and remainder */\n         npyv_store_@sfx@(dst1, rem);\n         if (NPY_UNLIKELY(vec_any_eq(b, vzero))) {\n@@ -340,17 +359,27 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n     }\n \n #if @id@ == 2 /* divmod */\n-    if (!vec_all_eq(warn, vzero)) {\n+    if (!vec_all_eq(warn_zero, vzero)) {\n         npy_set_floatstatus_divbyzero();\n     }\n+    if (!vec_all_eq(warn_overflow, vzero)) {\n+        npy_set_floatstatus_overflow();\n+    }\n \n     for (; len > 0; --len, ++src1, ++src2, ++dst1, ++dst2) {\n         const npyv_lanetype_@sfx@ a = *src1;\n         const npyv_lanetype_@sfx@ b = *src2;\n-        if (b == 0 || (a == NPY_MIN_INT@len@ && b == -1)) {\n-            npy_set_floatstatus_divbyzero();\n-            *dst1 = 0;\n-            *dst2 = 0;\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(a, b, NPY_MIN_INT@len@, NPY_TRUE)) {\n+            if (b == 0) {\n+                npy_set_floatstatus_divbyzero();\n+                *dst1 = 0;\n+                *dst2 = 0;\n+            }\n+            else {\n+                npy_set_floatstatus_overflow();\n+                *dst1 = NPY_MIN_INT@len@;\n+                *dst2 = 0;\n+            }\n         }\n         else {\n             *dst1 = a / b;\n@@ -365,8 +394,8 @@ vsx4_simd_@func@_contig_@sfx@(char **args, npy_intp len)\n     for (; len > 0; --len, ++src1, ++src2, ++dst1) {\n         const npyv_lanetype_@sfx@ a = *src1;\n         const npyv_lanetype_@sfx@ b = *src2;\n-        if (NPY_UNLIKELY(b == 0)) {\n-            npy_set_floatstatus_divbyzero();\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(a, b, NPY_MIN_INT@len@, NPY_TRUE)) {\n+            FLAG_IF_DIVIDEBYZERO(b);\n             *dst1 = 0;\n         } else{\n             *dst1 = a % b;\n@@ -415,8 +444,6 @@ vsx4_simd_@func@_by_scalar_contig_@sfx@(char **args, npy_intp len)\n         // (a == NPY_MIN_INT@len@ && b == -1)\n         npyv_b@len@ amin     = npyv_cmpeq_@sfx@(a, vmin);\n         npyv_b@len@ overflow = npyv_and_@sfx@(bneg_one, amin);\n-        // in case of overflow, 'cvtozero' forces quo/rem to be 0\n-        npyv_@sfx@ cvtozero  = npyv_select_@sfx@(overflow, vzero, vneg_one);\n                         warn = npyv_or_@sfx@(overflow, warn);\n #endif\n #if @id@ >= 1 /* remainder and divmod */\n@@ -432,23 +459,26 @@ vsx4_simd_@func@_by_scalar_contig_@sfx@(char **args, npy_intp len)\n #if @id@ == 2 /* divmod */\n         npyv_@sfx@ to_sub = npyv_select_@sfx@(or, vzero, vneg_one);\n         quo               = npyv_add_@sfx@(quo, to_sub);\n-        npyv_store_@sfx@(dst1, npyv_and_@sfx@(cvtozero, quo));\n-        npyv_store_@sfx@(dst2, npyv_and_@sfx@(cvtozero, rem));\n+        // Overflow: set quo to minimum and rem to 0\n+        quo               = npyv_select_@sfx@(overflow, vmin, quo);\n+        rem               = npyv_select_@sfx@(overflow, vzero, rem);\n+        npyv_store_@sfx@(dst1, quo);\n+        npyv_store_@sfx@(dst2, rem);\n #else /* fmod and remainder */\n         npyv_store_@sfx@(dst1, rem);\n #endif\n     }\n \n #if @id@ == 2 /* divmod */\n     if (!vec_all_eq(warn, vzero)) {\n-        npy_set_floatstatus_divbyzero();\n+        npy_set_floatstatus_overflow();\n     }\n \n     for (; len > 0; --len, ++src1, ++dst1, ++dst2) {\n         const npyv_lanetype_@sfx@ a = *src1;\n-        if (a == NPY_MIN_INT@len@ && scalar == -1) {\n-            npy_set_floatstatus_divbyzero();\n-            *dst1 = 0;\n+        if (NPY_UNLIKELY(a == NPY_MIN_INT@len@ && scalar == -1)) {\n+            npy_set_floatstatus_overflow();\n+            *dst1 = NPY_MIN_INT@len@;\n             *dst2 = 0;\n         }\n         else {\n@@ -524,8 +554,12 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_fmod)\n     BINARY_LOOP {\n         const @type@ in1 = *(@type@ *)ip1;\n         const @type@ in2 = *(@type@ *)ip2;\n-        if (NPY_UNLIKELY(in2 == 0)) {\n-            npy_set_floatstatus_divbyzero();\n+#if @signed@\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(in1, in2, NPY_MIN_@TYPE@, NPY_TRUE)) {\n+#else\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(in1, in2, 0, NPY_FALSE)) {\n+#endif\n+            FLAG_IF_DIVIDEBYZERO(in2);\n             *((@type@ *)op1) = 0;\n         } else{\n             *((@type@ *)op1)= in1 % in2;\n@@ -552,8 +586,12 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_remainder)\n     BINARY_LOOP {\n         const @type@ in1 = *(@type@ *)ip1;\n         const @type@ in2 = *(@type@ *)ip2;\n-        if (NPY_UNLIKELY(in2 == 0)) {\n-            npy_set_floatstatus_divbyzero();\n+#if @signed@\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(in1, in2, NPY_MIN_@TYPE@, NPY_TRUE)) {\n+#else\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(in1, in2, 0, NPY_FALSE)) {\n+#endif\n+            FLAG_IF_DIVIDEBYZERO(in2);\n             *((@type@ *)op1) = 0;\n         } else{\n #if @signed@\n@@ -593,10 +631,17 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_divmod)\n         const @type@ in1 = *(@type@ *)ip1;\n         const @type@ in2 = *(@type@ *)ip2;\n         /* see FIXME note for divide above */\n-        if (NPY_UNLIKELY(in2 == 0 || (in1 == NPY_MIN_@TYPE@ && in2 == -1))) {\n-            npy_set_floatstatus_divbyzero();\n-            *((@type@ *)op1) = 0;\n-            *((@type@ *)op2) = 0;\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(in1, in2, NPY_MIN_@TYPE@, NPY_TRUE)) {\n+            if (in2 == 0) {\n+                npy_set_floatstatus_divbyzero();\n+                *((@type@ *)op1) = 0;\n+                *((@type@ *)op2) = 0;\n+            }\n+            else {\n+                npy_set_floatstatus_overflow();\n+                *((@type@ *)op1) = NPY_MIN_@TYPE@;\n+                *((@type@ *)op2) = 0;\n+            }\n         }\n         else {\n             /* handle mixed case the way Python does */\n@@ -616,7 +661,7 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_divmod)\n     BINARY_LOOP_TWO_OUT {\n         const @type@ in1 = *(@type@ *)ip1;\n         const @type@ in2 = *(@type@ *)ip2;\n-        if (NPY_UNLIKELY(in2 == 0)) {\n+        if (DIVIDEBYZERO_OVERFLOW_CHECK(in1, in2, 0, NPY_FALSE)) {\n             npy_set_floatstatus_divbyzero();\n             *((@type@ *)op1) = 0;\n             *((@type@ *)op2) = 0;"
            },
            {
                "filename": "numpy/core/src/umath/scalarmath.c.src",
                "patch": "@@ -161,6 +161,13 @@ static NPY_INLINE int\n  * #NAME = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n  *         LONG, ULONG, LONGLONG, ULONGLONG#\n  */\n+\n+#if @neg@\n+    #define DIVIDEBYZERO_CHECK (b == 0 || (a == NPY_MIN_@NAME@ && b == -1))\n+#else\n+    #define DIVIDEBYZERO_CHECK (b == 0)\n+#endif\n+\n static NPY_INLINE int\n @name@_ctype_divide(@type@ a, @type@ b, @type@ *out) {\n     if (b == 0) {\n@@ -169,7 +176,7 @@ static NPY_INLINE int\n     }\n #if @neg@\n     else if (b == -1 && a == NPY_MIN_@NAME@) {\n-        *out = a / b;\n+        *out = NPY_MIN_@NAME@;\n         return NPY_FPE_OVERFLOW;\n     }\n #endif\n@@ -192,7 +199,7 @@ static NPY_INLINE int\n \n static NPY_INLINE int\n @name@_ctype_remainder(@type@ a, @type@ b, @type@ *out) {\n-    if (a == 0 || b == 0) {\n+    if (DIVIDEBYZERO_CHECK) {\n         *out = 0;\n         if (b == 0) {\n             return NPY_FPE_DIVIDEBYZERO;\n@@ -213,6 +220,7 @@ static NPY_INLINE int\n #endif\n     return 0;\n }\n+#undef DIVIDEBYZERO_CHECK\n /**end repeat**/\n \n /**begin repeat"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -5,8 +5,10 @@\n import pytest\n import sys\n import os\n+import operator\n from fractions import Fraction\n from functools import reduce\n+from collections import namedtuple\n \n import numpy.core.umath as ncu\n from numpy.core import _umath_tests as ncu_tests\n@@ -20,6 +22,62 @@\n from numpy.testing._private.utils import _glibc_older_than\n \n \n+def interesting_binop_operands(val1, val2, dtype):\n+    \"\"\"\n+    Helper to create \"interesting\" operands to cover common code paths:\n+    * scalar inputs\n+    * only first \"values\" is an array (e.g. scalar division fast-paths)\n+    * Longer array (SIMD) placing the value of interest at different positions\n+    * Oddly strided arrays which may not be SIMD compatible\n+\n+    It does not attempt to cover unaligned access or mixed dtypes.\n+    These are normally handled by the casting/buffering machinery.\n+\n+    This is not a fixture (currently), since I believe a fixture normally\n+    only yields once?\n+    \"\"\"\n+    fill_value = 1  # could be a parameter, but maybe not an optional one?\n+\n+    arr1 = np.full(10003, dtype=dtype, fill_value=fill_value)\n+    arr2 = np.full(10003, dtype=dtype, fill_value=fill_value)\n+\n+    arr1[0] = val1\n+    arr2[0] = val2\n+\n+    extractor = lambda res: res\n+    yield arr1[0], arr2[0], extractor, \"scalars\"\n+\n+    extractor = lambda res: res\n+    yield arr1[0, ...], arr2[0, ...], extractor, \"scalar-arrays\"\n+\n+    # reset array values to fill_value:\n+    arr1[0] = fill_value\n+    arr2[0] = fill_value\n+\n+    for pos in [0, 1, 2, 3, 4, 5, -1, -2, -3, -4]:\n+        arr1[pos] = val1\n+        arr2[pos] = val2\n+\n+        extractor = lambda res: res[pos]\n+        yield arr1, arr2, extractor, f\"off-{pos}\"\n+        yield arr1, arr2[pos], extractor, f\"off-{pos}-with-scalar\"\n+\n+        arr1[pos] = fill_value\n+        arr2[pos] = fill_value\n+\n+    for stride in [-1, 113]:\n+        op1 = arr1[::stride]\n+        op2 = arr2[::stride]\n+        op1[10] = val1\n+        op2[10] = val2\n+\n+        extractor = lambda res: res[10]\n+        yield op1, op2, extractor, f\"stride-{stride}\"\n+\n+        op1[10] = fill_value\n+        op2[10] = fill_value\n+\n+\n def on_powerpc():\n     \"\"\" True if we are running on a Power PC platform.\"\"\"\n     return platform.processor() == 'powerpc' or \\\n@@ -740,6 +798,140 @@ def test_float_remainder_corner_cases(self):\n                 assert_(np.isnan(fmod), 'dt: %s, fmod: %s' % (dt, rem))\n \n \n+class TestDivisionIntegerOverflowsAndDivideByZero:\n+    result_type = namedtuple('result_type',\n+            ['nocast', 'casted'])\n+    helper_lambdas = {\n+        'zero': lambda dtype: 0,\n+        'min': lambda dtype: np.iinfo(dtype).min,\n+        'neg_min': lambda dtype: -np.iinfo(dtype).min,\n+        'min-zero': lambda dtype: (np.iinfo(dtype).min, 0),\n+        'neg_min-zero': lambda dtype: (-np.iinfo(dtype).min, 0),\n+    }\n+    overflow_results = {\n+        np.remainder: result_type(\n+            helper_lambdas['zero'], helper_lambdas['zero']),\n+        np.fmod: result_type(\n+            helper_lambdas['zero'], helper_lambdas['zero']),\n+        operator.mod: result_type(\n+            helper_lambdas['zero'], helper_lambdas['zero']),\n+        operator.floordiv: result_type(\n+            helper_lambdas['min'], helper_lambdas['neg_min']),\n+        np.floor_divide: result_type(\n+            helper_lambdas['min'], helper_lambdas['neg_min']),\n+        np.divmod: result_type(\n+            helper_lambdas['min-zero'], helper_lambdas['neg_min-zero'])\n+    }\n+\n+    @pytest.mark.parametrize(\"dtype\", np.typecodes[\"Integer\"])\n+    def test_signed_division_overflow(self, dtype):\n+        to_check = interesting_binop_operands(np.iinfo(dtype).min, -1, dtype)\n+        for op1, op2, extractor, operand_identifier in to_check:\n+            with pytest.warns(RuntimeWarning, match=\"overflow encountered\"):\n+                res = op1 // op2\n+\n+            assert res.dtype == op1.dtype\n+            assert extractor(res) == np.iinfo(op1.dtype).min\n+\n+            # Remainder is well defined though, and does not warn:\n+            res = op1 % op2\n+            assert res.dtype == op1.dtype\n+            assert extractor(res) == 0\n+            # Check fmod as well:\n+            res = np.fmod(op1, op2)\n+            assert extractor(res) == 0\n+\n+            # Divmod warns for the division part:\n+            with pytest.warns(RuntimeWarning, match=\"overflow encountered\"):\n+                res1, res2 = np.divmod(op1, op2)\n+\n+            assert res1.dtype == res2.dtype == op1.dtype\n+            assert extractor(res1) == np.iinfo(op1.dtype).min\n+            assert extractor(res2) == 0\n+\n+    @pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n+    def test_divide_by_zero(self, dtype):\n+        # Note that the return value cannot be well defined here, but NumPy\n+        # currently uses 0 consistently.  This could be changed.\n+        to_check = interesting_binop_operands(1, 0, dtype)\n+        for op1, op2, extractor, operand_identifier in to_check:\n+            with pytest.warns(RuntimeWarning, match=\"divide by zero\"):\n+                res = op1 // op2\n+\n+            assert res.dtype == op1.dtype\n+            assert extractor(res) == 0\n+\n+            with pytest.warns(RuntimeWarning, match=\"divide by zero\"):\n+                res1, res2 = np.divmod(op1, op2)\n+\n+            assert res1.dtype == res2.dtype == op1.dtype\n+            assert extractor(res1) == 0\n+            assert extractor(res2) == 0\n+\n+    @pytest.mark.parametrize(\"dividend_dtype\",\n+            np.sctypes['int'])\n+    @pytest.mark.parametrize(\"divisor_dtype\",\n+            np.sctypes['int'])\n+    @pytest.mark.parametrize(\"operation\",\n+            [np.remainder, np.fmod, np.divmod, np.floor_divide,\n+             operator.mod, operator.floordiv])\n+    @np.errstate(divide='warn', over='warn')\n+    def test_overflows(self, dividend_dtype, divisor_dtype, operation):\n+        # SIMD tries to perform the operation on as many elements as possible\n+        # that is a multiple of the register's size. We resort to the\n+        # default implementation for the leftover elements.\n+        # We try to cover all paths here.\n+        arrays = [np.array([np.iinfo(dividend_dtype).min]*i,\n+                           dtype=dividend_dtype) for i in range(1, 129)]\n+        divisor = np.array([-1], dtype=divisor_dtype)\n+        # If dividend is a larger type than the divisor (`else` case),\n+        # then, result will be a larger type than dividend and will not\n+        # result in an overflow for `divmod` and `floor_divide`.\n+        if np.dtype(dividend_dtype).itemsize >= np.dtype(\n+                divisor_dtype).itemsize and operation in (\n+                        np.divmod, np.floor_divide, operator.floordiv):\n+            with pytest.warns(\n+                    RuntimeWarning,\n+                    match=\"overflow encountered in\"):\n+                result = operation(\n+                            dividend_dtype(np.iinfo(dividend_dtype).min),\n+                            divisor_dtype(-1)\n+                        )\n+                assert result == self.overflow_results[operation].nocast(\n+                        dividend_dtype)\n+\n+            # Arrays\n+            for a in arrays:\n+                # In case of divmod, we need to flatten the result\n+                # column first as we get a column vector of quotient and\n+                # remainder and a normal flatten of the expected result.\n+                with pytest.warns(\n+                        RuntimeWarning,\n+                        match=\"overflow encountered in\"):\n+                    result = np.array(operation(a, divisor)).flatten('f')\n+                    expected_array = np.array(\n+                            [self.overflow_results[operation].nocast(\n+                                dividend_dtype)]*len(a)).flatten()\n+                    assert_array_equal(result, expected_array)\n+        else:\n+            # Scalars\n+            result = operation(\n+                        dividend_dtype(np.iinfo(dividend_dtype).min),\n+                        divisor_dtype(-1)\n+                    )\n+            assert result == self.overflow_results[operation].casted(\n+                    dividend_dtype)\n+\n+            # Arrays\n+            for a in arrays:\n+                # See above comment on flatten\n+                result = np.array(operation(a, divisor)).flatten('f')\n+                expected_array = np.array(\n+                        [self.overflow_results[operation].casted(\n+                            dividend_dtype)]*len(a)).flatten()\n+                assert_array_equal(result, expected_array)\n+\n+\n class TestCbrt:\n     def test_cbrt_scalar(self):\n         assert_almost_equal((np.cbrt(np.float32(-2.5)**3)), -2.5)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22204,
        "body": "Why the test failed\r\n\r\n * Since Python 3.8, the default start method for multiprocessing has been changed from `fork` to `spawn` on macOS\r\n * The default start method is still `fork` on other Unix platforms[1], causing inconsistency on memory sharing model\r\n * It will cause a memory-sharing problem for the test `test_large_zip` on macOS as the memory sharing model between `spawn` and `fork` is different\r\n\r\nThe fix\r\n\r\n * Change the start method for this test back to `fork` under this testcase context\r\n * In this test case context, [the bug](/python/cpython/issues/77906) that caused default start method changed to `spawn` for macOS will not be triggered\r\n * It is context limited, so this change will not affect default start method other than `test_large_zip`\r\n * All platforms have the **same memory sharing model** now\r\n * After the change, `test_large_zip` is passed on macOS\r\n\r\n1. https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods",
        "changed_files": [
            {
                "filename": "numpy/lib/tests/test_io.py",
                "patch": "@@ -13,7 +13,7 @@\n from io import BytesIO, StringIO\n from datetime import datetime\n import locale\n-from multiprocessing import Process, Value\n+from multiprocessing import Value, get_context\n from ctypes import c_bool\n \n import numpy as np\n@@ -595,7 +595,12 @@ def check_large_zip(memoryerror_raised):\n         # Use an object in shared memory to re-raise the MemoryError exception\n         # in our process if needed, see gh-16889\n         memoryerror_raised = Value(c_bool)\n-        p = Process(target=check_large_zip, args=(memoryerror_raised,))\n+\n+        # Since Python 3.8, the default start method for multiprocessing has \n+        # been changed from 'fork' to 'spawn' on macOS, causing inconsistency \n+        # on memory sharing model, lead to failed test for check_large_zip\n+        ctx = get_context('fork')\n+        p = ctx.Process(target=check_large_zip, args=(memoryerror_raised,))\n         p.start()\n         p.join()\n         if memoryerror_raised.value:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24473,
        "body": "* The docstring of `numpy.polynomial.polyutils.trimseq` stated \"This routine fails for empty sequences.\". This is incorrect, as `trimseq` works fine on empty sequences (there is in fact an if statement especially for this case). In this PR we modify the docstring and add tests to make sure the `trimseq` works fine on empty sequences.\r\n* The most common case for `trimseq` are non-empty sequences (the coefficients of polynomials are non-empty by construction) with final coefficient non-zero. We add a fast path for the case where the final coefficient is non-zero.\r\n\r\nBenchmark\r\n```\r\nfrom numpy.polynomial import Polynomial\r\nfrom numpy.polynomial.polyutils import trimseq\r\n\r\np = Polynomial([1,2,3])\r\n\r\n%timeit trimseq(p.coef)\r\n``` \r\nResults:\r\n```\r\nmain: 565 ns \u00b1 36 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\nPR: 224 ns \u00b1 21.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n```\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/polynomial/polyutils.py",
                "patch": "@@ -57,8 +57,7 @@ def trimseq(seq):\n     Parameters\n     ----------\n     seq : sequence\n-        Sequence of Poly series coefficients. This routine fails for\n-        empty sequences.\n+        Sequence of Poly series coefficients.\n \n     Returns\n     -------\n@@ -72,7 +71,7 @@ def trimseq(seq):\n     Do not lose the type info if the sequence contains unknown objects.\n \n     \"\"\"\n-    if len(seq) == 0:\n+    if len(seq) == 0 or seq[-1] != 0:\n         return seq\n     else:\n         for i in range(len(seq) - 1, -1, -1):"
            },
            {
                "filename": "numpy/polynomial/tests/test_polyutils.py",
                "patch": "@@ -11,11 +11,15 @@\n class TestMisc:\n \n     def test_trimseq(self):\n-        for i in range(5):\n-            tgt = [1]\n-            res = pu.trimseq([1] + [0]*5)\n+        tgt = [1]\n+        for num_trailing_zeros in range(5):\n+            res = pu.trimseq([1] + [0] * num_trailing_zeros)\n             assert_equal(res, tgt)\n \n+    def test_trimseq_empty_input(self):\n+        for empty_seq in [[], np.array([], dtype=np.int32)]:\n+            assert_equal(pu.trimseq(empty_seq), empty_seq)\n+\n     def test_as_series(self):\n         # check exceptions\n         assert_raises(ValueError, pu.as_series, [[]])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24379,
        "body": "This is a much more minimal alternative to gh-24365. It only vendors Meson, and uses PATH manipulation to get `meson-python` and `spin` to pick up our vendored Meson - which is a friendly fork which lives at https://github.com/numpy/meson and is meant to temporarily include the feature we need for SIMD support.\r\n\r\nTested in combination with gh-23096, and it looks like everything works. Let's see how happy CI is ....",
        "changed_files": [
            {
                "filename": ".github/workflows/linux_musl.yml",
                "patch": "@@ -44,6 +44,7 @@ jobs:\n             git checkout $GITHUB_BASE_REF\n             git -c user.email=\"you@example.com\" merge --no-commit my_ref_name\n         fi\n+        git submodule update --init\n \n         ln -s /usr/local/bin/python3.10 /usr/local/bin/python\n "
            },
            {
                "filename": ".gitmodules",
                "patch": "@@ -7,3 +7,6 @@\n [submodule \"numpy/core/src/npysort/x86-simd-sort\"]\n \tpath = numpy/core/src/npysort/x86-simd-sort\n \turl = https://github.com/intel/x86-simd-sort\n+[submodule \"vendored-meson/meson\"]\n+\tpath = vendored-meson/meson\n+\turl = https://github.com/numpy/meson.git"
            },
            {
                "filename": ".spin/cmds.py",
                "patch": "@@ -11,6 +11,54 @@\n from spin import util\n \n \n+# The numpy-vendored version of Meson. Put the directory that the executable\n+# `meson` is in at the front of the PATH.\n+curdir = pathlib.Path(__file__).parent.resolve()\n+meson_executable_dir = str(curdir.parent / 'vendored-meson' / 'entrypoint')\n+os.environ['PATH'] = meson_executable_dir + os.pathsep + os.environ['PATH']\n+\n+# Check that the meson git submodule is present\n+meson_import_dir = curdir.parent / 'vendored-meson' / 'meson' / 'mesonbuild'\n+if not meson_import_dir.exists():\n+    raise RuntimeError(\n+        'The `vendored-meson/meson` git submodule does not exist! ' +\n+        'Run `git submodule update --init` to fix this problem.'\n+    )\n+\n+\n+@click.command()\n+@click.option(\n+    \"-j\", \"--jobs\",\n+    help=\"Number of parallel tasks to launch\",\n+    type=int\n+)\n+@click.option(\n+    \"--clean\", is_flag=True,\n+    help=\"Clean build directory before build\"\n+)\n+@click.option(\n+    \"-v\", \"--verbose\", is_flag=True,\n+    help=\"Print all build output, even installation\"\n+)\n+@click.argument(\"meson_args\", nargs=-1)\n+@click.pass_context\n+def build(ctx, meson_args, jobs=None, clean=False, verbose=False):\n+    \"\"\"\ud83d\udd27 Build package with Meson/ninja and install\n+\n+    MESON_ARGS are passed through e.g.:\n+\n+    spin build -- -Dpkg_config_path=/lib64/pkgconfig\n+\n+    The package is installed to build-install\n+\n+    By default builds for release, to be able to use a debugger set CFLAGS\n+    appropriately. For example, for linux use\n+\n+    CFLAGS=\"-O0 -g\" spin build\n+    \"\"\"\n+    ctx.forward(meson.build)\n+\n+\n @click.command()\n @click.argument(\"sphinx_target\", default=\"html\")\n @click.option(\n@@ -32,7 +80,7 @@\n )\n @click.option(\n     \"--install-deps/--no-install-deps\",\n-    default=True,\n+    default=False,\n     help=\"Install dependencies before building\"\n )\n @click.pass_context"
            },
            {
                "filename": "LICENSES_bundled.txt",
                "patch": "@@ -20,3 +20,12 @@ Name: libdivide\n Files: numpy/core/include/numpy/libdivide/*\n License: Zlib\n   For license text, see numpy/core/include/numpy/libdivide/LICENSE.txt\n+\n+\n+Note that the following files are vendored in the repository and sdist but not\n+installed in built numpy packages:\n+\n+Name: Meson\n+Files: vendored-meson/meson/*\n+License: Apache 2.0\n+  For license text, see vendored-meson/meson/COPYING"
            },
            {
                "filename": "pyproject.toml",
                "patch": "@@ -1,5 +1,6 @@\n [build-system]\n-build-backend = \"mesonpy\"\n+build-backend = \"npbuild\"\n+backend-path = ['./vendored-meson/build-backend-wrapper']\n requires = [\n     \"Cython>=3.0\",\n     \"meson-python>=0.13.1\",\n@@ -184,7 +185,7 @@ repair-wheel-command = \"bash ./tools/wheels/repair_windows.sh {wheel} {dest_dir}\n package = 'numpy'\n \n [tool.spin.commands]\n-\"Build\" = [\"spin.cmds.meson.build\", \".spin/cmds.py:test\"]\n+\"Build\" = [\".spin/cmds.py:build\", \".spin/cmds.py:test\"]\n \"Environments\" = [\n   \".spin/cmds.py:run\", \".spin/cmds.py:ipython\",\n   \".spin/cmds.py:python\", \".spin/cmds.py:gdb\""
            },
            {
                "filename": "tools/travis-test.sh",
                "patch": "@@ -16,7 +16,7 @@ fi\n \n source builds/venv/bin/activate\n \n-pip install --upgrade pip 'setuptools<49.2.0'\n+pip install --upgrade pip 'setuptools<49.2.0' build\n \n pip install -r build_requirements.txt\n \n@@ -223,7 +223,7 @@ elif [ -n \"$USE_SDIST\" ] && [ $# -eq 0 ]; then\n   $PYTHON -c \"import fcntl; fcntl.fcntl(1, fcntl.F_SETFL, 0)\"\n   # ensure some warnings are not issued\n   export CFLAGS=$CFLAGS\" -Wno-sign-compare -Wno-unused-result -Wno-error=undef\"\n-  $PYTHON setup.py sdist\n+  $PYTHON -m build --sdist\n   # Make another virtualenv to install into\n   $PYTHON -m venv venv-for-wheel\n   . venv-for-wheel/bin/activate"
            },
            {
                "filename": "vendored-meson/build-backend-wrapper/npbuild/__init__.py",
                "patch": "@@ -0,0 +1,27 @@\n+import os\n+import sys\n+import pathlib\n+\n+from mesonpy import (\n+    build_sdist,\n+    build_wheel,\n+    build_editable,\n+    get_requires_for_build_sdist,\n+    get_requires_for_build_wheel,\n+    get_requires_for_build_editable,\n+)\n+\n+\n+# The numpy-vendored version of Meson. Put the directory that the executable\n+# `meson` is in at the front of the PATH.\n+curdir = pathlib.Path(__file__).parent.resolve()\n+meson_executable_dir = str(curdir.parent.parent / 'entrypoint')\n+os.environ['PATH'] = meson_executable_dir + os.pathsep + os.environ['PATH']\n+\n+# Check that the meson git submodule is present\n+meson_import_dir = curdir.parent.parent / 'meson' / 'mesonbuild'\n+if not meson_import_dir.exists():\n+    raise RuntimeError(\n+        'The `vendored-meson/meson` git submodule does not exist! ' +\n+        'Run `git submodule update --init` to fix this problem.'\n+    )"
            },
            {
                "filename": "vendored-meson/entrypoint/meson",
                "patch": "@@ -0,0 +1,23 @@\n+#!/usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+import re\n+import sys\n+import pathlib\n+\n+\n+# The numpy-vendored version of Meson\n+meson_dir = str(pathlib.Path(__file__).resolve().parent.parent / 'meson')\n+sys.path.insert(0, meson_dir)\n+\n+from mesonbuild.mesonmain import main\n+import mesonbuild\n+if not 'vendored-meson' in mesonbuild.__path__[0]:\n+    # Note: only the print statement will show most likely, not the exception.\n+    # If this goes wrong, it first fails inside meson-python on the `meson\n+    # --version` check.\n+    print(f'picking up the wrong `meson`: {mesonbuild.__path__}')\n+    raise RuntimeError('incorrect mesonbuild module, exiting')\n+\n+if __name__ == '__main__':\n+    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n+    sys.exit(main())"
            },
            {
                "filename": "vendored-meson/meson",
                "patch": "@@ -0,0 +1 @@\n+Subproject commit 1f8351f16f9ce55965449b8e299c6d0fbca7f5df"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21199,
        "body": "xref #9675 would be nice to cimport these rather than duplicating them in pandas.\r\n\r\nNeed to add a `cdef extern from \"numpy/foobar.h\":` for these in `__init__.pxd`, need guidance and what that foobar.h should be.\r\n\r\nIs there a standard way of testing what is exposed to cython?  cc @bashtage \r\n",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/21199.c_api.rst",
                "patch": "@@ -0,0 +1,10 @@\n+Datetime functionality exposed in the C API and Cython bindings\n+---------------------------------------------------------------\n+\n+The functions ``NpyDatetime_ConvertDatetime64ToDatetimeStruct``,\n+``NpyDatetime_ConvertDatetimeStructToDatetime64``,\n+``NpyDatetime_ConvertPyDateTimeToDatetimeStruct``,\n+``NpyDatetime_GetDatetimeISO8601StrLen``, ``NpyDatetime_MakeISO8601Datetime``,\n+and ``NpyDatetime_ParseISO8601Datetime`` have been added to the C API to\n+facilitate converting between strings, Python datetimes, and NumPy datetimes in\n+external libraries."
            },
            {
                "filename": "doc/source/reference/c-api/datetimes.rst",
                "patch": "@@ -0,0 +1,225 @@\n+Datetime API\n+============\n+\n+NumPy represents dates internally using an int64 counter and a unit metadata\n+struct. Time differences are represented similarly using an int64 and a unit\n+metadata struct. The functions described below are available to to facilitate\n+converting between ISO 8601 date strings, NumPy datetimes, and Python datetime\n+objects in C.\n+\n+Data types\n+----------\n+\n+In addition to the `npy_datetime` and `npy_timedelta` typedefs for `npy_int64`,\n+NumPy defines two additional structs that represent time unit metadata and\n+an \"exploded\" view of a datetime.\n+\n+.. c:type:: PyArray_DatetimeMetaData\n+\n+   Represents datetime unit metadata.\n+\n+   .. code-block:: c\n+\n+       typedef struct {\n+           NPY_DATETIMEUNIT base;\n+           int num;\n+       } PyArray_DatetimeMetaData;\n+\n+   .. c:member:: NPY_DATETIMEUNIT base\n+\n+       The unit of the datetime.\n+\n+   .. c:member:: num\n+\n+       A multiplier for the unit.\n+\n+.. c:type:: npy_datetimestruct\n+\n+   An \"exploded\" view of a datetime value\n+\n+   .. code-block:: c\n+\n+       typedef struct {\n+           npy_int64 year;\n+           npy_int32 month, day, hour, min, sec, us, ps, as;\n+       } npy_datetimestruct;\n+\n+.. c:enum:: NPY_DATETIMEUNIT\n+\n+   Time units supported by NumPy. The \"FR\" in the names of the enum variants\n+   is short for frequency.\n+\n+   .. c:enumerator:: NPY_FR_ERROR\n+\n+       Error or undetermined units.\n+\n+   .. c::enumerator:: NPY_FR_Y\n+\n+       Years\n+\n+   .. c::enumerator:: NPY_FR_M\n+\n+       Months\n+\n+   .. c::enumerator:: NPY_FR_W\n+\n+       Weeks\n+\n+   .. c::enumerator:: NPY_FR_D\n+\n+       Days\n+\n+   .. c::enumerator:: NPY_FR_h\n+\n+       Hours\n+\n+   .. c::enumerator:: NPY_FR_m\n+\n+       Minutes\n+\n+   .. c::enumerator:: NPY_FR_s\n+\n+       Seconds\n+\n+   .. c::enumerator:: NPY_FR_ms\n+\n+       Milliseconds\n+\n+   .. c::enumerator:: NPY_FR_us\n+\n+       Microseconds\n+\n+   .. c::enumerator:: NPY_FR_ns\n+\n+       Nanoseconds\n+\n+   .. c::enumerator:: NPY_FR_ps\n+\n+       Picoseconds\n+\n+   .. c::enumerator:: NPY_FR_fs\n+\n+       Femtoseconds\n+\n+   .. c::enumerator:: NPY_FR_as\n+\n+       Attoseconds\n+\n+   .. c::enumerator:: NPY_FR_GENERIC\n+\n+       Unbound units, can convert to anything\n+\n+\n+Conversion functions\n+--------------------\n+\n+.. c:function:: int NpyDatetime_ConvertDatetimeStructToDatetime64( \\\n+        PyArray_DatetimeMetaData *meta, const npy_datetimestruct *dts, \\\n+        npy_datetime *out)\n+\n+    Converts a datetime from a datetimestruct to a datetime in the units\n+    specified by the unit metadata. The date is assumed to be valid.\n+\n+    If the ``num`` member of the metadata struct is large, there may\n+    be integer overflow in this function.\n+\n+    Returns 0 on success and -1 on failure.\n+\n+.. c:function:: int NpyDatetime_ConvertDatetime64ToDatetimeStruct( \\\n+        PyArray_DatetimeMetaData *meta, npy_datetime dt, \\\n+        npy_datetimestruct *out)\n+\n+    Converts a datetime with units specified by the unit metadata to an\n+    exploded datetime struct.\n+\n+    Returns 0 on success and -1 on failure.\n+\n+.. c:function:: int NpyDatetime_ConvertPyDateTimeToDatetimeStruct( \\\n+        PyObject *obj, npy_datetimestruct *out, \\\n+        NPY_DATETIMEUNIT *out_bestunit, int apply_tzinfo)\n+\n+    Tests for and converts a Python ``datetime.datetime`` or ``datetime.date``\n+    object into a NumPy ``npy_datetimestruct``.\n+\n+    ``out_bestunit`` gives a suggested unit based on whether the object\n+    was a ``datetime.date`` or ``datetime.datetime`` object.\n+\n+    If ``apply_tzinfo`` is 1, this function uses the tzinfo to convert\n+    to UTC time, otherwise it returns the struct with the local time.\n+\n+    Returns -1 on error, 0 on success, and 1 (with no error set)\n+    if obj doesn't have the needed date or datetime attributes.\n+\n+.. c:function:: NpyDatetime_ParseISO8601Datetime( \\\n+        char const *str, Py_ssize_t len, NPY_DATETIMEUNIT unit, \\\n+        NPY_CASTING casting, npy_datetimestruct *out, \\\n+        NPY_DATETIMEUNIT *out_bestunit, npy_bool *out_special)\n+\n+    Parses (almost) standard ISO 8601 date strings. The differences are:\n+\n+    * The date \"20100312\" is parsed as the year 20100312, not as\n+      equivalent to \"2010-03-12\". The '-' in the dates are not optional.\n+    * Only seconds may have a decimal point, with up to 18 digits after it\n+      (maximum attoseconds precision).\n+    * Either a 'T' as in ISO 8601 or a ' ' may be used to separate\n+      the date and the time. Both are treated equivalently.\n+    * Doesn't (yet) handle the \"YYYY-DDD\" or \"YYYY-Www\" formats.\n+    * Doesn't handle leap seconds (seconds value has 60 in these cases).\n+    * Doesn't handle 24:00:00 as synonym for midnight (00:00:00) tomorrow\n+    * Accepts special values \"NaT\" (not a time), \"Today\", (current\n+      day according to local time) and \"Now\" (current time in UTC).\n+\n+    ``str`` must be a NULL-terminated string, and ``len`` must be its length.\n+    ``unit`` should contain -1 if the unit is unknown, or the unit\n+       which will be used if it is.\n+    ``casting`` controls how the detected unit from the string is allowed\n+       to be cast to the 'unit' parameter.\n+    ``out`` gets filled with the parsed date-time.\n+    ``out_bestunit`` gives a suggested unit based on the amount of\n+       resolution provided in the string, or -1 for NaT.\n+    ``out_special`` gets set to 1 if the parsed time was 'today',\n+       'now', or ''/'NaT'. For 'today', the unit recommended is\n+       'D', for 'now', the unit recommended is 's', and for 'NaT'\n+       the unit recommended is 'Y'.\n+\n+    Returns 0 on success, -1 on failure.\n+\n+.. c:function:: NpyDatetime_GetDatetimeISO8601StrLen(\\\n+        int local, NPY_DATETIMEUNIT base)\n+\n+    Returns the string length to use for converting datetime\n+    objects with the given local time and unit settings to strings.\n+    Use this when constructings strings to supply to\n+    ``NpyDatetime_MakeISO8601Datetime``.\n+\n+.. c:function:: NpyDatetime_MakeISO8601Datetime(\\\n+        npy_datetimestruct *dts, char *outstr, npy_intp outlen, \\\n+        int local, int utc, NPY_DATETIMEUNIT base, int tzoffset, \\\n+        NPY_CASTING casting)\n+\n+    Converts an ``npy_datetimestruct`` to an (almost) ISO 8601\n+    NULL-terminated string. If the string fits in the space exactly,\n+    it leaves out the NULL terminator and returns success.\n+\n+    The differences from ISO 8601 are the 'NaT' string, and\n+    the number of year digits is >= 4 instead of strictly 4.\n+\n+    If ``local`` is non-zero, it produces a string in local time with\n+    a +-#### timezone offset. If ``local`` is zero and ``utc`` is non-zero,\n+    produce a string ending with 'Z' to denote UTC. By default, no time\n+    zone information is attached.\n+\n+    ``base`` restricts the output to that unit. Set ``base`` to\n+    -1 to auto-detect a base after which all the values are zero.\n+\n+     ``tzoffset`` is used if ``local`` is enabled, and ``tzoffset`` is\n+     set to a value other than -1. This is a manual override for\n+     the local time zone to use, as an offset in minutes.\n+\n+     ``casting`` controls whether data loss is allowed by truncating\n+     the data to a coarser unit. This interacts with ``local``, slightly,\n+     in order to form a date unit string as a local time, the casting\n+     must be unsafe.\n+\n+     Returns 0 on success, -1 on failure (for example if the output\n+     string was too short)."
            },
            {
                "filename": "doc/source/reference/c-api/index.rst",
                "patch": "@@ -48,5 +48,6 @@ code.\n    ufunc\n    generalized-ufuncs\n    coremath\n+   datetimes\n    deprecations\n    data_memory"
            },
            {
                "filename": "numpy/__init__.cython-30.pxd",
                "patch": "@@ -712,6 +712,8 @@ cdef extern from \"numpy/arrayobject.h\":\n     int PyArray_CompareString (char *, char *, size_t)\n     int PyArray_SetBaseObject(ndarray, base) except -1 # NOTE: steals a reference to base! Use \"set_array_base()\" instead.\n \n+    # additional datetime related functions are defined below\n+\n \n # Typedefs that matches the runtime dtype objects in\n # the numpy module.\n@@ -795,6 +797,10 @@ cdef extern from \"numpy/ndarraytypes.h\":\n         NPY_DATETIMEUNIT base\n         int64_t num\n \n+    ctypedef struct npy_datetimestruct:\n+        int64_t year\n+        int32_t month, day, hour, min, sec, us, ps, as\n+\n cdef extern from \"numpy/arrayscalars.h\":\n \n     # abstract types\n@@ -846,6 +852,31 @@ cdef extern from \"numpy/arrayscalars.h\":\n         NPY_FR_as\n \n \n+cdef extern from \"numpy/arrayobject.h\":\n+    # These are part of the C-API defined in `__multiarray_api.h`\n+\n+    # NumPy internal definitions in datetime_strings.c:\n+    int get_datetime_iso_8601_strlen \"NpyDatetime_GetDatetimeISO8601StrLen\" (\n+            int local, NPY_DATETIMEUNIT base)\n+    int make_iso_8601_datetime \"NpyDatetime_MakeISO8601Datetime\" (\n+            npy_datetimestruct *dts, char *outstr, npy_intp outlen,\n+            int local, int utc, NPY_DATETIMEUNIT base, int tzoffset,\n+            NPY_CASTING casting) except -1\n+\n+    # NumPy internal definition in datetime.c:\n+    # May return 1 to indicate that object does not appear to be a datetime\n+    # (returns 0 on success).\n+    int convert_pydatetime_to_datetimestruct \"NpyDatetime_ConvertPyDateTimeToDatetimeStruct\" (\n+            PyObject *obj, npy_datetimestruct *out,\n+            NPY_DATETIMEUNIT *out_bestunit, int apply_tzinfo) except -1\n+    int convert_datetime64_to_datetimestruct \"NpyDatetime_ConvertDatetime64ToDatetimeStruct\" (\n+            PyArray_DatetimeMetaData *meta, npy_datetime dt,\n+            npy_datetimestruct *out) except -1\n+    int convert_datetimestruct_to_datetime64 \"NpyDatetime_ConvertDatetimeStructToDatetime64\"(\n+            PyArray_DatetimeMetaData *meta, const npy_datetimestruct *dts,\n+            npy_datetime *out) except -1\n+\n+\n #\n # ufunc API\n #"
            },
            {
                "filename": "numpy/__init__.pxd",
                "patch": "@@ -670,6 +670,8 @@ cdef extern from \"numpy/arrayobject.h\":\n     int PyArray_CompareString (char *, char *, size_t)\n     int PyArray_SetBaseObject(ndarray, base) except -1 # NOTE: steals a reference to base! Use \"set_array_base()\" instead.\n \n+    # additional datetime related functions are defined below\n+\n \n # Typedefs that matches the runtime dtype objects in\n # the numpy module.\n@@ -753,6 +755,11 @@ cdef extern from \"numpy/ndarraytypes.h\":\n         NPY_DATETIMEUNIT base\n         int64_t num\n \n+    ctypedef struct npy_datetimestruct:\n+        int64_t year\n+        int32_t month, day, hour, min, sec, us, ps, as\n+\n+\n cdef extern from \"numpy/arrayscalars.h\":\n \n     # abstract types\n@@ -804,6 +811,31 @@ cdef extern from \"numpy/arrayscalars.h\":\n         NPY_FR_as\n \n \n+cdef extern from \"numpy/arrayobject.h\":\n+    # These are part of the C-API defined in `__multiarray_api.h`\n+\n+    # NumPy internal definitions in datetime_strings.c:\n+    int get_datetime_iso_8601_strlen \"NpyDatetime_GetDatetimeISO8601StrLen\" (\n+            int local, NPY_DATETIMEUNIT base)\n+    int make_iso_8601_datetime \"NpyDatetime_MakeISO8601Datetime\" (\n+            npy_datetimestruct *dts, char *outstr, npy_intp outlen,\n+            int local, int utc, NPY_DATETIMEUNIT base, int tzoffset,\n+            NPY_CASTING casting) except -1\n+\n+    # NumPy internal definition in datetime.c:\n+    # May return 1 to indicate that object does not appear to be a datetime\n+    # (returns 0 on success).\n+    int convert_pydatetime_to_datetimestruct \"NpyDatetime_ConvertPyDateTimeToDatetimeStruct\" (\n+            PyObject *obj, npy_datetimestruct *out,\n+            NPY_DATETIMEUNIT *out_bestunit, int apply_tzinfo) except -1\n+    int convert_datetime64_to_datetimestruct \"NpyDatetime_ConvertDatetime64ToDatetimeStruct\" (\n+            PyArray_DatetimeMetaData *meta, npy_datetime dt,\n+            npy_datetimestruct *out) except -1\n+    int convert_datetimestruct_to_datetime64 \"NpyDatetime_ConvertDatetimeStructToDatetime64\"(\n+            PyArray_DatetimeMetaData *meta, const npy_datetimestruct *dts,\n+            npy_datetime *out) except -1\n+\n+\n #\n # ufunc API\n #"
            },
            {
                "filename": "numpy/core/code_generators/cversions.txt",
                "patch": "@@ -73,4 +73,4 @@\n 0x00000011 = ca1aebdad799358149567d9d93cbca09\n \n # Version 18 (NumPy 2.0.0)\n-0x00000012 = 5af92e858ce8e95409ae1fcc8f508ddd\n+0x00000012 = 52eabc83680fd0d381c08d63e264e72e"
            },
            {
                "filename": "numpy/core/code_generators/numpy_api.py",
                "patch": "@@ -371,6 +371,12 @@ def get_annotations():\n     'PyDataMem_SetHandler':                 (304, MinVersion(\"1.22\")),\n     'PyDataMem_GetHandler':                 (305, MinVersion(\"1.22\")),\n     # End 1.22 API\n+    'NpyDatetime_ConvertDatetime64ToDatetimeStruct': (307, MinVersion(\"2.0\")),\n+    'NpyDatetime_ConvertDatetimeStructToDatetime64': (308, MinVersion(\"2.0\")),\n+    'NpyDatetime_ConvertPyDateTimeToDatetimeStruct': (309, MinVersion(\"2.0\")),\n+    'NpyDatetime_GetDatetimeISO8601StrLen':          (310, MinVersion(\"2.0\")),\n+    'NpyDatetime_MakeISO8601Datetime':               (311, MinVersion(\"2.0\")),\n+    'NpyDatetime_ParseISO8601Datetime':              (312, MinVersion(\"2.0\")),\n }\n \n ufunc_types_api = {"
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -55,7 +55,7 @@\n # 0x00000012 - 2.0.x\n C_API_VERSION = 0x00000012\n \n-# By default, when compiling downstream libraries against NumPy,```\n+# By default, when compiling downstream libraries against NumPy,\n # pick an older feature version.  For example, for 1.25.x we default to the\n # 1.19 API and support going back all the way to 1.15.x (if so desired).\n # This is set up in `numpyconfig.h`."
            },
            {
                "filename": "numpy/core/src/multiarray/_datetime.h",
                "patch": "@@ -50,15 +50,6 @@ find_string_array_datetime64_type(PyArrayObject *arr,\n NPY_NO_EXPORT PyArray_Descr *\n datetime_type_promotion(PyArray_Descr *type1, PyArray_Descr *type2);\n \n-/*\n- * Converts a datetime from a datetimestruct to a datetime based\n- * on some metadata.\n- */\n-NPY_NO_EXPORT int\n-convert_datetimestruct_to_datetime(PyArray_DatetimeMetaData *meta,\n-                                    const npy_datetimestruct *dts,\n-                                    npy_datetime *out);\n-\n /*\n  * Extracts the month number, within the current year,\n  * from a 'datetime64[D]' value. January is 1, etc.\n@@ -210,24 +201,6 @@ NPY_NO_EXPORT PyObject *\n metastr_to_unicode(PyArray_DatetimeMetaData *meta, int skip_brackets);\n \n \n-/*\n- * Tests for and converts a Python datetime.datetime or datetime.date\n- * object into a NumPy npy_datetimestruct.\n- *\n- * 'out_bestunit' gives a suggested unit based on whether the object\n- *      was a datetime.date or datetime.datetime object.\n- *\n- * If 'apply_tzinfo' is 1, this function uses the tzinfo to convert\n- * to UTC time, otherwise it returns the struct with the local time.\n- *\n- * Returns -1 on error, 0 on success, and 1 (with no error set)\n- * if obj doesn't have the needed date or datetime attributes.\n- */\n-NPY_NO_EXPORT int\n-convert_pydatetime_to_datetimestruct(PyObject *obj, npy_datetimestruct *out,\n-                                     NPY_DATETIMEUNIT *out_bestunit,\n-                                     int apply_tzinfo);\n-\n /*\n  * Converts a PyObject * into a datetime, in any of the forms supported.\n  *\n@@ -286,27 +259,6 @@ convert_datetime_to_pyobject(npy_datetime dt, PyArray_DatetimeMetaData *meta);\n NPY_NO_EXPORT PyObject *\n convert_timedelta_to_pyobject(npy_timedelta td, PyArray_DatetimeMetaData *meta);\n \n-/*\n- * Converts a datetime based on the given metadata into a datetimestruct\n- */\n-NPY_NO_EXPORT int\n-convert_datetime_to_datetimestruct(PyArray_DatetimeMetaData *meta,\n-                                    npy_datetime dt,\n-                                    npy_datetimestruct *out);\n-\n-/*\n- * Converts a datetime from a datetimestruct to a datetime based\n- * on some metadata. The date is assumed to be valid.\n- *\n- * TODO: If meta->num is really big, there could be overflow\n- *\n- * Returns 0 on success, -1 on failure.\n- */\n-NPY_NO_EXPORT int\n-convert_datetimestruct_to_datetime(PyArray_DatetimeMetaData *meta,\n-                                    const npy_datetimestruct *dts,\n-                                    npy_datetime *out);\n-\n /*\n  * Adjusts a datetimestruct based on a seconds offset. Assumes\n  * the current values are valid."
            },
            {
                "filename": "numpy/core/src/multiarray/datetime.c",
                "patch": "@@ -269,7 +269,8 @@ set_datetimestruct_days(npy_int64 days, npy_datetimestruct *dts)\n     }\n }\n \n-/*\n+/*NUMPY_API\n+ *\n  * Converts a datetime from a datetimestruct to a datetime based\n  * on some metadata. The date is assumed to be valid.\n  *\n@@ -278,7 +279,7 @@ set_datetimestruct_days(npy_int64 days, npy_datetimestruct *dts)\n  * Returns 0 on success, -1 on failure.\n  */\n NPY_NO_EXPORT int\n-convert_datetimestruct_to_datetime(PyArray_DatetimeMetaData *meta,\n+NpyDatetime_ConvertDatetimeStructToDatetime64(PyArray_DatetimeMetaData *meta,\n                                     const npy_datetimestruct *dts,\n                                     npy_datetime *out)\n {\n@@ -442,13 +443,14 @@ PyArray_TimedeltaStructToTimedelta(\n     return -1;\n }\n \n-/*\n+/*NUMPY_API\n+ *\n  * Converts a datetime based on the given metadata into a datetimestruct\n  */\n NPY_NO_EXPORT int\n-convert_datetime_to_datetimestruct(PyArray_DatetimeMetaData *meta,\n-                                    npy_datetime dt,\n-                                    npy_datetimestruct *out)\n+NpyDatetime_ConvertDatetime64ToDatetimeStruct(\n+        PyArray_DatetimeMetaData *meta, npy_datetime dt,\n+        npy_datetimestruct *out)\n {\n     npy_int64 days;\n \n@@ -2090,7 +2092,8 @@ add_minutes_to_datetimestruct(npy_datetimestruct *dts, int minutes)\n     }\n }\n \n-/*\n+/*NUMPY_API\n+ *\n  * Tests for and converts a Python datetime.datetime or datetime.date\n  * object into a NumPy npy_datetimestruct.\n  *\n@@ -2109,9 +2112,9 @@ add_minutes_to_datetimestruct(npy_datetimestruct *dts, int minutes)\n  * if obj doesn't have the needed date or datetime attributes.\n  */\n NPY_NO_EXPORT int\n-convert_pydatetime_to_datetimestruct(PyObject *obj, npy_datetimestruct *out,\n-                                     NPY_DATETIMEUNIT *out_bestunit,\n-                                     int apply_tzinfo)\n+NpyDatetime_ConvertPyDateTimeToDatetimeStruct(\n+        PyObject *obj, npy_datetimestruct *out, NPY_DATETIMEUNIT *out_bestunit,\n+        int apply_tzinfo)\n {\n     PyObject *tmp;\n     int isleap;\n@@ -2343,7 +2346,7 @@ get_tzoffset_from_pytzinfo(PyObject *timezone_obj, npy_datetimestruct *dts)\n     }\n \n     /* Convert the local datetime into a datetimestruct */\n-    if (convert_pydatetime_to_datetimestruct(loc_dt, &loc_dts, NULL, 0) < 0) {\n+    if (NpyDatetime_ConvertPyDateTimeToDatetimeStruct(loc_dt, &loc_dts, NULL, 0) < 0) {\n         Py_DECREF(loc_dt);\n         return -1;\n     }\n@@ -2399,8 +2402,9 @@ convert_pyobject_to_datetime(PyArray_DatetimeMetaData *meta, PyObject *obj,\n         /* Parse the ISO date */\n         npy_datetimestruct dts;\n         NPY_DATETIMEUNIT bestunit = NPY_FR_ERROR;\n-        if (parse_iso_8601_datetime(str, len, meta->base, casting,\n-                                &dts, &bestunit, NULL) < 0) {\n+        if (NpyDatetime_ParseISO8601Datetime(\n+                str, len, meta->base, casting,\n+                &dts, &bestunit, NULL) < 0) {\n             Py_DECREF(utf8);\n             return -1;\n         }\n@@ -2411,7 +2415,7 @@ convert_pyobject_to_datetime(PyArray_DatetimeMetaData *meta, PyObject *obj,\n             meta->num = 1;\n         }\n \n-        if (convert_datetimestruct_to_datetime(meta, &dts, out) < 0) {\n+        if (NpyDatetime_ConvertDatetimeStructToDatetime64(meta, &dts, out) < 0) {\n             Py_DECREF(utf8);\n             return -1;\n         }\n@@ -2503,7 +2507,7 @@ convert_pyobject_to_datetime(PyArray_DatetimeMetaData *meta, PyObject *obj,\n         npy_datetimestruct dts;\n         NPY_DATETIMEUNIT bestunit = NPY_FR_ERROR;\n \n-        code = convert_pydatetime_to_datetimestruct(obj, &dts, &bestunit, 1);\n+        code = NpyDatetime_ConvertPyDateTimeToDatetimeStruct(obj, &dts, &bestunit, 1);\n         if (code == -1) {\n             return -1;\n         }\n@@ -2526,7 +2530,7 @@ convert_pyobject_to_datetime(PyArray_DatetimeMetaData *meta, PyObject *obj,\n                 }\n             }\n \n-            return convert_datetimestruct_to_datetime(meta, &dts, out);\n+            return NpyDatetime_ConvertDatetimeStructToDatetime64(meta, &dts, out);\n         }\n     }\n \n@@ -2860,7 +2864,7 @@ convert_datetime_to_pyobject(npy_datetime dt, PyArray_DatetimeMetaData *meta)\n     }\n \n     /* Convert to a datetimestruct */\n-    if (convert_datetime_to_datetimestruct(meta, dt, &dts) < 0) {\n+    if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(meta, dt, &dts) < 0) {\n         return NULL;\n     }\n \n@@ -3025,11 +3029,11 @@ cast_datetime_to_datetime(PyArray_DatetimeMetaData *src_meta,\n     }\n \n     /* Otherwise convert through a datetimestruct */\n-    if (convert_datetime_to_datetimestruct(src_meta, src_dt, &dts) < 0) {\n+    if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(src_meta, src_dt, &dts) < 0) {\n             *dst_dt = NPY_DATETIME_NAT;\n             return -1;\n     }\n-    if (convert_datetimestruct_to_datetime(dst_meta, &dts, dst_dt) < 0) {\n+    if (NpyDatetime_ConvertDatetimeStructToDatetime64(dst_meta, &dts, dst_dt) < 0) {\n         *dst_dt = NPY_DATETIME_NAT;\n         return -1;\n     }\n@@ -3531,18 +3535,20 @@ find_string_array_datetime64_type(PyArrayObject *arr,\n                 tmp_buffer[maxlen] = '\\0';\n \n                 tmp_meta.base = NPY_FR_ERROR;\n-                if (parse_iso_8601_datetime(tmp_buffer, maxlen, -1,\n-                                    NPY_UNSAFE_CASTING, &dts,\n-                                    &tmp_meta.base, NULL) < 0) {\n+                if (NpyDatetime_ParseISO8601Datetime(\n+                        tmp_buffer, maxlen, -1,\n+                        NPY_UNSAFE_CASTING, &dts,\n+                        &tmp_meta.base, NULL) < 0) {\n                     goto fail;\n                 }\n             }\n             /* Otherwise parse the data in place */\n             else {\n                 tmp_meta.base = NPY_FR_ERROR;\n-                if (parse_iso_8601_datetime(data, tmp - data, -1,\n-                                    NPY_UNSAFE_CASTING, &dts,\n-                                    &tmp_meta.base, NULL) < 0) {\n+                if (NpyDatetime_ParseISO8601Datetime(\n+                        data, tmp - data, -1,\n+                        NPY_UNSAFE_CASTING, &dts,\n+                        &tmp_meta.base, NULL) < 0) {\n                     goto fail;\n                 }\n             }\n@@ -3961,7 +3967,7 @@ time_to_string_resolve_descriptors(\n         if (given_descrs[0]->type_num == NPY_DATETIME) {\n             PyArray_DatetimeMetaData *meta = get_datetime_metadata_from_dtype(given_descrs[0]);\n             assert(meta != NULL);\n-            size = get_datetime_iso_8601_strlen(0, meta->base);\n+            size = NpyDatetime_GetDatetimeISO8601StrLen(0, meta->base);\n         }\n         else {\n             /*"
            },
            {
                "filename": "numpy/core/src/multiarray/datetime_strings.c",
                "patch": "@@ -187,7 +187,8 @@ convert_datetimestruct_utc_to_local(npy_datetimestruct *out_dts_local,\n     return 0;\n }\n \n-/*\n+/*NUMPY_API\n+ *\n  * Parses (almost) standard ISO 8601 date strings. The differences are:\n  *\n  * + The date \"20100312\" is parsed as the year 20100312, not as\n@@ -219,12 +220,13 @@ convert_datetimestruct_utc_to_local(npy_datetimestruct *out_dts_local,\n  * Returns 0 on success, -1 on failure.\n  */\n NPY_NO_EXPORT int\n-parse_iso_8601_datetime(char const *str, Py_ssize_t len,\n-                    NPY_DATETIMEUNIT unit,\n-                    NPY_CASTING casting,\n-                    npy_datetimestruct *out,\n-                    NPY_DATETIMEUNIT *out_bestunit,\n-                    npy_bool *out_special)\n+NpyDatetime_ParseISO8601Datetime(\n+        char const *str, Py_ssize_t len,\n+        NPY_DATETIMEUNIT unit,\n+        NPY_CASTING casting,\n+        npy_datetimestruct *out,\n+        NPY_DATETIMEUNIT *out_bestunit,\n+        npy_bool *out_special)\n {\n     int year_leap = 0;\n     int i, numdigits;\n@@ -357,7 +359,7 @@ parse_iso_8601_datetime(char const *str, Py_ssize_t len,\n             return -1;\n         }\n \n-        return convert_datetime_to_datetimestruct(&meta, rawtime, out);\n+        return NpyDatetime_ConvertDatetime64ToDatetimeStruct(&meta, rawtime, out);\n     }\n \n     /* Anything else isn't a special value */\n@@ -752,12 +754,13 @@ parse_iso_8601_datetime(char const *str, Py_ssize_t len,\n     return -1;\n }\n \n-/*\n+/*NUMPY_API\n+ *\n  * Provides a string length to use for converting datetime\n  * objects with the given local and unit settings.\n  */\n NPY_NO_EXPORT int\n-get_datetime_iso_8601_strlen(int local, NPY_DATETIMEUNIT base)\n+NpyDatetime_GetDatetimeISO8601StrLen(int local, NPY_DATETIMEUNIT base)\n {\n     int len = 0;\n \n@@ -855,7 +858,8 @@ lossless_unit_from_datetimestruct(npy_datetimestruct *dts)\n     }\n }\n \n-/*\n+/*NUMPY_API\n+ *\n  * Converts an npy_datetimestruct to an (almost) ISO 8601\n  * NULL-terminated string. If the string fits in the space exactly,\n  * it leaves out the NULL terminator and returns success.\n@@ -884,9 +888,10 @@ lossless_unit_from_datetimestruct(npy_datetimestruct *dts)\n  *  string was too short).\n  */\n NPY_NO_EXPORT int\n-make_iso_8601_datetime(npy_datetimestruct *dts, char *outstr, npy_intp outlen,\n-                    int local, int utc, NPY_DATETIMEUNIT base, int tzoffset,\n-                    NPY_CASTING casting)\n+NpyDatetime_MakeISO8601Datetime(\n+        npy_datetimestruct *dts, char *outstr, npy_intp outlen,\n+        int local, int utc, NPY_DATETIMEUNIT base, int tzoffset,\n+        NPY_CASTING casting)\n {\n     npy_datetimestruct dts_local;\n     int timezone_offset = 0;\n@@ -1492,7 +1497,7 @@ array_datetime_as_string(PyObject *NPY_UNUSED(self), PyObject *args,\n     }\n \n     /* Get a string size long enough for any datetimes we're given */\n-    strsize = get_datetime_iso_8601_strlen(local, unit);\n+    strsize = NpyDatetime_GetDatetimeISO8601StrLen(local, unit);\n     /*\n      * For Python3, allocate the output array as a UNICODE array, so\n      * that it will behave as strings properly\n@@ -1548,7 +1553,7 @@ array_datetime_as_string(PyObject *NPY_UNUSED(self), PyObject *args,\n             dt = *(npy_datetime *)dataptr[0];\n \n             /* Convert it to a struct */\n-            if (convert_datetime_to_datetimestruct(meta, dt, &dts) < 0) {\n+            if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(meta, dt, &dts) < 0) {\n                 goto fail;\n             }\n \n@@ -1563,7 +1568,7 @@ array_datetime_as_string(PyObject *NPY_UNUSED(self), PyObject *args,\n             /* Zero the destination string completely */\n             memset(dataptr[1], 0, strsize);\n             /* Convert that into a string */\n-            if (make_iso_8601_datetime(&dts, (char *)dataptr[1], strsize,\n+            if (NpyDatetime_MakeISO8601Datetime(&dts, (char *)dataptr[1], strsize,\n                                 local, utc, unit, tzoffset, casting) < 0) {\n                 goto fail;\n             }"
            },
            {
                "filename": "numpy/core/src/multiarray/datetime_strings.h",
                "patch": "@@ -1,79 +1,6 @@\n #ifndef NUMPY_CORE_SRC_MULTIARRAY_DATETIME_STRINGS_H_\n #define NUMPY_CORE_SRC_MULTIARRAY_DATETIME_STRINGS_H_\n \n-/*\n- * Parses (almost) standard ISO 8601 date strings. The differences are:\n- *\n- * + The date \"20100312\" is parsed as the year 20100312, not as\n- *   equivalent to \"2010-03-12\". The '-' in the dates are not optional.\n- * + Only seconds may have a decimal point, with up to 18 digits after it\n- *   (maximum attoseconds precision).\n- * + Either a 'T' as in ISO 8601 or a ' ' may be used to separate\n- *   the date and the time. Both are treated equivalently.\n- * + Doesn't (yet) handle the \"YYYY-DDD\" or \"YYYY-Www\" formats.\n- * + Doesn't handle leap seconds (seconds value has 60 in these cases).\n- * + Doesn't handle 24:00:00 as synonym for midnight (00:00:00) tomorrow\n- * + Accepts special values \"NaT\" (not a time), \"Today\", (current\n- *   day according to local time) and \"Now\" (current time in UTC).\n- *\n- * 'str' must be a NULL-terminated string, and 'len' must be its length.\n- * 'unit' should contain -1 if the unit is unknown, or the unit\n- *      which will be used if it is.\n- * 'casting' controls how the detected unit from the string is allowed\n- *           to be cast to the 'unit' parameter.\n- *\n- * 'out' gets filled with the parsed date-time.\n- * 'out_bestunit' gives a suggested unit based on the amount of\n- *      resolution provided in the string, or -1 for NaT.\n- * 'out_special' gets set to 1 if the parsed time was 'today',\n- *      'now', or ''/'NaT'. For 'today', the unit recommended is\n- *      'D', for 'now', the unit recommended is 's', and for 'NaT'\n- *      the unit recommended is 'Y'.\n- *\n- * Returns 0 on success, -1 on failure.\n- */\n-NPY_NO_EXPORT int\n-parse_iso_8601_datetime(char const *str, Py_ssize_t len,\n-                    NPY_DATETIMEUNIT unit,\n-                    NPY_CASTING casting,\n-                    npy_datetimestruct *out,\n-                    NPY_DATETIMEUNIT *out_bestunit,\n-                    npy_bool *out_special);\n-\n-/*\n- * Provides a string length to use for converting datetime\n- * objects with the given local and unit settings.\n- */\n-NPY_NO_EXPORT int\n-get_datetime_iso_8601_strlen(int local, NPY_DATETIMEUNIT base);\n-\n-/*\n- * Converts an npy_datetimestruct to an (almost) ISO 8601\n- * NULL-terminated string.\n- *\n- * If 'local' is non-zero, it produces a string in local time with\n- * a +-#### timezone offset, otherwise it uses timezone Z (UTC).\n- *\n- * 'base' restricts the output to that unit. Set 'base' to\n- * -1 to auto-detect a base after which all the values are zero.\n- *\n- *  'tzoffset' is used if 'local' is enabled, and 'tzoffset' is\n- *  set to a value other than -1. This is a manual override for\n- *  the local time zone to use, as an offset in minutes.\n- *\n- *  'casting' controls whether data loss is allowed by truncating\n- *  the data to a coarser unit. This interacts with 'local', slightly,\n- *  in order to form a date unit string as a local time, the casting\n- *  must be unsafe.\n- *\n- *  Returns 0 on success, -1 on failure (for example if the output\n- *  string was too short).\n- */\n-NPY_NO_EXPORT int\n-make_iso_8601_datetime(npy_datetimestruct *dts, char *outstr, npy_intp outlen,\n-                    int local, int utc, NPY_DATETIMEUNIT base, int tzoffset,\n-                    NPY_CASTING casting);\n-\n /*\n  * This is the Python-exposed datetime_as_string function.\n  */"
            },
            {
                "filename": "numpy/core/src/multiarray/dtype_transfer.c",
                "patch": "@@ -783,12 +783,12 @@ _strided_to_strided_datetime_general_cast(\n     while (N > 0) {\n         memcpy(&dt, src, sizeof(dt));\n \n-        if (convert_datetime_to_datetimestruct(&d->src_meta,\n+        if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(&d->src_meta,\n                                                dt, &dts) < 0) {\n             return -1;\n         }\n         else {\n-            if (convert_datetimestruct_to_datetime(&d->dst_meta,\n+            if (NpyDatetime_ConvertDatetimeStructToDatetime64(&d->dst_meta,\n                                                    &dts, &dt) < 0) {\n                 return -1;\n             }\n@@ -893,15 +893,15 @@ _strided_to_strided_datetime_to_string(\n     while (N > 0) {\n         memcpy(&dt, src, sizeof(dt));\n \n-        if (convert_datetime_to_datetimestruct(&d->src_meta,\n+        if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(&d->src_meta,\n                                                dt, &dts) < 0) {\n             return -1;\n         }\n \n         /* Initialize the destination to all zeros */\n         memset(dst, 0, dst_itemsize);\n \n-        if (make_iso_8601_datetime(&dts, dst, dst_itemsize,\n+        if (NpyDatetime_MakeISO8601Datetime(&dts, dst, dst_itemsize,\n                                 0, 0, d->src_meta.base, -1,\n                                 NPY_UNSAFE_CASTING) < 0) {\n             return -1;\n@@ -941,24 +941,26 @@ _strided_to_strided_string_to_datetime(\n             memcpy(tmp_buffer, src, src_itemsize);\n             tmp_buffer[src_itemsize] = '\\0';\n \n-            if (parse_iso_8601_datetime(tmp_buffer, src_itemsize,\n-                                    d->dst_meta.base, NPY_SAME_KIND_CASTING,\n-                                    &dts, NULL, NULL) < 0) {\n+            if (NpyDatetime_ParseISO8601Datetime(\n+                    tmp_buffer, src_itemsize,\n+                    d->dst_meta.base, NPY_SAME_KIND_CASTING,\n+                    &dts, NULL, NULL) < 0) {\n                 return -1;\n             }\n         }\n         /* Otherwise parse the data in place */\n         else {\n-            if (parse_iso_8601_datetime(src, tmp - src,\n-                                    d->dst_meta.base, NPY_SAME_KIND_CASTING,\n-                                    &dts, NULL, NULL) < 0) {\n+            if (NpyDatetime_ParseISO8601Datetime(\n+                    src, tmp - src,\n+                    d->dst_meta.base, NPY_SAME_KIND_CASTING,\n+                    &dts, NULL, NULL) < 0) {\n                 return -1;\n             }\n         }\n \n         /* Convert to the datetime */\n         if (dt != NPY_DATETIME_NAT &&\n-                convert_datetimestruct_to_datetime(&d->dst_meta,\n+                NpyDatetime_ConvertDatetimeStructToDatetime64(&d->dst_meta,\n                                                &dts, &dt) < 0) {\n             return -1;\n         }"
            },
            {
                "filename": "numpy/core/src/multiarray/scalartypes.c.src",
                "patch": "@@ -574,13 +574,13 @@ datetimetype_repr(PyObject *self)\n \n     scal = (PyDatetimeScalarObject *)self;\n \n-    if (convert_datetime_to_datetimestruct(&scal->obmeta,\n+    if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(&scal->obmeta,\n                 scal->obval, &dts) < 0) {\n         return NULL;\n     }\n \n     unit = scal->obmeta.base;\n-    if (make_iso_8601_datetime(&dts, iso, sizeof(iso), 0, 0,\n+    if (NpyDatetime_MakeISO8601Datetime(&dts, iso, sizeof(iso), 0, 0,\n                             unit, -1, NPY_SAFE_CASTING) < 0) {\n         return NULL;\n     }\n@@ -669,13 +669,13 @@ datetimetype_str(PyObject *self)\n \n     scal = (PyDatetimeScalarObject *)self;\n \n-    if (convert_datetime_to_datetimestruct(&scal->obmeta, scal->obval,\n+    if (NpyDatetime_ConvertDatetime64ToDatetimeStruct(&scal->obmeta, scal->obval,\n                                                             &dts) < 0) {\n         return NULL;\n     }\n \n     unit = scal->obmeta.base;\n-    if (make_iso_8601_datetime(&dts, iso, sizeof(iso), 0, 0,\n+    if (NpyDatetime_MakeISO8601Datetime(&dts, iso, sizeof(iso), 0, 0,\n                             unit, -1, NPY_SAFE_CASTING) < 0) {\n         return NULL;\n     }"
            },
            {
                "filename": "numpy/core/tests/examples/cython/checks.pyx",
                "patch": "@@ -30,3 +30,50 @@ def get_dt64_unit(obj):\n \n def is_integer(obj):\n     return isinstance(obj, (cnp.integer, int))\n+\n+\n+def get_datetime_iso_8601_strlen():\n+    return cnp.get_datetime_iso_8601_strlen(0, cnp.NPY_FR_ns)\n+\n+\n+def convert_datetime64_to_datetimestruct():\n+    cdef:\n+        cnp.npy_datetimestruct dts\n+        cnp.PyArray_DatetimeMetaData meta\n+        cnp.int64_t value = 1647374515260292\n+        # i.e. (time.time() * 10**6) at 2022-03-15 20:01:55.260292 UTC\n+\n+    meta.base = cnp.NPY_FR_us\n+    meta.num = 1\n+    cnp.convert_datetime64_to_datetimestruct(&meta, value, &dts)\n+    return dts\n+\n+\n+def make_iso_8601_datetime(dt: \"datetime\"):\n+    cdef:\n+        cnp.npy_datetimestruct dts\n+        char result[36]  # 36 corresponds to NPY_FR_s passed below\n+        int local = 0\n+        int utc = 0\n+        int tzoffset = 0\n+\n+    dts.year = dt.year\n+    dts.month = dt.month\n+    dts.day = dt.day\n+    dts.hour = dt.hour\n+    dts.min = dt.minute\n+    dts.sec = dt.second\n+    dts.us = dt.microsecond\n+    dts.ps = dts.as = 0\n+\n+    cnp.make_iso_8601_datetime(\n+        &dts,\n+        result,\n+        sizeof(result),\n+        local,\n+        utc,\n+        cnp.NPY_FR_s,\n+        tzoffset,\n+        cnp.NPY_NO_CASTING,\n+    )\n+    return result"
            },
            {
                "filename": "numpy/core/tests/examples/cython/setup.py",
                "patch": "@@ -9,7 +9,11 @@\n from setuptools.extension import Extension\n import os\n \n-macros = [(\"NPY_NO_DEPRECATED_API\", 0)]\n+macros = [\n+    (\"NPY_NO_DEPRECATED_API\", 0),\n+    # Require 1.25+ to test datetime additions\n+    (\"NPY_TARGET_VERSION\", \"NPY_2_0_API_VERSION\"),\n+]\n \n checks = Extension(\n     \"checks\","
            },
            {
                "filename": "numpy/core/tests/test_cython.py",
                "patch": "@@ -1,7 +1,9 @@\n+from datetime import datetime\n import os\n import shutil\n import subprocess\n import sys\n+import time\n import pytest\n \n import numpy as np\n@@ -135,3 +137,41 @@ def test_abstract_scalars(install_temp):\n     assert checks.is_integer(1)\n     assert checks.is_integer(np.int8(1))\n     assert checks.is_integer(np.uint64(1))\n+\n+\n+def test_convert_datetime64_to_datetimestruct(install_temp):\n+    # GH#21199\n+    import checks\n+\n+    res = checks.convert_datetime64_to_datetimestruct()\n+\n+    exp = {\n+        \"year\": 2022,\n+        \"month\": 3,\n+        \"day\": 15,\n+        \"hour\": 20,\n+        \"min\": 1,\n+        \"sec\": 55,\n+        \"us\": 260292,\n+        \"ps\": 0,\n+        \"as\": 0,\n+    }\n+\n+    assert res == exp\n+\n+\n+class TestDatetimeStrings:\n+    def test_make_iso_8601_datetime(self, install_temp):\n+        # GH#21199\n+        import checks\n+        dt = datetime(2016, 6, 2, 10, 45, 19)\n+        # uses NPY_FR_s\n+        result = checks.make_iso_8601_datetime(dt)\n+        assert result == b\"2016-06-02T10:45:19\"\n+\n+    def test_get_datetime_iso_8601_strlen(self, install_temp):\n+        # GH#21199\n+        import checks\n+        # uses NPY_FR_ns\n+        res = checks.get_datetime_iso_8601_strlen()\n+        assert res == 48"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24279,
        "body": "The change of default in build system behavior follows up on the discussion in gh-24200. It avoids large accidental regression in performance. If users really want to build with fallback routines, it's a matter of passing a single `-Dallow-noblas` to opt into that.\r\n\r\nThe 32-bit Python on Windows CI job was removed from Azure when switching to Meson. This re-introduces it, on GitHub Actions because it's much easier to debug there. This should be low-maintenance, and exercise the `-Dallow-noblas` flag at the same time. It addressed one of the TODO's in gh-23981.\r\n\r\nThere was one failing test on this config, not surprising because it hasn't been tested before. Those were for floating point exceptions, which already had a note about issues with MSVC + 32-bit Python. Given that the behavior is going to change when SIMD support comes back, I simply disabled the test for now.",
        "changed_files": [
            {
                "filename": ".circleci/config.yml",
                "patch": "@@ -63,7 +63,7 @@ jobs:\n             . venv/bin/activate\n             pip install --progress-bar=off -r test_requirements.txt\n             pip install --progress-bar=off -r doc_requirements.txt\n-            pip install .\n+            pip install . --config-settings=setup-args=\"-Dallow-noblas=true\"\n \n       - run:\n           name: create release notes"
            },
            {
                "filename": ".github/workflows/wheels.yml",
                "patch": "@@ -186,12 +186,13 @@ jobs:\n           python-version: \"3.9\"\n       - name: Build sdist\n         run: |\n-          python setup.py sdist\n+          python -m pip install -U pip build\n+          python -m build --sdist -Csetup-args=-Dallow-noblas=true\n       - name: Test the sdist\n         run: |\n           # TODO: Don't run test suite, and instead build wheels from sdist\n           # Depends on pypa/cibuildwheel#1020\n-          python -m pip install dist/*.gz\n+          python -m pip install dist/*.gz -Csetup-args=-Dallow-noblas=true\n           pip install ninja\n           pip install -r test_requirements.txt\n           cd .. # Can't import numpy within numpy src directory"
            },
            {
                "filename": ".github/workflows/windows_meson.yml",
                "patch": "@@ -17,10 +17,10 @@ permissions:\n   contents: read # to fetch code (actions/checkout)\n \n jobs:\n-  meson:\n-    name: Meson windows build/test\n+  msvc_64bit_python_openblas:\n+    name: MSVC, x86-64, LP64 OpenBLAS\n     runs-on: windows-2019\n-    # if: \"github.repository == 'numpy/numpy'\"\n+    if: \"github.repository == 'numpy/numpy'\"\n     steps:\n     - name: Checkout\n       uses: actions/checkout@c85c95e3d7251135ab7dc9ce3241c5835cc595a9 # v3.5.3\n@@ -86,3 +86,38 @@ jobs:\n         echo \"LASTEXITCODE is '$LASTEXITCODE'\"\n         python -c \"import numpy, sys; sys.exit(numpy.test(verbose=3) is False)\"\n         echo \"LASTEXITCODE is '$LASTEXITCODE'\"\n+\n+  msvc_32bit_python_openblas:\n+    name: MSVC, 32-bit Python, no BLAS\n+    runs-on: windows-2019\n+    if: \"github.repository == 'numpy/numpy'\"\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@c85c95e3d7251135ab7dc9ce3241c5835cc595a9 # v3.5.3\n+        with:\n+          submodules: recursive\n+          fetch-depth: 0\n+\n+      - name: Setup Python (32-bit)\n+        uses: actions/setup-python@61a6322f88396a6271a6ee3565807d608ecaddd1 # v4.7.0\n+        with:\n+          python-version: '3.10'\n+          architecture: 'x86'\n+\n+      - name: Setup MSVC (32-bit)\n+        uses: bus1/cabuild/action/msdevshell@e22aba57d6e74891d059d66501b6b5aed8123c4d  # v1\n+        with:\n+          architecture: 'x86'\n+\n+      - name: Build and install\n+        run: |\n+          python -m pip install . -v -Ccompile-args=\"-j2\" -Csetup-args=\"-Dallow-noblas=true\"\n+\n+      - name: Install test dependencies\n+        run: |\n+          python -m pip install -r test_requirements.txt\n+\n+      - name: Run test suite (fast)\n+        run: |\n+          cd tools\n+          python -m pytest --pyargs numpy -m \"not slow\" -n2"
            },
            {
                "filename": "azure-pipelines.yml",
                "patch": "@@ -227,6 +227,8 @@ stages:\n             TEST_MODE: fast\n             BITS: 64\n             NPY_USE_BLAS_ILP64: '1'\n+            # Broken - it builds but _multiarray_umath doesn't import - needs investigating\n+            DISABLE_BLAS: '1'\n \n     steps:\n     - template: azure-steps-windows.yml"
            },
            {
                "filename": "azure-steps-windows.yml",
                "patch": "@@ -28,26 +28,33 @@ steps:\n     mkdir  C:/opt/openblas/openblas_dll\n     mkdir  C:/opt/32/lib/pkgconfig\n     mkdir  C:/opt/64/lib/pkgconfig\n-    # TBD: support 32 bit testing\n     $target=$(python -c \"import tools.openblas_support as obs; plat=obs.get_plat(); ilp64=obs.get_ilp64(); target=f'openblas_{plat}.zip'; obs.download_openblas(target, plat, ilp64);print(target)\")\n     unzip -o -d c:/opt/ $target\n     echo \"##vso[task.setvariable variable=PKG_CONFIG_PATH]c:/opt/64/lib/pkgconfig\"\n     copy C:/opt/64/bin/*.dll C:/opt/openblas/openblas_dll\n   displayName: 'Download / Install OpenBLAS'\n \n - powershell: |\n+    # Note: ensure the `pip install .` command remains the last one here, to\n+    # avoid \"green on failure\" issues\n     python -c \"from tools import openblas_support; openblas_support.make_init('numpy')\"\n-    If ( Test-Path env:NPY_USE_BLAS_ILP64 ) {\n-        python -m pip install . -Csetup-args=\"--vsenv\" -Csetup-args=\"-Duse-ilp64=true\" -Csetup-args=\"-Dblas-symbol-suffix=64_\"\n+    If ( Test-Path env:DISABLE_BLAS ) {\n+        python -m pip install . -v -Csetup-args=\"--vsenv\" -Csetup-args=\"-Dblas=none\" -Csetup-args=\"-Dlapack=none\" -Csetup-args=\"-Dallow-noblas=true\"\n+    }\n+    elseif ( Test-Path env:NPY_USE_BLAS_ILP64 ) {\n+        python -m pip install . -v -Csetup-args=\"--vsenv\" -Csetup-args=\"-Duse-ilp64=true\" -Csetup-args=\"-Dblas-symbol-suffix=64_\"\n     } else {\n-        python -m pip install . -Csetup-args=\"--vsenv\"\n+        python -m pip install . -v -Csetup-args=\"--vsenv\"\n     }\n+  displayName: 'Build NumPy'\n+\n+- powershell: |\n     # copy from c:/opt/openblas/openblas_dll to numpy/.libs to ensure it can\n     # get loaded when numpy is imported (no RPATH on Windows)\n     $target = $(python -c \"import sysconfig; print(sysconfig.get_path('platlib'))\")\n     mkdir $target/numpy/.libs\n     copy C:/opt/openblas/openblas_dll/*.dll $target/numpy/.libs\n-  displayName: 'Build NumPy'\n+  displayName: 'Copy OpenBLAS DLL to site-packages'\n \n - script: |\n     python -m pip install threadpoolctl"
            },
            {
                "filename": "meson_options.txt",
                "patch": "@@ -2,6 +2,8 @@ option('blas', type: 'string', value: 'openblas',\n         description: 'option for BLAS library switching')\n option('lapack', type: 'string', value: 'openblas',\n         description: 'option for LAPACK library switching')\n+option('allow-noblas', type: 'boolean', value: false,\n+        description: 'If set to true, allow building with (slow!) internal fallback routines')\n option('use-ilp64', type: 'boolean', value: false,\n        description: 'Use ILP64 (64-bit integer) BLAS and LAPACK interfaces')\n option('blas-symbol-suffix', type: 'string', value: '',"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -1764,6 +1764,8 @@ def test_expm1(self):\n         np.log, np.log2, np.log10, np.reciprocal, np.arccosh\n     ]\n \n+    @pytest.mark.skipif(sys.platform == \"win32\" and sys.maxsize < 2**31 + 1,\n+                        reason='failures on 32-bit Python, see FIXME below')\n     @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\n     @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\n     @pytest.mark.parametrize(\"data, escape\", (\n@@ -1810,6 +1812,8 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n         # FIXME: NAN raises FP invalid exception:\n         #  - ceil/float16 on MSVC:32-bit\n         #  - spacing/float16 on almost all platforms\n+        # FIXME: skipped on MSVC:32-bit during switch to Meson, 10 cases fail\n+        #        when SIMD support not present / disabled\n         if ufunc in (np.spacing, np.ceil) and dtype == 'e':\n             return\n         array = np.array(data, dtype=dtype)"
            },
            {
                "filename": "numpy/linalg/meson.build",
                "patch": "@@ -14,7 +14,6 @@ lapack_lite_sources = [\n \n lapack_lite_module_src = ['lapack_litemodule.c']\n if not have_lapack\n-  warning('LAPACK was not found, NumPy is using an unoptimized, naive build from sources!')\n   lapack_lite_module_src += lapack_lite_sources\n endif\n "
            },
            {
                "filename": "numpy/meson.build",
                "patch": "@@ -53,6 +53,19 @@ endif\n # (see cibuildwheel settings in pyproject.toml), but used by CI jobs already\n blas_symbol_suffix = get_option('blas-symbol-suffix')\n \n+use_ilp64 = get_option('use-ilp64')\n+if not use_ilp64\n+  # For now, keep supporting this environment variable too (same as in setup.py)\n+  # `false is the default for the CLI flag, so check if env var was set\n+  use_ilp64 = run_command(py,\n+    [\n+      '-c',\n+      'import os; print(1) if os.environ.get(\"NPY_USE_BLAS_ILP64\", \"0\") != \"0\" else print(0)'\n+    ],\n+    check: true\n+  ).stdout().strip() == '1'\n+endif\n+\n \n # TODO: 64-bit (ILP64) BLAS and LAPACK support (e.g., check for more .pc files\n # so we detect `openblas64_.so` directly). Partially supported now, needs more\n@@ -70,7 +83,12 @@ lapack_name = get_option('lapack')\n # pkg-config uses a lower-case name while CMake uses a capitalized name, so try\n # that too to make the fallback detection with CMake work\n if blas_name == 'openblas'\n-  blas = dependency(['openblas', 'OpenBLAS'], required: false)\n+  if use_ilp64\n+    _openblas_names = ['openblas64', 'openblas', 'OpenBLAS']\n+  else\n+    _openblas_names = ['openblas', 'OpenBLAS']\n+  endif\n+  blas = dependency(_openblas_names, required: false)\n else\n   blas = dependency(blas_name, required: false)\n endif\n@@ -121,27 +139,22 @@ if have_blas\n   endif\n endif\n \n-use_ilp64 = get_option('use-ilp64')\n-if not use_ilp64\n-  # For now, keep supporting this environment variable too (same as in setup.py)\n-  # `false is the default for the CLI flag, so check if env var was set\n-  use_ilp64 = run_command(py,\n-    [\n-      '-c',\n-      'import os; print(1) if os.environ.get(\"NPY_USE_BLAS_ILP64\", \"0\") != \"0\" else print(0)'\n-    ],\n-    check: true\n-  ).stdout().strip() == '1'\n-endif\n-\n-# BLAS and LAPACK are optional dependencies for NumPy. We can only use a BLAS\n-# which provides a CBLAS interface. So disable BLAS completely if CBLAS is not\n-# found (lapack-lite will be used instead; xref gh-24200 for a discussion on\n-# whether this silent disabling should stay as-is)\n+# BLAS and LAPACK are dependencies for NumPy. Since NumPy 2.0, by default the\n+# build will fail if they are missing; the performance impact is large, so\n+# using fallback routines must be explicitly opted into by the user. xref\n+# gh-24200 for a discussion on this.\n+#\n+# Note that we can only use a BLAS which provides a CBLAS interface. So disable\n+# BLAS completely if CBLAS is not found.\n+allow_noblas = get_option('allow-noblas')\n if have_blas\n   _args_blas = []  # note: used for C and C++ via `blas_dep` below\n   if have_cblas\n     _args_blas += ['-DHAVE_CBLAS']\n+  elif not allow_noblas\n+    error('No CBLAS interface detected! Install a BLAS library with CBLAS ' + \\\n+          'support, or use the `allow-noblas` build option (note, this ' + \\\n+          'may be up to 100x slower for some linear algebra operations).')\n   endif\n   if use_ilp64\n     _args_blas += ['-DHAVE_BLAS_ILP64']\n@@ -154,15 +167,25 @@ if have_blas\n     compile_args: _args_blas,\n   )\n else\n-  blas_dep = []\n+  if allow_noblas\n+    blas_dep = []\n+  else\n+    error('No BLAS library detected! Install one, or use the ' + \\\n+          '`allow-noblas` build option (note, this may be up to 100x slower ' + \\\n+          'for some linear algebra operations).')\n+  endif\n endif\n \n if lapack_name == 'openblas'\n   lapack_name = ['openblas', 'OpenBLAS']\n endif\n lapack_dep = dependency(lapack_name, required: false)\n have_lapack = lapack_dep.found()\n-\n+if not have_lapack and not allow_noblas\n+  error('No LAPACK library detected! Install one, or use the ' + \\\n+        '`allow-noblas` build option (note, this may be up to 100x slower ' + \\\n+        'for some linear algebra operations).')\n+endif\n \n # Copy the main __init__.py|pxd files to the build dir (needed for Cython)\n __init__py = fs.copyfile('__init__.py')"
            },
            {
                "filename": "tools/ci/cirrus_macosx_arm64.yml",
                "patch": "@@ -53,5 +53,5 @@ macos_arm64_test_task:\n     pip install -r build_requirements.txt\n     pip install pytest pytest-xdist hypothesis typing_extensions\n     \n-    spin build\n+    spin build -- -Dallow-noblas=true\n     spin test -j auto"
            },
            {
                "filename": "tools/openblas_support.py",
                "patch": "@@ -324,7 +324,7 @@ def test_version(expected_version=None):\n     data = threadpoolctl.threadpool_info()\n     if len(data) != 1:\n         if platform.python_implementation() == 'PyPy':\n-            print(f\"Check broken in CI on PyPy, data is: {data}\")\n+            print(f\"Not using OpenBLAS for PyPy in Azure CI, so skip this\")\n             return\n         raise ValueError(f\"expected single threadpool_info result, got {data}\")\n     if not expected_version:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24274,
        "body": "This API is very slow to use compared to normal advanced indexing\r\n(which reaches deep into the much richer internals).\r\nMaking this fast is difficult and the only known users is Theano:\r\n* While probably still in use Theano is end-of-life for a long time.\r\n* The use-case in Theano would be better served with `ufunc.at` or\r\n  direct advanced indexing use probably (even if that requires\r\n  wrapping into NumPy arrays).  Because it will be vastly faster\r\n  normally.\r\n\r\nSo remove it (make it private) to allow making the ABI of it not be\r\na strange mix for backwards compatibility.\r\n\r\n---\r\n\r\nIt could also make sense to only make the struct almost fully opaque (with some macros to fetch the pointers needed in a version agnostic way).  But considering that the only user is Theano, and that this is rather complicated...\r\n\r\nLooking it partially, because I think we we need to improve `ufunc.at`.  At that point we won't use the \"truly\" public portion of the API anymore, ourselves.",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/24274.c_api_removal.rst",
                "patch": "@@ -0,0 +1,6 @@\n+* The advanced indexing ``MapIter`` and related API has been removed.\n+  The (truly) public part of it was not well tested and had only one\n+  known user (Theano).  Making it private will simplify improvements\n+  to speed up ``ufunc.at`` and make advanced indexing more maintainable.\n+  Please let us know if this API is important to you so we can find a\n+  solution together."
            },
            {
                "filename": "doc/source/reference/c-api/array.rst",
                "patch": "@@ -2686,44 +2686,6 @@ cost of a slight overhead.\n     neighborhood. Calling this function after every point of the\n     neighborhood has been visited is undefined.\n \n-Array mapping\n--------------\n-\n-Array mapping is the machinery behind advanced indexing.\n-\n-.. c:function:: PyObject* PyArray_MapIterArray(PyArrayObject *a, \\\n-                 PyObject *index)\n-\n-    Use advanced indexing to iterate an array.\n-\n-.. c:function:: void PyArray_MapIterSwapAxes(PyArrayMapIterObject *mit, \\\n-                PyArrayObject **ret, int getmap)\n-\n-    Swap the axes to or from their inserted form. ``MapIter`` always puts the\n-    advanced (array) indices first in the iteration. But if they are\n-    consecutive, it will insert/transpose them back before returning.\n-    This is stored as ``mit->consec != 0`` (the place where they are inserted).\n-    For assignments, the opposite happens: the values to be assigned are\n-    transposed (``getmap=1`` instead of ``getmap=0``). ``getmap=0`` and\n-    ``getmap=1`` undo the other operation.\n-\n-.. c:function:: void PyArray_MapIterNext(PyArrayMapIterObject *mit)\n-\n-    This function needs to update the state of the map iterator\n-    and point ``mit->dataptr`` to the memory-location of the next object.\n-\n-    Note that this function never handles an extra operand but provides\n-    compatibility for an old (exposed) API.\n-\n-.. c:function:: PyObject* PyArray_MapIterArrayCopyIfOverlap(PyArrayObject *a, \\\n-                PyObject *index, int copy_if_overlap, PyArrayObject *extra_op)\n-\n-    Similar to :c:func:`PyArray_MapIterArray` but with an additional\n-    ``copy_if_overlap`` argument. If ``copy_if_overlap != 0``, checks if ``a``\n-    has memory overlap with any of the arrays in ``index`` and with\n-    ``extra_op``, and make copies as appropriate to avoid problems if the\n-    input is modified during the iteration. ``iter->array`` may contain a\n-    copied array (WRITEBACKIFCOPY set).\n \n Array Scalars\n -------------"
            },
            {
                "filename": "numpy/core/code_generators/cversions.txt",
                "patch": "@@ -73,4 +73,4 @@\n 0x00000011 = ca1aebdad799358149567d9d93cbca09\n \n # Version 18 (NumPy 2.0.0)\n-0x00000012 = 52eabc83680fd0d381c08d63e264e72e\n+0x00000012 = 05791c885295a8943e00530fa66827fe"
            },
            {
                "filename": "numpy/core/code_generators/generate_numpy_api.py",
                "patch": "@@ -17,7 +17,6 @@\n         npy_bool obval;\n } PyBoolScalarObject;\n \n-extern NPY_NO_EXPORT PyTypeObject PyArrayMapIter_Type;\n extern NPY_NO_EXPORT PyTypeObject PyArrayNeighborhoodIter_Type;\n extern NPY_NO_EXPORT PyBoolScalarObject _PyArrayScalar_BoolValues[2];\n "
            },
            {
                "filename": "numpy/core/code_generators/numpy_api.py",
                "patch": "@@ -98,7 +98,8 @@ def get_annotations():\n # define _PyArrayScalar_BoolValues ((PyBoolScalarObject *)PyArray_API[8])\n \n multiarray_funcs_api = {\n-    '__unused_indices__': [40, 41, 67, 68, 163, 164, 201, 202, 278, 291],\n+    '__unused_indices__': [\n+        40, 41, 67, 68, 163, 164, 201, 202, 278, 291, 293, 294, 295, 301],\n     'PyArray_GetNDArrayCVersion':           (0,),\n     # Unused slot 40, was `PyArray_SetNumericOps`\n     # Unused slot 41, was `PyArray_GetNumericOps`,\n@@ -351,9 +352,9 @@ def get_annotations():\n     'PyDataMem_FREE':                       (289,),\n     'PyDataMem_RENEW':                      (290,),\n     # Unused slot 291, was `PyDataMem_SetEventHook`\n-    'PyArray_MapIterSwapAxes':              (293,),\n-    'PyArray_MapIterArray':                 (294,),\n-    'PyArray_MapIterNext':                  (295,),\n+    # Unused slot 293, was `PyArray_MapIterSwapAxes`\n+    # Unused slot 294, was `PyArray_MapIterArray`\n+    # Unused slot 295, was `PyArray_MapIterNext`\n     # End 1.7 API\n     'PyArray_Partition':                    (296,),\n     'PyArray_ArgPartition':                 (297,),\n@@ -363,7 +364,7 @@ def get_annotations():\n     # End 1.9 API\n     'PyArray_CheckAnyScalarExact':          (300,),\n     # End 1.10 API\n-    'PyArray_MapIterArrayCopyIfOverlap':    (301,),\n+    # Unused slot 301, was `PyArray_MapIterArrayCopyIfOverlap`\n     # End 1.13 API\n     'PyArray_ResolveWritebackIfCopy':       (302,),\n     'PyArray_SetWritebackIfCopyBase':       (303,),"
            },
            {
                "filename": "numpy/core/include/numpy/ndarraytypes.h",
                "patch": "@@ -1291,94 +1291,6 @@ typedef struct {\n #define PyArray_MultiIter_NOTDONE(multi)                \\\n         (_PyMIT(multi)->index < _PyMIT(multi)->size)\n \n-/*\n- * Store the information needed for fancy-indexing over an array. The\n- * fields are slightly unordered to keep consec, dataptr and subspace\n- * where they were originally.\n- */\n-typedef struct {\n-        PyObject_HEAD\n-        /*\n-         * Multi-iterator portion --- needs to be present in this\n-         * order to work with PyArray_Broadcast\n-         */\n-\n-        int                   numiter;                 /* number of index-array\n-                                                          iterators */\n-        npy_intp              size;                    /* size of broadcasted\n-                                                          result */\n-        npy_intp              index;                   /* current index */\n-        int                   nd;                      /* number of dims */\n-        npy_intp              dimensions[NPY_MAXDIMS]; /* dimensions */\n-        NpyIter               *outer;                  /* index objects\n-                                                          iterator */\n-        void                  *unused[NPY_MAXDIMS - 2];\n-        PyArrayObject         *array;\n-        /* Flat iterator for the indexed array. For compatibility solely. */\n-        PyArrayIterObject     *ait;\n-\n-        /*\n-         * Subspace array. For binary compatibility (was an iterator,\n-         * but only the check for NULL should be used).\n-         */\n-        PyArrayObject         *subspace;\n-\n-        /*\n-         * if subspace iteration, then this is the array of axes in\n-         * the underlying array represented by the index objects\n-         */\n-        int                   iteraxes[NPY_MAXDIMS];\n-        npy_intp              fancy_strides[NPY_MAXDIMS];\n-\n-        /* pointer when all fancy indices are 0 */\n-        char                  *baseoffset;\n-\n-        /*\n-         * after binding consec denotes at which axis the fancy axes\n-         * are inserted.\n-         */\n-        int                   consec;\n-        char                  *dataptr;\n-\n-        int                   nd_fancy;\n-        npy_intp              fancy_dims[NPY_MAXDIMS];\n-\n-        /*\n-         * Whether the iterator (any of the iterators) requires API.  This is\n-         * unused by NumPy itself; ArrayMethod flags are more precise.\n-         */\n-        int                   needs_api;\n-\n-        /*\n-         * Extra op information.\n-         */\n-        PyArrayObject         *extra_op;\n-        PyArray_Descr         *extra_op_dtype;         /* desired dtype */\n-        npy_uint32            *extra_op_flags;         /* Iterator flags */\n-\n-        NpyIter               *extra_op_iter;\n-        NpyIter_IterNextFunc  *extra_op_next;\n-        char                  **extra_op_ptrs;\n-\n-        /*\n-         * Information about the iteration state.\n-         */\n-        NpyIter_IterNextFunc  *outer_next;\n-        char                  **outer_ptrs;\n-        npy_intp              *outer_strides;\n-\n-        /*\n-         * Information about the subspace iterator.\n-         */\n-        NpyIter               *subspace_iter;\n-        NpyIter_IterNextFunc  *subspace_next;\n-        char                  **subspace_ptrs;\n-        npy_intp              *subspace_strides;\n-\n-        /* Count for the external loop (which ever it is) for API iteration */\n-        npy_intp              iter_count;\n-\n-} PyArrayMapIterObject;\n \n enum {\n     NPY_NEIGHBORHOOD_ITER_ZERO_PADDING,"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -991,6 +991,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('include', 'numpy', 'halffloat.h'),\n             join('src', 'multiarray', 'common.h'),\n             join('src', 'multiarray', 'number.h'),\n+            join('src', 'multiarray', 'mapping.h'),\n             join('src', 'common', 'templ_common.h.src'),\n             join('src', 'umath', 'override.h'),\n             join(codegen_dir, 'generate_ufunc_api.py'),"
            },
            {
                "filename": "numpy/core/src/common/lowlevel_strided_loops.h",
                "patch": "@@ -5,6 +5,7 @@\n #include \"array_method.h\"\n #include \"dtype_transfer.h\"\n #include \"mem_overlap.h\"\n+#include \"mapping.h\"\n \n /* For PyArray_ macros used below */\n #include \"numpy/ndarrayobject.h\""
            },
            {
                "filename": "numpy/core/src/multiarray/_multiarray_tests.c.src",
                "patch": "@@ -467,132 +467,6 @@ clean_ax:\n }\n \n \n-typedef void (*inplace_map_binop)(PyArrayMapIterObject *, PyArrayIterObject *);\n-\n-static void npy_float64_inplace_add(PyArrayMapIterObject *mit, PyArrayIterObject *it)\n-{\n-    int index = mit->size;\n-    while (index--) {\n-        ((npy_float64*)mit->dataptr)[0] = ((npy_float64*)mit->dataptr)[0] + ((npy_float64*)it->dataptr)[0];\n-\n-        PyArray_MapIterNext(mit);\n-        PyArray_ITER_NEXT(it);\n-    }\n-}\n-\n-inplace_map_binop addition_funcs[] = {\n-npy_float64_inplace_add,\n-NULL};\n-\n-int type_numbers[] = {\n-NPY_FLOAT64,\n--1000};\n-\n-\n-\n-static int\n-map_increment(PyArrayMapIterObject *mit, PyObject *op, inplace_map_binop add_inplace)\n-{\n-    PyArrayObject *arr = NULL;\n-    PyArrayIterObject *it;\n-    PyArray_Descr *descr;\n-\n-    if (mit->ait == NULL) {\n-        return -1;\n-    }\n-    descr = PyArray_DESCR(mit->ait->ao);\n-    Py_INCREF(descr);\n-    arr = (PyArrayObject *)PyArray_FromAny(op, descr,\n-                                0, 0, NPY_ARRAY_FORCECAST, NULL);\n-    if (arr == NULL) {\n-        return -1;\n-    }\n-\n-    if ((mit->subspace != NULL) && (mit->consec)) {\n-        PyArray_MapIterSwapAxes(mit, (PyArrayObject **)&arr, 0);\n-        if (arr == NULL) {\n-            return -1;\n-        }\n-    }\n-\n-    if ((it = (PyArrayIterObject *)\\\n-            PyArray_BroadcastToShape((PyObject *)arr, mit->dimensions,\n-                                     mit->nd)) == NULL) {\n-        Py_DECREF(arr);\n-\n-        return -1;\n-    }\n-\n-    (*add_inplace)(mit, it);\n-\n-    Py_DECREF(arr);\n-    Py_DECREF(it);\n-    return 0;\n-}\n-\n-\n-static PyObject *\n-inplace_increment(PyObject *dummy, PyObject *args)\n-{\n-    PyObject *arg_a = NULL, *index=NULL, *inc=NULL;\n-    PyArrayObject *a;\n-    inplace_map_binop add_inplace = NULL;\n-    int type_number = -1;\n-    int i =0;\n-    PyArrayMapIterObject * mit;\n-\n-    if (!PyArg_ParseTuple(args, \"OOO\", &arg_a, &index,\n-            &inc)) {\n-        return NULL;\n-    }\n-    if (!PyArray_Check(arg_a)) {\n-         PyErr_SetString(PyExc_ValueError, \"needs an ndarray as first argument\");\n-         return NULL;\n-    }\n-    a = (PyArrayObject *) arg_a;\n-\n-    if (PyArray_FailUnlessWriteable(a, \"input/output array\") < 0) {\n-        return NULL;\n-    }\n-\n-    if (PyArray_NDIM(a) == 0) {\n-        PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed.\");\n-        return NULL;\n-    }\n-    type_number = PyArray_TYPE(a);\n-\n-    while (type_numbers[i] >= 0 && addition_funcs[i] != NULL){\n-        if (type_number == type_numbers[i]) {\n-            add_inplace = addition_funcs[i];\n-            break;\n-        }\n-        i++ ;\n-    }\n-\n-    if (add_inplace == NULL) {\n-        PyErr_SetString(PyExc_TypeError, \"unsupported type for a\");\n-        return NULL;\n-    }\n-\n-    mit = (PyArrayMapIterObject *) PyArray_MapIterArray(a, index);\n-    if (mit == NULL) {\n-        goto fail;\n-    }\n-\n-    if (map_increment(mit, inc, add_inplace) != 0) {\n-        goto fail;\n-    }\n-\n-    Py_DECREF(mit);\n-\n-    Py_RETURN_NONE;\n-\n-fail:\n-    Py_XDECREF(mit);\n-\n-    return NULL;\n-}\n-\n /*\n  * Helper to test fromstring of 0 terminated strings, as the C-API supports\n  * the -1 length identifier.\n@@ -2302,9 +2176,6 @@ static PyMethodDef Multiarray_TestsMethods[] = {\n     {\"test_neighborhood_iterator_oob\",\n         test_neighborhood_iterator_oob,\n         METH_VARARGS, NULL},\n-    {\"test_inplace_increment\",\n-        inplace_increment,\n-        METH_VARARGS, NULL},\n     {\"fromstring_null_term_c_api\",\n         fromstring_null_term_c_api,\n         METH_O, NULL},"
            },
            {
                "filename": "numpy/core/src/multiarray/mapping.c",
                "patch": "@@ -112,8 +112,7 @@ _get_transpose(int fancy_ndim, int consec, int ndim, int getmap, npy_intp *dims)\n }\n \n \n-/*NUMPY_API\n- *\n+/*\n  * Swap the axes to or from their inserted form. MapIter always puts the\n  * advanced (array) indices first in the iteration. But if they are\n  * consecutive, will insert/transpose them back before returning.\n@@ -2299,7 +2298,7 @@ PyArray_MapIterReset(PyArrayMapIterObject *mit)\n }\n \n \n-/*NUMPY_API\n+/*\n  * This function needs to update the state of the map iterator\n  * and point mit->dataptr to the memory-location of the next object\n  *\n@@ -3105,13 +3104,6 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n     if (!uses_subspace) {\n         mit->outer_strides = NpyIter_GetInnerStrideArray(mit->outer);\n     }\n-    if (NpyIter_IterationNeedsAPI(mit->outer)) {\n-        mit->needs_api = 1;\n-        /* We may be doing a cast for the buffer, and that may have failed */\n-        if (PyErr_Occurred()) {\n-            goto fail;\n-        }\n-    }\n \n     /* Get the allocated extra_op */\n     if (extra_op_flags) {\n@@ -3233,14 +3225,6 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n     mit->subspace_ptrs = NpyIter_GetDataPtrArray(mit->subspace_iter);\n     mit->subspace_strides = NpyIter_GetInnerStrideArray(mit->subspace_iter);\n \n-    if (NpyIter_IterationNeedsAPI(mit->subspace_iter)) {\n-        mit->needs_api = 1;\n-        /*\n-         * NOTE: In this case, need to call PyErr_Occurred() after\n-         *       basepointer resetting (buffer allocation)\n-         */\n-    }\n-\n     Py_XDECREF(extra_op);\n     Py_DECREF(intp_descr);\n     return (PyObject *)mit;\n@@ -3309,9 +3293,8 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n }\n \n \n-/*NUMPY_API\n- *\n- * Same as PyArray_MapIterArray, but:\n+/*\n+ * Use advanced indexing to iterate an array.\n  *\n  * If copy_if_overlap != 0, check if `a` has memory overlap with any of the\n  * arrays in `index` and with `extra_op`. If yes, make copies as appropriate\n@@ -3374,12 +3357,6 @@ PyArray_MapIterArrayCopyIfOverlap(PyArrayObject * a, PyObject * index,\n         goto fail;\n     }\n \n-    /* Required for backward compatibility */\n-    mit->ait = (PyArrayIterObject *)PyArray_IterNew((PyObject *)a);\n-    if (mit->ait == NULL) {\n-        goto fail;\n-    }\n-\n     if (PyArray_MapIterCheckIndices(mit) < 0) {\n         goto fail;\n     }\n@@ -3408,17 +3385,6 @@ PyArray_MapIterArrayCopyIfOverlap(PyArrayObject * a, PyObject * index,\n }\n \n \n-/*NUMPY_API\n- *\n- * Use advanced indexing to iterate an array.\n- */\n-NPY_NO_EXPORT PyObject *\n-PyArray_MapIterArray(PyArrayObject * a, PyObject * index)\n-{\n-    return PyArray_MapIterArrayCopyIfOverlap(a, index, 0, NULL);\n-}\n-\n-\n #undef HAS_INTEGER\n #undef HAS_NEWAXIS\n #undef HAS_SLICE\n@@ -3434,7 +3400,6 @@ arraymapiter_dealloc(PyArrayMapIterObject *mit)\n {\n     PyArray_ResolveWritebackIfCopy(mit->array);\n     Py_XDECREF(mit->array);\n-    Py_XDECREF(mit->ait);\n     Py_XDECREF(mit->subspace);\n     Py_XDECREF(mit->extra_op);\n     Py_XDECREF(mit->extra_op_dtype);"
            },
            {
                "filename": "numpy/core/src/multiarray/mapping.h",
                "patch": "@@ -4,6 +4,80 @@\n extern NPY_NO_EXPORT PyMappingMethods array_as_mapping;\n \n \n+/*\n+ * Store the information needed for fancy-indexing over an array. The\n+ * fields are slightly unordered to keep consec, dataptr and subspace\n+ * where they were originally.\n+ */\n+typedef struct {\n+        PyObject_HEAD\n+\n+        int                   numiter;                 /* number of index-array\n+                                                          iterators */\n+        npy_intp              size;                    /* size of broadcasted\n+                                                          result */\n+        int                   nd;                      /* number of dims */\n+        npy_intp              dimensions[NPY_MAXDIMS]; /* dimensions */\n+        NpyIter               *outer;                  /* index objects\n+                                                          iterator */\n+        PyArrayObject         *array;\n+\n+        /* Subspace array. */\n+        PyArrayObject         *subspace;\n+\n+        /*\n+         * if subspace iteration, then this is the array of axes in\n+         * the underlying array represented by the index objects\n+         */\n+        int                   iteraxes[NPY_MAXDIMS];\n+        npy_intp              fancy_strides[NPY_MAXDIMS];\n+\n+        /* pointer when all fancy indices are 0 */\n+        char                  *baseoffset;\n+\n+        /*\n+         * after binding consec denotes at which axis the fancy axes\n+         * are inserted.\n+         */\n+        int                   consec;\n+        char                  *dataptr;\n+\n+        int                   nd_fancy;\n+        npy_intp              fancy_dims[NPY_MAXDIMS];\n+\n+        /*\n+         * Extra op information.\n+         */\n+        PyArrayObject         *extra_op;\n+        PyArray_Descr         *extra_op_dtype;         /* desired dtype */\n+        npy_uint32            *extra_op_flags;         /* Iterator flags */\n+\n+        NpyIter               *extra_op_iter;\n+        NpyIter_IterNextFunc  *extra_op_next;\n+        char                  **extra_op_ptrs;\n+\n+        /*\n+         * Information about the iteration state.\n+         */\n+        NpyIter_IterNextFunc  *outer_next;\n+        char                  **outer_ptrs;\n+        npy_intp              *outer_strides;\n+\n+        /*\n+         * Information about the subspace iterator.\n+         */\n+        NpyIter               *subspace_iter;\n+        NpyIter_IterNextFunc  *subspace_next;\n+        char                  **subspace_ptrs;\n+        npy_intp              *subspace_strides;\n+\n+        /* Count for the external loop (which ever it is) for API iteration */\n+        npy_intp              iter_count;\n+\n+} PyArrayMapIterObject;\n+\n+extern NPY_NO_EXPORT PyTypeObject PyArrayMapIter_Type;\n+\n /*\n  * Struct into which indices are parsed.\n  * I.e. integer ones should only be parsed once, slices and arrays\n@@ -70,4 +144,9 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n                    npy_uint32 subspace_iter_flags, npy_uint32 subspace_flags,\n                    npy_uint32 extra_op_flags, PyArrayObject *extra_op,\n                    PyArray_Descr *extra_op_dtype);\n+\n+NPY_NO_EXPORT PyObject *\n+PyArray_MapIterArrayCopyIfOverlap(PyArrayObject * a, PyObject * index,\n+                                  int copy_if_overlap, PyArrayObject *extra_op);\n+\n #endif  /* NUMPY_CORE_SRC_MULTIARRAY_MAPPING_H_ */"
            },
            {
                "filename": "numpy/core/src/umath/ufunc_object.c",
                "patch": "@@ -59,6 +59,7 @@\n #include \"convert_datatype.h\"\n #include \"legacy_array_method.h\"\n #include \"abstractdtypes.h\"\n+#include \"mapping.h\"\n \n /* TODO: Only for `NpyIter_GetTransferFlags` until it is public */\n #define NPY_ITERATOR_IMPLEMENTATION_CODE"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -8656,28 +8656,6 @@ def test_scalar_element_deletion():\n     assert_raises(ValueError, a[0].__delitem__, 'x')\n \n \n-class TestMapIter:\n-    def test_mapiter(self):\n-        # The actual tests are within the C code in\n-        # multiarray/_multiarray_tests.c.src\n-\n-        a = np.arange(12).reshape((3, 4)).astype(float)\n-        index = ([1, 1, 2, 0],\n-                 [0, 0, 2, 3])\n-        vals = [50, 50, 30, 16]\n-\n-        _multiarray_tests.test_inplace_increment(a, index, vals)\n-        assert_equal(a, [[0.00, 1., 2.0, 19.],\n-                         [104., 5., 6.0, 7.0],\n-                         [8.00, 9., 40., 11.]])\n-\n-        b = np.arange(6).astype(float)\n-        index = (np.array([1, 2, 0]),)\n-        vals = [50, 4, 100.1]\n-        _multiarray_tests.test_inplace_increment(b, index, vals)\n-        assert_equal(b, [100.1,  51.,   6.,   3.,   4.,   5.])\n-\n-\n class TestAsCArray:\n     def test_1darray(self):\n         array = np.arange(24, dtype=np.double)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24248,
        "body": "Using `getenv` regularly is probably not great anyway, but it seems very slow on windows which leads to a large overhead for every array deallocation here.\r\n\r\nRefactor it out to only check on first import and add helper because the tests are set up slightly differently.\r\n(Manually checked that the startup works, tests run with policy set to 1, not modifying it and passing.)\r\n\r\nCloses gh-24232.\r\n",
        "changed_files": [
            {
                "filename": "doc/source/reference/global_state.rst",
                "patch": "@@ -82,3 +82,6 @@ memory allocation policy, the default will be to call ``free``. If\n ``NUMPY_WARN_IF_NO_MEM_POLICY`` is set to ``\"1\"``, a ``RuntimeWarning`` will\n be emitted. A better alternative is to use a ``PyCapsule`` with a deallocator\n and set the ``ndarray.base``.\n+\n+.. versionchanged:: 1.25.2\n+    This variable is only checked on the first import.\n\\ No newline at end of file"
            },
            {
                "filename": "numpy/core/src/multiarray/arrayobject.c",
                "patch": "@@ -62,6 +62,9 @@ maintainer email:  oliphant.travis@ieee.org\n #include \"binop_override.h\"\n #include \"array_coercion.h\"\n \n+\n+NPY_NO_EXPORT npy_bool numpy_warn_if_no_mem_policy = 0;\n+\n /*NUMPY_API\n   Compute the size of an array (in number of items)\n */\n@@ -460,9 +463,8 @@ array_dealloc(PyArrayObject *self)\n             }\n         }\n         if (fa->mem_handler == NULL) {\n-            char *env = getenv(\"NUMPY_WARN_IF_NO_MEM_POLICY\");\n-            if ((env != NULL) && (strncmp(env, \"1\", 1) == 0)) {\n-                char const * msg = \"Trying to dealloc data, but a memory policy \"\n+            if (numpy_warn_if_no_mem_policy) {\n+                char const *msg = \"Trying to dealloc data, but a memory policy \"\n                     \"is not set. If you take ownership of the data, you must \"\n                     \"set a base owning the data (e.g. a PyCapsule).\";\n                 WARN_IN_DEALLOC(PyExc_RuntimeWarning, msg);"
            },
            {
                "filename": "numpy/core/src/multiarray/arrayobject.h",
                "patch": "@@ -5,6 +5,8 @@\n #ifndef NUMPY_CORE_SRC_MULTIARRAY_ARRAYOBJECT_H_\n #define NUMPY_CORE_SRC_MULTIARRAY_ARRAYOBJECT_H_\n \n+extern NPY_NO_EXPORT npy_bool numpy_warn_if_no_mem_policy;\n+\n NPY_NO_EXPORT PyObject *\n _strings_richcompare(PyArrayObject *self, PyArrayObject *other, int cmp_op,\n                      int rstrip);"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -4378,6 +4378,24 @@ normalize_axis_index(PyObject *NPY_UNUSED(self),\n }\n \n \n+static PyObject *\n+_set_numpy_warn_if_no_mem_policy(PyObject *NPY_UNUSED(self), PyObject *arg)\n+{\n+    int res = PyObject_IsTrue(arg);\n+    if (res < 0) {\n+        return NULL;\n+    }\n+    int old_value = numpy_warn_if_no_mem_policy;\n+    numpy_warn_if_no_mem_policy = res;\n+    if (old_value) {\n+        Py_RETURN_TRUE;\n+    }\n+    else {\n+        Py_RETURN_FALSE;\n+    }\n+}\n+\n+\n static PyObject *\n _reload_guard(PyObject *NPY_UNUSED(self), PyObject *NPY_UNUSED(args)) {\n     static int initialized = 0;\n@@ -4625,6 +4643,9 @@ static struct PyMethodDef array_module_methods[] = {\n          METH_O, \"Set the NEP 50 promotion state.  This is not thread-safe.\\n\"\n                  \"The optional warnings can be safely silenced using the \\n\"\n                  \"`np._no_nep50_warning()` context manager.\"},\n+    {\"_set_numpy_warn_if_no_mem_policy\",\n+         (PyCFunction)_set_numpy_warn_if_no_mem_policy,\n+         METH_O, \"Change the warn if no mem policy flag for testing.\"},\n     {\"_add_newdoc_ufunc\", (PyCFunction)add_newdoc_ufunc,\n         METH_VARARGS, NULL},\n     {\"_get_sfloat_dtype\",\n@@ -4913,6 +4934,14 @@ initialize_static_globals(void)\n         return -1;\n     }\n \n+    char *env = getenv(\"NUMPY_WARN_IF_NO_MEM_POLICY\");\n+    if ((env != NULL) && (strncmp(env, \"1\", 1) == 0)) {\n+        numpy_warn_if_no_mem_policy = 1;\n+    }\n+    else {\n+        numpy_warn_if_no_mem_policy = 0;\n+    }\n+\n     return 0;\n }\n "
            },
            {
                "filename": "numpy/core/tests/test_mem_policy.py",
                "patch": "@@ -409,16 +409,19 @@ def test_switch_owner(get_module, policy):\n     a = get_module.get_array()\n     assert np.core.multiarray.get_handler_name(a) is None\n     get_module.set_own(a)\n-    oldval = os.environ.get('NUMPY_WARN_IF_NO_MEM_POLICY', None)\n+\n     if policy is None:\n-        if 'NUMPY_WARN_IF_NO_MEM_POLICY' in os.environ:\n-            os.environ.pop('NUMPY_WARN_IF_NO_MEM_POLICY')\n+        # See what we expect to be set based on the env variable\n+        policy = os.getenv(\"NUMPY_WARN_IF_NO_MEM_POLICY\", \"0\") == \"1\"\n+        oldval = None\n     else:\n-        os.environ['NUMPY_WARN_IF_NO_MEM_POLICY'] = policy\n+        policy = policy == \"1\"\n+        oldval = np.core._multiarray_umath._set_numpy_warn_if_no_mem_policy(\n+            policy)\n     try:\n         # The policy should be NULL, so we have to assume we can call\n         # \"free\".  A warning is given if the policy == \"1\"\n-        if policy == \"1\":\n+        if policy:\n             with assert_warns(RuntimeWarning) as w:\n                 del a\n                 gc.collect()\n@@ -427,11 +430,8 @@ def test_switch_owner(get_module, policy):\n             gc.collect()\n \n     finally:\n-        if oldval is None:\n-            if 'NUMPY_WARN_IF_NO_MEM_POLICY' in os.environ:\n-                os.environ.pop('NUMPY_WARN_IF_NO_MEM_POLICY')\n-        else:\n-            os.environ['NUMPY_WARN_IF_NO_MEM_POLICY'] = oldval\n+        if oldval is not None:\n+            np.core._multiarray_umath._set_numpy_warn_if_no_mem_policy(oldval)\n \n \n @pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23113,
        "body": "I use the mixins in a few different file backed arrays.\r\n\r\nHowever, the lack of slots make it difficult for me to use slots.\r\n\r\nI mostly use slots to ensure that performance optimized code doesn't create unecessary references to large chunks of memory.\r\n\r\nIf all parent classes do not have `__slots__` defined, I think that Python (3.9) just ignores `__slots__` alltogether.\r\n\r\nThank you for considering.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/23113.improvement.rst",
                "patch": "@@ -0,0 +1,3 @@\n+- The ``NDArrayOperatorsMixin`` class now specifies that it contains no\n+  ``__slots__`` ensureing that subclasses can now make use of this feature in\n+  Python."
            },
            {
                "filename": "numpy/lib/mixins.py",
                "patch": "@@ -133,6 +133,7 @@ def __repr__(self):\n \n     .. versionadded:: 1.13\n     \"\"\"\n+    __slots__ = ()\n     # Like np.ndarray, this mixin class implements \"Option 1\" from the ufunc\n     # overrides NEP.\n "
            },
            {
                "filename": "numpy/ma/tests/test_subclassing.py",
                "patch": "@@ -154,6 +154,7 @@ class WrappedArray(NDArrayOperatorsMixin):\n     ufunc deferrals are commutative.\n     See: https://github.com/numpy/numpy/issues/15200)\n     \"\"\"\n+    __slots__ = ('_array', 'attrs')\n     __array_priority__ = 20\n \n     def __init__(self, array, **attrs):\n@@ -448,3 +449,12 @@ def test_masked_binary_operations(self):\n         assert_(isinstance(np.divide(wm, m2), WrappedArray))\n         assert_(isinstance(np.divide(m2, wm), WrappedArray))\n         assert_equal(np.divide(m2, wm), np.divide(wm, m2))\n+\n+    def test_mixins_have_slots(self):\n+        mixin = NDArrayOperatorsMixin()\n+        # Should raise an error\n+        assert_raises(AttributeError, mixin.__setattr__, \"not_a_real_attr\", 1)\n+\n+        m = np.ma.masked_array([1, 3, 5], mask=[False, True, False])\n+        wm = WrappedArray(m)\n+        assert_raises(AttributeError, wm.__setattr__, \"not_an_attr\", 2)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23789,
        "body": "This adds a parameter to api.reshape to specify if data should be copied. This parameter is required so that api.reshape conforms to the standard. See #23410",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/23789.new_feature.rst",
                "patch": "No changes"
            },
            {
                "filename": "numpy/array_api/_manipulation_functions.py",
                "patch": "@@ -53,13 +53,27 @@ def permute_dims(x: Array, /, axes: Tuple[int, ...]) -> Array:\n \n \n # Note: the optional argument is called 'shape', not 'newshape'\n-def reshape(x: Array, /, shape: Tuple[int, ...]) -> Array:\n+def reshape(x: Array, \n+            /, \n+            shape: Tuple[int, ...],\n+            *,\n+            copy: Optional[Bool] = None) -> Array:\n     \"\"\"\n     Array API compatible wrapper for :py:func:`np.reshape <numpy.reshape>`.\n \n     See its docstring for more information.\n     \"\"\"\n-    return Array._new(np.reshape(x._array, shape))\n+\n+    data = x._array\n+    if copy:\n+        data = np.copy(data)\n+\n+    reshaped = np.reshape(data, shape)\n+\n+    if copy is False and not np.shares_memory(data, reshaped):\n+        raise AttributeError(\"Incompatible shape for in-place modification.\")\n+\n+    return Array._new(reshaped)\n \n \n def roll("
            },
            {
                "filename": "numpy/array_api/tests/test_manipulation_functions.py",
                "patch": "@@ -0,0 +1,37 @@\n+from numpy.testing import assert_raises\n+import numpy as np\n+\n+from .. import all\n+from .._creation_functions import asarray\n+from .._dtypes import float64, int8\n+from .._manipulation_functions import (\n+        concat,\n+        reshape,\n+        stack\n+)\n+\n+\n+def test_concat_errors():\n+    assert_raises(TypeError, lambda: concat((1, 1), axis=None))\n+    assert_raises(TypeError, lambda: concat([asarray([1], dtype=int8),\n+                                             asarray([1], dtype=float64)]))\n+\n+\n+def test_stack_errors():\n+    assert_raises(TypeError, lambda: stack([asarray([1, 1], dtype=int8),\n+                                            asarray([2, 2], dtype=float64)]))\n+\n+\n+def test_reshape_copy():\n+    a = asarray(np.ones((2, 3)))\n+    b = reshape(a, (3, 2), copy=True)\n+    assert not np.shares_memory(a._array, b._array)\n+    \n+    a = asarray(np.ones((2, 3)))\n+    b = reshape(a, (3, 2), copy=False)\n+    assert np.shares_memory(a._array, b._array)\n+\n+    a = asarray(np.ones((2, 3)).T)\n+    b = reshape(a, (3, 2), copy=True)\n+    assert_raises(AttributeError, lambda: reshape(a, (2, 3), copy=False))\n+"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23980,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/array_api/tests/test_array_object.py",
                "patch": "@@ -219,8 +219,7 @@ def _array_vals():\n     # Finally, matmul() must be tested separately, because it works a bit\n     # different from the other operations.\n     def _matmul_array_vals():\n-        for a in _array_vals():\n-            yield a\n+        yield from _array_vals()\n         for d in _all_dtypes:\n             yield ones((3, 4), dtype=d)\n             yield ones((4, 2), dtype=d)"
            },
            {
                "filename": "numpy/lib/tests/test_loadtxt.py",
                "patch": "@@ -459,8 +459,7 @@ def gen():\n \n def test_read_from_bad_generator():\n     def gen():\n-        for entry in [\"1,2\", b\"3, 5\", 12738]:\n-            yield entry\n+        yield from [\"1,2\", b\"3, 5\", 12738]\n \n     with pytest.raises(\n             TypeError, match=r\"non-string returned while reading data\"):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 8528,
        "body": "Currently a work in progress.  Adds functionality from issue #8513 \r\n\r\nQuestions:\r\n\r\n- How should this be tied into numpy setup?  Currently the build fails when clean because logical_gufuncs.c.src isnt being expanded.  If I uncomment line 905 numpy.core.setup that file gets expanded into the *.c, but now the compile fails because its included twice.  If I leave the expanded *.c files and recomment line 905, now the build works.  \"generate_umath_templated_sources\" sounds like a helpful function but I don't think it is ever called.  Sorry but this is my first time in the bowels of a complicated setup.py.  Any advice?\r\n\r\n- I still need to add complex data types.  Should I add datetimes also?  Also need to add unit tests,  docstrings, release notes, and probably others\r\n\r\n- Any other suggestions welcomed\r\n\r\nThanks",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -140,6 +140,7 @@ numpy/core/src/umath/simd.inc\n numpy/core/src/umath/struct_ufunc_test.c\n numpy/core/src/umath/test_rational.c\n numpy/core/src/umath/umath_tests.c\n+numpy/core/src/umath/logical_gufuncs.c\n numpy/distutils/__config__.py\n numpy/linalg/umath_linalg.c\n doc/source/reference/generated"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -899,7 +899,8 @@ def generate_umath_c(ext, build_dir):\n             join('src', 'umath', 'ufunc_object.c'),\n             join('src', 'umath', 'scalarmath.c.src'),\n             join('src', 'umath', 'ufunc_type_resolution.c'),\n-            join('src', 'private', 'mem_overlap.c')]\n+            join('src', 'private', 'mem_overlap.c'),\n+            join('src', 'umath', 'logical_gufuncs.c.src')]\n \n     umath_deps = [\n             generate_umath_py,"
            },
            {
                "filename": "numpy/core/src/umath/logical_gufuncs.c.src",
                "patch": "@@ -0,0 +1,528 @@\n+#define _UMATHMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define NO_IMPORT_ARRAY\n+#include \"npy_config.h\"\n+#include \"numpy/ndarraytypes.h\"\n+#include \"numpy/ufuncobject.h\"\n+#include \"numpy/halffloat.h\"\n+#include \"ufunc_type_resolution.h\"\n+#include \"logical_gufuncs.h\"\n+#include \"lowlevel_strided_loops.h\"\n+\n+\n+/* define the basic real version similar to the macro for complex numbers */\n+#define REQ(a,b) ((a) == (b))\n+#define RNE(a,b) ((a) != (b))\n+#define RLT(a,b) ((a) < (b))\n+#define RLE(a,b) ((a) <= (b))\n+#define RGT(a,b) ((a) > (b))\n+#define RGE(a,b) ((a) >= (b))\n+\n+#define HEQ(a,b) (npy_half_eq(a, b))\n+#define HNE(a,b) (npy_half_ne(a, b))\n+#define HLT(a,b) (npy_half_lt(a, b))\n+#define HLE(a,b) (npy_half_le(a, b))\n+#define HGT(a,b) (npy_half_gt(a, b))\n+#define HGE(a,b) (npy_half_ge(a, b))\n+\n+/* create the family of functions using a template  */\n+\n+#define BLOCK_SIZE 32\n+\n+/**begin repeat\n+ * #TYPE = npy_bool,npy_int8,npy_uint8,npy_int16,npy_uint16,\n+ *         npy_int32,npy_uint32,npy_int64,npy_uint64,\n+ *         npy_half,npy_float,npy_double,npy_longdouble,\n+ *         npy_cfloat,npy_cdouble,npy_clongdouble#\n+ * #OP_PREFIX = R,R,R,R,R,R,R,R,R,H,R,R,R,PyArray_C,PyArray_C,PyArray_C#\n+ * #ISBOOL = != 0,,,,,,,,,,,,,,,#\n+ * #VECTORIZE = 1*9,0,1,1,0,1,1,0#\n+ */\n+\n+/**begin repeat1\n+ * #OP = EQ,NE,LT,LE,GT,GE#\n+ * #OPNAME = equal,not_equal,less,less_equal,greater,greater_equal#\n+ */\n+\n+/**begin repeat2\n+ * #fname = all,any#\n+ * #ALL_OR_ANY = NPY_TRUE,NPY_FALSE#\n+ * #INV = ,!#\n+ */\n+\n+/**begin repeat3\n+ * #isa = , _avx#\n+ * #CHK = 1, defined HAVE_ATTRIBUTE_TARGET_AVX && defined DO_VECTORIZE#\n+ * #ATTR = , NPY_GCC_TARGET_AVX#\n+ */\n+#if @VECTORIZE@\n+#define DO_VECTORIZE\n+#endif\n+\n+#if @CHK@\n+#define HAVE_@TYPE@_@fname@_@OPNAME@@isa@\n+static NPY_GCC_OPT_3 @ATTR@ void\n+@TYPE@_@fname@_@OPNAME@@isa@(char **args, npy_intp *dimensions,\n+                             npy_intp* steps, void* data)\n+{\n+    npy_intp n;\n+    npy_intp N = dimensions[0], nI = dimensions[1];\n+    char *a_n = args[0], *b_n = args[1], *c_n = args[2];\n+    npy_intp a_N = steps[0], b_N = steps[1], c_N = steps[2];\n+    npy_intp a_I = steps[3], b_I = steps[4];\n+\n+    for (n = 0; n < N; n++) {\n+        npy_intp i;\n+        char * a_i = a_n;\n+        char * b_i = b_n;\n+\n+        *((npy_bool *)c_n) = @ALL_OR_ANY@;\n+\n+        i = 0;\n+        /* main loop in chunks with auto vectorize simd instructions */\n+        if (@VECTORIZE@ && a_I == sizeof(@TYPE@) && b_I == sizeof(@TYPE@)) {\n+            for (i=0; i < npy_blocked_end(0, 1, BLOCK_SIZE, nI);\n+                 i+=BLOCK_SIZE) {\n+                unsigned int true_count = 0, j;\n+                for (j=0 ; j<BLOCK_SIZE ; j++){\n+                    @TYPE@ a = (*(@TYPE@ *)a_i) @ISBOOL@;\n+                    @TYPE@ b = (*(@TYPE@ *)b_i) @ISBOOL@;\n+                    true_count += @INV@@OP_PREFIX@@OP@(a, b);\n+                    a_i += a_I;\n+                    b_i += b_I;\n+                }\n+                if (true_count != BLOCK_SIZE) {\n+                    *((npy_bool *)c_n) = !@ALL_OR_ANY@;\n+                    i = nI;\n+                    break;\n+                }\n+            }\n+        }\n+\n+        /* check remaining elements */\n+        for ( ; i < nI; i++) {\n+            @TYPE@ a = (*(@TYPE@ *)a_i) @ISBOOL@;\n+            @TYPE@ b = (*(@TYPE@ *)b_i) @ISBOOL@;\n+            if (@INV@@OP_PREFIX@@OP@(a, b)) {\n+                a_i += a_I;\n+                b_i += b_I;\n+            } else {\n+                *((npy_bool *)c_n) = !@ALL_OR_ANY@;\n+                break;\n+            }\n+        }\n+\n+        a_n += a_N;\n+        b_n += b_N;\n+        c_n += c_N;\n+    }\n+}\n+#endif\n+\n+#undef DO_VECTORIZE\n+\n+/**end repeat3**/\n+/**end repeat2**/\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #TYPE = npy_datetime, npy_timedelta#\n+ */\n+\n+/**begin repeat1\n+ * #NAME = all,any#\n+ * #ALL_OR_ANY = NPY_TRUE,NPY_FALSE#\n+ * #INV = ,!#\n+ */\n+\n+/**begin repeat2\n+ * #OPNAME = equal,less,less_equal,greater,greater_equal#\n+ * #OP = ==,<,<=,>,>=#\n+ */\n+\n+static  void\n+@TYPE@_@NAME@_@OPNAME@(char **args, npy_intp *dimensions,\n+                       npy_intp* steps, void* data)\n+{\n+    npy_intp n;\n+    npy_intp N = dimensions[0], nI = dimensions[1];\n+    char *a_n = args[0], *b_n = args[1], *c_n = args[2];\n+    npy_intp a_N = steps[0], b_N = steps[1], c_N = steps[2];\n+    npy_intp a_I = steps[3], b_I = steps[4];\n+\n+    for (n = 0; n < N; n++) {\n+        npy_intp i;\n+        char * a_i = a_n;\n+        char * b_i = b_n;\n+\n+        *((npy_bool *)c_n) = @ALL_OR_ANY@;\n+\n+        for (i=0; i < nI; i++) {\n+            @TYPE@ a = *(@TYPE@ *)a_i;\n+            @TYPE@ b = *(@TYPE@ *)b_i;\n+            npy_bool res = @INV@(a @OP@ b);\n+\n+            if ((a == NPY_DATETIME_NAT || b == NPY_DATETIME_NAT) && @INV@res) {\n+                NPY_ALLOW_C_API_DEF;\n+                NPY_ALLOW_C_API;\n+                /* 2016-01-18, 1.11 */\n+                if (DEPRECATE_FUTUREWARNING(\n+                                \"In the future, 'NAT @OP@ x' and 'x @OP@ NAT' \"\n+                                \"will always be False.\") < 0) {\n+                    NPY_DISABLE_C_API;\n+                    return;\n+                }\n+                NPY_DISABLE_C_API;\n+            }\n+\n+            if (res) {\n+                a_i += a_I;\n+                b_i += b_I;\n+            } else {\n+                *((npy_bool *)c_n) = !@ALL_OR_ANY@;\n+                break;\n+            }\n+        }\n+\n+        a_n += a_N;\n+        b_n += b_N;\n+        c_n += c_N;\n+    }\n+}\n+\n+/**end repeat2**/\n+\n+static  void\n+@TYPE@_@NAME@_not_equal(char **args, npy_intp *dimensions,\n+                        npy_intp* steps, void* data)\n+{\n+    npy_intp n;\n+    npy_intp N = dimensions[0], nI = dimensions[1];\n+    char *a_n = args[0], *b_n = args[1], *c_n = args[2];\n+    npy_intp a_N = steps[0], b_N = steps[1], c_N = steps[2];\n+    npy_intp a_I = steps[3], b_I = steps[4];\n+\n+    for (n = 0; n < N; n++) {\n+        npy_intp i;\n+        char * a_i = a_n;\n+        char * b_i = b_n;\n+\n+        *((npy_bool *)c_n) = @ALL_OR_ANY@;\n+\n+        for (i=0; i < nI; i++) {\n+            @TYPE@ a = *(@TYPE@ *)a_i;\n+            @TYPE@ b = *(@TYPE@ *)b_i;\n+            npy_bool res = @INV@(a != b);\n+\n+            if (a == NPY_DATETIME_NAT && b == NPY_DATETIME_NAT) {\n+                NPY_ALLOW_C_API_DEF\n+                NPY_ALLOW_C_API;\n+                /* 2016-01-18, 1.11 */\n+                if (DEPRECATE_FUTUREWARNING(\n+                                    \"In the future, NAT != NAT will be True \"\n+                                    \"rather than False.\") < 0) {\n+                    NPY_DISABLE_C_API;\n+                    return;\n+                }\n+                NPY_DISABLE_C_API;\n+            }\n+\n+            if (res) {\n+                a_i += a_I;\n+                b_i += b_I;\n+            } else {\n+                *((npy_bool *)c_n) = !@ALL_OR_ANY@;\n+                break;\n+            }\n+        }\n+\n+        a_n += a_N;\n+        b_n += b_N;\n+        c_n += c_N;\n+    }\n+}\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #NAME = all,any#\n+ * #ALL_OR_ANY = NPY_TRUE,NPY_FALSE#\n+ * #INV = ,!#\n+ */\n+\n+/**begin repeat1\n+ * #OPNAME = equal, not_equal, greater, greater_equal, less, less_equal#\n+ * #OP = EQ, NE, GT, GE, LT, LE#\n+ */\n+NPY_NO_EXPORT void\n+OBJECT_@NAME@_@OPNAME@(char **args, npy_intp *dimensions,\n+                       npy_intp *steps, void *NPY_UNUSED(func))\n+{\n+    npy_intp n;\n+    npy_intp N = dimensions[0], nI = dimensions[1];\n+    char *a_n = args[0], *b_n = args[1], *c_n = args[2];\n+    npy_intp a_N = steps[0], b_N = steps[1], c_N = steps[2];\n+    npy_intp a_I = steps[3], b_I = steps[4];\n+\n+    for (n = 0; n < N; n++) {\n+        npy_intp i;\n+        char * a_i = a_n;\n+        char * b_i = b_n;\n+\n+        *((npy_bool *)c_n) = @ALL_OR_ANY@;\n+\n+        for (i=0; i < nI; i++) {\n+            int res;\n+            PyObject *ret_obj;\n+            PyObject * a = *(PyObject **)a_i;\n+            PyObject * b = *(PyObject **)b_i;\n+\n+            a = a ? a : Py_None;\n+            b = b ? b : Py_None;\n+\n+            /*\n+             * Do not use RichCompareBool because it includes an identity check\n+             * for == and !=. This is wrong for elementwise behaviour, since it\n+             * means that NaN can be equal to NaN and an array is equal to\n+             * itself.\n+             */\n+            ret_obj = PyObject_RichCompare(a, b, Py_@OP@);\n+            if (ret_obj == NULL) {\n+                return;\n+            }\n+            res = PyObject_IsTrue(ret_obj);\n+            Py_DECREF(ret_obj);\n+            if (res == -1) {\n+                return;\n+            }\n+\n+            if (@INV@res) {\n+                a_i += a_I;\n+                b_i += b_I;\n+            } else {\n+                *((npy_bool *)c_n) = !@ALL_OR_ANY@;\n+                break;\n+            }\n+        }\n+\n+        a_n += a_N;\n+        b_n += b_N;\n+        c_n += c_N;\n+    }\n+}\n+/**end repeat**/\n+\n+/* create type arrays for each gufunc, which are all identical */\n+static char types[] = {NPY_BOOL, NPY_BOOL, NPY_BOOL,\n+                       NPY_BYTE, NPY_BYTE, NPY_BOOL,\n+                       NPY_UBYTE, NPY_UBYTE, NPY_BOOL,\n+                       NPY_SHORT, NPY_SHORT, NPY_BOOL,\n+                       NPY_USHORT, NPY_USHORT, NPY_BOOL,\n+\n+                       NPY_INT, NPY_INT, NPY_BOOL,\n+                       NPY_UINT, NPY_UINT, NPY_BOOL,\n+                       NPY_LONG, NPY_LONG, NPY_BOOL,\n+                       NPY_ULONG, NPY_ULONG, NPY_BOOL,\n+\n+                       NPY_LONGLONG, NPY_LONGLONG, NPY_BOOL,\n+                       NPY_ULONGLONG, NPY_ULONGLONG, NPY_BOOL,\n+                       NPY_HALF, NPY_HALF, NPY_BOOL,\n+                       NPY_FLOAT, NPY_FLOAT, NPY_BOOL,\n+                       NPY_DOUBLE, NPY_DOUBLE, NPY_BOOL,\n+\n+                       NPY_LONGDOUBLE, NPY_LONGDOUBLE, NPY_BOOL,\n+                       NPY_CFLOAT, NPY_CFLOAT, NPY_BOOL,\n+                       NPY_CDOUBLE, NPY_CDOUBLE, NPY_BOOL,\n+                       NPY_CLONGDOUBLE, NPY_CLONGDOUBLE, NPY_BOOL,\n+\n+                       NPY_OBJECT, NPY_OBJECT, NPY_BOOL,\n+                       NPY_DATETIME, NPY_DATETIME, NPY_BOOL,\n+                       NPY_TIMEDELTA, NPY_TIMEDELTA, NPY_BOOL};\n+\n+\n+/* create array of nulls for \"data\" for each gufunc type */\n+\n+static void *array_of_nulls[] = {\n+    (void *)NULL, (void *)NULL, (void *)NULL, (void *)NULL,\n+    (void *)NULL, (void *)NULL, (void *)NULL, (void *)NULL,\n+    (void *)NULL, (void *)NULL, (void *)NULL, (void *)NULL,\n+    (void *)NULL, (void *)NULL, (void *)NULL, (void *)NULL,\n+    (void *)NULL, (void *)NULL, (void *)NULL, (void *)NULL,\n+    (void *)NULL\n+};\n+\n+\n+/* define docstrings */\n+\n+/**begin repeat\n+* #name = equal,not_equal,less,less_equal,greater,greater_equal,\n+*         equal,not_equal,less,less_equal,greater,greater_equal#\n+* #prefix = all*6, any*6#\n+* #op = ==,!=,<,<=,>,>=,==,!=,<,<=,>,>=#\n+* #res1 = True,False,False,True,False,True,\n+*         True,False,False,True,False,True#\n+* #res2 = array([ True False  True]),\n+*         array([False  True False]),\n+*         array([False  True False]),\n+*         array([ True  True  True]),\n+*         array([False False False]),\n+*         array([ True False  True]),\n+*         array([ True False  True]),\n+*         array([False  True False]),\n+*         array([False  True False]),\n+*         array([ True  True  True]),\n+*         array([False False False]),\n+*         array([ True False  True])#\n+*/\n+\n+static char const * const @prefix@_@name@_doc =\n+    \"Return True if x1 @op@ x2 for @prefix@ elements along the last axis, \"\n+    \"False\\n\"\n+    \"otherwise.  Similar to (x1 @op@ x2).@prefix@(axis=-1), except the last \"\n+    \"dimension\\n\"\n+    \"of x1 and x2 must be equal and greater than 1.\\n\"\n+    \"\\n\"\n+    \"Parameters\\n\"\n+    \"----------\\n\"\n+    \"x1, x2 : array_like\\n\"\n+    \"    Input arrays of the same shape.\\n\"\n+    \"\\n\"\n+    \"Returns\\n\"\n+    \"-------\\n\"\n+    \"out : ndarray or bool\\n\"\n+    \"   Output array of bools, or a single bool if x1 and x2 are 1D.\\n\"\n+    \"\\n\"\n+    \"\\n\"\n+    \"Examples\\n\"\n+    \"-------\\n\"\n+    \">>> np.@prefix@_@name@(np.arange(3), np.arange(3))\\n\"\n+    \"@res1@\\n\"\n+    \">>> np.@prefix@_@name@([[1, 2], [0, 0], [1, 2]], [1, 2])\\n\"\n+    \"@res2@\";\n+\n+/**end repeat**/\n+\n+/* function to create and register all gufuncs */\n+\n+/*\n+ * define bad integer names to sized names to assign long and longlong to the\n+ * int64 functions (or int and long to int32)\n+ * this avoids code duplication\n+ */\n+#define NPY_FUNCNAME__(type, name, suffix) type##_##name##suffix\n+#define NPY_FUNCNAME_(type, name, suffix) NPY_FUNCNAME__(type, name, suffix)\n+#define NPY_FUNCNAME(type, name, suffix) NPY_FUNCNAME_(type, name, suffix)\n+\n+/**begin repeat\n+ * #name = byte, short, int, long, longlong#\n+ * #NAME = BYTE, SHORT, INT, LONG, LONGLONG#\n+ */\n+/**begin repeat1\n+ * #SIZE = 1, 2, 4, 8#\n+ * #BITS = 8, 16, 32, 64#\n+ */\n+#if NPY_SIZEOF_@NAME@ == @SIZE@\n+#define npy_@name@ npy_int@BITS@\n+#define npy_u@name@ npy_uint@BITS@\n+\n+/**begin repeat2\n+ * #OPNAME = equal,not_equal,less,less_equal,greater,greater_equal#\n+ */\n+/**begin repeat3\n+ * #fname = all,any#\n+ */\n+#ifdef HAVE_npy_int@BITS@_@fname@_@OPNAME@_avx\n+#define HAVE_npy_@name@_@fname@_@OPNAME@_avx\n+#endif\n+#ifdef HAVE_npy_uint@BITS@_@fname@_@OPNAME@_avx\n+#define HAVE_npy_u@name@_@fname@_@OPNAME@_avx\n+#endif\n+/**end repeat3**/\n+/**end repeat2**/\n+\n+#endif\n+/**end repeat1**/\n+/**end repeat**/\n+\n+void InitLogicalGufuncs(PyObject *dictionary,\n+                        PyUFunc_FromFuncAndDataAndSignature_t createPyUFunc)\n+{\n+    PyObject *f;\n+\n+/**begin repeat\n+ * #NAME = all_equal,all_not_equal,all_less,all_less_equal,all_greater,\n+ *         all_greater_equal,any_equal,any_not_equal,any_less,any_less_equal,\n+ *         any_greater,any_greater_equal#\n+ */\n+\n+    { /* open bracket surrounding inner repeat */\n+\n+        static PyUFuncGenericFunction @NAME@_funcs_base[] = {\n+\n+/**begin repeat1\n+ * #TYPE = npy_bool,npy_byte,npy_ubyte,npy_short,npy_ushort,npy_int,npy_uint,\n+ *         npy_long,npy_ulong,npy_longlong,npy_ulonglong,\n+ *         npy_half,npy_float,npy_double,\n+ *         npy_longdouble,npy_cfloat,npy_cdouble,npy_clongdouble#\n+ */\n+            NPY_FUNCNAME(@TYPE@,@NAME@,),\n+\n+/**end repeat1**/\n+            OBJECT_@NAME@,\n+            npy_datetime_@NAME@,\n+            npy_timedelta_@NAME@\n+        };\n+        static PyUFuncGenericFunction @NAME@_funcs_avx[] = {\n+\n+/**begin repeat1\n+ * #TYPE = npy_bool,npy_byte,npy_ubyte,npy_short,npy_ushort,npy_int,npy_uint,\n+ *         npy_long,npy_ulong,npy_longlong,npy_ulonglong,\n+ *         npy_half,npy_float,npy_double,\n+ *         npy_longdouble,npy_cfloat,npy_cdouble,npy_clongdouble#\n+ */\n+#ifdef HAVE_@TYPE@_@NAME@_avx\n+            NPY_FUNCNAME(@TYPE@,@NAME@,_avx),\n+#else\n+            NPY_FUNCNAME(@TYPE@,@NAME@,),\n+#endif\n+\n+/**end repeat1**/\n+            OBJECT_@NAME@,\n+            npy_datetime_@NAME@,\n+            npy_timedelta_@NAME@\n+        };\n+\n+        PyUFuncGenericFunction * funcs = @NAME@_funcs_base;\n+\n+#ifdef HAVE_ATTRIBUTE_TARGET_AVX\n+        if (NPY_CPU_SUPPORTS_AVX) {\n+            funcs = @NAME@_funcs_avx;\n+        }\n+#endif\n+\n+        assert(sizeof(array_of_nulls) / sizeof(void*) == sizeof(types) / 3);\n+        assert(sizeof(@NAME@_funcs_base) / sizeof(void*) == sizeof(types) / 3);\n+        assert(sizeof(@NAME@_funcs_avx) / sizeof(void*) == sizeof(types) / 3);\n+        f = createPyUFunc(funcs,\n+                          array_of_nulls,\n+                          types,\n+                          sizeof(types) / 3,  /* number of types */\n+                          2,                  /* number of inputs */\n+                          1,                  /* number of outputs */\n+                          PyUFunc_None,\n+                          \"@NAME@\",\n+                          (char*) @NAME@_doc,\n+                          0,              /* unused */\n+                          \"(i),(i)->()\");\n+\n+        ((PyUFuncObject *)f)->type_resolver =\n+            &PyUFunc_SimpleBinaryComparisonTypeResolver;\n+        PyDict_SetItemString(dictionary, \"@NAME@\", f);\n+        Py_DECREF(f);\n+\n+    }; /* close bracket surrounding inner repeat */\n+\n+/**end repeat**/\n+}"
            },
            {
                "filename": "numpy/core/src/umath/logical_gufuncs.h",
                "patch": "@@ -0,0 +1,18 @@\n+#ifndef _NPY_LOGICAL_GUFUNCS_H_\n+#define _NPY_LOGICAL_GUFUNCS_H_\n+typedef PyObject*\n+(*PyUFunc_FromFuncAndDataAndSignature_t)(PyUFuncGenericFunction*,\n+                                         void**,\n+                                         char*,\n+                                         int,\n+                                         int,\n+                                         int,\n+                                         int,\n+                                         const char*,\n+                                         const char*,\n+                                         int,\n+                                         const char*);\n+\n+void InitLogicalGufuncs(PyObject *dictionary,\n+                        PyUFunc_FromFuncAndDataAndSignature_t createPyUFunc);\n+#endif"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1196,6 +1196,7 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @TYPE@_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func))\n {\n+    /* NOTE logical_gufuncs.c.src implements the same code */\n     BINARY_LOOP {\n         const @type@ in1 = *(@type@ *)ip1;\n         const @type@ in2 = *(@type@ *)ip2;\n@@ -1221,6 +1222,7 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @TYPE@_not_equal(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func))\n {\n+    /* NOTE logical_gufuncs.c.src implements the same code */\n     BINARY_LOOP {\n         const @type@ in1 = *(@type@ *)ip1;\n         const @type@ in2 = *(@type@ *)ip2;\n@@ -2661,6 +2663,7 @@ NPY_NO_EXPORT void\n  */\n NPY_NO_EXPORT void\n OBJECT_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func)) {\n+    /* NOTE logical_gufuncs.c.src implements the same code */\n     BINARY_LOOP {\n         int ret;\n         PyObject *ret_obj;"
            },
            {
                "filename": "numpy/core/src/umath/umathmodule.c",
                "patch": "@@ -41,6 +41,7 @@\n #include \"ufunc_type_resolution.h\"\n #include \"__umath_generated.c\"\n #include \"__ufunc_api.c\"\n+#include \"logical_gufuncs.h\"\n \n NPY_NO_EXPORT int initscalarmath(PyObject *);\n \n@@ -365,6 +366,7 @@ PyMODINIT_FUNC initumath(void)\n \n     /* Load the ufunc operators into the array module's namespace */\n     InitOperators(d);\n+    InitLogicalGufuncs(d, PyUFunc_FromFuncAndDataAndSignature);\n \n     PyDict_SetItemString(d, \"pi\", s = PyFloat_FromDouble(NPY_PI));\n     Py_DECREF(s);"
            },
            {
                "filename": "numpy/core/tests/test_datetime.py",
                "patch": "@@ -1093,39 +1093,46 @@ def test_datetime_compare(self):\n         assert_equal(np.greater_equal(a, b), [1, 1, 0, 1, 0])\n \n     def test_datetime_compare_nat(self):\n-        dt_nat = np.datetime64('NaT', 'D')\n-        dt_other = np.datetime64('2000-01-01')\n-        td_nat = np.timedelta64('NaT', 'h')\n-        td_other = np.timedelta64(1, 'h')\n+        dt_nat = np.array([np.datetime64('NaT', 'D')])\n+        dt_other = np.array([np.datetime64('2000-01-01')])\n+        td_nat = np.array([np.timedelta64('NaT', 'h')])\n+        td_other = np.array([np.timedelta64(1, 'h')])\n \n         with suppress_warnings() as sup:\n             # The assert warns contexts will again see the warning:\n             sup.filter(FutureWarning, \".*NAT\")\n \n             for op in [np.equal, np.less, np.less_equal,\n-                       np.greater, np.greater_equal]:\n-                if op(dt_nat, dt_nat):\n+                       np.greater, np.greater_equal,\n+                       np.all_equal, np.all_less, np.all_less_equal,\n+                       np.all_greater, np.all_greater_equal,\n+                       np.any_equal, np.any_less, np.any_less_equal,\n+                       np.any_greater, np.any_greater_equal]:\n+                if op(dt_nat, dt_nat).all():\n                     assert_warns(FutureWarning, op, dt_nat, dt_nat)\n-                if op(dt_nat, dt_other):\n+                if op(dt_nat, dt_other).all():\n                     assert_warns(FutureWarning, op, dt_nat, dt_other)\n-                if op(dt_other, dt_nat):\n+                if op(dt_other, dt_nat).all():\n                     assert_warns(FutureWarning, op, dt_other, dt_nat)\n-                if op(td_nat, td_nat):\n+                if op(td_nat, td_nat).all():\n                     assert_warns(FutureWarning, op, td_nat, td_nat)\n-                if op(td_nat, td_other):\n+                if op(td_nat, td_other).all():\n                     assert_warns(FutureWarning, op, td_nat, td_other)\n-                if op(td_other, td_nat):\n+                if op(td_other, td_nat).all():\n                     assert_warns(FutureWarning, op, td_other, td_nat)\n \n             assert_warns(FutureWarning, np.not_equal, dt_nat, dt_nat)\n             assert_warns(FutureWarning, np.not_equal, td_nat, td_nat)\n+            assert_warns(FutureWarning, np.all_not_equal, dt_nat, dt_nat)\n+            assert_warns(FutureWarning, np.any_not_equal, td_nat, td_nat)\n \n         with suppress_warnings() as sup:\n             sup.record(FutureWarning)\n-            assert_(np.not_equal(dt_nat, dt_other))\n-            assert_(np.not_equal(dt_other, dt_nat))\n-            assert_(np.not_equal(td_nat, td_other))\n-            assert_(np.not_equal(td_other, td_nat))\n+            for op in [np.not_equal, np.all_not_equal, np.any_not_equal]:\n+                assert_(op(dt_nat, dt_other).all())\n+                assert_(op(dt_other, dt_nat).all())\n+                assert_(op(td_nat, td_other).all())\n+                assert_(op(td_other, td_nat).all())\n             self.assertEqual(len(sup.log), 0)\n \n     def test_datetime_minmax(self):"
            },
            {
                "filename": "numpy/core/tests/test_logical_gufuncs.py",
                "patch": "@@ -0,0 +1,94 @@\n+from __future__ import division, absolute_import, print_function\n+\n+import numpy as np\n+from numpy.testing import (\n+    TestCase, run_module_suite, assert_equal\n+)\n+\n+float_types = [np.float16, np.float32, np.float64, np.longdouble]\n+complex_types = [np.cfloat, np.cdouble, np.clongdouble]\n+int_types = [np.bool, np.int8, np.uint8, np.int16, np.uint16, np.int32,\n+             np.uint32, np.int64, np.uint64, np.longlong, np.ulonglong]\n+datetime = ['M8[s]', 'm8[h]']\n+\n+# helper functions\n+def check(f, x1, x2, expected):\n+    assert_equal(f(x1, x2), expected)\n+\n+\n+def check_all(x1, x2):\n+    yield check, np.all_equal,          x1, x2, (x1==x2).all()\n+    yield check, np.any_equal,          x1, x2, (x1==x2).any()\n+    yield check, np.all_not_equal,      x1, x2, (x1!=x2).all()\n+    yield check, np.any_not_equal,      x1, x2, (x1!=x2).any()\n+    yield check, np.all_greater,        x1, x2, (x1>x2).all()\n+    yield check, np.any_greater,        x1, x2, (x1>x2).any()\n+    yield check, np.all_greater_equal,  x1, x2, (x1>=x2).all()\n+    yield check, np.any_greater_equal,  x1, x2, (x1>=x2).any()\n+    yield check, np.all_less,           x1, x2, (x1<x2).all()\n+    yield check, np.any_less,           x1, x2, (x1<x2).any()\n+    yield check, np.all_less_equal,     x1, x2, (x1<=x2).all()\n+    yield check, np.any_less_equal,     x1, x2, (x1<=x2).any()\n+\n+\n+def test_real():\n+    inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n+    for i in range(inputs.shape[0]):\n+        for j in range(inputs.shape[0]):\n+            for dtype in (int_types + float_types + complex_types +\n+                          [object] + datetime):\n+                x1 = inputs[i, :].astype(dtype)\n+                x2 = inputs[j, :].astype(dtype)\n+                for x in check_all(x1, x2):\n+                    yield x\n+\n+\n+def test_bool():\n+    inputs = np.array([[0,0],[0,12],[1,0],[1,16]])\n+    for i in range(inputs.shape[0]):\n+        for j in range(inputs.shape[0]):\n+            x1 = inputs[i, :].astype(np.int8).view(np.bool)\n+            x2 = inputs[j, :].astype(np.int8).view(np.bool)\n+            for x in check_all(x1, x2):\n+                yield x\n+\n+\n+def test_complex():\n+    j = 1j\n+    for m in range(-1, 2):\n+        for n in range(-1, 2):\n+            for dtype in complex_types:\n+                x1 = np.zeros(2, dtype=dtype)\n+                x2 = x1 + m + n * j\n+                for x in check_all(x1, x2):\n+                    yield x\n+\n+\n+def test_simd():\n+    for dtype in [np.float32, np.float64, np.int32]:\n+        x1 = np.arange(4000, dtype=dtype)\n+        x2 = x1.copy()\n+        yield check, np.all_equal, x1, x2, (x1==x2).all()\n+        x2[-1] = -1\n+        yield check, np.all_equal, x1, x2, (x1==x2).all()\n+        x2 = x1.copy()\n+        x2[500] = -2\n+        yield check, np.all_equal, x1, x2, (x1==x2).all()\n+        x2 = x1.copy()\n+        x2[0] = -3\n+        yield check, np.all_equal, x1, x2, (x1==x2).all()\n+\n+\n+class TestLogicalGUFuncs(TestCase):\n+    def test_structure(self):\n+        for op in [np.all_equal, np.all_less, np.all_less_equal,\n+                   np.all_greater, np.all_greater_equal,\n+                   np.any_equal, np.any_less, np.any_less_equal,\n+                   np.any_greater, np.any_greater_equal]:\n+            assert_equal(op.types, np.equal.types)\n+            self.assertEqual(op.nin, 2)\n+            self.assertEqual(op.nout, 1)\n+\n+\n+if __name__ == \"__main__\":\n+    run_module_suite()"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -179,6 +179,8 @@ def test_ignore_object_identity_in_equal(self):\n         # is not a simple boolean, e.g., arrays that are compared elementwise.\n         a = np.array([np.array([1, 2, 3]), None], dtype=object)\n         assert_raises(ValueError, np.equal, a, a)\n+        assert_raises(ValueError, np.all_equal, a, a)\n+        assert_raises(ValueError, np.any_equal, a, a)\n \n         # Check error raised when comparing identical non-comparable objects.\n         class FunkyType(object):\n@@ -187,16 +189,22 @@ def __eq__(self, other):\n \n         a = np.array([FunkyType()])\n         assert_raises(TypeError, np.equal, a, a)\n+        assert_raises(TypeError, np.all_equal, a, a)\n+        assert_raises(TypeError, np.any_equal, a, a)\n \n         # Check identity doesn't override comparison mismatch.\n         a = np.array([np.nan], dtype=object)\n         assert_equal(np.equal(a, a), [False])\n+        assert_equal(np.all_equal(a, a), False)\n+        assert_equal(np.any_equal(a, a), False)\n \n     def test_ignore_object_identity_in_not_equal(self):\n         # Check error raised when comparing identical objects whose comparison\n         # is not a simple boolean, e.g., arrays that are compared elementwise.\n         a = np.array([np.array([1, 2, 3]), None], dtype=object)\n         assert_raises(ValueError, np.not_equal, a, a)\n+        assert_raises(ValueError, np.all_not_equal, a, a)\n+        assert_raises(ValueError, np.any_not_equal, a, a)\n \n         # Check error raised when comparing identical non-comparable objects.\n         class FunkyType(object):\n@@ -205,10 +213,14 @@ def __ne__(self, other):\n \n         a = np.array([FunkyType()])\n         assert_raises(TypeError, np.not_equal, a, a)\n+        assert_raises(TypeError, np.all_not_equal, a, a)\n+        assert_raises(TypeError, np.any_not_equal, a, a)\n \n         # Check identity doesn't override comparison mismatch.\n         a = np.array([np.nan], dtype=object)\n         assert_equal(np.not_equal(a, a), [True])\n+        assert_equal(np.all_not_equal(a, a), True)\n+        assert_equal(np.any_not_equal(a, a), True)\n \n \n class TestDivision(TestCase):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24560,
        "body": "The changed jobs are:\r\n- The `basic` job was de-matrixed (the basic Python 3.9 job served no purpose, duplicate with other jobs) and rename to be specifically for PyPy\r\n- The `debug` job (this was straightforward)\r\n- The `benchmark` job (this one was broken in many ways, and the benchmarks weren't even running before despite the job being green)\r\n\r\nMaking the `benchmark` job run uncovered a ton of warnings and bugs in both our benchmark suite and in `asv`. Everything should work after this PR with `asv` 0.6 (a recent release), some open issues in `asv` are worked around. The fixes were mostly in these categories:\r\n- Avoid using deprecated or removed APIs\r\n- Avoid raising `NotImplementedError` where possible - this is extremely noisy, because `spin bench` will show stderr output (that is a good idea, because it should run cleanly) - and instead split up benchmarks to avoid trying unsupported dtypes/values,\r\n- Remove long double types from the default list of types in `common.py`. They caused a lot of problems, and they're not very interesting dtypes to test that extensively. For the few performance-critical ops, whoever cares can write a dedicated benchmark,\r\n- Reduce the level of parametrization of a number of tests. This is a good idea for at least two reasons: reduce runtime, and keep the CI log output reasonable (it went from >50,000 lines with all the warnings and heavy parametrization to ~8.000 lines) \r\n- `spin bench` improvements:\r\n  - ensure the exit code is always propagated so that when benchmarks fail the CI job fails, and\r\n  - add a `--quick` option to `spin bench` - necessary to make the CI job run in a reasonable amount of time.\r\n\r\n",
        "changed_files": [
            {
                "filename": ".github/workflows/linux.yml",
                "patch": "@@ -64,41 +64,50 @@ jobs:\n         python-version: '3.9'\n     - uses: ./.github/meson_actions\n \n-  basic:\n+  pypy:\n     needs: [smoke_test]\n     runs-on: ubuntu-latest\n     if: github.event_name != 'push'\n-    strategy:\n-      matrix:\n-        python-version: [\"3.9\", \"pypy3.9-v7.3.12\"]\n-    env:\n-      EXPECT_CPU_FEATURES: \"SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL\"\n     steps:\n     - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0\n       with:\n         submodules: recursive\n         fetch-depth: 0\n     - uses: actions/setup-python@61a6322f88396a6271a6ee3565807d608ecaddd1 # v4.7.0\n       with:\n-        python-version: ${{ matrix.python-version }}\n-    - uses: ./.github/actions\n+        python-version: 'pypy3.9-v7.3.12'\n+    - name: Install system dependencies\n+      run: |\n+        sudo apt-get install libopenblas-dev ninja-build\n+    - uses: ./.github/meson_actions\n \n   debug:\n     needs: [smoke_test]\n     runs-on: ubuntu-latest\n     if: github.event_name != 'push'\n-    env:\n-      USE_DEBUG: 1\n     steps:\n     - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0\n       with:\n         submodules: recursive\n         fetch-depth: 0\n-    - uses: actions/setup-python@61a6322f88396a6271a6ee3565807d608ecaddd1 # v4.7.0\n-      with:\n-        python-version: '3.11'\n-\n-    - uses: ./.github/actions\n+    - name: Install debug Python\n+      run: |\n+        sudo apt-get install python3-dbg ninja-build\n+    - name: Build NumPy and install into venv\n+      run: |\n+        python3-dbg -m venv venv\n+        source venv/bin/activate\n+        pip install -U pip\n+        pip install . -v -Csetup-args=-Dbuildtype=debug -Csetup-args=-Dallow-noblas=true\n+    - name: Install test dependencies\n+      run: |\n+        source venv/bin/activate\n+        pip install -r test_requirements.txt\n+    - name: Run test suite\n+      run: |\n+        source venv/bin/activate\n+        cd tools\n+        pytest --pyargs numpy -m \"not slow\"\n \n   full:\n     # Build a wheel, install it, then run the full test suite with code coverage\n@@ -137,14 +146,6 @@ jobs:\n     needs: [smoke_test]\n     runs-on: ubuntu-latest\n     if: github.event_name != 'push'\n-    env:\n-      PYTHONOPTIMIZE: 2\n-      BLAS: None\n-      LAPACK: None\n-      ATLAS: None\n-      NPY_BLAS_ORDER: mkl,blis,openblas,atlas,blas\n-      NPY_LAPACK_ORDER: MKL,OPENBLAS,ATLAS,LAPACK\n-      USE_ASV: 1\n     steps:\n     - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0\n       with:\n@@ -153,7 +154,23 @@ jobs:\n     - uses: actions/setup-python@61a6322f88396a6271a6ee3565807d608ecaddd1 # v4.7.0\n       with:\n         python-version: '3.9'\n-    - uses: ./.github/actions\n+    - name: Install build and benchmarking dependencies\n+      run: |\n+        sudo apt-get install libopenblas-dev ninja-build\n+        pip install spin cython asv virtualenv packaging\n+    - name: Install NumPy\n+      run: |\n+        spin build -- -Dcpu-dispatch=none\n+    # Ensure to keep the below steps as single-line bash commands (it's a\n+    # workaround for asv#1333, and it may have side-effects on multi-line commands)\n+    - name: Appease asv's need for machine info\n+      shell: 'script -q -e -c \"bash --noprofile --norc -eo pipefail {0}\"'\n+      run: |\n+        asv machine --yes --config benchmarks/asv.conf.json\n+    - name: Run benchmarks\n+      shell: 'script -q -e -c \"bash --noprofile --norc -eo pipefail {0}\"'\n+      run: |\n+        spin bench --quick\n \n   relaxed_strides_debug:\n     needs: [smoke_test]"
            },
            {
                "filename": ".spin/cmds.py",
                "patch": "@@ -303,19 +303,7 @@ def _run_asv(cmd):\n     except (ImportError, RuntimeError):\n         pass\n \n-    try:\n-        util.run(cmd, cwd='benchmarks', env=env, sys_exit=False)\n-    except FileNotFoundError:\n-        click.secho((\n-            \"Cannot find `asv`. \"\n-            \"Please install Airspeed Velocity:\\n\\n\"\n-            \"  https://asv.readthedocs.io/en/latest/installing.html\\n\"\n-            \"\\n\"\n-            \"Depending on your system, one of the following should work:\\n\\n\"\n-            \"  pip install asv\\n\"\n-            \"  conda install asv\\n\"\n-        ), fg=\"red\")\n-        sys.exit(1)\n+    util.run(cmd, cwd='benchmarks', env=env)\n \n \n @click.command()\n@@ -336,13 +324,17 @@ def _run_asv(cmd):\n @click.option(\n     '--verbose', '-v', is_flag=True, default=False\n )\n+@click.option(\n+    '--quick', '-q', is_flag=True, default=False,\n+    help=\"Run each benchmark only once (timings won't be accurate)\"\n+)\n @click.argument(\n     'commits', metavar='',\n     required=False,\n     nargs=-1\n )\n @click.pass_context\n-def bench(ctx, tests, compare, verbose, commits):\n+def bench(ctx, tests, compare, verbose, quick, commits):\n     \"\"\"\ud83c\udfcb Run benchmarks.\n \n     \\b\n@@ -382,6 +374,9 @@ def bench(ctx, tests, compare, verbose, commits):\n     if verbose:\n         bench_args = ['-v'] + bench_args\n \n+    if quick:\n+        bench_args = ['--quick'] + bench_args\n+\n     if not compare:\n         # No comparison requested; we build and benchmark the current version\n \n@@ -409,27 +404,21 @@ def bench(ctx, tests, compare, verbose, commits):\n         cmd = [\n             'asv', 'run', '--dry-run', '--show-stderr', '--python=same'\n         ] + bench_args\n-\n         _run_asv(cmd)\n-\n     else:\n-        # Benchmark comparison\n-\n         # Ensure that we don't have uncommited changes\n         commit_a, commit_b = [_commit_to_sha(c) for c in commits]\n \n-        if commit_b == 'HEAD':\n-            if _dirty_git_working_dir():\n-                click.secho(\n-                    \"WARNING: you have uncommitted changes --- \"\n-                    \"these will NOT be benchmarked!\",\n-                    fg=\"red\"\n-                )\n+        if commit_b == 'HEAD' and _dirty_git_working_dir():\n+            click.secho(\n+                \"WARNING: you have uncommitted changes --- \"\n+                \"these will NOT be benchmarked!\",\n+                fg=\"red\"\n+            )\n \n         cmd_compare = [\n             'asv', 'continuous', '--factor', '1.05',\n         ] + bench_args + [commit_a, commit_b]\n-\n         _run_asv(cmd_compare)\n \n "
            },
            {
                "filename": "benchmarks/benchmarks/bench_core.py",
                "patch": "@@ -217,19 +217,13 @@ def time_indices(self):\n \n \n class StatsMethods(Benchmark):\n-    # Not testing, but in array_api (redundant)\n-    # 8, 16, 32 bit variants, and 128 complexes\n-    params = [['int64', 'uint64', 'float64', 'intp',\n-               'complex64', 'bool', 'float', 'int',\n-               'complex', 'complex256'],\n-              [100**n for n in range(0, 2)]]\n+    params = [['int64', 'uint64', 'float32', 'float64',\n+               'complex64', 'bool_'],\n+              [100, 10000]]\n     param_names = ['dtype', 'size']\n \n     def setup(self, dtype, size):\n-        try:\n-            self.data = np.ones(size, dtype=getattr(np, dtype))\n-        except AttributeError:  # builtins throw AttributeError after 1.20\n-            self.data = np.ones(size, dtype=dtype)\n+        self.data = np.ones(size, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = np.random.randn(size) + 1j * np.random.randn(size)\n "
            },
            {
                "filename": "benchmarks/benchmarks/bench_creation.py",
                "patch": "@@ -1,4 +1,4 @@\n-from .common import Benchmark, TYPES1\n+from .common import Benchmark, TYPES1, get_squares_\n \n import numpy as np\n \n@@ -23,57 +23,49 @@ def time_meshgrid(self, size, ndims, ind, ndtype):\n class Create(Benchmark):\n     \"\"\" Benchmark for creation functions\n     \"\"\"\n-    # (64, 64), (128, 128), (256, 256)\n-    # , (512, 512), (1024, 1024)\n-    params = [[16, 32, 128, 256, 512,\n-               (16, 16), (32, 32)],\n-              ['C', 'F'],\n+    params = [[16, 512, (32, 32)],\n               TYPES1]\n-    param_names = ['shape', 'order', 'npdtypes']\n+    param_names = ['shape', 'npdtypes']\n     timeout = 10\n \n-    def setup(self, shape, order, npdtypes):\n+    def setup(self, shape, npdtypes):\n         values = get_squares_()\n         self.xarg = values.get(npdtypes)[0]\n \n-    def time_full(self, shape, order, npdtypes):\n-        np.full(shape, self.xarg[1], dtype=npdtypes, order=order)\n+    def time_full(self, shape, npdtypes):\n+        np.full(shape, self.xarg[1], dtype=npdtypes)\n \n-    def time_full_like(self, shape, order, npdtypes):\n-        np.full_like(self.xarg, self.xarg[0], order=order)\n+    def time_full_like(self, shape, npdtypes):\n+        np.full_like(self.xarg, self.xarg[0])\n \n-    def time_ones(self, shape, order, npdtypes):\n-        np.ones(shape, dtype=npdtypes, order=order)\n+    def time_ones(self, shape, npdtypes):\n+        np.ones(shape, dtype=npdtypes)\n \n-    def time_ones_like(self, shape, order, npdtypes):\n-        np.ones_like(self.xarg, order=order)\n+    def time_ones_like(self, shape, npdtypes):\n+        np.ones_like(self.xarg)\n \n-    def time_zeros(self, shape, order, npdtypes):\n-        np.zeros(shape, dtype=npdtypes, order=order)\n+    def time_zeros(self, shape, npdtypes):\n+        np.zeros(shape, dtype=npdtypes)\n \n-    def time_zeros_like(self, shape, order, npdtypes):\n-        np.zeros_like(self.xarg, order=order)\n+    def time_zeros_like(self, shape, npdtypes):\n+        np.zeros_like(self.xarg)\n \n-    def time_empty(self, shape, order, npdtypes):\n-        np.empty(shape, dtype=npdtypes, order=order)\n+    def time_empty(self, shape, npdtypes):\n+        np.empty(shape, dtype=npdtypes)\n \n-    def time_empty_like(self, shape, order, npdtypes):\n-        np.empty_like(self.xarg, order=order)\n+    def time_empty_like(self, shape, npdtypes):\n+        np.empty_like(self.xarg)\n \n \n class UfuncsFromDLP(Benchmark):\n     \"\"\" Benchmark for creation functions\n     \"\"\"\n-    params = [[16, 32, (16, 16),\n-               (32, 32), (64, 64)],\n+    params = [[16, 32, (16, 16), (64, 64)],\n               TYPES1]\n     param_names = ['shape', 'npdtypes']\n     timeout = 10\n \n     def setup(self, shape, npdtypes):\n-        if npdtypes in ['longdouble', 'clongdouble']:\n-            raise NotImplementedError(\n-                'Only IEEE dtypes are supported')\n         values = get_squares_()\n         self.xarg = values.get(npdtypes)[0]\n "
            },
            {
                "filename": "benchmarks/benchmarks/bench_itemselection.py",
                "patch": "@@ -5,7 +5,7 @@\n \n class Take(Benchmark):\n     params = [\n-        [(1000, 1), (1000, 2), (2, 1000, 1), (1000, 3)],\n+        [(1000, 1), (2, 1000, 1), (1000, 3)],\n         [\"raise\", \"wrap\", \"clip\"],\n         TYPES1 + [\"O\", \"i,O\"]]\n     param_names = [\"shape\", \"mode\", \"dtype\"]"
            },
            {
                "filename": "benchmarks/benchmarks/bench_linalg.py",
                "patch": "@@ -72,37 +72,39 @@ def time_tensordot_a_b_axes_1_0_0_1(self):\n \n \n class Linalg(Benchmark):\n-    params = [['svd', 'pinv', 'det', 'norm'],\n-              TYPES1]\n-    param_names = ['op', 'type']\n+    params = set(TYPES1) - set(['float16'])\n+    param_names = ['dtype']\n \n-    def setup(self, op, typename):\n+    def setup(self, typename):\n         np.seterr(all='ignore')\n+        self.a = get_squares_()[typename]\n+\n+    def time_svd(self, typename):\n+        np.linalg.svd(self.a)\n+\n+    def time_pinv(self, typename):\n+        np.linalg.pinv(self.a)\n \n-        self.func = getattr(np.linalg, op)\n+    def time_det(self, typename):\n+        np.linalg.det(self.a)\n \n-        if op == 'cholesky':\n-            # we need a positive definite\n-            self.a = np.dot(get_squares_()[typename],\n-                            get_squares_()[typename].T)\n-        else:\n-            self.a = get_squares_()[typename]\n \n-        # check that dtype is supported at all\n-        try:\n-            self.func(self.a[:2, :2])\n-        except TypeError as e:\n-            raise NotImplementedError() from e\n+class LinalgNorm(Benchmark):\n+    params = TYPES1\n+    param_names = ['dtype']\n+\n+    def setup(self, typename):\n+        self.a = get_squares_()[typename]\n \n-    def time_op(self, op, typename):\n-        self.func(self.a)\n+    def time_norm(self, typename):\n+        np.linalg.norm(self.a)\n \n \n class LinalgSmallArrays(Benchmark):\n     \"\"\" Test overhead of linalg methods for small arrays \"\"\"\n     def setup(self):\n         self.array_5 = np.arange(5.)\n-        self.array_5_5 = np.reshape(np.arange(25.), (5., 5.))\n+        self.array_5_5 = np.reshape(np.arange(25.), (5, 5))\n \n     def time_norm_small_array(self):\n         np.linalg.norm(self.array_5)"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ma.py",
                "patch": "@@ -128,10 +128,10 @@ class MAFunctions1v(Benchmark):\n               ['small', 'big']]\n \n     def setup(self, mtype, func, msize):\n-        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        xs = 2.0 + np.random.uniform(-1, 1, 6).reshape(2, 3)\n         m1 = [[True, False, False], [False, False, True]]\n-        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n-        maskx = xl > 0.8\n+        xl = 2.0 + np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 2.8\n         self.nmxs = np.ma.array(xs, mask=m1)\n         self.nmxl = np.ma.array(xl, mask=maskx)\n \n@@ -173,17 +173,17 @@ class MAFunctions2v(Benchmark):\n \n     def setup(self, mtype, func, msize):\n         # Small arrays\n-        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n-        ys = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        xs = 2.0 + np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        ys = 2.0 + np.random.uniform(-1, 1, 6).reshape(2, 3)\n         m1 = [[True, False, False], [False, False, True]]\n         m2 = [[True, False, True], [False, False, True]]\n         self.nmxs = np.ma.array(xs, mask=m1)\n         self.nmys = np.ma.array(ys, mask=m2)\n         # Big arrays\n-        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n-        yl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n-        maskx = xl > 0.8\n-        masky = yl < -0.8\n+        xl = 2.0 + np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        yl = 2.0 + np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 2.8\n+        masky = yl < 1.8\n         self.nmxl = np.ma.array(xl, mask=maskx)\n         self.nmyl = np.ma.array(yl, mask=masky)\n "
            },
            {
                "filename": "benchmarks/benchmarks/bench_manipulate.py",
                "patch": "@@ -4,9 +4,7 @@\n from collections import deque\n \n class BroadcastArrays(Benchmark):\n-    params = [[(16, 32), (32, 64),\n-               (64, 128), (128, 256),\n-               (256, 512), (512, 1024)],\n+    params = [[(16, 32), (128, 256), (512, 1024)],\n               TYPES1]\n     param_names = ['shape', 'ndtype']\n     timeout = 10\n@@ -22,7 +20,7 @@ def time_broadcast_arrays(self, shape, ndtype):\n \n \n class BroadcastArraysTo(Benchmark):\n-    params = [[16, 32, 64, 128, 256, 512],\n+    params = [[16, 64, 512],\n               TYPES1]\n     param_names = ['size', 'ndtype']\n     timeout = 10\n@@ -39,9 +37,8 @@ def time_broadcast_to(self, size, ndtype):\n \n \n class ConcatenateStackArrays(Benchmark):\n-    # (64, 128), (128, 256), (256, 512)\n     params = [[(16, 32), (32, 64)],\n-              [2, 3, 4, 5],\n+              [2, 5],\n               TYPES1]\n     param_names = ['shape', 'narrays', 'ndtype']\n     timeout = 10"
            },
            {
                "filename": "benchmarks/benchmarks/bench_reduce.py",
                "patch": "@@ -46,18 +46,11 @@ def time_any_slow(self):\n \n \n class StatsReductions(Benchmark):\n-    # Not testing, but in array_api (redundant)\n-    # 8, 16, 32 bit variants, and 128 complexes\n-    params = ['int64', 'uint64', 'float64', 'intp',\n-               'complex64', 'bool', 'float', 'int',\n-               'complex', 'complex256'],\n+    params = ['int64', 'uint64', 'float32', 'float64', 'complex64', 'bool_'],\n     param_names = ['dtype']\n \n     def setup(self, dtype):\n-        try:\n-            self.data = np.ones(200, dtype=getattr(np, dtype))\n-        except AttributeError:  # builtins throw AttributeError after 1.20\n-            self.data = np.ones(200, dtype=dtype)\n+        self.data = np.ones(200, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = self.data * self.data.T*1j\n "
            },
            {
                "filename": "benchmarks/benchmarks/bench_shape_base.py",
                "patch": "@@ -68,7 +68,7 @@ def time_no_lists(self, n):\n \n \n class Block2D(Benchmark):\n-    params = [[(16, 16), (32, 32), (64, 64), (128, 128), (256, 256), (512, 512), (1024, 1024)],\n+    params = [[(16, 16), (64, 64), (256, 256), (1024, 1024)],\n               ['uint8', 'uint16', 'uint32', 'uint64'],\n               [(2, 2), (4, 4)]]\n     param_names = ['shape', 'dtype', 'n_chunks']"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -135,47 +135,98 @@ def time_ndarray_meth(self, methname, npdtypes):\n         getattr(operator, methname)(*[self.vals, 2])\n \n \n-class Methods0D(Benchmark):\n+class Methods0DBoolComplex(Benchmark):\n     \"\"\"Zero dimension array methods\n     \"\"\"\n-    params = [['__bool__', '__complex__', '__invert__',\n-               '__float__', '__int__'], TYPES1]\n+    params = [['__bool__', '__complex__'],\n+              TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        self.xarg = np.array(3, dtype=npdtypes)\n+\n+    def time_ndarray__0d__(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        meth()\n+\n+\n+class Methods0DFloatInt(Benchmark):\n+    \"\"\"Zero dimension array methods\n+    \"\"\"\n+    params = [['__int__', '__float__'],\n+              [dt for dt in TYPES1 if not dt.startswith('complex')]]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n         self.xarg = np.array(3, dtype=npdtypes)\n-        if (npdtypes.startswith('complex') and\n-           methname in ['__float__', '__int__']) or \\\n-           (npdtypes.startswith('int') and methname == '__invert__'):\n-            # Skip\n-            raise NotImplementedError\n \n     def time_ndarray__0d__(self, methname, npdtypes):\n         meth = getattr(self.xarg, methname)\n         meth()\n \n \n+class Methods0DInvert(Benchmark):\n+    \"\"\"Zero dimension array methods\n+    \"\"\"\n+    params = ['int16', 'int32', 'int64']\n+    param_names = ['npdtypes']\n+    timeout = 10\n+\n+    def setup(self, npdtypes):\n+        self.xarg = np.array(3, dtype=npdtypes)\n+\n+    def time_ndarray__0d__(self, npdtypes):\n+        self.xarg.__invert__()\n+\n+\n class MethodsV1(Benchmark):\n     \"\"\" Benchmark for the methods which take an argument\n     \"\"\"\n-    params = [['__and__', '__add__', '__eq__', '__floordiv__', '__ge__',\n-               '__gt__', '__le__', '__lt__', '__matmul__',\n-               '__mod__', '__mul__', '__ne__', '__or__',\n-               '__pow__', '__sub__', '__truediv__', '__xor__'],\n+    params = [['__add__', '__eq__', '__ge__', '__gt__', '__le__',\n+               '__lt__', '__matmul__', '__mul__', '__ne__',\n+               '__pow__', '__sub__', '__truediv__'],\n               TYPES1]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n-        if (\n-            npdtypes.startswith(\"complex\")\n-                and methname in [\"__floordiv__\", \"__mod__\"]\n-        ) or (\n-            not npdtypes.startswith(\"int\")\n-            and methname in [\"__and__\", \"__or__\", \"__xor__\"]\n-        ):\n-            raise NotImplementedError  # skip\n+        values = get_squares_().get(npdtypes)\n+        self.xargs = [values[0], values[1]]\n+        if np.issubdtype(npdtypes, np.inexact):\n+            # avoid overflow in __pow__/__matmul__ for low-precision dtypes\n+            self.xargs[1] *= 0.01\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(*self.xargs)\n+\n+\n+class MethodsV1IntOnly(Benchmark):\n+    \"\"\" Benchmark for the methods which take an argument\n+    \"\"\"\n+    params = [['__and__', '__or__', '__xor__'],\n+              ['int16', 'int32', 'int64']]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_().get(npdtypes)\n+        self.xargs = [values[0], values[1]]\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(*self.xargs)\n+\n+\n+class MethodsV1NoComplex(Benchmark):\n+    \"\"\" Benchmark for the methods which take an argument\n+    \"\"\"\n+    params = [['__floordiv__', '__mod__'],\n+              [dt for dt in TYPES1 if not dt.startswith('complex')]]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n         values = get_squares_().get(npdtypes)\n         self.xargs = [values[0], values[1]]\n \n@@ -391,25 +442,36 @@ def time_less_than_scalar2(self, dtype):\n \n \n class CustomScalarFloorDivideInt(Benchmark):\n-    params = (np.core.sctypes['int'] + \n-              np.core.sctypes['uint'], [8, -8, 43, -43])\n+    params = (np.core.sctypes['int'],\n+              [8, -8, 43, -43])\n     param_names = ['dtype', 'divisors']\n \n     def setup(self, dtype, divisor):\n-        if dtype in np.core.sctypes['uint'] and divisor < 0:\n-            raise NotImplementedError(\n-                    \"Skipping test for negative divisor with unsigned type\")\n-\n         iinfo = np.iinfo(dtype)\n         self.x = np.random.randint(\n                     iinfo.min, iinfo.max, size=10000, dtype=dtype)\n \n     def time_floor_divide_int(self, dtype, divisor):\n         self.x // divisor\n \n+\n+class CustomScalarFloorDivideUInt(Benchmark):\n+    params = (np.core.sctypes['uint'],\n+              [8, 43])\n+    param_names = ['dtype', 'divisors']\n+\n+    def setup(self, dtype, divisor):\n+        iinfo = np.iinfo(dtype)\n+        self.x = np.random.randint(\n+                    iinfo.min, iinfo.max, size=10000, dtype=dtype)\n+\n+    def time_floor_divide_uint(self, dtype, divisor):\n+        self.x // divisor\n+\n+\n class CustomArrayFloorDivideInt(Benchmark):\n-    params = (np.core.sctypes['int'] + \n-              np.core.sctypes['uint'], [100, 10000, 1000000])\n+    params = (np.core.sctypes['int'] + np.core.sctypes['uint'],\n+              [100, 10000, 1000000])\n     param_names = ['dtype', 'size']\n \n     def setup(self, dtype, size):"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc_strides.py",
                "patch": "@@ -100,7 +100,10 @@ def time_unary(self, ufunc, stride_in, stride_out, dtype):\n         ufunc(*self.ufunc_args)\n \n class UnaryFP(_AbstractUnary):\n-    params = [UFUNCS_UNARY, [1, 2, 4], [1, 2, 4], ['e', 'f', 'd']]\n+    params = [[uf for uf in UFUNCS_UNARY if uf != np.invert],\n+              [1, 4],\n+              [1, 2],\n+              ['e', 'f', 'd']]\n \n     def setup(self, ufunc, stride_in, stride_out, dtype):\n         _AbstractUnary.setup(self, ufunc, stride_in, stride_out, dtype)\n@@ -115,7 +118,7 @@ class UnaryFPSpecial(UnaryFP):\n class BinaryFP(_AbstractBinary):\n     params = [\n         [np.maximum, np.minimum, np.fmax, np.fmin, np.ldexp],\n-        [1, 2, 4], [1, 2, 4], [1, 2, 4], ['f', 'd']\n+        [1, 2], [1, 4], [1, 2, 4], ['f', 'd']\n     ]\n \n class BinaryFPSpecial(BinaryFP):"
            },
            {
                "filename": "benchmarks/benchmarks/common.py",
                "patch": "@@ -22,10 +22,8 @@\n     'int16', 'float16',\n     'int32', 'float32',\n     'int64', 'float64',  'complex64',\n-    'longdouble', 'complex128',\n+    'complex128',\n ]\n-if 'complex256' in np.core.sctypeDict:\n-    TYPES1.append('clongdouble')\n \n DLPACK_TYPES = [\n     'int16', 'float16',"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24479,
        "body": "This patch implements cross-compile builds for armhf, ppc64le, and IBMZ architectures in the CI pipeline.\r\nIn this setup, QEMU manages the Python interpreter, meson, and runtime tests, while ninja,\r\nthe toolchain, and any binutils binaries are executed natively to speed up the build.\r\nWhile it might not be highly efficient due to qemu's quirks and slower performance,\r\nit still does extend testing to include multiarray, umath, ufunc, and simd operations.\r\n    \r\n",
        "changed_files": [
            {
                "filename": ".github/workflows/linux_qemu.yml",
                "patch": "@@ -0,0 +1,147 @@\n+# Meson's Python module doesn't support crosscompiling,\n+# and python dependencies may be another potential hurdle.\n+# There might also be a need to run runtime tests during configure time.\n+#\n+# The recommended practice is to rely on Docker to provide the x86_64 crosscompile toolchain,\n+# enabling native execution via binfmt.\n+#\n+# In simpler terms, everything except the crosscompile toolchain will be emulated.\n+\n+name: Linux Qemu tests\n+\n+on:\n+  pull_request:\n+    branches:\n+      - main\n+      - maintenance/**\n+\n+defaults:\n+  run:\n+    shell: bash\n+\n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n+  cancel-in-progress: true\n+\n+jobs:\n+  linux_qemu:\n+    if: \"github.repository == 'numpy/numpy'\"\n+    runs-on: ubuntu-22.04\n+    continue-on-error: true\n+    strategy:\n+      matrix:\n+        BUILD_PROP:\n+          - [\n+              \"armhf\",\n+              \"arm-linux-gnueabihf\",\n+              \"arm32v7/ubuntu:22.04\",\n+              \"-Dallow-noblas=true\",\n+              # test_unary_spurious_fpexception is currently skipped\n+              # FIXME(@seiko2plus): Requires confirmation for the following issue:\n+              # The presence of an FP invalid exception caused by sqrt. Unsure if this is a qemu bug or not.\n+              \"(test_kind or test_multiarray or test_simd or test_umath or test_ufunc) and not test_unary_spurious_fpexception\"\n+          ]\n+          - [\n+              \"ppc64le\",\n+              \"powerpc64le-linux-gnu\",\n+              \"ppc64le/ubuntu:22.04\",\n+              \"-Dallow-noblas=true\",\n+              \"test_kind or test_multiarray or test_simd or test_umath or test_ufunc\",\n+          ]\n+          - [\n+              \"s390x\",\n+              \"s390x-linux-gnu\",\n+              \"s390x/ubuntu:22.04\",\n+              \"-Dallow-noblas=true\",\n+              # Skipping TestRationalFunctions.test_gcd_overflow test\n+              # because of a possible qemu bug that appears to be related to int64 overflow in absolute operation.\n+              # TODO(@seiko2plus): Confirm the bug and provide a minimal reproducer, then report it to upstream.\n+              \"(test_kind or test_multiarray or test_simd or test_umath or test_ufunc) and not test_gcd_overflow\"\n+          ]\n+          - [\n+              \"s390x - baseline(Z13)\",\n+              \"s390x-linux-gnu\",\n+              \"s390x/ubuntu:22.04\",\n+              \"-Dallow-noblas=true -Dcpu-baseline=vx\",\n+              \"(test_kind or test_multiarray or test_simd or test_umath or test_ufunc) and not test_gcd_overflow\"\n+          ]\n+    env:\n+      TOOLCHAIN_NAME: ${{ matrix.BUILD_PROP[1] }}\n+      DOCKER_CONTAINER: ${{ matrix.BUILD_PROP[2] }}\n+      MESON_OPTIONS: ${{ matrix.BUILD_PROP[3] }}\n+      RUNTIME_TEST_FILTER: ${{ matrix.BUILD_PROP[4] }}\n+      TERM: xterm-256color\n+\n+    name: \"${{ matrix.BUILD_PROP[0] }}\"\n+    steps:\n+    - uses: actions/checkout@c85c95e3d7251135ab7dc9ce3241c5835cc595a9 # v3.5.3\n+      with:\n+        submodules: recursive\n+        fetch-depth: 0\n+\n+    - name: Initialize binfmt_misc for qemu-user-static\n+      run: |\n+        docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n+\n+    - name: Install GCC cross-compilers\n+      run: |\n+        sudo apt update\n+        sudo apt install -y ninja-build gcc-${TOOLCHAIN_NAME} g++-${TOOLCHAIN_NAME} gfortran-${TOOLCHAIN_NAME}\n+\n+    - name: Cache docker container\n+      uses: actions/cache@v3\n+      id: container-cache\n+      with:\n+        path: ~/docker_${{ matrix.BUILD_PROP[1] }}\n+        key: container-${{ runner.os }}-${{ matrix.BUILD_PROP[1] }}-${{ matrix.BUILD_PROP[2] }}-${{ hashFiles('build_requirements.txt') }}\n+\n+    - name: Creates new container\n+      if: steps.container-cache.outputs.cache-hit != 'true'\n+      run: |\n+        docker run --name the_container --interactive -v /:/host -v $(pwd):/numpy ${DOCKER_CONTAINER} /bin/bash -c \"\n+          apt update &&\n+          apt install -y cmake git python3 python-is-python3 python3-dev python3-pip &&\n+          mkdir -p /lib64 && ln -s /host/lib64/ld-* /lib64/ &&\n+          ln -s /host/lib/x86_64-linux-gnu /lib/x86_64-linux-gnu &&\n+          rm -rf /usr/${TOOLCHAIN_NAME} && ln -s /host/usr/${TOOLCHAIN_NAME} /usr/${TOOLCHAIN_NAME} &&\n+          rm -rf /usr/lib/gcc/${TOOLCHAIN_NAME} && ln -s /host/usr/lib/gcc-cross/${TOOLCHAIN_NAME} /usr/lib/gcc/${TOOLCHAIN_NAME} &&\n+          rm -f /usr/bin/gcc && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-gcc /usr/bin/gcc &&\n+          rm -f /usr/bin/g++ && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-g++ /usr/bin/g++ &&\n+          rm -f /usr/bin/gfortran && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-gfortran /usr/bin/gfortran &&\n+          rm -f /usr/bin/ar && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-ar /usr/bin/ar &&\n+          rm -f /usr/bin/as && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-as /usr/bin/as &&\n+          rm -f /usr/bin/ld && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-ld /usr/bin/ld &&\n+          rm -f /usr/bin/ld.bfd && ln -s /host/usr/bin/${TOOLCHAIN_NAME}-ld.bfd /usr/bin/ld.bfd &&\n+          rm -f /usr/bin/ninja && ln -s /host/usr/bin/ninja /usr/bin/ninja &&\n+          git config --global --add safe.directory /numpy &&\n+          python -m pip install -r /numpy/build_requirements.txt &&\n+          python -m pip install pytest pytest-xdist hypothesis typing_extensions &&\n+          rm -f /usr/local/bin/ninja && mkdir -p /usr/local/bin && ln -s /host/usr/bin/ninja /usr/local/bin/ninja\n+        \"\n+        docker commit the_container the_container\n+        mkdir -p \"~/docker_${TOOLCHAIN_NAME}\"\n+        docker save -o \"~/docker_${TOOLCHAIN_NAME}/the_container.tar\" the_container\n+\n+    - name: Load container from cache\n+      if: steps.container-cache.outputs.cache-hit == 'true'\n+      run: docker load -i \"~/docker_${TOOLCHAIN_NAME}/the_container.tar\"\n+\n+    - name: Meson Build\n+      run: |\n+        docker run --rm -e \"TERM=xterm-256color\" -v $(pwd):/numpy -v /:/host the_container \\\n+        /bin/script -e -q -c \"/bin/bash --noprofile --norc -eo pipefail -c '\n+          cd /numpy && spin build --clean -- ${MESON_OPTIONS}\n+        '\"\n+\n+    - name: Meson Log\n+      if: always()\n+      run: 'cat build/meson-logs/meson-log.txt'\n+\n+    - name: Run Tests\n+      run: |\n+        docker run --rm -e \"TERM=xterm-256color\" -v $(pwd):/numpy -v /:/host the_container \\\n+        /bin/script -e -q -c \"/bin/bash --noprofile --norc -eo pipefail -c '\n+          export F90=/usr/bin/gfortran\n+          cd /numpy && spin test -- -k \\\"${RUNTIME_TEST_FILTER}\\\"\n+        '\"\n+"
            },
            {
                "filename": "numpy/core/tests/test_simd_module.py",
                "patch": "@@ -86,6 +86,8 @@ def test_signed_overflow(self, sfx):\n         assert lanes == [0] * nlanes\n \n     def test_truncate_f32(self):\n+        if not npyv.simd_f32:\n+            pytest.skip(\"F32 isn't support by the SIMD extension\")\n         f32 = npyv.setall_f32(0.1)[0]\n         assert f32 != 0.1\n         assert round(f32, 1) == 0.1"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 24176,
        "body": "Fixes #23486.\r\n\r\nUpdates all the code paths used by the flatiter getter and setter to use single-element casts instead of copyswap. This allows new dtypes to use these code paths without seg faulting.\r\n\r\nI ran the flatiter benchmarks and didn't see any significant performance changes, but there seem to only be benchmarks for boolean indexing. I could probably substantially improve performance by only using copyswap in situations where it's required but using a hand-unrolled `memcpy` like `fasttake` otherwise but only want to fix the seg faults for now.",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/iterators.c",
                "patch": "@@ -19,6 +19,8 @@\n #include \"conversion_utils.h\"\n #include \"array_coercion.h\"\n #include \"item_selection.h\"\n+#include \"lowlevel_strided_loops.h\"\n+#include \"array_assign.h\"\n \n #define NEWAXIS_INDEX -1\n #define ELLIPSIS_INDEX -2\n@@ -416,16 +418,14 @@ iter_length(PyArrayIterObject *self)\n \n \n static PyArrayObject *\n-iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind)\n+iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind,\n+                    NPY_cast_info *cast_info)\n {\n     npy_intp counter, strides;\n     int itemsize;\n     npy_intp count = 0;\n     char *dptr, *optr;\n     PyArrayObject *ret;\n-    int swap;\n-    PyArray_CopySwapFunc *copyswap;\n-\n \n     if (PyArray_NDIM(ind) != 1) {\n         PyErr_SetString(PyExc_ValueError,\n@@ -444,9 +444,10 @@ iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind)\n     count = count_boolean_trues(PyArray_NDIM(ind), PyArray_DATA(ind),\n                                 PyArray_DIMS(ind), PyArray_STRIDES(ind));\n     itemsize = PyArray_DESCR(self->ao)->elsize;\n-    Py_INCREF(PyArray_DESCR(self->ao));\n+    PyArray_Descr *dtype = PyArray_DESCR(self->ao);\n+    Py_INCREF(dtype);\n     ret = (PyArrayObject *)PyArray_NewFromDescr(Py_TYPE(self->ao),\n-                             PyArray_DESCR(self->ao), 1, &count,\n+                             dtype, 1, &count,\n                              NULL, NULL,\n                              0, (PyObject *)self->ao);\n     if (ret == NULL) {\n@@ -457,12 +458,15 @@ iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind)\n         optr = PyArray_DATA(ret);\n         counter = PyArray_DIMS(ind)[0];\n         dptr = PyArray_DATA(ind);\n-        copyswap = PyArray_DESCR(self->ao)->f->copyswap;\n-        /* Loop over Boolean array */\n-        swap = (PyArray_ISNOTSWAPPED(self->ao) != PyArray_ISNOTSWAPPED(ret));\n+        npy_intp one = 1;\n         while (counter--) {\n             if (*((npy_bool *)dptr) != 0) {\n-                copyswap(optr, self->dataptr, swap, self->ao);\n+                char *args[2] = {self->dataptr, optr};\n+                npy_intp transfer_strides[2] = {itemsize, itemsize};\n+                if (cast_info->func(&cast_info->context, args, &one,\n+                                    transfer_strides, cast_info->auxdata) < 0) {\n+                    return NULL;\n+                }\n                 optr += itemsize;\n             }\n             dptr += strides;\n@@ -474,18 +478,16 @@ iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind)\n }\n \n static PyObject *\n-iter_subscript_int(PyArrayIterObject *self, PyArrayObject *ind)\n+iter_subscript_int(PyArrayIterObject *self, PyArrayObject *ind,\n+                   NPY_cast_info *cast_info)\n {\n     npy_intp num;\n     PyArrayObject *ret;\n     PyArrayIterObject *ind_it;\n     int itemsize;\n-    int swap;\n     char *optr;\n     npy_intp counter;\n-    PyArray_CopySwapFunc *copyswap;\n \n-    itemsize = PyArray_DESCR(self->ao)->elsize;\n     if (PyArray_NDIM(ind) == 0) {\n         num = *((npy_intp *)PyArray_DATA(ind));\n         if (check_and_adjust_index(&num, self->size, -1, NULL) < 0) {\n@@ -501,9 +503,10 @@ iter_subscript_int(PyArrayIterObject *self, PyArrayObject *ind)\n         }\n     }\n \n-    Py_INCREF(PyArray_DESCR(self->ao));\n+    PyArray_Descr *dtype = PyArray_DESCR(self->ao);\n+    Py_INCREF(dtype);\n     ret = (PyArrayObject *)PyArray_NewFromDescr(Py_TYPE(self->ao),\n-                             PyArray_DESCR(self->ao),\n+                             dtype,\n                              PyArray_NDIM(ind),\n                              PyArray_DIMS(ind),\n                              NULL, NULL,\n@@ -517,22 +520,29 @@ iter_subscript_int(PyArrayIterObject *self, PyArrayObject *ind)\n         Py_DECREF(ret);\n         return NULL;\n     }\n+\n+    npy_intp one = 1;\n+    itemsize = dtype->elsize;\n     counter = ind_it->size;\n-    copyswap = PyArray_DESCR(ret)->f->copyswap;\n-    swap = (PyArray_ISNOTSWAPPED(ret) != PyArray_ISNOTSWAPPED(self->ao));\n     while (counter--) {\n         num = *((npy_intp *)(ind_it->dataptr));\n         if (check_and_adjust_index(&num, self->size, -1, NULL) < 0) {\n-            Py_DECREF(ind_it);\n-            Py_DECREF(ret);\n-            PyArray_ITER_RESET(self);\n-            return NULL;\n+            Py_CLEAR(ret);\n+            goto finish;\n         }\n         PyArray_ITER_GOTO1D(self, num);\n-        copyswap(optr, self->dataptr, swap, ret);\n+        char *args[2] = {self->dataptr, optr};\n+        npy_intp transfer_strides[2] = {itemsize, itemsize};\n+        if (cast_info->func(&cast_info->context, args, &one,\n+                            transfer_strides, cast_info->auxdata) < 0) {\n+            Py_CLEAR(ret);\n+            goto finish;\n+        }\n         optr += itemsize;\n         PyArray_ITER_NEXT(ind_it);\n     }\n+\n+ finish:\n     Py_DECREF(ind_it);\n     PyArray_ITER_RESET(self);\n     return (PyObject *)ret;\n@@ -551,7 +561,7 @@ iter_subscript(PyArrayIterObject *self, PyObject *ind)\n     int size;\n     PyObject *obj = NULL;\n     PyObject *new;\n-    PyArray_CopySwapFunc *copyswap;\n+    NPY_cast_info cast_info = {.func = NULL};\n \n     if (ind == Py_Ellipsis) {\n         ind = PySlice_New(NULL, NULL, NULL);\n@@ -596,6 +606,21 @@ iter_subscript(PyArrayIterObject *self, PyObject *ind)\n         }\n     }\n \n+    dtype = PyArray_DESCR(self->ao);\n+    size = dtype->elsize;\n+\n+    /* set up a cast to handle item copying */\n+\n+    NPY_ARRAYMETHOD_FLAGS transfer_flags = 0;\n+    npy_intp one = 1;\n+    /* We can assume the newly allocated output array is aligned */\n+    int is_aligned = IsUintAligned(self->ao);\n+    if (PyArray_GetDTypeTransferFunction(\n+                is_aligned, size, size, dtype, dtype, 0, &cast_info,\n+                &transfer_flags) < 0) {\n+        goto fail;\n+    }\n+\n     /* Check for Integer or Slice */\n     if (PyLong_Check(ind) || PySlice_Check(ind)) {\n         start = parse_index_entry(ind, &step_size, &n_steps,\n@@ -613,10 +638,9 @@ iter_subscript(PyArrayIterObject *self, PyObject *ind)\n             PyObject *tmp;\n             tmp = PyArray_ToScalar(self->dataptr, self->ao);\n             PyArray_ITER_RESET(self);\n+            NPY_cast_info_xfree(&cast_info);\n             return tmp;\n         }\n-        size = PyArray_DESCR(self->ao)->elsize;\n-        dtype = PyArray_DESCR(self->ao);\n         Py_INCREF(dtype);\n         ret = (PyArrayObject *)PyArray_NewFromDescr(Py_TYPE(self->ao),\n                                  dtype,\n@@ -627,14 +651,19 @@ iter_subscript(PyArrayIterObject *self, PyObject *ind)\n             goto fail;\n         }\n         dptr = PyArray_DATA(ret);\n-        copyswap = PyArray_DESCR(ret)->f->copyswap;\n         while (n_steps--) {\n-            copyswap(dptr, self->dataptr, 0, ret);\n+            char *args[2] = {self->dataptr, dptr};\n+            npy_intp transfer_strides[2] = {size, size};\n+            if (cast_info.func(&cast_info.context, args, &one,\n+                               transfer_strides, cast_info.auxdata) < 0) {\n+                goto fail;\n+            }\n             start += step_size;\n             PyArray_ITER_GOTO1D(self, start);\n             dptr += size;\n         }\n         PyArray_ITER_RESET(self);\n+        NPY_cast_info_xfree(&cast_info);\n         return (PyObject *)ret;\n     }\n \n@@ -659,10 +688,8 @@ iter_subscript(PyArrayIterObject *self, PyObject *ind)\n \n     /* Check for Boolean array */\n     if (PyArray_TYPE((PyArrayObject *)obj) == NPY_BOOL) {\n-        ret = iter_subscript_Bool(self, (PyArrayObject *)obj);\n-        Py_DECREF(indtype);\n-        Py_DECREF(obj);\n-        return (PyObject *)ret;\n+        ret = iter_subscript_Bool(self, (PyArrayObject *)obj, &cast_info);\n+        goto finish;\n     }\n \n     /* Only integer arrays left */\n@@ -676,31 +703,35 @@ iter_subscript(PyArrayIterObject *self, PyObject *ind)\n     if (new == NULL) {\n         goto fail;\n     }\n+    ret = (PyArrayObject *)iter_subscript_int(self, (PyArrayObject *)new,\n+                                              &cast_info);\n+    Py_DECREF(new);\n+\n+ finish:\n     Py_DECREF(indtype);\n     Py_DECREF(obj);\n-    ret = (PyArrayObject *)iter_subscript_int(self, (PyArrayObject *)new);\n-    Py_DECREF(new);\n+    NPY_cast_info_xfree(&cast_info);\n     return (PyObject *)ret;\n \n-\n  fail:\n     if (!PyErr_Occurred()) {\n         PyErr_SetString(PyExc_IndexError, \"unsupported iterator index\");\n     }\n     Py_XDECREF(indtype);\n     Py_XDECREF(obj);\n+    NPY_cast_info_xfree(&cast_info);\n+\n     return NULL;\n \n }\n \n \n static int\n iter_ass_sub_Bool(PyArrayIterObject *self, PyArrayObject *ind,\n-                  PyArrayIterObject *val, int swap)\n+                  PyArrayIterObject *val, NPY_cast_info *cast_info)\n {\n     npy_intp counter, strides;\n     char *dptr;\n-    PyArray_CopySwapFunc *copyswap;\n \n     if (PyArray_NDIM(ind) != 1) {\n         PyErr_SetString(PyExc_ValueError,\n@@ -719,10 +750,17 @@ iter_ass_sub_Bool(PyArrayIterObject *self, PyArrayObject *ind,\n     dptr = PyArray_DATA(ind);\n     PyArray_ITER_RESET(self);\n     /* Loop over Boolean array */\n-    copyswap = PyArray_DESCR(self->ao)->f->copyswap;\n+    npy_intp one = 1;\n+    PyArray_Descr *dtype = PyArray_DESCR(self->ao);\n+    int itemsize = dtype->elsize;\n+    npy_intp transfer_strides[2] = {itemsize, itemsize};\n     while (counter--) {\n         if (*((npy_bool *)dptr) != 0) {\n-            copyswap(self->dataptr, val->dataptr, swap, self->ao);\n+            char *args[2] = {val->dataptr, self->dataptr};\n+            if (cast_info->func(&cast_info->context, args, &one,\n+                                transfer_strides, cast_info->auxdata) < 0) {\n+                return -1;\n+            }\n             PyArray_ITER_NEXT(val);\n             if (val->index == val->size) {\n                 PyArray_ITER_RESET(val);\n@@ -737,21 +775,28 @@ iter_ass_sub_Bool(PyArrayIterObject *self, PyArrayObject *ind,\n \n static int\n iter_ass_sub_int(PyArrayIterObject *self, PyArrayObject *ind,\n-                 PyArrayIterObject *val, int swap)\n+                 PyArrayIterObject *val, NPY_cast_info *cast_info)\n {\n     npy_intp num;\n     PyArrayIterObject *ind_it;\n     npy_intp counter;\n-    PyArray_CopySwapFunc *copyswap;\n \n-    copyswap = PyArray_DESCR(self->ao)->f->copyswap;\n+    npy_intp one = 1;\n+    PyArray_Descr *dtype = PyArray_DESCR(self->ao);\n+    int itemsize = dtype->elsize;\n+    npy_intp transfer_strides[2] = {itemsize, itemsize};\n+\n     if (PyArray_NDIM(ind) == 0) {\n         num = *((npy_intp *)PyArray_DATA(ind));\n         if (check_and_adjust_index(&num, self->size, -1, NULL) < 0) {\n             return -1;\n         }\n         PyArray_ITER_GOTO1D(self, num);\n-        copyswap(self->dataptr, val->dataptr, swap, self->ao);\n+        char *args[2] = {val->dataptr, self->dataptr};\n+        if (cast_info->func(&cast_info->context, args, &one,\n+                            transfer_strides, cast_info->auxdata) < 0) {\n+            return -1;\n+        }\n         return 0;\n     }\n     ind_it = (PyArrayIterObject *)PyArray_IterNew((PyObject *)ind);\n@@ -766,7 +811,12 @@ iter_ass_sub_int(PyArrayIterObject *self, PyArrayObject *ind,\n             return -1;\n         }\n         PyArray_ITER_GOTO1D(self, num);\n-        copyswap(self->dataptr, val->dataptr, swap, self->ao);\n+        char *args[2] = {val->dataptr, self->dataptr};\n+        if (cast_info->func(&cast_info->context, args, &one,\n+                            transfer_strides, cast_info->auxdata) < 0) {\n+            Py_DECREF(ind_it);\n+            return -1;\n+        }\n         PyArray_ITER_NEXT(ind_it);\n         PyArray_ITER_NEXT(val);\n         if (val->index == val->size) {\n@@ -784,12 +834,11 @@ iter_ass_subscript(PyArrayIterObject *self, PyObject *ind, PyObject *val)\n     PyArrayIterObject *val_it = NULL;\n     PyArray_Descr *type;\n     PyArray_Descr *indtype = NULL;\n-    int swap, retval = -1;\n+    int retval = -1;\n     npy_intp start, step_size;\n     npy_intp n_steps;\n     PyObject *obj = NULL;\n-    PyArray_CopySwapFunc *copyswap;\n-\n+    NPY_cast_info cast_info = {.func = NULL};\n \n     if (val == NULL) {\n         PyErr_SetString(PyExc_TypeError,\n@@ -868,8 +917,17 @@ iter_ass_subscript(PyArrayIterObject *self, PyObject *ind, PyObject *val)\n         goto finish;\n     }\n \n-    copyswap = PyArray_DESCR(arrval)->f->copyswap;\n-    swap = (PyArray_ISNOTSWAPPED(self->ao)!=PyArray_ISNOTSWAPPED(arrval));\n+    /* set up cast to handle single-element copies into arrval */\n+    NPY_ARRAYMETHOD_FLAGS transfer_flags = 0;\n+    npy_intp one = 1;\n+    int itemsize = type->elsize;\n+    /* We can assume the newly allocated array is aligned */\n+    int is_aligned = IsUintAligned(self->ao);\n+    if (PyArray_GetDTypeTransferFunction(\n+                is_aligned, itemsize, itemsize, type, type, 0,\n+                &cast_info, &transfer_flags) < 0) {\n+        goto finish;\n+    }\n \n     /* Check Slice */\n     if (PySlice_Check(ind)) {\n@@ -883,15 +941,23 @@ iter_ass_subscript(PyArrayIterObject *self, PyObject *ind, PyObject *val)\n             goto finish;\n         }\n         PyArray_ITER_GOTO1D(self, start);\n+        npy_intp transfer_strides[2] = {itemsize, itemsize};\n         if (n_steps == SINGLE_INDEX) {\n-            /* Integer */\n-            copyswap(self->dataptr, PyArray_DATA(arrval), swap, arrval);\n+            char *args[2] = {PyArray_DATA(arrval), self->dataptr};\n+            if (cast_info.func(&cast_info.context, args, &one,\n+                               transfer_strides, cast_info.auxdata) < 0) {\n+                goto finish;\n+            }\n             PyArray_ITER_RESET(self);\n             retval = 0;\n             goto finish;\n         }\n         while (n_steps--) {\n-            copyswap(self->dataptr, val_it->dataptr, swap, arrval);\n+            char *args[2] = {val_it->dataptr, self->dataptr};\n+            if (cast_info.func(&cast_info.context, args, &one,\n+                               transfer_strides, cast_info.auxdata) < 0) {\n+                goto finish;\n+            }\n             start += step_size;\n             PyArray_ITER_GOTO1D(self, start);\n             PyArray_ITER_NEXT(val_it);\n@@ -919,7 +985,7 @@ iter_ass_subscript(PyArrayIterObject *self, PyObject *ind, PyObject *val)\n         /* Check for Boolean object */\n         if (PyArray_TYPE((PyArrayObject *)obj)==NPY_BOOL) {\n             if (iter_ass_sub_Bool(self, (PyArrayObject *)obj,\n-                                  val_it, swap) < 0) {\n+                                  val_it, &cast_info) < 0) {\n                 goto finish;\n             }\n             retval=0;\n@@ -936,7 +1002,7 @@ iter_ass_subscript(PyArrayIterObject *self, PyObject *ind, PyObject *val)\n                 goto finish;\n             }\n             if (iter_ass_sub_int(self, (PyArrayObject *)obj,\n-                                 val_it, swap) < 0) {\n+                                 val_it, &cast_info) < 0) {\n                 goto finish;\n             }\n             retval = 0;\n@@ -951,6 +1017,7 @@ iter_ass_subscript(PyArrayIterObject *self, PyObject *ind, PyObject *val)\n     Py_XDECREF(obj);\n     Py_XDECREF(val_it);\n     Py_XDECREF(arrval);\n+    NPY_cast_info_xfree(&cast_info);\n     return retval;\n \n }"
            },
            {
                "filename": "numpy/core/tests/test_custom_dtypes.py",
                "patch": "@@ -1,3 +1,4 @@\n+import sys\n from tempfile import NamedTemporaryFile\n \n import pytest\n@@ -266,7 +267,27 @@ def test_np_save_load(self):\n \n         del np._ScaledFloatTestDType\n \n+    def test_flatiter(self):\n+        arr = np.array([1.0, 2.0, 3.0], dtype=SF(1.0))\n+\n+        for i, val in enumerate(arr.flat):\n+            assert arr[i] == val\n \n+    @pytest.mark.parametrize(\n+        \"index\", [\n+            [1, 2], ..., slice(None, 2, None),\n+            np.array([True, True, False]), np.array([0, 1])\n+        ], ids=[\"int_list\", \"ellipsis\", \"slice\", \"bool_array\", \"int_array\"])\n+    def test_flatiter_index(self, index):\n+        arr = np.array([1.0, 2.0, 3.0], dtype=SF(1.0))\n+        np.testing.assert_array_equal(\n+            arr[index].view(np.float64), arr.flat[index].view(np.float64))\n+\n+        arr2 = arr.copy()\n+        arr[index] = 5.0\n+        arr2.flat[index] = 5.0\n+        np.testing.assert_array_equal(\n+            arr.view(np.float64), arr2.view(np.float64))\n \n def test_type_pickle():\n     # can't actually unpickle, but we can pickle (if in namespace)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23953,
        "body": "closes #23896 \r\n\r\nThis is a workaround for a compiler bug, still, the new changes don't affect performance.\r\n\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/common/simd/avx2/memory.h",
                "patch": "@@ -372,10 +372,29 @@ NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32\n NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n {\n     assert(nlane > 0);\n+#ifdef _MSC_VER\n+   /*\n+    * Although this version is compatible with all other compilers,\n+    * there is no performance benefit in retaining the other branch.\n+    * However, it serves as evidence of a newly emerging bug in MSVC\n+    * that started to appear since v19.30.\n+    * For some reason, the MSVC optimizer chooses to ignore the lower store (128-bit mov)\n+    * and replace with full mov counting on ymmword pointer.\n+    *\n+    * For more details, please refer to the discussion on https://github.com/numpy/numpy/issues/23896.\n+    */\n+    if (nlane > 1) {\n+        npyv_store_s64(ptr, a);\n+    }\n+    else {\n+        npyv_storel_s64(ptr, a);\n+    }\n+#else\n     npyv_storel_s64(ptr, a);\n     if (nlane > 1) {\n         npyv_storeh_s64(ptr + 2, a);\n     }\n+#endif\n }\n /*********************************\n  * Non-contiguous partial store"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23956,
        "body": "Backport of #23953.\r\n\r\ncloses #23896\r\n\r\nThis a workaround for compiler bug, still the new changes doesn't affect on performance.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/common/simd/avx2/memory.h",
                "patch": "@@ -372,10 +372,29 @@ NPY_FINLINE void npyv_store2_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32\n NPY_FINLINE void npyv_store2_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n {\n     assert(nlane > 0);\n+#ifdef _MSC_VER\n+   /*\n+    * Although this version is compatible with all other compilers,\n+    * there is no performance benefit in retaining the other branch.\n+    * However, it serves as evidence of a newly emerging bug in MSVC\n+    * that started to appear since v19.30.\n+    * For some reason, the MSVC optimizer chooses to ignore the lower store (128-bit mov)\n+    * and replace with full mov counting on ymmword pointer.\n+    *\n+    * For more details, please refer to the discussion on https://github.com/numpy/numpy/issues/23896.\n+    */\n+    if (nlane > 1) {\n+        npyv_store_s64(ptr, a);\n+    }\n+    else {\n+        npyv_storel_s64(ptr, a);\n+    }\n+#else\n     npyv_storel_s64(ptr, a);\n     if (nlane > 1) {\n         npyv_storeh_s64(ptr + 2, a);\n     }\n+#endif\n }\n /*********************************\n  * Non-contiguous partial store"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23917,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nSpeed up flatiter boolean indexing by using `count_boolean_trues`, which is faster than a manual loop. Also only copy data if the number of trues is non-zero. Both of these changes are already used by [`array_boolean_subscript`](https://github.com/numpy/numpy/blob/e75214c1bc34903e6fe0f643dae8ada02529c0d5/numpy/core/src/multiarray/mapping.c#L918).\r\n\r\n```\r\n       before           after         ratio\r\n     [e75214c1]       [e9d29d00]\r\n     <main>           <optimize_flatiter_mask>\r\n-     23.2\u00b10.07ms       20.3\u00b10.2ms     0.88  bench_indexing.FlatIterIndexing.time_flat_bool_index_all\r\n-      22.9\u00b10.1ms       17.1\u00b10.4ms     0.75  bench_indexing.FlatIterIndexing.time_flat_bool_index_half\r\n-     12.0\u00b10.02ms          355\u00b11\u03bcs     0.03  bench_indexing.FlatIterIndexing.time_flat_bool_index_none\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_indexing.py",
                "patch": "@@ -125,3 +125,21 @@ def time_scalar_slice(self):\n \n     def time_scalar_all(self):\n         self.b['a'] = self.a['a']\n+\n+\n+class FlatIterIndexing(Benchmark):\n+    def setup(self):\n+        self.a = np.ones((200, 50000))\n+        self.m_all = np.repeat(True, 200 * 50000)\n+        self.m_half = np.copy(self.m_all)\n+        self.m_half[::2] = False\n+        self.m_none = np.repeat(False, 200 * 50000)\n+\n+    def time_flat_bool_index_none(self):\n+        self.a.flat[self.m_none]\n+\n+    def time_flat_bool_index_half(self):\n+        self.a.flat[self.m_half]\n+\n+    def time_flat_bool_index_all(self):\n+        self.a.flat[self.m_all]"
            },
            {
                "filename": "numpy/core/src/multiarray/iterators.c",
                "patch": "@@ -18,6 +18,7 @@\n #include \"common.h\"\n #include \"conversion_utils.h\"\n #include \"array_coercion.h\"\n+#include \"item_selection.h\"\n \n #define NEWAXIS_INDEX -1\n #define ELLIPSIS_INDEX -2\n@@ -439,14 +440,9 @@ iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind)\n     }\n \n     strides = PyArray_STRIDES(ind)[0];\n-    dptr = PyArray_DATA(ind);\n     /* Get size of return array */\n-    while (counter--) {\n-        if (*((npy_bool *)dptr) != 0) {\n-            count++;\n-        }\n-        dptr += strides;\n-    }\n+    count = count_boolean_trues(PyArray_NDIM(ind), PyArray_DATA(ind),\n+                                PyArray_DIMS(ind), PyArray_STRIDES(ind));\n     itemsize = PyArray_DESCR(self->ao)->elsize;\n     Py_INCREF(PyArray_DESCR(self->ao));\n     ret = (PyArrayObject *)PyArray_NewFromDescr(Py_TYPE(self->ao),\n@@ -456,22 +452,24 @@ iter_subscript_Bool(PyArrayIterObject *self, PyArrayObject *ind)\n     if (ret == NULL) {\n         return NULL;\n     }\n-    /* Set up loop */\n-    optr = PyArray_DATA(ret);\n-    counter = PyArray_DIMS(ind)[0];\n-    dptr = PyArray_DATA(ind);\n-    copyswap = PyArray_DESCR(self->ao)->f->copyswap;\n-    /* Loop over Boolean array */\n-    swap = (PyArray_ISNOTSWAPPED(self->ao) != PyArray_ISNOTSWAPPED(ret));\n-    while (counter--) {\n-        if (*((npy_bool *)dptr) != 0) {\n-            copyswap(optr, self->dataptr, swap, self->ao);\n-            optr += itemsize;\n+    if (count > 0) {\n+        /* Set up loop */\n+        optr = PyArray_DATA(ret);\n+        counter = PyArray_DIMS(ind)[0];\n+        dptr = PyArray_DATA(ind);\n+        copyswap = PyArray_DESCR(self->ao)->f->copyswap;\n+        /* Loop over Boolean array */\n+        swap = (PyArray_ISNOTSWAPPED(self->ao) != PyArray_ISNOTSWAPPED(ret));\n+        while (counter--) {\n+            if (*((npy_bool *)dptr) != 0) {\n+                copyswap(optr, self->dataptr, swap, self->ao);\n+                optr += itemsize;\n+            }\n+            dptr += strides;\n+            PyArray_ITER_NEXT(self);\n         }\n-        dptr += strides;\n-        PyArray_ITER_NEXT(self);\n+        PyArray_ITER_RESET(self);\n     }\n-    PyArray_ITER_RESET(self);\n     return ret;\n }\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23906,
        "body": "Mainly, I think this is a bit simpler, it also is a a fair bit faster when keyword arguments are being used.  (not that it matters most of the time)\r\n\r\n\r\n---\r\n\r\nReally, I mainly think this is a bit nicer, I ran a few benchmarks (not all) and this dropped out, which is probably true:\r\n```\r\n       before           after         ratio\r\n     [4e69d516]       [e4bb12da]\r\n     <main>           <forward-call-vectorcall>\r\n-        2.21\u00b10\u03bcs      2.10\u00b10.02\u03bcs     0.95  bench_core.CountNonzero.time_count_nonzero_axis(2, 100, <class 'numpy.int32'>)\r\n-     2.08\u00b10.03\u03bcs      1.96\u00b10.01\u03bcs     0.95  bench_core.CountNonzero.time_count_nonzero_multi_axis(1, 100, <class 'bool'>)\r\n-        759\u00b130ns          718\u00b12ns     0.95  bench_core.StatsMethods.time_sum('intp', 1)\r\n-     2.13\u00b10.02\u03bcs      1.97\u00b10.02\u03bcs     0.92  bench_core.CountNonzero.time_count_nonzero_multi_axis(3, 100, <class 'bool'>)\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\nif you have lots of kwargs on a `sum` or so, its quite noticable, of course for most calls it isn't a whole lot of a difference.",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/methods.c",
                "patch": "@@ -63,34 +63,45 @@ NpyArg_ParseKeywords(PyObject *keys, const char *format, char **kwlist, ...)\n \n \n /*\n- * Forwards an ndarray method to a the Python function\n- * numpy.core._methods.<name>(...)\n+ * Forwards a method call to a Python function while adding `self`:\n+ * callable(self, ...)\n  */\n static PyObject *\n-forward_ndarray_method(PyArrayObject *self, PyObject *args, PyObject *kwds,\n-                            PyObject *forwarding_callable)\n+npy_forward_method(\n+        PyObject *callable, PyObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n-    PyObject *sargs, *ret;\n-    int i, n;\n+    PyObject *args_buffer[NPY_MAXARGS];\n+    /* Practically guaranteed NPY_MAXARGS is enough. */\n+    PyObject **new_args = args_buffer;\n \n-    /* Combine 'self' and 'args' together into one tuple */\n-    n = PyTuple_GET_SIZE(args);\n-    sargs = PyTuple_New(n + 1);\n-    if (sargs == NULL) {\n-        return NULL;\n-    }\n-    Py_INCREF(self);\n-    PyTuple_SET_ITEM(sargs, 0, (PyObject *)self);\n-    for (i = 0; i < n; ++i) {\n-        PyObject *item = PyTuple_GET_ITEM(args, i);\n-        Py_INCREF(item);\n-        PyTuple_SET_ITEM(sargs, i+1, item);\n+    /*\n+     * `PY_VECTORCALL_ARGUMENTS_OFFSET` seems never set, probably `args[-1]`\n+     * is always `self` but do not rely on it unless Python documents that.\n+     */\n+    npy_intp len_kwargs = kwnames != NULL ? PyTuple_GET_SIZE(kwnames) : 0;\n+    size_t original_arg_size = (len_args + len_kwargs) * sizeof(PyObject *);\n+\n+    if (NPY_UNLIKELY(len_args + len_kwargs > NPY_MAXARGS)) {\n+        new_args = (PyObject **)PyMem_MALLOC(original_arg_size + sizeof(PyObject *));\n+        if (new_args == NULL) {\n+            /*\n+             * If this fails Python uses `PY_VECTORCALL_ARGUMENTS_OFFSET` and\n+             * we should probably add a fast-path for that (hopefully almost)\n+             * always taken.\n+             */\n+            return PyErr_NoMemory();\n+        }\n     }\n \n-    /* Call the function and return */\n-    ret = PyObject_Call(forwarding_callable, sargs, kwds);\n-    Py_DECREF(sargs);\n-    return ret;\n+    new_args[0] = self;\n+    memcpy(&new_args[1], args, original_arg_size);\n+    PyObject *res = PyObject_Vectorcall(callable, new_args, len_args+1, kwnames);\n+\n+    if (NPY_UNLIKELY(len_args + len_kwargs > NPY_MAXARGS)) {\n+        PyMem_FREE(new_args);\n+    }\n+    return res;\n }\n \n /*\n@@ -105,7 +116,7 @@ forward_ndarray_method(PyArrayObject *self, PyObject *args, PyObject *kwds,\n         if (callable == NULL) { \\\n             return NULL; \\\n         } \\\n-        return forward_ndarray_method(self, args, kwds, callable)\n+        return npy_forward_method(callable, (PyObject *)self, args, len_args, kwnames)\n \n \n static PyObject *\n@@ -339,19 +350,22 @@ array_argmin(PyArrayObject *self,\n }\n \n static PyObject *\n-array_max(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_max(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_amax\");\n }\n \n static PyObject *\n-array_min(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_min(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_amin\");\n }\n \n static PyObject *\n-array_ptp(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_ptp(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_ptp\");\n }\n@@ -2344,14 +2358,16 @@ PyArray_Dumps(PyObject *self, int protocol)\n \n \n static PyObject *\n-array_dump(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_dump(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_dump\");\n }\n \n \n static PyObject *\n-array_dumps(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_dumps(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_dumps\");\n }\n@@ -2402,13 +2418,15 @@ array_transpose(PyArrayObject *self, PyObject *args)\n #define _CHKTYPENUM(typ) ((typ) ? (typ)->type_num : NPY_NOTYPE)\n \n static PyObject *\n-array_mean(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_mean(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_mean\");\n }\n \n static PyObject *\n-array_sum(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_sum(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_sum\");\n }\n@@ -2437,7 +2455,8 @@ array_cumsum(PyArrayObject *self, PyObject *args, PyObject *kwds)\n }\n \n static PyObject *\n-array_prod(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_prod(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_prod\");\n }\n@@ -2496,26 +2515,30 @@ array_dot(PyArrayObject *self,\n \n \n static PyObject *\n-array_any(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_any(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_any\");\n }\n \n \n static PyObject *\n-array_all(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_all(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_all\");\n }\n \n static PyObject *\n-array_stddev(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_stddev(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_std\");\n }\n \n static PyObject *\n-array_variance(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_variance(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_var\");\n }\n@@ -2595,7 +2618,8 @@ array_trace(PyArrayObject *self,\n \n \n static PyObject *\n-array_clip(PyArrayObject *self, PyObject *args, PyObject *kwds)\n+array_clip(PyArrayObject *self,\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n {\n     NPY_FORWARD_NDARRAY_METHOD(\"_clip\");\n }\n@@ -2906,10 +2930,10 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         METH_VARARGS, NULL},\n     {\"dumps\",\n         (PyCFunction) array_dumps,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"dump\",\n         (PyCFunction) array_dump,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n \n     {\"__complex__\",\n         (PyCFunction) array_complex,\n@@ -2927,10 +2951,10 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n     /* Original and Extended methods added 2005 */\n     {\"all\",\n         (PyCFunction)array_all,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"any\",\n         (PyCFunction)array_any,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"argmax\",\n         (PyCFunction)array_argmax,\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n@@ -2954,7 +2978,7 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"clip\",\n         (PyCFunction)array_clip,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"compress\",\n         (PyCFunction)array_compress,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n@@ -2996,13 +3020,13 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         METH_VARARGS, NULL},\n     {\"max\",\n         (PyCFunction)array_max,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"mean\",\n         (PyCFunction)array_mean,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"min\",\n         (PyCFunction)array_min,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"newbyteorder\",\n         (PyCFunction)array_newbyteorder,\n         METH_VARARGS, NULL},\n@@ -3014,10 +3038,10 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"prod\",\n         (PyCFunction)array_prod,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"ptp\",\n         (PyCFunction)array_ptp,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"put\",\n         (PyCFunction)array_put,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n@@ -3053,10 +3077,10 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"std\",\n         (PyCFunction)array_stddev,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"sum\",\n         (PyCFunction)array_sum,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"swapaxes\",\n         (PyCFunction)array_swapaxes,\n         METH_VARARGS, NULL},\n@@ -3083,7 +3107,7 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         METH_VARARGS, NULL},\n     {\"var\",\n         (PyCFunction)array_variance,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"view\",\n         (PyCFunction)array_view,\n         METH_FASTCALL | METH_KEYWORDS, NULL},"
            },
            {
                "filename": "numpy/core/tests/test_argparse.py",
                "patch": "@@ -60,3 +60,12 @@ def test_string_fallbacks():\n             match=\"got an unexpected keyword argument 'missing_arg'\"):\n         func(2, **{missing_arg: 3})\n \n+\n+def test_too_many_arguments_method_forwarding():\n+    # Not directly related to the standard argument parsing, but we sometimes\n+    # forward methods to Python: arr.mean() calls np.core._methods._mean()\n+    # This adds code coverage for this `npy_forward_method`.\n+    arr = np.arange(3)\n+    args = range(1000)\n+    with pytest.raises(TypeError):\n+        arr.mean(*args)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23699,
        "body": "See #23248 for a similar refactor.\r\n\r\nThis makes it possible to call `np.repeat` and `ndarray.repeat` on new-style dtypes that have references. There aren't any dtypes like that in numpy itself so I can't easily add a test for the new code path, but this change works with `StringDType`.\r\n\r\nPerformance before:\r\n\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: arr = np.array([1, 2, 3], dtype=np.int_)\r\n\r\nIn [3]: %timeit arr.repeat(1_000_000)\r\n5.8 ms \u00b1 10.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [4]: arr = np.array([1, 2, 3], dtype=object)\r\n\r\nIn [5]: %timeit arr.repeat(1_000_000)\r\n15.3 ms \u00b1 35.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n```\r\n\r\nAnd after:\r\n\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: arr = np.array([1, 2, 3], dtype=np.int_)\r\n\r\nIn [3]: %timeit arr.repeat(1_000_000)\r\n5.83 ms \u00b1 29.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [4]: arr = np.array([1, 2, 3], dtype=object)\r\n\r\nIn [5]: %timeit arr.repeat(1_000_000)\r\n16.7 ms \u00b1 57.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n```\r\n\r\nSo negligible performance impact for non-`REFCHK` dtypes and a ~10% slowdown for object dtypes.\r\n\r\nHappy to add more tests and do more benchmarks if any reviewers have suggestions.",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/item_selection.c",
                "patch": "@@ -761,20 +761,98 @@ PyArray_PutMask(PyArrayObject *self, PyObject* values0, PyObject* mask0)\n     return NULL;\n }\n \n+static NPY_GCC_OPT_3 inline int\n+npy_fastrepeat_impl(\n+    npy_intp n_outer, npy_intp n, npy_intp nel, npy_intp chunk,\n+    npy_bool broadcast, npy_intp* counts, char* new_data, char* old_data,\n+    npy_intp elsize, NPY_cast_info cast_info, int needs_refcounting)\n+{\n+    npy_intp i, j, k;\n+    for (i = 0; i < n_outer; i++) {\n+        for (j = 0; j < n; j++) {\n+            npy_intp tmp = broadcast ? counts[0] : counts[j];\n+            for (k = 0; k < tmp; k++) {\n+                if (!needs_refcounting) {\n+                    memcpy(new_data, old_data, chunk);\n+                }\n+                else {\n+                    char *data[2] = {old_data, new_data};\n+                    npy_intp strides[2] = {elsize, elsize};\n+                    if (cast_info.func(&cast_info.context, data, &nel,\n+                                       strides, cast_info.auxdata) < 0) {\n+                        return -1;\n+                    }\n+                }\n+                new_data += chunk;\n+            }\n+            old_data += chunk;\n+        }\n+    }\n+    return 0;\n+}\n+\n+static NPY_GCC_OPT_3 int\n+npy_fastrepeat(\n+    npy_intp n_outer, npy_intp n, npy_intp nel, npy_intp chunk,\n+    npy_bool broadcast, npy_intp* counts, char* new_data, char* old_data,\n+    npy_intp elsize, NPY_cast_info cast_info, int needs_refcounting)\n+{\n+    if (!needs_refcounting) {\n+        if (chunk == 1) {\n+            return npy_fastrepeat_impl(\n+                n_outer, n, nel, chunk, broadcast, counts, new_data, old_data,\n+                elsize, cast_info, needs_refcounting);\n+        }\n+        if (chunk == 2) {\n+            return npy_fastrepeat_impl(\n+                n_outer, n, nel, chunk, broadcast, counts, new_data, old_data,\n+                elsize, cast_info, needs_refcounting);\n+        }\n+        if (chunk == 4) {\n+            return npy_fastrepeat_impl(\n+                n_outer, n, nel, chunk, broadcast, counts, new_data, old_data,\n+                elsize, cast_info, needs_refcounting);\n+        }\n+        if (chunk == 8) {\n+            return npy_fastrepeat_impl(\n+                n_outer, n, nel, chunk, broadcast, counts, new_data, old_data,\n+                elsize, cast_info, needs_refcounting);\n+        }\n+        if (chunk == 16) {\n+            return npy_fastrepeat_impl(\n+                n_outer, n, nel, chunk, broadcast, counts, new_data, old_data,\n+                elsize, cast_info, needs_refcounting);\n+        }\n+        if (chunk == 32) {\n+            return npy_fastrepeat_impl(\n+                n_outer, n, nel, chunk, broadcast, counts, new_data, old_data,\n+                elsize, cast_info, needs_refcounting);\n+        }\n+    }\n+\n+    return npy_fastrepeat_impl(\n+        n_outer, n, nel, chunk, broadcast, counts, new_data, old_data, elsize,\n+        cast_info, needs_refcounting);    \n+}\n+\n+\n /*NUMPY_API\n  * Repeat the array.\n  */\n NPY_NO_EXPORT PyObject *\n PyArray_Repeat(PyArrayObject *aop, PyObject *op, int axis)\n {\n     npy_intp *counts;\n-    npy_intp n, n_outer, i, j, k, chunk;\n+    npy_intp i, j, n, n_outer, chunk, elsize, nel;\n     npy_intp total = 0;\n     npy_bool broadcast = NPY_FALSE;\n     PyArrayObject *repeats = NULL;\n     PyObject *ap = NULL;\n     PyArrayObject *ret = NULL;\n     char *new_data, *old_data;\n+    NPY_cast_info cast_info;\n+    NPY_ARRAYMETHOD_FLAGS flags;\n+    int needs_refcounting;\n \n     repeats = (PyArrayObject *)PyArray_ContiguousFromAny(op, NPY_INTP, 0, 1);\n     if (repeats == NULL) {\n@@ -798,6 +876,8 @@ PyArray_Repeat(PyArrayObject *aop, PyObject *op, int axis)\n \n     aop = (PyArrayObject *)ap;\n     n = PyArray_DIM(aop, axis);\n+    NPY_cast_info_init(&cast_info);\n+    needs_refcounting = PyDataType_REFCHK(PyArray_DESCR(aop));\n \n     if (!broadcast && PyArray_SIZE(repeats) != n) {\n         PyErr_Format(PyExc_ValueError,\n@@ -835,35 +915,41 @@ PyArray_Repeat(PyArrayObject *aop, PyObject *op, int axis)\n     new_data = PyArray_DATA(ret);\n     old_data = PyArray_DATA(aop);\n \n-    chunk = PyArray_DESCR(aop)->elsize;\n+    nel = 1;\n+    elsize = PyArray_DESCR(aop)->elsize;\n     for(i = axis + 1; i < PyArray_NDIM(aop); i++) {\n-        chunk *= PyArray_DIMS(aop)[i];\n+        nel *= PyArray_DIMS(aop)[i];\n     }\n+    chunk = nel*elsize;\n \n     n_outer = 1;\n     for (i = 0; i < axis; i++) {\n         n_outer *= PyArray_DIMS(aop)[i];\n     }\n-    for (i = 0; i < n_outer; i++) {\n-        for (j = 0; j < n; j++) {\n-            npy_intp tmp = broadcast ? counts[0] : counts[j];\n-            for (k = 0; k < tmp; k++) {\n-                memcpy(new_data, old_data, chunk);\n-                new_data += chunk;\n-            }\n-            old_data += chunk;\n+\n+    if (needs_refcounting) {\n+        if (PyArray_GetDTypeTransferFunction(\n+                1, elsize, elsize, PyArray_DESCR(aop), PyArray_DESCR(aop), 0,\n+                &cast_info, &flags) < 0) {\n+            goto fail;\n         }\n     }\n \n+    if (npy_fastrepeat(n_outer, n, nel, chunk, broadcast, counts, new_data,\n+                       old_data, elsize, cast_info, needs_refcounting) < 0) {\n+        goto fail;\n+    }\n+\n     Py_DECREF(repeats);\n-    PyArray_INCREF(ret);\n     Py_XDECREF(aop);\n+    NPY_cast_info_xfree(&cast_info);\n     return (PyObject *)ret;\n \n  fail:\n     Py_DECREF(repeats);\n     Py_XDECREF(aop);\n     Py_XDECREF(ret);\n+    NPY_cast_info_xfree(&cast_info);\n     return NULL;\n }\n "
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -1934,8 +1934,9 @@ def test_prod(self):\n                 assert_array_equal(a2.prod(axis=-1),\n                                    np.array([24, 1890, 600], ctype))\n \n-    def test_repeat(self):\n-        m = np.array([1, 2, 3, 4, 5, 6])\n+    @pytest.mark.parametrize('dtype', [None, object])\n+    def test_repeat(self, dtype):\n+        m = np.array([1, 2, 3, 4, 5, 6], dtype=dtype)\n         m_rect = m.reshape((2, 3))\n \n         A = m.repeat([1, 3, 2, 1, 1, 2])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23668,
        "body": "See #23661.\r\nI was not sure if I should tag it as an \"improvement\" or as a \"performance\" change.\r\nLet me know if it needs to be renamed ;)\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/23661.performance.rst",
                "patch": "@@ -0,0 +1,4 @@\n+Faster membership test on ``NpzFile``\n+-------------------------------------\n+Membership test on ``NpzFile`` will no longer\n+decompress the archive if it is successful."
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18923,
        "body": "Backport of #18900. \r\n \r\n Supporting PowerPC/AltiVec on macOS is no longer necessary,\r\n  even if the Mac/G5 is still running having `-faltivec` or\r\n  replacing it with the new AltiVec flag `-maltivec` wouldn't\r\n  increase that much performance without raw SIMD.\r\n\r\n  note: the flag was enabled on non-intel platforms which\r\n  causes fatal build errors on macOS/arm64.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/distutils/system_info.py",
                "patch": "@@ -2443,8 +2443,6 @@ def calc_info(self):\n                     'accelerate' in libraries):\n                 if intel:\n                     args.extend(['-msse3'])\n-                else:\n-                    args.extend(['-faltivec'])\n                 args.extend([\n                     '-I/System/Library/Frameworks/vecLib.framework/Headers'])\n                 link_args.extend(['-Wl,-framework', '-Wl,Accelerate'])\n@@ -2453,8 +2451,6 @@ def calc_info(self):\n                       'veclib' in libraries):\n                 if intel:\n                     args.extend(['-msse3'])\n-                else:\n-                    args.extend(['-faltivec'])\n                 args.extend([\n                     '-I/System/Library/Frameworks/vecLib.framework/Headers'])\n                 link_args.extend(['-Wl,-framework', '-Wl,vecLib'])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18906,
        "body": "Closes #16313\r\n\r\nThis PR adds the `PCG64DXSM` `BitGenerator`. \r\n\r\nI have a documentation page explaining the issue and when you should worry about it. It is my usual overcomplicated prose (with my penchant for parentheticals), but I did try to give clear practical advice upfront and shunt all of the technical explanation down to the bottom. I imagine it could use a few more editing passes, but probably nothing that would prevent a release. Inserting references to it in all of the relevant places was even less elegant, but it gets the job done. As in NEP 19, I name myself as the maker of mistakes, which I feel is appropriate, but I'm not wedded to it if it doesn't match the tone we want in the documentation.\r\n\r\nI updated the performance benchmarking script that feeds the tables in the documentation, but I did not replace the tables as I don't have ready access to all of the platforms. @bashtage I think you ran those, am I right?",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/18906.new_function.rst",
                "patch": "@@ -0,0 +1,17 @@\n+.. currentmodule:: numpy.random\n+\n+Add `PCG64DXSM` `BitGenerator`\n+------------------------------\n+\n+Uses of the `PCG64` `BitGenerator` in a massively-parallel context have been\n+shown to have statistical weaknesses that were not apparent at the first\n+release in numpy 1.17. Most users will never observe this weakness and are\n+safe to continue to use `PCG64`. We have introduced a new `PCG64DXSM`\n+`BitGenerator` that will eventually become the new default `BitGenerator`\n+implementation used by `default_rng` in future releases. `PCG64DXSM` solves\n+the statistical weakness while preserving the performance and the features of\n+`PCG64`.\n+\n+See :ref:`upgrading-pcg64` for more details.\n+\n+.. currentmodule:: numpy"
            },
            {
                "filename": "doc/source/reference/random/bit_generators/index.rst",
                "patch": "@@ -15,10 +15,13 @@ Supported BitGenerators\n \n The included BitGenerators are:\n \n-* PCG-64 - The default. A fast generator that supports many parallel streams\n-  and can be advanced by an arbitrary amount. See the documentation for\n-  :meth:`~.PCG64.advance`. PCG-64 has a period of :math:`2^{128}`. See the `PCG\n-  author's page`_ for more details about this class of PRNG.\n+* PCG-64 - The default. A fast generator that can be advanced by an arbitrary\n+  amount. See the documentation for :meth:`~.PCG64.advance`. PCG-64 has\n+  a period of :math:`2^{128}`. See the `PCG author's page`_ for more details\n+  about this class of PRNG.\n+* PCG-64 DXSM - An upgraded version of PCG-64 with better statistical\n+  properties in parallel contexts. See :ref:`upgrading-pcg64` for more\n+  information on these improvements.\n * MT19937 - The standard Python BitGenerator. Adds a `MT19937.jumped`\n   function that returns a new generator with state as-if :math:`2^{128}` draws have\n   been made.\n@@ -43,6 +46,7 @@ The included BitGenerators are:\n \n     MT19937 <mt19937>\n     PCG64 <pcg64>\n+    PCG64DXSM <pcg64dxsm>\n     Philox <philox>\n     SFC64 <sfc64>\n "
            },
            {
                "filename": "doc/source/reference/random/bit_generators/pcg64dxsm.rst",
                "patch": "@@ -0,0 +1,32 @@\n+Permuted Congruential Generator (64-bit, PCG64 DXSM)\n+----------------------------------------------------\n+\n+.. currentmodule:: numpy.random\n+\n+.. autoclass:: PCG64DXSM\n+    :members: __init__\n+    :exclude-members: __init__\n+\n+State\n+=====\n+\n+.. autosummary::\n+   :toctree: generated/\n+\n+   ~PCG64DXSM.state\n+\n+Parallel generation\n+===================\n+.. autosummary::\n+   :toctree: generated/\n+\n+   ~PCG64DXSM.advance\n+   ~PCG64DXSM.jumped\n+\n+Extending\n+=========\n+.. autosummary::\n+   :toctree: generated/\n+\n+   ~PCG64DXSM.cffi\n+   ~PCG64DXSM.ctypes"
            },
            {
                "filename": "doc/source/reference/random/index.rst",
                "patch": "@@ -222,6 +222,9 @@ one of three ways:\n * :ref:`independent-streams`\n * :ref:`parallel-jumped`\n \n+Users with a very large amount of parallelism will want to consult\n+:ref:`upgrading-pcg64`.\n+\n Concepts\n --------\n .. toctree::\n@@ -230,6 +233,7 @@ Concepts\n    generator\n    Legacy Generator (RandomState) <legacy>\n    BitGenerators, SeedSequences <bit_generators/index>\n+   Upgrading PCG64 with PCG64DXSM <upgrading-pcg64>\n \n Features\n --------"
            },
            {
                "filename": "doc/source/reference/random/parallel.rst",
                "patch": "@@ -88,10 +88,11 @@ territory ([2]_).\n        estimate the naive upper bound on a napkin and take comfort knowing\n        that the probability is actually lower.\n \n-.. [2] In this calculation, we can ignore the amount of numbers drawn from each\n-       stream. Each of the PRNGs we provide has some extra protection built in\n+.. [2] In this calculation, we can mostly ignore the amount of numbers drawn from each\n+       stream. See :ref:`upgrading-pcg64` for the technical details about\n+       `PCG64`. The other PRNGs we provide have some extra protection built in\n        that avoids overlaps if the `~SeedSequence` pools differ in the\n-       slightest bit. `PCG64` has :math:`2^{127}` separate cycles\n+       slightest bit. `PCG64DXSM` has :math:`2^{127}` separate cycles\n        determined by the seed in addition to the position in the\n        :math:`2^{128}` long period for each cycle, so one has to both get on or\n        near the same cycle *and* seed a nearby position in the cycle.\n@@ -150,12 +151,14 @@ BitGenerator, the size of the jump and the bits in the default unsigned random\n are listed below.\n \n +-----------------+-------------------------+-------------------------+-------------------------+\n-| BitGenerator    | Period                  |  Jump Size              | Bits                    |\n+| BitGenerator    | Period                  |  Jump Size              | Bits per Draw           |\n +=================+=========================+=========================+=========================+\n-| MT19937         | :math:`2^{19937}`       | :math:`2^{128}`         | 32                      |\n+| MT19937         | :math:`2^{19937}-1`     | :math:`2^{128}`         | 32                      |\n +-----------------+-------------------------+-------------------------+-------------------------+\n | PCG64           | :math:`2^{128}`         | :math:`~2^{127}` ([3]_) | 64                      |\n +-----------------+-------------------------+-------------------------+-------------------------+\n+| PCG64DXSM       | :math:`2^{128}`         | :math:`~2^{127}` ([3]_) | 64                      |\n++-----------------+-------------------------+-------------------------+-------------------------+\n | Philox          | :math:`2^{256}`         | :math:`2^{128}`         | 64                      |\n +-----------------+-------------------------+-------------------------+-------------------------+\n "
            },
            {
                "filename": "doc/source/reference/random/performance.py",
                "patch": "@@ -3,9 +3,9 @@\n import pandas as pd\n \n import numpy as np\n-from numpy.random import MT19937, PCG64, Philox, SFC64\n+from numpy.random import MT19937, PCG64, PCG64DXSM, Philox, SFC64\n \n-PRNGS = [MT19937, PCG64, Philox, SFC64]\n+PRNGS = [MT19937, PCG64, PCG64DXSM, Philox, SFC64]\n \n funcs = {}\n integers = 'integers(0, 2**{bits},size=1000000, dtype=\"uint{bits}\")'\n@@ -53,7 +53,7 @@\n     col[key] = 1000 * min(t)\n table['RandomState'] = pd.Series(col)\n \n-columns = ['MT19937','PCG64','Philox','SFC64', 'RandomState']\n+columns = ['MT19937', 'PCG64', 'PCG64DXSM', 'Philox', 'SFC64', 'RandomState']\n table = pd.DataFrame(table)\n order = np.log(table).mean().sort_values().index\n table = table.T"
            },
            {
                "filename": "doc/source/reference/random/performance.rst",
                "patch": "@@ -5,9 +5,12 @@ Performance\n \n Recommendation\n **************\n-The recommended generator for general use is `PCG64`. It is\n-statistically high quality, full-featured, and fast on most platforms, but\n-somewhat slow when compiled for 32-bit processes.\n+\n+The recommended generator for general use is `PCG64` or its upgraded variant\n+`PCG64DXSM` for heavily-parallel use cases. They are statistically high quality,\n+full-featured, and fast on most platforms, but somewhat slow when compiled for\n+32-bit processes. See :ref:`upgrading-pcg64` for details on when heavy\n+parallelism would indicate using `PCG64DXSM`.\n \n `Philox` is fairly slow, but its statistical properties have\n very high quality, and it is easy to get assuredly-independent stream by using"
            },
            {
                "filename": "doc/source/reference/random/upgrading-pcg64.rst",
                "patch": "@@ -0,0 +1,152 @@\n+.. _upgrading-pcg64:\n+\n+.. currentmodule:: numpy.random\n+\n+Upgrading ``PCG64`` with ``PCG64DXSM``\n+--------------------------------------\n+\n+Uses of the `PCG64` `BitGenerator` in a massively-parallel context have been\n+shown to have statistical weaknesses that were not apparent at the first\n+release in numpy 1.17. Most users will never observe this weakness and are\n+safe to continue to use `PCG64`. We have introduced a new `PCG64DXSM`\n+`BitGenerator` that will eventually become the new default `BitGenerator`\n+implementation used by `default_rng` in future releases. `PCG64DXSM` solves\n+the statistical weakness while preserving the performance and the features of\n+`PCG64`.\n+\n+Does this affect me?\n+====================\n+\n+If you\n+\n+  1. only use a single `Generator` instance,\n+  2. only use `RandomState` or the functions in `numpy.random`,\n+  3. only use the `PCG64.jumped` method to generate parallel streams,\n+  4. explicitly use a `BitGenerator` other than `PCG64`,\n+\n+then this weakness does not affect you at all. Carry on.\n+\n+If you use moderate numbers of parallel streams created with `default_rng` or\n+`SeedSequence.spawn`, in the 1000s, then the chance of observing this weakness\n+is negligibly small. You can continue to use `PCG64` comfortably.\n+\n+If you use very large numbers of parallel streams, in the millions, and draw\n+large amounts of numbers from each, then the chance of observing this weakness\n+can become non-negligible, if still small. An example of such a use case would\n+be a very large distributed reinforcement learning problem with millions of\n+long Monte Carlo playouts each generating billions of random number draws. Such\n+use cases should consider using `PCG64DXSM` explicitly or another\n+modern `BitGenerator` like `SFC64` or `Philox`, but it is unlikely that any\n+old results you may have calculated are invalid. In any case, the weakness is\n+a kind of `Birthday Paradox <https://en.wikipedia.org/wiki/Birthday_problem>`_\n+collision. That is, a single pair of parallel streams out of the millions,\n+considered together, might fail a stringent set of statistical tests of\n+randomness. The remaining millions of streams would all be perfectly fine, and\n+the effect of the bad pair in the whole calculation is very likely to be\n+swamped by the remaining streams in most applications.\n+\n+.. _upgrading-pcg64-details:\n+\n+Technical Details\n+=================\n+\n+Like many PRNG algorithms, `PCG64` is constructed from a transition function,\n+which advances a 128-bit state, and an output function, that mixes the 128-bit\n+state into a 64-bit integer to be output. One of the guiding design principles\n+of the PCG family of PRNGs is to balance the computational cost (and\n+pseudorandomness strength) between the transition function and the output\n+function. The transition function is a 128-bit linear congruential generator\n+(LCG), which consists of multiplying the 128-bit state with a fixed\n+multiplication constant and then adding a user-chosen increment, in 128-bit\n+modular arithmetic. LCGs are well-analyzed PRNGs with known weaknesses, though\n+128-bit LCGs are large enough to pass stringent statistical tests on their own,\n+with only the trivial output function. The output function of `PCG64` is\n+intended to patch up some of those known weaknesses by doing \"just enough\"\n+scrambling of the bits to assist in the statistical properties without adding\n+too much computational cost.\n+\n+One of these known weaknesses is that advancing the state of the LCG by steps\n+numbering a power of two (``bg.advance(2**N)``) will leave the lower ``N`` bits\n+identical to the state that was just left. For a single stream drawn from\n+sequentially, this is of little consequence. The remaining :math:`128-N` bits provide\n+plenty of pseudorandomness that will be mixed in for any practical ``N`` that can\n+be observed in a single stream, which is why one does not need to worry about\n+this if you only use a single stream in your application. Similarly, the\n+`PCG64.jumped` method uses a carefully chosen number of steps to avoid creating\n+these collisions. However, once you start creating \"randomly-initialized\"\n+parallel streams, either using OS entropy by calling `default_rng` repeatedly\n+or using `SeedSequence.spawn`, then we need to consider how many lower bits\n+need to \"collide\" in order to create a bad pair of streams, and then evaluate\n+the probability of creating such a collision.\n+`Empirically <https://github.com/numpy/numpy/issues/16313>`_, it has been\n+determined that if one shares the lower 58 bits of state and shares an\n+increment, then the pair of streams, when interleaved, will fail \n+`PractRand <http://pracrand.sourceforge.net/>`_ in\n+a reasonable amount of time, after drawing a few gigabytes of data. Following\n+the standard Birthday Paradox calculations for a collision of 58 bits, we can\n+see that we can create :math:`2^{29}`, or about half a billion, streams which is when\n+the probability of such a collision becomes high. Half a billion streams is\n+quite high, and the amount of data each stream needs to draw before the\n+statistical correlations become apparent to even the strict ``PractRand`` tests\n+is in the gigabytes. But this is on the horizon for very large applications\n+like distributed reinforcement learning. There are reasons to expect that even\n+in these applications a collision probably will not have a practical effect in\n+the total result, since the statistical problem is constrained to just the\n+colliding pair.\n+\n+Now, let us consider the case when the increment is not constrained to be the\n+same. Our implementation of `PCG64` seeds both the state and the increment;\n+that is, two calls to `default_rng` (almost certainly) have different states\n+and increments. Upon our first release, we believed that having the seeded\n+increment would provide a certain amount of extra protection, that one would\n+have to be \"close\" in both the state space and increment space in order to\n+observe correlations (``PractRand`` failures) in a pair of streams. If that were\n+true, then the \"bottleneck\" for collisions would be the 128-bit entropy pool\n+size inside of `SeedSequence` (and 128-bit collisions are in the\n+\"preposterously unlikely\" category). Unfortunately, this is not true.\n+\n+One of the known properties of an LCG is that different increments create\n+*distinct* streams, but with a known relationship. Each LCG has an orbit that\n+traverses all :math:`2^{128}` different 128-bit states. Two LCGs with different\n+increments are related in that one can \"rotate\" the orbit of the first LCG\n+(advance it by a number of steps that we can compute from the two increments)\n+such that then both LCGs will always then have the same state, up to an\n+additive constant and maybe an inversion of the bits. If you then iterate both\n+streams in lockstep, then the states will *always* remain related by that same\n+additive constant (and the inversion, if present). Recall that `PCG64` is\n+constructed from both a transition function (the LCG) and an output function.\n+It was expected that the scrambling effect of the output function would have\n+been strong enough to make the distinct streams practically independent (i.e.\n+\"passing the ``PractRand`` tests\") unless the two increments were\n+pathologically related to each other (e.g. 1 and 3). The output function XSL-RR\n+of the then-standard PCG algorithm that we implemented in `PCG64` turns out to\n+be too weak to cover up for the 58-bit collision of the underlying LCG that we\n+described above. For any given pair of increments, the size of the \"colliding\"\n+space of states is the same, so for this weakness, the extra distinctness\n+provided by the increments does not translate into extra protection from\n+statistical correlations that ``PractRand`` can detect.\n+\n+Fortunately, strengthening the output function is able to correct this weakness\n+and *does* turn the extra distinctness provided by differing increments into\n+additional protection from these low-bit collisions. To the `PCG author's\n+credit <https://github.com/numpy/numpy/issues/13635#issuecomment-506088698>`_,\n+she had developed a stronger output function in response to related discussions\n+during the long birth of the new `BitGenerator` system. We NumPy developers\n+chose to be \"conservative\" and use the XSL-RR variant that had undergone\n+a longer period of testing at that time. The DXSM output function adopts\n+a \"xorshift-multiply\" construction used in strong integer hashes that has much\n+better avalanche properties than the XSL-RR output function. While there are\n+\"pathological\" pairs of increments that induce \"bad\" additive constants that\n+relate the two streams, the vast majority of pairs induce \"good\" additive\n+constants that make the merely-distinct streams of LCG states into\n+practically-independent output streams. Indeed, now the claim we once made\n+about `PCG64` is actually true of `PCG64DXSM`: collisions are possible, but\n+both streams have to simultaneously be both \"close\" in the 128 bit state space\n+*and* \"close\" in the 127-bit increment space, so that would be less likely than\n+the negligible chance of colliding in the 128-bit internal `SeedSequence` pool.\n+The DXSM output function is more computationally intensive than XSL-RR, but\n+some optimizations in the LCG more than make up for the performance hit on most\n+machines, so `PCG64DXSM` is a good, safe upgrade. There are, of course, an\n+infinite number of stronger output functions that one could consider, but most\n+will have a greater computational cost, and the DXSM output function has now\n+received many CPU cycles of testing via ``PractRand`` at this time."
            },
            {
                "filename": "numpy/random/__init__.py",
                "patch": "@@ -17,6 +17,7 @@\n --------------------------------------------- ---\n MT19937\n PCG64\n+PCG64DXSM\n Philox\n SFC64\n ============================================= ===\n@@ -183,13 +184,14 @@\n from ._generator import Generator, default_rng\n from .bit_generator import SeedSequence, BitGenerator\n from ._mt19937 import MT19937\n-from ._pcg64 import PCG64\n+from ._pcg64 import PCG64, PCG64DXSM\n from ._philox import Philox\n from ._sfc64 import SFC64\n from .mtrand import *\n \n __all__ += ['Generator', 'RandomState', 'SeedSequence', 'MT19937',\n-            'Philox', 'PCG64', 'SFC64', 'default_rng', 'BitGenerator']\n+            'Philox', 'PCG64', 'PCG64DXSM', 'SFC64', 'default_rng',\n+            'BitGenerator']\n \n \n def __RandomState_ctor():"
            },
            {
                "filename": "numpy/random/__init__.pyi",
                "patch": "@@ -3,7 +3,10 @@ from typing import List\n from numpy.random._generator import Generator as Generator\n from numpy.random._generator import default_rng as default_rng\n from numpy.random._mt19937 import MT19937 as MT19937\n-from numpy.random._pcg64 import PCG64 as PCG64\n+from numpy.random._pcg64 import (\n+    PCG64 as PCG64,\n+    PCG64DXSM as PCG64DXSM,\n+)\n from numpy.random._philox import Philox as Philox\n from numpy.random._sfc64 import SFC64 as SFC64\n from numpy.random.bit_generator import BitGenerator as BitGenerator"
            },
            {
                "filename": "numpy/random/_pcg64.pyi",
                "patch": "@@ -32,3 +32,17 @@ class PCG64(BitGenerator):\n         value: _PCG64State,\n     ) -> None: ...\n     def advance(self, delta: int) -> PCG64: ...\n+\n+class PCG64DXSM(BitGenerator):\n+    def __init__(self, seed: Union[None, _ArrayLikeInt_co, SeedSequence] = ...) -> None: ...\n+    def jumped(self, jumps: int = ...) -> PCG64DXSM: ...\n+    @property\n+    def state(\n+        self,\n+    ) -> _PCG64State: ...\n+    @state.setter\n+    def state(\n+        self,\n+        value: _PCG64State,\n+    ) -> None: ...\n+    def advance(self, delta: int) -> PCG64DXSM: ..."
            },
            {
                "filename": "numpy/random/_pcg64.pyx",
                "patch": "@@ -26,6 +26,10 @@ cdef extern from \"src/pcg64/pcg64.h\":\n     void pcg64_get_state(pcg64_state *state, uint64_t *state_arr, int *has_uint32, uint32_t *uinteger)\n     void pcg64_set_state(pcg64_state *state, uint64_t *state_arr, int has_uint32, uint32_t uinteger)\n \n+    uint64_t pcg64_cm_next64(pcg64_state *state)  nogil\n+    uint32_t pcg64_cm_next32(pcg64_state *state)  nogil\n+    void pcg64_cm_advance(pcg64_state *state, uint64_t *step)\n+\n cdef uint64_t pcg64_uint64(void* st) nogil:\n     return pcg64_next64(<pcg64_state *>st)\n \n@@ -35,6 +39,14 @@ cdef uint32_t pcg64_uint32(void *st) nogil:\n cdef double pcg64_double(void* st) nogil:\n     return uint64_to_double(pcg64_next64(<pcg64_state *>st))\n \n+cdef uint64_t pcg64_cm_uint64(void* st) nogil:\n+    return pcg64_cm_next64(<pcg64_state *>st)\n+\n+cdef uint32_t pcg64_cm_uint32(void *st) nogil:\n+    return pcg64_cm_next32(<pcg64_state *> st)\n+\n+cdef double pcg64_cm_double(void* st) nogil:\n+    return uint64_to_double(pcg64_cm_next64(<pcg64_state *>st))\n \n cdef class PCG64(BitGenerator):\n     \"\"\"\n@@ -268,3 +280,239 @@ cdef class PCG64(BitGenerator):\n         pcg64_advance(&self.rng_state, <uint64_t *>np.PyArray_DATA(d))\n         self._reset_state_variables()\n         return self\n+\n+\n+cdef class PCG64DXSM(BitGenerator):\n+    \"\"\"\n+    PCG64DXSM(seed=None)\n+\n+    BitGenerator for the PCG-64 DXSM pseudo-random number generator.\n+\n+    Parameters\n+    ----------\n+    seed : {None, int, array_like[ints], SeedSequence}, optional\n+        A seed to initialize the `BitGenerator`. If None, then fresh,\n+        unpredictable entropy will be pulled from the OS. If an ``int`` or\n+        ``array_like[ints]`` is passed, then it will be passed to\n+        `SeedSequence` to derive the initial `BitGenerator` state. One may also\n+        pass in a `SeedSequence` instance.\n+\n+    Notes\n+    -----\n+    PCG-64 DXSM is a 128-bit implementation of O'Neill's permutation congruential\n+    generator ([1]_, [2]_). PCG-64 DXSM has a period of :math:`2^{128}` and supports\n+    advancing an arbitrary number of steps as well as :math:`2^{127}` streams.\n+    The specific member of the PCG family that we use is PCG CM DXSM 128/64. It\n+    differs from ``PCG64`` in that it uses the stronger DXSM output function,\n+    a 64-bit \"cheap multiplier\" in the LCG, and outputs from the state before\n+    advancing it rather than advance-then-output.\n+\n+    ``PCG64DXSM`` provides a capsule containing function pointers that produce\n+    doubles, and unsigned 32 and 64- bit integers. These are not\n+    directly consumable in Python and must be consumed by a ``Generator``\n+    or similar object that supports low-level access.\n+\n+    Supports the method :meth:`advance` to advance the RNG an arbitrary number of\n+    steps. The state of the PCG-64 DXSM RNG is represented by 2 128-bit unsigned\n+    integers.\n+\n+    **State and Seeding**\n+\n+    The ``PCG64DXSM`` state vector consists of 2 unsigned 128-bit values,\n+    which are represented externally as Python ints. One is the state of the\n+    PRNG, which is advanced by a linear congruential generator (LCG). The\n+    second is a fixed odd increment used in the LCG.\n+\n+    The input seed is processed by `SeedSequence` to generate both values. The\n+    increment is not independently settable.\n+\n+    **Parallel Features**\n+\n+    The preferred way to use a BitGenerator in parallel applications is to use\n+    the `SeedSequence.spawn` method to obtain entropy values, and to use these\n+    to generate new BitGenerators:\n+\n+    >>> from numpy.random import Generator, PCG64DXSM, SeedSequence\n+    >>> sg = SeedSequence(1234)\n+    >>> rg = [Generator(PCG64DXSM(s)) for s in sg.spawn(10)]\n+\n+    **Compatibility Guarantee**\n+\n+    ``PCG64DXSM`` makes a guarantee that a fixed seed and will always produce\n+    the same random integer stream.\n+\n+    References\n+    ----------\n+    .. [1] `\"PCG, A Family of Better Random Number Generators\"\n+           <http://www.pcg-random.org/>`_\n+    .. [2] O'Neill, Melissa E. `\"PCG: A Family of Simple Fast Space-Efficient\n+           Statistically Good Algorithms for Random Number Generation\"\n+           <https://www.cs.hmc.edu/tr/hmc-cs-2014-0905.pdf>`_\n+    \"\"\"\n+    cdef pcg64_state rng_state\n+    cdef pcg64_random_t pcg64_random_state\n+\n+    def __init__(self, seed=None):\n+        BitGenerator.__init__(self, seed)\n+        self.rng_state.pcg_state = &self.pcg64_random_state\n+\n+        self._bitgen.state = <void *>&self.rng_state\n+        self._bitgen.next_uint64 = &pcg64_cm_uint64\n+        self._bitgen.next_uint32 = &pcg64_cm_uint32\n+        self._bitgen.next_double = &pcg64_cm_double\n+        self._bitgen.next_raw = &pcg64_cm_uint64\n+        # Seed the _bitgen\n+        val = self._seed_seq.generate_state(4, np.uint64)\n+        pcg64_set_seed(&self.rng_state,\n+                       <uint64_t *>np.PyArray_DATA(val),\n+                       (<uint64_t *>np.PyArray_DATA(val) + 2))\n+        self._reset_state_variables()\n+\n+    cdef _reset_state_variables(self):\n+        self.rng_state.has_uint32 = 0\n+        self.rng_state.uinteger = 0\n+\n+    cdef jump_inplace(self, jumps):\n+        \"\"\"\n+        Jump state in-place\n+        Not part of public API\n+\n+        Parameters\n+        ----------\n+        jumps : integer, positive\n+            Number of times to jump the state of the rng.\n+\n+        Notes\n+        -----\n+        The step size is phi-1 when multiplied by 2**128 where phi is the\n+        golden ratio.\n+        \"\"\"\n+        step = 0x9e3779b97f4a7c15f39cc0605cedc835\n+        self.advance(step * int(jumps))\n+\n+    def jumped(self, jumps=1):\n+        \"\"\"\n+        jumped(jumps=1)\n+\n+        Returns a new bit generator with the state jumped.\n+\n+        Jumps the state as-if jumps * 210306068529402873165736369884012333109\n+        random numbers have been generated.\n+\n+        Parameters\n+        ----------\n+        jumps : integer, positive\n+            Number of times to jump the state of the bit generator returned\n+\n+        Returns\n+        -------\n+        bit_generator : PCG64DXSM\n+            New instance of generator jumped iter times\n+\n+        Notes\n+        -----\n+        The step size is phi-1 when multiplied by 2**128 where phi is the\n+        golden ratio.\n+        \"\"\"\n+        cdef PCG64DXSM bit_generator\n+\n+        bit_generator = self.__class__()\n+        bit_generator.state = self.state\n+        bit_generator.jump_inplace(jumps)\n+\n+        return bit_generator\n+\n+    @property\n+    def state(self):\n+        \"\"\"\n+        Get or set the PRNG state\n+\n+        Returns\n+        -------\n+        state : dict\n+            Dictionary containing the information required to describe the\n+            state of the PRNG\n+        \"\"\"\n+        cdef np.ndarray state_vec\n+        cdef int has_uint32\n+        cdef uint32_t uinteger\n+\n+        # state_vec is state.high, state.low, inc.high, inc.low\n+        state_vec = <np.ndarray>np.empty(4, dtype=np.uint64)\n+        pcg64_get_state(&self.rng_state,\n+                        <uint64_t *>np.PyArray_DATA(state_vec),\n+                        &has_uint32, &uinteger)\n+        state = int(state_vec[0]) * 2**64 + int(state_vec[1])\n+        inc = int(state_vec[2]) * 2**64 + int(state_vec[3])\n+        return {'bit_generator': self.__class__.__name__,\n+                'state': {'state': state, 'inc': inc},\n+                'has_uint32': has_uint32,\n+                'uinteger': uinteger}\n+\n+    @state.setter\n+    def state(self, value):\n+        cdef np.ndarray state_vec\n+        cdef int has_uint32\n+        cdef uint32_t uinteger\n+        if not isinstance(value, dict):\n+            raise TypeError('state must be a dict')\n+        bitgen = value.get('bit_generator', '')\n+        if bitgen != self.__class__.__name__:\n+            raise ValueError('state must be for a {0} '\n+                             'RNG'.format(self.__class__.__name__))\n+        state_vec = <np.ndarray>np.empty(4, dtype=np.uint64)\n+        state_vec[0] = value['state']['state'] // 2 ** 64\n+        state_vec[1] = value['state']['state'] % 2 ** 64\n+        state_vec[2] = value['state']['inc'] // 2 ** 64\n+        state_vec[3] = value['state']['inc'] % 2 ** 64\n+        has_uint32 = value['has_uint32']\n+        uinteger = value['uinteger']\n+        pcg64_set_state(&self.rng_state,\n+                        <uint64_t *>np.PyArray_DATA(state_vec),\n+                        has_uint32, uinteger)\n+\n+    def advance(self, delta):\n+        \"\"\"\n+        advance(delta)\n+\n+        Advance the underlying RNG as-if delta draws have occurred.\n+\n+        Parameters\n+        ----------\n+        delta : integer, positive\n+            Number of draws to advance the RNG. Must be less than the\n+            size state variable in the underlying RNG.\n+\n+        Returns\n+        -------\n+        self : PCG64\n+            RNG advanced delta steps\n+\n+        Notes\n+        -----\n+        Advancing a RNG updates the underlying RNG state as-if a given\n+        number of calls to the underlying RNG have been made. In general\n+        there is not a one-to-one relationship between the number output\n+        random values from a particular distribution and the number of\n+        draws from the core RNG.  This occurs for two reasons:\n+\n+        * The random values are simulated using a rejection-based method\n+          and so, on average, more than one value from the underlying\n+          RNG is required to generate an single draw.\n+        * The number of bits required to generate a simulated value\n+          differs from the number of bits generated by the underlying\n+          RNG.  For example, two 16-bit integer values can be simulated\n+          from a single draw of a 32-bit RNG.\n+\n+        Advancing the RNG state resets any pre-computed random numbers.\n+        This is required to ensure exact reproducibility.\n+        \"\"\"\n+        delta = wrap_int(delta, 128)\n+\n+        cdef np.ndarray d = np.empty(2, dtype=np.uint64)\n+        d[0] = delta // 2**64\n+        d[1] = delta % 2**64\n+        pcg64_cm_advance(&self.rng_state, <uint64_t *>np.PyArray_DATA(d))\n+        self._reset_state_variables()\n+        return self\n+"
            },
            {
                "filename": "numpy/random/_pickle.py",
                "patch": "@@ -1,13 +1,14 @@\n from .mtrand import RandomState\n from ._philox import Philox\n-from ._pcg64 import PCG64\n+from ._pcg64 import PCG64, PCG64DXSM\n from ._sfc64 import SFC64\n \n from ._generator import Generator\n from ._mt19937 import MT19937\n \n BitGenerators = {'MT19937': MT19937,\n                  'PCG64': PCG64,\n+                 'PCG64DXSM': PCG64DXSM,\n                  'Philox': Philox,\n                  'SFC64': SFC64,\n                  }"
            },
            {
                "filename": "numpy/random/src/pcg64/pcg64.c",
                "patch": "@@ -61,6 +61,10 @@ pcg_setseq_128_xsl_rr_64_boundedrand_r(pcg_state_setseq_128 *rng,\n                                        uint64_t bound);\n extern inline void pcg_setseq_128_advance_r(pcg_state_setseq_128 *rng,\n                                             pcg128_t delta);\n+extern inline uint64_t pcg_cm_random_r(pcg_state_setseq_128 *rng);\n+extern inline void pcg_cm_step_r(pcg_state_setseq_128 *rng);\n+extern inline uint64_t pcg_output_cm_128_64(pcg128_t state);\n+extern inline void pcg_cm_srandom_r(pcg_state_setseq_128 *rng, pcg128_t initstate, pcg128_t initseq);\n \n /* Multi-step advance functions (jump-ahead, jump-back)\n  *\n@@ -117,6 +121,9 @@ pcg128_t pcg_advance_lcg_128(pcg128_t state, pcg128_t delta, pcg128_t cur_mult,\n extern inline uint64_t pcg64_next64(pcg64_state *state);\n extern inline uint32_t pcg64_next32(pcg64_state *state);\n \n+extern inline uint64_t pcg64_cm_next64(pcg64_state *state);\n+extern inline uint32_t pcg64_cm_next32(pcg64_state *state);\n+\n extern void pcg64_advance(pcg64_state *state, uint64_t *step) {\n   pcg128_t delta;\n #ifndef PCG_EMULATED_128BIT_MATH\n@@ -128,6 +135,17 @@ extern void pcg64_advance(pcg64_state *state, uint64_t *step) {\n   pcg64_advance_r(state->pcg_state, delta);\n }\n \n+extern void pcg64_cm_advance(pcg64_state *state, uint64_t *step) {\n+  pcg128_t delta;\n+#ifndef PCG_EMULATED_128BIT_MATH\n+  delta = (((pcg128_t)step[0]) << 64) | step[1];\n+#else\n+  delta.high = step[0];\n+  delta.low = step[1];\n+#endif\n+  pcg_cm_advance_r(state->pcg_state, delta);\n+}\n+\n extern void pcg64_set_seed(pcg64_state *state, uint64_t *seed, uint64_t *inc) {\n   pcg128_t s, i;\n #ifndef PCG_EMULATED_128BIT_MATH"
            },
            {
                "filename": "numpy/random/src/pcg64/pcg64.h",
                "patch": "@@ -104,6 +104,9 @@ typedef struct {\n     , PCG_128BIT_CONSTANT(0x0000000000000001ULL, 0xda3e39cb94b95bdbULL)        \\\n   }\n \n+#define PCG_CHEAP_MULTIPLIER_128 (0xda942042e4dd58b5ULL)\n+\n+\n static inline uint64_t pcg_rotr_64(uint64_t value, unsigned int rot) {\n #ifdef _WIN32\n   return _rotr64(value, rot);\n@@ -198,6 +201,62 @@ pcg_setseq_128_xsl_rr_64_random_r(pcg_state_setseq_128 *rng) {\n #endif\n }\n \n+static inline pcg128_t pcg128_mult_64(pcg128_t a, uint64_t b) {\n+  uint64_t h1;\n+  pcg128_t result;\n+\n+  h1 = a.high * b;\n+  _pcg_mult64(a.low, b, &(result.high), &(result.low));\n+  result.high += h1;\n+  return result;\n+}\n+\n+static inline void pcg_cm_step_r(pcg_state_setseq_128 *rng) {\n+#if defined _WIN32 && _MSC_VER >= 1900 && _M_AMD64\n+  uint64_t h1;\n+  pcg128_t product;\n+\n+  /* Manually inline the multiplication and addition using intrinsics */\n+  h1 = rng->state.high * PCG_CHEAP_MULTIPLIER_128;\n+  product.low =\n+      _umul128(rng->state.low, PCG_CHEAP_MULTIPLIER_128, &(product.high));\n+  product.high += h1;\n+  _addcarry_u64(_addcarry_u64(0, product.low, rng->inc.low, &(rng->state.low)),\n+                product.high, rng->inc.high, &(rng->state.high));\n+#else\n+  rng->state = pcg128_add(pcg128_mult_64(rng->state, PCG_CHEAP_MULTIPLIER_128),\n+                           rng->inc);\n+#endif\n+}\n+\n+static inline uint64_t pcg_output_cm_128_64(pcg128_t state) {\n+  uint64_t hi = state.high;\n+  uint64_t lo = state.low;\n+\n+  lo |= 1;\n+  hi ^= hi >> 32;\n+  hi *= 0xda942042e4dd58b5ULL;\n+  hi ^= hi >> 48;\n+  hi *= lo;\n+  return hi;\n+}\n+\n+static inline void pcg_cm_srandom_r(pcg_state_setseq_128 *rng, pcg128_t initstate, pcg128_t initseq) {\n+  rng->state = PCG_128BIT_CONSTANT(0ULL, 0ULL);\n+  rng->inc.high = initseq.high << 1u;\n+  rng->inc.high |= initseq.low >> 63u;\n+  rng->inc.low = (initseq.low << 1u) | 1u;\n+  pcg_cm_step_r(rng);\n+  rng->state = pcg128_add(rng->state, initstate);\n+  pcg_cm_step_r(rng);\n+}\n+\n+static inline uint64_t pcg_cm_random_r(pcg_state_setseq_128* rng)\n+{\n+    uint64_t ret = pcg_output_cm_128_64(rng->state);\n+    pcg_cm_step_r(rng);\n+    return ret;\n+}\n #else /* PCG_EMULATED_128BIT_MATH */\n \n static inline void pcg_setseq_128_step_r(pcg_state_setseq_128 *rng) {\n@@ -209,6 +268,37 @@ static inline uint64_t pcg_output_xsl_rr_128_64(pcg128_t state) {\n                      state >> 122u);\n }\n \n+static inline void pcg_cm_step_r(pcg_state_setseq_128 *rng) {\n+  rng-> state = rng->state * PCG_CHEAP_MULTIPLIER_128 + rng->inc;\n+}\n+\n+static inline uint64_t pcg_output_cm_128_64(pcg128_t state) {\n+  uint64_t hi = state >> 64;\n+  uint64_t lo = state;\n+\n+  lo |= 1;\n+  hi ^= hi >> 32;\n+  hi *= 0xda942042e4dd58b5ULL;\n+  hi ^= hi >> 48;\n+  hi *= lo;\n+  return hi;\n+}\n+\n+static inline void pcg_cm_srandom_r(pcg_state_setseq_128 *rng, pcg128_t initstate, pcg128_t initseq) {\n+  rng->state = 0U;\n+  rng->inc = (initseq << 1u) | 1u;\n+  pcg_cm_step_r(rng);\n+  rng->state += initstate;\n+  pcg_cm_step_r(rng);\n+}\n+\n+static inline uint64_t pcg_cm_random_r(pcg_state_setseq_128* rng)\n+{\n+    uint64_t ret = pcg_output_cm_128_64(rng->state);\n+    pcg_cm_step_r(rng);\n+    return ret;\n+}\n+\n static inline uint64_t\n pcg_setseq_128_xsl_rr_64_random_r(pcg_state_setseq_128* rng)\n {\n@@ -248,6 +338,12 @@ static inline void pcg_setseq_128_advance_r(pcg_state_setseq_128 *rng,\n                                    PCG_DEFAULT_MULTIPLIER_128, rng->inc);\n }\n \n+static inline void pcg_cm_advance_r(pcg_state_setseq_128 *rng, pcg128_t delta) {\n+    rng->state = pcg_advance_lcg_128(rng->state, delta,\n+                                     PCG_128BIT_CONSTANT(0, PCG_CHEAP_MULTIPLIER_128),\n+                                     rng->inc);\n+}\n+\n typedef pcg_state_setseq_128 pcg64_random_t;\n #define pcg64_random_r pcg_setseq_128_xsl_rr_64_random_r\n #define pcg64_boundedrand_r pcg_setseq_128_xsl_rr_64_boundedrand_r\n@@ -281,7 +377,24 @@ static inline uint32_t pcg64_next32(pcg64_state *state) {\n   return (uint32_t)(next & 0xffffffff);\n }\n \n+static inline uint64_t pcg64_cm_next64(pcg64_state *state) {\n+  return pcg_cm_random_r(state->pcg_state);\n+}\n+\n+static inline uint32_t pcg64_cm_next32(pcg64_state *state) {\n+  uint64_t next;\n+  if (state->has_uint32) {\n+    state->has_uint32 = 0;\n+    return state->uinteger;\n+  }\n+  next = pcg_cm_random_r(state->pcg_state);\n+  state->has_uint32 = 1;\n+  state->uinteger = (uint32_t)(next >> 32);\n+  return (uint32_t)(next & 0xffffffff);\n+}\n+\n void pcg64_advance(pcg64_state *state, uint64_t *step);\n+void pcg64_cm_advance(pcg64_state *state, uint64_t *step);\n \n void pcg64_set_seed(pcg64_state *state, uint64_t *seed, uint64_t *inc);\n "
            },
            {
                "filename": "numpy/random/tests/data/pcg64dxsm-testset-1.csv",
                "patch": "@@ -0,0 +1,1001 @@\n+seed, 0xdeadbeaf\n+0, 0xdf1ddcf1e22521fe\n+1, 0xc71b2f9c706cf151\n+2, 0x6922a8cc24ad96b2\n+3, 0x82738c549beccc30\n+4, 0x5e8415cdb1f17580\n+5, 0x64c54ad0c09cb43\n+6, 0x361a17a607dce278\n+7, 0x4346f6afb7acad68\n+8, 0x6e9f14d4f6398d6b\n+9, 0xf818d4343f8ed822\n+10, 0x6327647daf508ed6\n+11, 0xe1d1dbe5496a262a\n+12, 0xfc081e619076b2e0\n+13, 0x37126563a956ab1\n+14, 0x8bb46e155db16b9\n+15, 0x56449f006c9f3fb4\n+16, 0x34a9273550941803\n+17, 0x5b4df62660f99462\n+18, 0xb8665cad532e3018\n+19, 0x72fc3e5f7f84216a\n+20, 0x71d3c47f6fd59939\n+21, 0xfd4218afa1de463b\n+22, 0xc84054c78e0a9a71\n+23, 0xae59034726be61a8\n+24, 0xa6a5f21de983654d\n+25, 0x3b633acf572009da\n+26, 0x6a0884f347ab54c8\n+27, 0x7a907ebe9adcab50\n+28, 0xbe779be53d7b8d4a\n+29, 0xf5976e8c69b9dcd1\n+30, 0x1d8302f114699e11\n+31, 0x7d37e43042c038a0\n+32, 0x2cc1d4edc2a40f35\n+33, 0x83e3347bb2d581f1\n+34, 0x253f8698651a844d\n+35, 0x4312dea0dd4e32f6\n+36, 0x10f106439964ea3a\n+37, 0x810eb374844868cc\n+38, 0x366342a54b1978cc\n+39, 0x9fb39b13aaddfb5e\n+40, 0xdb91fd0d9482bed7\n+41, 0x89f6ea4ca9c68204\n+42, 0x146b31ccca461792\n+43, 0x203fd9724deb2486\n+44, 0x58a84f23748e25cb\n+45, 0x2f20eb6aeb94e88\n+46, 0x14d3581460e473c\n+47, 0xad5bd0d25f37d047\n+48, 0x1cf88fa16de258b2\n+49, 0x3bcab6485b7a341\n+50, 0xb2433b37f227d90c\n+51, 0x2cffd7e0a8360cc8\n+52, 0x5d2eeff7c9ebc847\n+53, 0x6fd7c7ae23f9f64b\n+54, 0x381650b2d00f175d\n+55, 0x9d93edcedc873cae\n+56, 0x56e369a033d4cb49\n+57, 0x7547997116a3bac\n+58, 0x11debaa897fd4665\n+59, 0xdf799d2b73bd6fb8\n+60, 0x3747d299c66624d\n+61, 0xac9346701afd0cfa\n+62, 0xac90e150fa13c7bf\n+63, 0x85c56ad2248c2871\n+64, 0xdea66bf35c45f195\n+65, 0x59cf910ea079fb74\n+66, 0x2f841bb782274586\n+67, 0x9814df4384d92bd9\n+68, 0x15bc70824be09925\n+69, 0x16d4d0524c0503a3\n+70, 0xf04ea249135c0cc7\n+71, 0xa707ab509b7e3032\n+72, 0x465459efa869e372\n+73, 0x64cbf70a783fab67\n+74, 0x36b3541a14ca8ed7\n+75, 0x9a4dfae8f4c596bf\n+76, 0x11d9a04224281be3\n+77, 0xe09bbe6d5e98ec32\n+78, 0xa6c60d908973aa0d\n+79, 0x7c524c57dd5915c8\n+80, 0xa810c170b27f1fdc\n+81, 0xce5d409819621583\n+82, 0xfe2ee3d5332a3525\n+83, 0x162fb7c8b32045eb\n+84, 0x4a3327156b0b2d83\n+85, 0x808d0282f971064\n+86, 0x2e6f04cf5ed27e60\n+87, 0xaf6800699cca67a9\n+88, 0xc7590aae7244c3bf\n+89, 0x7824345f4713f5f9\n+90, 0x8f713505f8fd059b\n+91, 0x3d5b5b9bb6b1e80e\n+92, 0x8674f45e5dc40d79\n+93, 0xcb1e36846aa14773\n+94, 0xe0ae45b2b9b778c1\n+95, 0xd7254ce931eefcfb\n+96, 0xef34e15e4f55ac0a\n+97, 0xf17cc0ba15a99bc4\n+98, 0x77bb0f7ffe7b31f1\n+99, 0x6ee86438d2e71d38\n+100, 0x584890f86829a455\n+101, 0x7baf0d8d30ba70fe\n+102, 0xb1ac8f326b8403ae\n+103, 0xcc1963435c874ba7\n+104, 0x9c483b953d1334ce\n+105, 0xc0924bcbf3e10941\n+106, 0x21bcc581558717b1\n+107, 0x2c5ad1623f8d292b\n+108, 0xa8ea110f6124557e\n+109, 0x15f24a6c5c4c591\n+110, 0x40fe0d9cd7629126\n+111, 0xcfe8f2b3b081484d\n+112, 0x891383f4b4cac284\n+113, 0x76f2fcdef7fa845\n+114, 0x4edd12133aed0584\n+115, 0xd53c06d12308873d\n+116, 0xf7f22882c17f86bf\n+117, 0xfbaa4aad72f35e10\n+118, 0x627610da2e3c0cc3\n+119, 0x582b16a143634d9a\n+120, 0x9b4a7f69ed38f4a0\n+121, 0x2df694974d1e1cbe\n+122, 0xe5be6eaafed5d4b\n+123, 0xc48e2a288ad6605e\n+124, 0xbcb088149ce27c2b\n+125, 0x3cb6a7fb06ceecbe\n+126, 0x516735fff3b9e3ac\n+127, 0x5cbafc551ee5008d\n+128, 0xee27d1ab855c5fd5\n+129, 0xc99fb341f6baf846\n+130, 0x7ad8891b92058e6d\n+131, 0xf50310d03c1ac6c7\n+132, 0x947e281d998cbd3e\n+133, 0x1d4d94a93824fe80\n+134, 0x5568b77289e7ee73\n+135, 0x7d82d1b2b41e3c8b\n+136, 0x1af462c7abc787b\n+137, 0xcfd8dfe80bfae1ef\n+138, 0xd314caeb723a63ea\n+139, 0x1c63ddcfc1145429\n+140, 0x3801b7cc6cbf2437\n+141, 0xc327d5b9fdafddd3\n+142, 0xe140278430ca3c78\n+143, 0x4d0345a685cb6ef8\n+144, 0x47640dc86e261ff9\n+145, 0xab817f158523ebf4\n+146, 0x37c51e35fbe65a6b\n+147, 0xab090f475d30a178\n+148, 0x4d3ec225bf599fc1\n+149, 0xefd517b0041679b1\n+150, 0x20ad50bca4da32c5\n+151, 0x75e1f7cd07fad86d\n+152, 0x348cf781ee655f4b\n+153, 0x9375f0e5ffc2d2ec\n+154, 0x7689082fd5f7279c\n+155, 0x633e56f763561e77\n+156, 0x9d1752d70861f9fd\n+157, 0xa3c994b4e70b0b0f\n+158, 0xabf7276a58701b88\n+159, 0xbfa18d1a0540d000\n+160, 0xc6a28a2475646d26\n+161, 0x7cdf108583f65085\n+162, 0x82dcefb9f32104be\n+163, 0xc6baadd0adc6b446\n+164, 0x7a63cff01075b1b4\n+165, 0x67ac62e575c89919\n+166, 0x96fa4320a0942035\n+167, 0xc4658859385b325f\n+168, 0xde22c17ff47808f6\n+169, 0xbb952c4d89e2f2ec\n+170, 0x638251fbc55bdc37\n+171, 0x38918b307a03b3ea\n+172, 0xccb60f2cedbb570b\n+173, 0x3c06f4086a28f012\n+174, 0x4e8d238388986e33\n+175, 0x1760b7793514a143\n+176, 0xa3f924efe49ee7d6\n+177, 0xaf6be2dbaebc0bdf\n+178, 0x6782682090dffe09\n+179, 0xb63a4d90d848e8ef\n+180, 0x5f649c7eaf4c54c5\n+181, 0xbe57582426a085ba\n+182, 0xb5dd825aa52fb76d\n+183, 0x74cb4e6ca4039617\n+184, 0x382e578bf0a49588\n+185, 0xc043e8ea6e1dcdae\n+186, 0xf902addd5c04fa7c\n+187, 0xf3337994612528db\n+188, 0x4e8fd48d6d15b4e6\n+189, 0x7190a509927c07ab\n+190, 0x864c2dee5b7108ae\n+191, 0xbb9972ddc196f467\n+192, 0x1ea02ab3ca10a448\n+193, 0xe50a8ffde35ddef9\n+194, 0x7bd2f59a67183541\n+195, 0x5a940b30d8fcd27a\n+196, 0x82b4cea62623d4d3\n+197, 0x6fbda76d4afef445\n+198, 0x8b1f6880f418328e\n+199, 0x8b69a025c72c54b7\n+200, 0xb71e0f3986a3835f\n+201, 0xa4a7ddb8b9816825\n+202, 0x945dcda28228b1d8\n+203, 0xb471abf2f8044d72\n+204, 0xf07d4af64742b1ba\n+205, 0xfca5190bc4dd6a2a\n+206, 0xd681497262e11bc5\n+207, 0xbe95d5f00c577028\n+208, 0x56313439fd8bde19\n+209, 0x3f3d9ac9b5ee6522\n+210, 0x7b8d457dd2b49bbe\n+211, 0xe76b5747885d214b\n+212, 0xa8a695b3deb493ea\n+213, 0x5292446548c95d71\n+214, 0xbf5cdf0d436412df\n+215, 0x7936abaed779d28d\n+216, 0x659c6e8073b3a06d\n+217, 0x86c9ff28f5543b71\n+218, 0x6faa748445a99146\n+219, 0xdcc1e6ab57904fd7\n+220, 0x770bd61233addc5f\n+221, 0x16963e041e46d94f\n+222, 0x158e6cb2934157ac\n+223, 0xb65088a8fd246441\n+224, 0x2b12ced6ce8a68c3\n+225, 0x59a18d02cd6082b3\n+226, 0x4ddbc318cb5488ee\n+227, 0x3d4cf520b3ed20a1\n+228, 0x7028b3a92e2b292d\n+229, 0xf141da264a250e4d\n+230, 0x9788d53e86041c37\n+231, 0x1bb91238a7c97dbf\n+232, 0x81953d0ddb634309\n+233, 0xfa39ccfe14d2d46\n+234, 0xf7c7861c9b7e8399\n+235, 0x18d27ca50d9dc249\n+236, 0x258dfdf38510d0d9\n+237, 0x9e72d8af910ea76f\n+238, 0x4f8ef24b96de50ad\n+239, 0xb9d9c12297e03dc9\n+240, 0x91994e41b4a1929c\n+241, 0x8defa79b2ccc83b9\n+242, 0x948566748706dac5\n+243, 0x7b0454946e70e4cf\n+244, 0x340b7cb298c70ed7\n+245, 0x6602005330cebd95\n+246, 0xf71cb803aa61f722\n+247, 0x4683fb07fc70ae8a\n+248, 0xc6db9f0c4de3ed88\n+249, 0x3e8dfae2a593cef9\n+250, 0x615f7c38e3862b33\n+251, 0x676c7996550d857\n+252, 0xc6d520d54a5c266a\n+253, 0x202b1e8eef14aa2e\n+254, 0xa3a84891a27a582\n+255, 0x84dbee451658d47f\n+256, 0x254c7cd97e777e3a\n+257, 0xf50b6e977f0eba50\n+258, 0x2898b1d3062a4798\n+259, 0x4096f7cbbb019773\n+260, 0x9fb8e75548062c50\n+261, 0x4647071e5ca318ec\n+262, 0x2b4750bdb3b3b01\n+263, 0x88ac41cc69a39786\n+264, 0x705e25476ef46fa3\n+265, 0xc0c1db19884a48a6\n+266, 0x1364c0afdbb465e5\n+267, 0x58e98534701272a6\n+268, 0x746a5ea9701517c0\n+269, 0x523a70bc6b300b67\n+270, 0x9b1c098eda8564ad\n+271, 0xfbaeb28d3637067f\n+272, 0xddd9a13551fdba65\n+273, 0x56461a670559e832\n+274, 0xab4fd79be85570ad\n+275, 0xd4b691ecaff8ca55\n+276, 0x11a4495939e7f004\n+277, 0x40d069d19477eb47\n+278, 0xe790783d285cd81e\n+279, 0xde8218b16d935bc7\n+280, 0x2635e8c65cd4182d\n+281, 0xeae402623e3454\n+282, 0x9f99c833184e0279\n+283, 0x3d0f79a0d52d84e7\n+284, 0xc1f8edb10c625b90\n+285, 0x9b4546363d1f0489\n+286, 0x98d86d0b1212a282\n+287, 0x386b53863161200d\n+288, 0xbe1165c7fe48a135\n+289, 0xb9658b04dbbfdc8c\n+290, 0xcea14eddfe84d71a\n+291, 0x55d03298be74abe7\n+292, 0x5be3b50d961ffd7e\n+293, 0xc76b1045dc4b78e1\n+294, 0x7830e3ff3f6c3d4c\n+295, 0xb617adb36ca3729\n+296, 0x4a51bdb194f14aa9\n+297, 0x246024e54e6b682a\n+298, 0x33d42fc9c6d33083\n+299, 0xadccba149f31e1d\n+300, 0x5183e66b9002f8b\n+301, 0x70eb2416404d51b7\n+302, 0x26c25eb225535351\n+303, 0xbc2d5b0d23076561\n+304, 0x5823019ddead1da\n+305, 0x85cfa109fca69f62\n+306, 0x26017933e7e1efd9\n+307, 0x3ec7be9a32212753\n+308, 0x697e8a0697cd6f60\n+309, 0x44735f6cca03920f\n+310, 0x8cc655eb94ee212e\n+311, 0x8b8b74eba84929a0\n+312, 0x7708ccedd0c98c80\n+313, 0x1b6f21f19777cbe1\n+314, 0x363e564bd5fadedb\n+315, 0x5921543a641591fe\n+316, 0xc390786d68ea8a1b\n+317, 0x9b293138dc033fca\n+318, 0x45447ca8dc843345\n+319, 0xee6ef6755bc49c5e\n+320, 0x70a3a1f5163c3be5\n+321, 0xf05e25448b6343b0\n+322, 0x4739f4f8717b7e69\n+323, 0xb006141975bf957\n+324, 0x31874a91b707f452\n+325, 0x3a07f2c90bae2869\n+326, 0xb73dae5499a55c5e\n+327, 0x489070893bb51575\n+328, 0x7129acf423940575\n+329, 0x38c41f4b90130972\n+330, 0xc5260ca65f5a84a1\n+331, 0x6e76194f39563932\n+332, 0x62ca1f9ca3de3ca6\n+333, 0xb4a97874e640853f\n+334, 0x38ed0f71e311cc02\n+335, 0xde183b81099e8f47\n+336, 0x9bb8bf8e6694346\n+337, 0xd15497b6bf81e0f2\n+338, 0xaaae52536c00111\n+339, 0x4e4e60d1435aaafd\n+340, 0x5a15512e5d6ea721\n+341, 0xff0f1ffabfc6664f\n+342, 0xba3ffcedc5f97fec\n+343, 0xef87f391c0c6bfb6\n+344, 0x4a888c5d31eb0f98\n+345, 0x559a3fbfd7946e95\n+346, 0xe45b44a0db5a9bad\n+347, 0x9457898964190af1\n+348, 0xd9357dfaab76cd9e\n+349, 0xa60e907178d965a1\n+350, 0x76b2dc3032dc2f4a\n+351, 0x13549b9c2802120\n+352, 0x8656b965a66a1800\n+353, 0x16802e6e22456a23\n+354, 0x23b62edc60efaa9\n+355, 0x6832a366e1e4ea3b\n+356, 0x46b1b41093ff2b1e\n+357, 0x55c857128143f219\n+358, 0x7fc35ddf5e138200\n+359, 0x790abe78be67467e\n+360, 0xa4446fc08babd466\n+361, 0xc23d70327999b855\n+362, 0x2e019d1597148196\n+363, 0xfefd98e560403ab8\n+364, 0xbe5f0a33da330d58\n+365, 0x3078a4e9d43ca395\n+366, 0x511bfedd6f12f2b3\n+367, 0x8bc138e335be987c\n+368, 0x24640f803465716d\n+369, 0xf6530b04d0bd618f\n+370, 0x9b7833e5aa782716\n+371, 0x778cd35aea5841b1\n+372, 0xecea3c458cefbc60\n+373, 0x5107ae83fc527f46\n+374, 0x278ad83d44bd2d1a\n+375, 0x7014a382295aeb16\n+376, 0xf326dd762048743f\n+377, 0x858633d56279e553\n+378, 0x76408154085f01bc\n+379, 0x3e77d3364d02e746\n+380, 0x2f26cea26cadd50b\n+381, 0x6d6846a4ecb84273\n+382, 0x4847e96f2df5f76\n+383, 0x5a8610f46e13ff61\n+384, 0x4e7a7cac403e10dd\n+385, 0x754bdf2e20c7bc90\n+386, 0x8bdd80e6c51bd0be\n+387, 0x61c655fae2b4bc52\n+388, 0x60873ef48e3d2f03\n+389, 0x9d7d8d3698a0b4a4\n+390, 0xdf48e9c355cd5d4b\n+391, 0x69ecf03e20be99ac\n+392, 0xc1a0c5a339bd1815\n+393, 0x2e3263a6a3adccb\n+394, 0x23557459719adbdc\n+395, 0xd1b709a3b330e5a\n+396, 0xade5ab00a5d88b9d\n+397, 0x69a6bd644120cfad\n+398, 0x40187ecceee92342\n+399, 0x1c41964ba1ac78da\n+400, 0x9ac5c51cbecabe67\n+401, 0xbdc075781cf36d55\n+402, 0xeaf5a32246ded56\n+403, 0xcda0b67e39c0fb71\n+404, 0x4839ee456ef7cc95\n+405, 0xf17092fdd41d5658\n+406, 0x2b5d422e60ae3253\n+407, 0x3effe71102008551\n+408, 0x20a47108e83934b7\n+409, 0xd02da65fe768a88f\n+410, 0xeb046bd56afa4026\n+411, 0x70c0509c08e0fbe0\n+412, 0x1d35c38d4f8bac6c\n+413, 0x9aa8eb6466f392e0\n+414, 0x587bd4a430740f30\n+415, 0x82978fe4bad4195\n+416, 0xdc4ebc4c0feb50ab\n+417, 0xd3b7164d0240c06f\n+418, 0x6e2ad6e5a5003a63\n+419, 0xa24b430e2ee6b59c\n+420, 0x2905f49fd5073094\n+421, 0x5f209e4de03aa941\n+422, 0x57b7da3e0bedb1dc\n+423, 0x5e054018875b01f5\n+424, 0xb2f2da6145658db3\n+425, 0xbd9c94a69a8eb651\n+426, 0x9c5f9a07cd6ac749\n+427, 0x2296c4af4d529c38\n+428, 0x522ed800fafdefab\n+429, 0xe2a447ced0c66791\n+430, 0x937f10d45e455fef\n+431, 0xc882987d9e29a24\n+432, 0x4610bfd6a247ee1a\n+433, 0x562ba3e50870059\n+434, 0x59d8d58793602189\n+435, 0xfe9a606e3e34abe\n+436, 0x6825f7932a5e9282\n+437, 0xe77f7061bab476ad\n+438, 0xbf42001da340ace3\n+439, 0x9c3e9230f5e47960\n+440, 0x2c0f700d96d5ad58\n+441, 0x330048b7cd18f1f9\n+442, 0xffc08785eca5cca9\n+443, 0xb5879046915f07a5\n+444, 0xef51fe26f83c988e\n+445, 0xfa4c2968e7881a9a\n+446, 0xc0a9744455a4aad\n+447, 0xbd2ad686d6313928\n+448, 0x6b9f0984c127682a\n+449, 0xc9aaa00a5da59ed8\n+450, 0x762a0c4b98980dbf\n+451, 0x52d1a2393d3ca2d1\n+452, 0x1e9308f2861db15c\n+453, 0xe7b3c74fe4b4a844\n+454, 0x485e15704a7fc594\n+455, 0x9e7f67ea44c221f6\n+456, 0xbab9ad47fde916e0\n+457, 0x50e383912b7fc1f4\n+458, 0xaad63db8abcef62d\n+459, 0xc2f0c5699f47f013\n+460, 0xee15b36ada826812\n+461, 0x2a1b1cf1e1777142\n+462, 0x8adb03ede79e937d\n+463, 0xf14105ef65643bf3\n+464, 0x752bbaefc374a3c7\n+465, 0xa4980a08a5a21d23\n+466, 0x418a1c05194b2db7\n+467, 0xdd6ff32efe1c3cd6\n+468, 0x272473ed1f0d3aa2\n+469, 0x1e7fdebadabe6c06\n+470, 0xd1baa90c17b3842f\n+471, 0xd3d3a778e9c8404a\n+472, 0x781ae7fda49fa1a0\n+473, 0x61c44fdbdacc672d\n+474, 0x6d447d0a1404f257\n+475, 0x9303e8bdfbfb894d\n+476, 0x3b3482cdec016244\n+477, 0xb149bf245d062e7b\n+478, 0x96f8d54b14cf992d\n+479, 0x4741549a01f8c3d0\n+480, 0x48270811b2992af\n+481, 0x7b58f175cd25d147\n+482, 0x8f19a840b56f4be9\n+483, 0x84a77f43c0951a93\n+484, 0x34e1a69381f0c374\n+485, 0xb158383c9b4040f\n+486, 0x372f1abc7cf3a9fa\n+487, 0x5439819a84571763\n+488, 0xabf8515e9084e2fa\n+489, 0xb02312b9387ff99\n+490, 0x238a85bb47a68b12\n+491, 0x2068cb83857c49bb\n+492, 0xc6170e743083664c\n+493, 0x745cf8470bcb8467\n+494, 0xe3a759a301670300\n+495, 0x292c7686ad3e67da\n+496, 0x359efedaff192a45\n+497, 0x511f2c31a2d8c475\n+498, 0x97fd041bf21c20b3\n+499, 0x25ef1fe841b7b3f6\n+500, 0xbb71739e656f262d\n+501, 0x2729b0e989b6b7b8\n+502, 0xd2142702ec7dbabf\n+503, 0x7008decd2488ee3f\n+504, 0x69daa95e303298d7\n+505, 0xc35eca4efb8baa5a\n+506, 0xf3f16d261cec3b6c\n+507, 0x22371c1d75396bd3\n+508, 0x7aefa08eccae857e\n+509, 0x255b493c5e3c2a2f\n+510, 0x779474a077d34241\n+511, 0x5199c42686bea241\n+512, 0x16c83931e293b8d3\n+513, 0xa57fe8db8c0302c7\n+514, 0xd7ace619e5312eb1\n+515, 0x8740f013306d217c\n+516, 0xb6a1ad5e29f4d453\n+517, 0x31abf7c964688597\n+518, 0xbc3d791daed71e7\n+519, 0x31ee4ca67b7056ed\n+520, 0x1ab5416bfe290ea3\n+521, 0x93db416f6d3b843a\n+522, 0xed83bbe5b1dd2fed\n+523, 0xece38271470d9b6d\n+524, 0x3a620f42663cd8ae\n+525, 0x50c87e02acafee5d\n+526, 0xcabeb8bedbc6dab5\n+527, 0x2880a6d09970c729\n+528, 0x4aba5dd3bfc81bc\n+529, 0xaba54edf41080cec\n+530, 0xb86bb916fc85a169\n+531, 0x4c41de87bc79d8ca\n+532, 0xcce2a202622945fe\n+533, 0x513f086fad94c107\n+534, 0x18b3960c11f8cc96\n+535, 0x2f0d1cfd1896e236\n+536, 0x1702ae3880d79b15\n+537, 0x88923749029ae81\n+538, 0x84810d4bdec668eb\n+539, 0xf85b0a123f4fc68d\n+540, 0x93efd68974b6e4d1\n+541, 0x5d16d6d993a071c9\n+542, 0x94436858f94ca43b\n+543, 0xb3dbb9ed0cb180b6\n+544, 0x6447030a010b8c99\n+545, 0xd7224897c62925d8\n+546, 0xb0c13c1d50605d3a\n+547, 0xdff02c7cb9d45f30\n+548, 0xe8103179f983570d\n+549, 0xbc552037d6d0a24e\n+550, 0x775e500b01486b0d\n+551, 0x2050ac632c694dd6\n+552, 0x218910387c4d7ae7\n+553, 0xf83e8b68ff885d5d\n+554, 0xe3374ec25fca51a3\n+555, 0xfa750ffa3a60f3af\n+556, 0x29ee40ba6df5592e\n+557, 0x70e21a68f48260d2\n+558, 0x3805ca72cd40886e\n+559, 0x2f23e73f8eabf062\n+560, 0x2296f80cdf6531ae\n+561, 0x903099ed968db43a\n+562, 0xf044445cf9f2929f\n+563, 0xcd47fdc2de1b7a1\n+564, 0xaab1cbd4f849da99\n+565, 0x5fc990688da01acb\n+566, 0xa9cee52ea7dab392\n+567, 0xecefc3a4349283a8\n+568, 0xdd6b572972e3fafc\n+569, 0xc1f0b1a2ffb155da\n+570, 0xc30d53fc17bd25c8\n+571, 0x8afa89c77834db28\n+572, 0x5569a596fb32896c\n+573, 0x36f207fc8df3e3d4\n+574, 0x57c2bd58517d81db\n+575, 0xb524693e73d0061c\n+576, 0xb69f6eb233f5c48b\n+577, 0x4f0fb23cab8dc695\n+578, 0x492c1ad0a48df8df\n+579, 0xf6dcc348ec8dec1f\n+580, 0xa4d8708d6eb2e262\n+581, 0x4c2072c2c9766ff1\n+582, 0xa9bf27c4304875f0\n+583, 0xfc8fb8066d4f9ae2\n+584, 0x188095f6235fec3c\n+585, 0x1d8227a2938c2864\n+586, 0x89ea50c599010378\n+587, 0xcac86df0a7c6d56d\n+588, 0x47a8c5df84c7d78\n+589, 0xe607ae24ea228bfa\n+590, 0x36624a7996efe104\n+591, 0x5d72881c1227d810\n+592, 0x78694a6750374c8\n+593, 0x7b9a217d4ab5ff45\n+594, 0xd53e5d6f7504becc\n+595, 0x197a72d3f4889a0e\n+596, 0xfdc70c4755a8df36\n+597, 0xd0fda83748c77f74\n+598, 0x7ddc919ac9d6dcc9\n+599, 0x785c810a6a2dc08b\n+600, 0xba4be83e7e36896c\n+601, 0x379d6fe80cf2bffe\n+602, 0x74cae2dabc429206\n+603, 0x1efac32d5d34c917\n+604, 0x3cb64e2f98d36e70\n+605, 0xc0a7c3cdc3c60aa7\n+606, 0x699dfadd38790ebe\n+607, 0x4861e61b3ecfbeac\n+608, 0x531744826c345baa\n+609, 0x5ec26427ad450cba\n+610, 0xf2c1741479abdcae\n+611, 0xe9328a78b2595458\n+612, 0x30cd1bdf087acd7f\n+613, 0x7491ced4e009adbe\n+614, 0xdcd942df1e2e7023\n+615, 0xfe63f01689fee35\n+616, 0x80282dfe5eaedc42\n+617, 0x6ecdea86495f8427\n+618, 0xe0adfdd5e9ed31c3\n+619, 0xf32bd2a7418127e\n+620, 0x8aabba078db6ee2\n+621, 0xa8a8e60499145aca\n+622, 0xf76b086ac4e8a0f2\n+623, 0x6e55b3c452ff27f8\n+624, 0xe18fa7cd025a71bf\n+625, 0xeed7b685fde0fa25\n+626, 0xba9b6c95867fa721\n+627, 0x4c2603bc69de2df2\n+628, 0xaac87eee1b58cd66\n+629, 0x3c9af6656e01282c\n+630, 0x2dfa05ce8ff476b6\n+631, 0xeae9143fcf92f23d\n+632, 0x3f0699f631be3bc8\n+633, 0xa0f5f79f2492bd67\n+634, 0x59c47722388131ed\n+635, 0x5f6e9d2941cef1de\n+636, 0xe9ad915c09788b7b\n+637, 0x92c6d37e4f9482f5\n+638, 0x57d301b7fdadd911\n+639, 0x7e952d23d2a8443\n+640, 0xbb2fa5e0704b3871\n+641, 0xe5642199be36e2d5\n+642, 0x5020b60d54358291\n+643, 0xa0b6317ec3f60343\n+644, 0xb57b08b99540bc5c\n+645, 0x21f1890adc997a88\n+646, 0xfcf824200dd9da2d\n+647, 0x8146293d83d425d1\n+648, 0xdadfbf5fbb99d420\n+649, 0x1eb9bbc5e6482b7d\n+650, 0xd40ff44f1bbd0f1c\n+651, 0xa9f948ba2d08afa5\n+652, 0x638cc07c5301e601\n+653, 0x1f984baa606e14e8\n+654, 0x44e153671081f398\n+655, 0xb17882eeb1d77a5d\n+656, 0x5fd8dbee995f14c\n+657, 0xff3533e87f81b7fe\n+658, 0x2f44124293c49795\n+659, 0x3bf6b51e9360248\n+660, 0x72d615edf1436371\n+661, 0x8fc5cf4a38adab9d\n+662, 0xfa517e9022078374\n+663, 0xf356733f3e26f4d8\n+664, 0x20ea099cdc6aad40\n+665, 0xe15b977deb37637d\n+666, 0xcc85601b89dae88d\n+667, 0x5768c62f8dd4905c\n+668, 0xa43cc632b4e56ea\n+669, 0xc4240cf980e82458\n+670, 0xb194e8ffb4b3eeb6\n+671, 0xee753cf2219c5fa1\n+672, 0xfe2500192181d44d\n+673, 0x2d03d7d6493dd821\n+674, 0xff0e787bb98e7f9b\n+675, 0xa05cf8d3bd810ce7\n+676, 0x718d5d6dcbbdcd65\n+677, 0x8d0b5343a06931c\n+678, 0xae3a00a932e7eaf9\n+679, 0x7ed3d8f18f983e18\n+680, 0x3bb778ee466dc143\n+681, 0x711c685c4e9062c0\n+682, 0x104c3af5d7ac9834\n+683, 0x17bdbb671fb5d5cf\n+684, 0xabf26caead4d2292\n+685, 0xa45f02866467c005\n+686, 0xf3769a32dc945d2d\n+687, 0xe78d0007f6aabb66\n+688, 0x34b60be4acbd8d4b\n+689, 0x58c0b04b69359084\n+690, 0x3a8bb354c212b1\n+691, 0x6b82a8f3d70058d5\n+692, 0x405bdef80a276a4a\n+693, 0xe20ca40ee9195cad\n+694, 0xf5dd96ba2446fefd\n+695, 0xc1e180c55fe55e3c\n+696, 0xa329caf6daa952b3\n+697, 0xb4809dd0c84a6b0a\n+698, 0xd27f82661070cee7\n+699, 0xa7121f15ee2b0d8a\n+700, 0x4bdaea70d6b34583\n+701, 0xe821dc2f310f7a49\n+702, 0x4c00a5a68e76f647\n+703, 0x331065b064a2d5ea\n+704, 0xac0c2ce3dc04fa37\n+705, 0x56b32b37b8229008\n+706, 0xe757cdb51534fcfa\n+707, 0xd3ff183576b2fad7\n+708, 0x179e1f4190f197a7\n+709, 0xf874c626a7c9aae5\n+710, 0xd58514ffc37c80e4\n+711, 0xc65de31d33fa7fd3\n+712, 0x6f6637052025769b\n+713, 0xca1c6bdadb519cc0\n+714, 0xd1f3534cde37828a\n+715, 0xc858c339eee4830a\n+716, 0x2371eacc215e02f4\n+717, 0x84e5022db85bbbe9\n+718, 0x5f71c50bba48610e\n+719, 0xe420192dad9c323f\n+720, 0x2889342721fca003\n+721, 0x83e64f63334f501d\n+722, 0xac2617172953f2c\n+723, 0xfa1f78d8433938ff\n+724, 0x5578382760051462\n+725, 0x375d7a2e3b90af16\n+726, 0xb93ff44e6c07552d\n+727, 0xded1d5ad811e818c\n+728, 0x7cf256b3b29e3a8c\n+729, 0x78d581b8e7bf95e8\n+730, 0x5b69192f2caa6ad3\n+731, 0xa9e25855a52de3ce\n+732, 0x69d8e8fc45cc188d\n+733, 0x5dd012c139ad347d\n+734, 0xfcb01c07b77db606\n+735, 0x56253e36ab3d1cce\n+736, 0x1181edbb3ea2192\n+737, 0x325bef47ff19a08d\n+738, 0xd3e231ceb27e5f7\n+739, 0x8e819dd2de7956d2\n+740, 0x34a9689fe6f84a51\n+741, 0x3e4eeb719a9c2927\n+742, 0x5c3b3440581d0aaf\n+743, 0x57caf51897d7c920\n+744, 0xec6a458130464b40\n+745, 0xe98f044e0da40e9b\n+746, 0xbe38662020eeb8e7\n+747, 0x7b8c407c632724ae\n+748, 0x16c7cfa97b33a544\n+749, 0xd23359e2e978ae5a\n+750, 0x4fdba458250933dd\n+751, 0x3c9e0713cfe616ba\n+752, 0x6f0df87b13163b42\n+753, 0xc460902cb852cc97\n+754, 0x289df8fefd6b0bce\n+755, 0x4ac2a2a1c3fb8029\n+756, 0x2fc3e24d8b68eef7\n+757, 0x34564386a59aab9a\n+758, 0x31047391ebd67ce4\n+759, 0x6c23d070a0564d41\n+760, 0xba6387b2b72545f7\n+761, 0xcdcf1008058387af\n+762, 0xc9308fa98db05192\n+763, 0xdbdbb5abd01a9d84\n+764, 0x937088275c7804ab\n+765, 0x6f6accfefe34ee81\n+766, 0x5c33c74c49cfdb2c\n+767, 0x5e1a771edfb92bd3\n+768, 0x6e89b009069ecae7\n+769, 0x34d64e17ec0e8968\n+770, 0x841203d0cde0c330\n+771, 0x7642cc9d7eb9e9cb\n+772, 0xca01d2e8c128b97e\n+773, 0x5b8390617b3304ab\n+774, 0x52ec4ed10de1eb2d\n+775, 0xb90f288b9616f237\n+776, 0x5bd43cd49617b2e2\n+777, 0x1a53e21d25230596\n+778, 0x36ccd15207a21cd6\n+779, 0xc8263d780618fd3c\n+780, 0x6eb520598c6ce1cb\n+781, 0x493c99a3b341564f\n+782, 0xab999e9c5aa8764f\n+783, 0xab2fa4ceaba84b\n+784, 0xbbd2f17e5cb2331b\n+785, 0xc8b4d377c0cc4e81\n+786, 0x31f71a6e165c4b1e\n+787, 0xd1011e55fb3addaa\n+788, 0x5f7ec34728dfa59\n+789, 0x2aef59e60a84eb0f\n+790, 0x5dde6f09aec9ad5f\n+791, 0x968c6cdbc0ef0438\n+792, 0x1957133afa15b13a\n+793, 0xbaf28f27573a64c2\n+794, 0xc6f6ddd543ebf862\n+795, 0xdd7534315ec9ae1e\n+796, 0xd2b80cd2758dd3b\n+797, 0xa38c3da00cc81538\n+798, 0x15c95b82d3f9b0f9\n+799, 0x6704930287ce2571\n+800, 0x9c40cc2f6f4ecb0c\n+801, 0xc8de91f50b22e94e\n+802, 0x39272e8fddbfdf0a\n+803, 0x879e0aa810a117d\n+804, 0xa312fff4e9e5f3bd\n+805, 0x10dd747f2835dfec\n+806, 0xeb8466db7171cdae\n+807, 0xaa808d87b9ad040a\n+808, 0xab4d2229a329243a\n+809, 0x7c622f70d46f789c\n+810, 0x5d41cef5965b2a8e\n+811, 0xce97ec4702410d99\n+812, 0x5beba2812c91211b\n+813, 0xf134b46c93a3fec7\n+814, 0x76401d5630127226\n+815, 0xc55fc9d9eacd4ec1\n+816, 0xaec8cefaa12f813f\n+817, 0x2f845dcfd7b00722\n+818, 0x3380ab4c20885921\n+819, 0xdb68ad2597691b74\n+820, 0x8a7e4951455f563f\n+821, 0x2372d007ed761c53\n+822, 0xcab691907714c4f1\n+823, 0x16bc31d6f3abec1a\n+824, 0x7dff639fbcf1824\n+825, 0x6666985fbcff543d\n+826, 0xb618948e3d8e6d0c\n+827, 0x77b87837c794e068\n+828, 0xcd48288d54fcb5a8\n+829, 0x47a773ed6ae30dc3\n+830, 0xba85ae44e203c942\n+831, 0xa7a7b21791a25b2d\n+832, 0x4029dd92e63f19e0\n+833, 0xc2ad66ab85e7d5aa\n+834, 0xa0f237c96fdab0db\n+835, 0xffefb0ab1ca18ed\n+836, 0x90cb4500785fd7d5\n+837, 0xa7dd3120f4876435\n+838, 0x53f7872624694300\n+839, 0xea111326ff0040d9\n+840, 0x5f83cb4cce40c83b\n+841, 0x918e04936c3b504d\n+842, 0x87a8db4c0e15e87c\n+843, 0x7cff39da6a0dedd0\n+844, 0x36f7de2037f85381\n+845, 0xd1d8d94022a1e9a7\n+846, 0x2c9930127dc33ec9\n+847, 0x6cb4719dcd0101c6\n+848, 0xc01868cde76935f7\n+849, 0x6b86f2ec1ab50143\n+850, 0x68af607d8d94ae61\n+851, 0xe216c5b95feedf34\n+852, 0x4b866bd91efe2e4b\n+853, 0x4bff79df08f92c99\n+854, 0x6ff664ea806acfd1\n+855, 0x7fce0b3f9ece39bc\n+856, 0x29bc90b59cb3db97\n+857, 0x833c4b419198607d\n+858, 0xf3573e36ca4d4768\n+859, 0x50d71c0a3c2a3fa8\n+860, 0xd754591aea2017e7\n+861, 0x3f9126f1ee1ebf3\n+862, 0xe775d7f4b1e43de8\n+863, 0xe93d51628c263060\n+864, 0x83e77f6fb32d6d82\n+865, 0x43dd7eef823408e4\n+866, 0x1c843c2c90180662\n+867, 0xe924dafb9a16066b\n+868, 0x6af3ee96e7b7fbd9\n+869, 0x94d5c4f37befcd1f\n+870, 0x40ffb04bedef4236\n+871, 0x71c17bbc20e553e\n+872, 0x101f7a0a6208729f\n+873, 0x5ca34570cf923548\n+874, 0x8e3139db2e96e814\n+875, 0x3ab96d96263d048d\n+876, 0x97f3c0bbc6755c3c\n+877, 0x31fc72daedaef3dc\n+878, 0x71f8d7855d10789b\n+879, 0xce6dc97b4662333b\n+880, 0xfddc2aabd342bc61\n+881, 0xefbd4007ff8c7d2e\n+882, 0xf72cd6c689ef8758\n+883, 0x932c8b0c0e755137\n+884, 0x94cc4dedd58ff69\n+885, 0xde4dfd6890535979\n+886, 0xdb00dcd2dcb4a50a\n+887, 0xb0466240b4548107\n+888, 0x9cb9264c7b90d1a3\n+889, 0x357e378e9be5766b\n+890, 0x6e0316ef03367bbf\n+891, 0x201ea18839544ca\n+892, 0x803ff3406be5f338\n+893, 0xf9d5e82fd4144bb2\n+894, 0x1b6b88ca701e9f47\n+895, 0xd1fe5ab8e1f89cc0\n+896, 0x14171fe176c4bece\n+897, 0x887948bdef78beaa\n+898, 0x80449ddc3eb9b977\n+899, 0x5f4e1f900fb4bcf3\n+900, 0xbe30f8701909f8e2\n+901, 0xd1f2a2fb5503306d\n+902, 0x6b1c77238dc23803\n+903, 0x102156a6c9860f66\n+904, 0x4cd446e099edf4c1\n+905, 0xc79ac6cbc911f33b\n+906, 0x3ee096ffe3384f1c\n+907, 0xb58f83b18a306dc7\n+908, 0x9f76582141de56b2\n+909, 0x9ddfa85e02c13866\n+910, 0x4d9a19d4ce90a543\n+911, 0xbf81ab39fd17d376\n+912, 0x5327e5054c6a74f1\n+913, 0xd5062dd31db1a9b7\n+914, 0x645853735527edc\n+915, 0x485393967f91af08\n+916, 0xeff9667dcf77ca68\n+917, 0xd012313f5fbec464\n+918, 0xbeae35bdfae55144\n+919, 0x302c41ebac8444a0\n+920, 0x9ccdb6c2fe58fba8\n+921, 0x567753af68ed23f8\n+922, 0xff90f790e43efec3\n+923, 0x970cc756fb799696\n+924, 0xe59239d1c44915\n+925, 0x4d2d189fb3941f05\n+926, 0x96f23085db165a9c\n+927, 0xa1202dec7a37b1a5\n+928, 0xc0c1ee74bcd7dc1a\n+929, 0x9edcf2048b30333a\n+930, 0xd848588ba7e865fb\n+931, 0x8d9f0897317cab40\n+932, 0x67b96f15e25924fb\n+933, 0xefc8d8536619ee42\n+934, 0xf3f621d22bdde0c2\n+935, 0x68610a0de862ae32\n+936, 0xa22ca5142de24cbd\n+937, 0x8815452f4e6b4801\n+938, 0x4e9c1b607b2750e5\n+939, 0x19b3c09ba6fc9b25\n+940, 0x9b2543c8836780ac\n+941, 0xe702b8f950e56431\n+942, 0xb357cc329cac3917\n+943, 0x387bf86a17a31e08\n+944, 0x9940b983d331b163\n+945, 0xf5d89d7fe9095e18\n+946, 0x4362682329e5c4d1\n+947, 0xd2132573f6ae7b42\n+948, 0xc0a5849e23a61606\n+949, 0xdadbddf47265bc02\n+950, 0x1b96f00339a705f7\n+951, 0x94e6642329288913\n+952, 0x825ab3f10e6d330b\n+953, 0x1a1c31ac9d883ea0\n+954, 0xb49076b7155c6f47\n+955, 0x920cf3085dfe3ccb\n+956, 0x9743407c9f28e825\n+957, 0x6ce8a28622402719\n+958, 0xce2fe67e06baf8a6\n+959, 0x3a16b34784ecf5e6\n+960, 0x140467cc1d162a0c\n+961, 0x32d4772692ab625\n+962, 0xa4f4b28562f43336\n+963, 0x885b4335457bd84a\n+964, 0x499d3ed26c87ad8a\n+965, 0xc7328bcedb9a545e\n+966, 0xc6dd76a6cbf5d2b2\n+967, 0xba9c22be404ee1aa\n+968, 0x70e6aee45f23521d\n+969, 0x61e03a798593c177\n+970, 0x171671f809c68213\n+971, 0x28d54872fc1d914c\n+972, 0x43c2fcd9bd098b53\n+973, 0x172ad4c4a98b9d37\n+974, 0x330860c9460f2516\n+975, 0x49547f472df984f4\n+976, 0x873b2436d3f0e114\n+977, 0x6f99accf4ea050b6\n+978, 0x5968ac874ed51613\n+979, 0x4939d70d29a3c611\n+980, 0x11f381ed28738d3d\n+981, 0xa97430d36ab3a869\n+982, 0xe6fa880801129e22\n+983, 0xf84decbd8f48c913\n+984, 0x4425c0ed1e9a82a5\n+985, 0x7a1f9485e9929d5a\n+986, 0xc7c51f155dfce1c6\n+987, 0x9619a39501d74f2b\n+988, 0x7c7035955dbf4c1b\n+989, 0xc61ee569cf57c2c9\n+990, 0x3eaf7c5b0df734e1\n+991, 0xe71cb4064d1ede05\n+992, 0x356e3cec80e418b2\n+993, 0xca04306243a15be6\n+994, 0x941cf3881fa18896\n+995, 0x30dbb0e819d644e0\n+996, 0xaae22c0bef02859a\n+997, 0x7bd30917bbaa8a94\n+998, 0x2672547bc8d7d329\n+999, 0x4955c92aaa231578"
            },
            {
                "filename": "numpy/random/tests/data/pcg64dxsm-testset-2.csv",
                "patch": "@@ -0,0 +1,1001 @@\n+seed, 0x0\n+0, 0xd97e4a147f788a70\n+1, 0x8dfa7bce56e3a253\n+2, 0x13556ed9f53d3c10\n+3, 0x55dbf1c241341e98\n+4, 0xa2cd98f722eb0e0a\n+5, 0x83dfc407203ade8\n+6, 0xeaa083df518f030d\n+7, 0x44968c87e432852b\n+8, 0x573107b9cb8d9ecc\n+9, 0x9eedd1da50b9daca\n+10, 0xb33a6735ca451e3c\n+11, 0x72830d2b39677262\n+12, 0x9da8c512fd0207e8\n+13, 0x1fc5c91954a2672b\n+14, 0xd33479437116e08\n+15, 0x9ccdd9390cee46f3\n+16, 0x1fd39bb01acd9e76\n+17, 0xedc1869a42ff7fe5\n+18, 0xbd68ca0b42a6e7e9\n+19, 0x620b67df09621b1f\n+20, 0xfa11d51bd6950221\n+21, 0xc8c45b36e7d28d08\n+22, 0xe9c91272fbaad777\n+23, 0x2dc87a143f220e90\n+24, 0x6376a7c82361f49d\n+25, 0x552c5e434232fe75\n+26, 0x468f7f872ac195bc\n+27, 0x32bed6858125cf89\n+28, 0xe4f06111494d09d3\n+29, 0xa5c166ffea248b80\n+30, 0x4e26605b97064a3f\n+31, 0xceafd9f6fc5569d\n+32, 0xb772f2f9eed9e106\n+33, 0x672c65e6a93534e2\n+34, 0xcdc5e1a28d1bd6a0\n+35, 0x1ed9c96daeebd3e3\n+36, 0x4d189dcfc0c93c3f\n+37, 0x50df5a95c62f4b43\n+38, 0xcccf4949fa65bbb8\n+39, 0x19b8073d53cdc984\n+40, 0x6fb40bba35483703\n+41, 0xb02de4aef86b515a\n+42, 0x4d90c63655350310\n+43, 0xea44e4089825b16c\n+44, 0x8d676958b1f9da2b\n+45, 0x6d313940917ae195\n+46, 0x1b1d35a4c1dd19f4\n+47, 0x117720f8397337ef\n+48, 0xcc073cf3ac11eeaa\n+49, 0x8331ec58a9ff8acb\n+50, 0xf3dc2a308b6b866f\n+51, 0x7eba1202663382b6\n+52, 0x8269839debeb4e5a\n+53, 0x87fd3dc0f9181a8e\n+54, 0xabe62ddd3c925f03\n+55, 0x7f56f146944fe8d4\n+56, 0xc535972150852068\n+57, 0x60b252d453bd3a68\n+58, 0x4251f0134634490a\n+59, 0x338950da210dfeb2\n+60, 0xcadfe932971c9471\n+61, 0xfb7049457fab470e\n+62, 0x9bfb8145a4459dff\n+63, 0x4a89dda3898f9d8a\n+64, 0x88cc560151483929\n+65, 0x277dc820f4b6796e\n+66, 0x3524bd07ea0afb88\n+67, 0x92eb6ffb2bf14311\n+68, 0xf6559be0783f3fe9\n+69, 0xf0844f9af54af00d\n+70, 0xdd5e0b59adcef8a\n+71, 0x4ff7e4f2ab18554c\n+72, 0x3fa22c8a02634587\n+73, 0x1db8e1a9442fe300\n+74, 0x40cf15953ad3d3e7\n+75, 0x92af15fe1a9f6f0a\n+76, 0xab4a0e466fb0cfd\n+77, 0x944f1555a06cca82\n+78, 0x10cf48412f1f6066\n+79, 0x7f51f9a455f9e8e1\n+80, 0x47ee93530f024c7e\n+81, 0x36cf2f0413e0f6f2\n+82, 0xa315e23731969407\n+83, 0xd8e2796327cf5f87\n+84, 0xa86072696a555c34\n+85, 0xee3f0b8804feaab7\n+86, 0x41e80dc858f8360b\n+87, 0x31ec2e9b78f5b29\n+88, 0xd397fb9b8561344c\n+89, 0x28081e724e649b74\n+90, 0x5c135fc3fc672348\n+91, 0x9a276ca70ce9caa0\n+92, 0x9216da059229050a\n+93, 0xcf7d375ed68007b0\n+94, 0xa68ad1963724a770\n+95, 0xd4350de8d3b6787c\n+96, 0xee7d2c2cc275b6d2\n+97, 0x71645ec738749735\n+98, 0x45abdf8c68d33dbb\n+99, 0xe71cadb692c705ea\n+100, 0x60af6f061fd90622\n+101, 0x1eabe2072632c99d\n+102, 0x947dda995a402cb6\n+103, 0xbb19f49a3454f3b\n+104, 0xe6e43e907407758c\n+105, 0xfe2b67016bd6873a\n+106, 0x7fdb4dd8ab30a722\n+107, 0x39d3265b0ff1a45b\n+108, 0xed24c0e4fce8d0c2\n+109, 0xf6e074f86faf669d\n+110, 0x9142040df8dc2a79\n+111, 0x9682ab16bc939a9c\n+112, 0x6a4e80c378d971c8\n+113, 0x31309c2c7fc2d3d6\n+114, 0xb7237ec682993339\n+115, 0x6a30c06bb83dccd9\n+116, 0x21c8e9b6d8e7c382\n+117, 0x258a24ae6f086a19\n+118, 0xb76edb5be7df5c35\n+119, 0x3c11d7d5c16e7175\n+120, 0xbdfc34c31eff66e1\n+121, 0x8af66e44be8bf3a2\n+122, 0x3053292e193dec28\n+123, 0xd0cc44545b454995\n+124, 0x408ac01a9289d56\n+125, 0x4e02d34318ec2e85\n+126, 0x9413ff3777c6eb6b\n+127, 0xa3a301f8e37eb3df\n+128, 0x14e6306bd8d8f9f9\n+129, 0xd3ea06ce16c4a653\n+130, 0x170abe5429122982\n+131, 0x7f9e6fddc6cacb85\n+132, 0xa41b93e10a10a4c8\n+133, 0x239216f9d5b6d0b5\n+134, 0x985fcb6cb4190d98\n+135, 0xb45e3e7c68f480c6\n+136, 0xc1b2fc2e0446211c\n+137, 0x4596adb28858c498\n+138, 0x2dd706f3458ddc75\n+139, 0x29c988c86f75464\n+140, 0xac33a65aa679a60\n+141, 0xa28fef762d39d938\n+142, 0x541e6fa48647f53\n+143, 0x27838d56b2649735\n+144, 0x8e143d318a796212\n+145, 0xaea6097745f586b8\n+146, 0x636143330f8ee2e6\n+147, 0xc2d05fd8b945b172\n+148, 0x6e355f9eb4353055\n+149, 0xeb64ca42e8bf282e\n+150, 0xe8202dfd9da0fe5\n+151, 0x7305689c9d790cba\n+152, 0xf122f8b1bef32970\n+153, 0x9562887e38c32ba5\n+154, 0xf9cd9be121b738d\n+155, 0x6238e0c398307913\n+156, 0x5f2e79bb07c30f47\n+157, 0x8ce8e45c465006e\n+158, 0x39281fe1e99e2441\n+159, 0xafb10c2ca2874fea\n+160, 0x6e52f91633f83cf\n+161, 0x8ff12c1ac73c4494\n+162, 0xe48608a09365af59\n+163, 0xefd9bbc7e76e6a33\n+164, 0xbe16a39d5c38ec92\n+165, 0x6a6ffbcaf5a2330f\n+166, 0xdd5d6ac7d998d43d\n+167, 0x207bf978226d4f11\n+168, 0xf8eec56bd2a0f62e\n+169, 0xa5bccf05dce0d975\n+170, 0x93cf3ec1afe457a6\n+171, 0x38651466d201f736\n+172, 0x3ad21473985c9184\n+173, 0xc6407a3bd38c92a6\n+174, 0xb1ec42c7afa90a25\n+175, 0xbdeca984df8b7dd3\n+176, 0xb6926b1d00aa6c55\n+177, 0x86141d0022352d49\n+178, 0x169316256135ee09\n+179, 0xffb1c7767af02a5c\n+180, 0x502af38ad19f5c91\n+181, 0xfbf6cbc080086658\n+182, 0x33cf9b219edae501\n+183, 0x46e69bebd77b8862\n+184, 0xf11e0cc91125d041\n+185, 0xb4cd1649f85e078f\n+186, 0xb49be408db4e952\n+187, 0xb0b8db46140cce3c\n+188, 0xba647f2174012be7\n+189, 0x4f0a09e406970ac9\n+190, 0xf868c7aec9890a5c\n+191, 0xde4c8fa7498ea090\n+192, 0x872ceb197978c1d4\n+193, 0x1eb5cd9c3269b258\n+194, 0x3ea189f91724f014\n+195, 0x41379656f7746f2c\n+196, 0x7bd18493aca60e51\n+197, 0x5380c23b0cbbf15e\n+198, 0x920b72835f88246b\n+199, 0x24d7f734a4548b8e\n+200, 0x9944edb57e5aa145\n+201, 0x4628e136ebb8afe1\n+202, 0xb4ee6a776356e2a7\n+203, 0x481cbe9744ccf7d7\n+204, 0x7e8d67e8b0b995d9\n+205, 0xeeacde100af7b47e\n+206, 0x103da08f2487dab7\n+207, 0x6b9890a91d831459\n+208, 0xd0c5beae37b572c7\n+209, 0xfdccc371ee73fcc\n+210, 0x65438f0a367a2003\n+211, 0x5d23b2c818a7e943\n+212, 0x9a8ed45ac04b58b3\n+213, 0xdaf3c3f1695dce10\n+214, 0x5960eec706fa2bc0\n+215, 0x98ca652facb80d40\n+216, 0x72970ae5e2194143\n+217, 0x18c6374d878c5c94\n+218, 0x20fa51f997381900\n+219, 0x3af253dba26d6e1d\n+220, 0x1b23d65db15c7f78\n+221, 0x9f53ae976259b0e3\n+222, 0x9a6addb28dc92d49\n+223, 0x1e085c4accd0a7d7\n+224, 0xe9d3f4cc9bad6ce5\n+225, 0xe018fad78b5b1059\n+226, 0x5ef7682232b4b95\n+227, 0xb2242aa649f5de80\n+228, 0x8f3e6d8dd99b9e4e\n+229, 0xb9be6cc22949d62a\n+230, 0xecbdc7beaa5ff1fe\n+231, 0xd388db43a855bdf0\n+232, 0xd71ee3238852568d\n+233, 0x85ab3056304c04b5\n+234, 0x2ed7ae7ad3cfc3cb\n+235, 0x781d1b03d40b6c48\n+236, 0x7d3c740886657e6d\n+237, 0x982cfa6828daa6b0\n+238, 0x278579599c529464\n+239, 0x773adecfae9f0e08\n+240, 0x63a243ea4b85c5d7\n+241, 0x59940074fc3709e1\n+242, 0xc914a2eed58a6363\n+243, 0x2602b04274dd724c\n+244, 0xdf636eb7636c2c42\n+245, 0x891a334d0d26c547\n+246, 0xde8cd586d499e22d\n+247, 0x3ea1aa4d9b7035b6\n+248, 0xd085cff6f9501523\n+249, 0xe82a872f374959e\n+250, 0x55cb495bbd42cc53\n+251, 0x5f42b3226e56ca97\n+252, 0xea463f6f203493a3\n+253, 0xeef3718e57731737\n+254, 0x1bd4f9d62b7f9f3c\n+255, 0x19284f5e74817511\n+256, 0xaf6e842c7450ca87\n+257, 0x1d27d2b08a6b3600\n+258, 0xfb4b912b396a52e3\n+259, 0x30804d4c5c710121\n+260, 0x4907e82564e36338\n+261, 0x6441cf3b2900ddb7\n+262, 0xd76de6f51988dc66\n+263, 0x4f298ef96fd5e6d2\n+264, 0x65432960c009f83d\n+265, 0x65ebed07e1d2e3df\n+266, 0xf83ee8078febca20\n+267, 0x7bb18e9d74fc5b29\n+268, 0x597b5fbc2261d91\n+269, 0xea4f8ed0732b15b2\n+270, 0xba2267f74f458268\n+271, 0x3f304acabd746bbb\n+272, 0x7bd187af85659a82\n+273, 0x88e20dbdb7a08ea3\n+274, 0x2a2dc948c772fcb4\n+275, 0x87784fec2993c867\n+276, 0x89163933cd362d4e\n+277, 0xfd7b24f04302f957\n+278, 0x9bdd544405dfb153\n+279, 0xddee0fac58ffc611\n+280, 0xa8e8993417e71ec1\n+281, 0x55e0ab46ff7757af\n+282, 0x53e7645f08d3d7df\n+283, 0xbf78e563bc656ba2\n+284, 0x1d162253b45ee2de\n+285, 0x15e2bfefedf29eb4\n+286, 0x4e2a4584aa394702\n+287, 0xa89fb12b01525897\n+288, 0x825bd98f0544e4df\n+289, 0xfc6c50da6750700\n+290, 0xc24aaabde7d28423\n+291, 0x79d6f4660fcb19e5\n+292, 0xee7d4fb40c8d659f\n+293, 0x70bc281b462e811d\n+294, 0x23ed4dc9636519a7\n+295, 0xcb7c3f5a5711b935\n+296, 0xe73090e0508c5d9d\n+297, 0xb25a331f375952a6\n+298, 0xa64c86e0c04740f6\n+299, 0xb8f3ffc8d56ac124\n+300, 0x2479266fc5ee6b15\n+301, 0x8d5792d27f5ffbcb\n+302, 0xb064298be946cd52\n+303, 0xf0934a98912ffe26\n+304, 0xbe805682c6634d98\n+305, 0xe0e6e2c010012b4f\n+306, 0x58c47d475f75976\n+307, 0x358c9a6e646b2b4a\n+308, 0x7e7c4ffca5b17ba7\n+309, 0x43585c8c9a24a04c\n+310, 0x5154ddbcd68d5c2c\n+311, 0x4a2b062d3742a5e\n+312, 0xca5691191da2b946\n+313, 0x696a542109457466\n+314, 0x9eb5d658a5022ba5\n+315, 0x8158cf6b599ab8dc\n+316, 0x1b95391eaa4af4a6\n+317, 0x9953e79bd0fc3107\n+318, 0x8639690086748123\n+319, 0x2d35781c287c6842\n+320, 0x393ef0001cd7bc8f\n+321, 0xe3a61be8c5f2c22a\n+322, 0x5e4ff21b847cc29b\n+323, 0x4c9c9389a370eb84\n+324, 0xd43a25a8fc3635fa\n+325, 0xf6790e4a85385508\n+326, 0x37edf0c81cb95e1d\n+327, 0x52db00d6e6e79af8\n+328, 0x3b202bceeb7f096\n+329, 0x2a164a1c776136bb\n+330, 0x73e03ee3fd80fd1b\n+331, 0xd2c58c0746b8d858\n+332, 0x2ed2cb0038153d22\n+333, 0x98996d0fc8ceeacc\n+334, 0xa4ed0589936b37f\n+335, 0x5f61cf41a6d2c172\n+336, 0xa6d4afb538c110d7\n+337, 0xe85834541baadf1a\n+338, 0x4c8967107fd49212\n+339, 0x49bafb762ab1a8c1\n+340, 0x45d540e2a834bf17\n+341, 0x1c0ec8b4ed671dac\n+342, 0x3d503ce2c83fe883\n+343, 0x437bfffd95f42022\n+344, 0xc82d1e3d5c2bc8d2\n+345, 0x7a0a9cbfcb0d3f24\n+346, 0xc0a4f00251b7a3be\n+347, 0xb5be24e74bb6a1c6\n+348, 0xa3104b94b57545b1\n+349, 0x86de7d0c4b97b361\n+350, 0x879c1483f26538a6\n+351, 0xd74c87557f6accfb\n+352, 0x2f9be40dbf0fe8a1\n+353, 0x445a93398f608d89\n+354, 0x7b3cb8a7211d7fdc\n+355, 0xe86cc51290d031e7\n+356, 0x33ef3594052ad79f\n+357, 0xc61911d241dbb590\n+358, 0x37cccb0c0e3de461\n+359, 0xb75259124080b48b\n+360, 0xd81e8961beb4abe5\n+361, 0xf4542deb84a754e\n+362, 0x6ea036d00385f02e\n+363, 0xa7b60b0ac3b88681\n+364, 0x108a6c36ca30baf5\n+365, 0x4a2adc5bbfe2bf07\n+366, 0x4079501f892a5342\n+367, 0x55e113963c5448f0\n+368, 0x8019ff4903b37242\n+369, 0x109c6dcdb7ec6618\n+370, 0x1239ac50944da450\n+371, 0xe1399c7f94c651c1\n+372, 0x5a6bbbae388d365a\n+373, 0x4d72be57b8810929\n+374, 0x3f067df24384e1fb\n+375, 0x4f8b9e0f7f6c7be\n+376, 0x202492c342a3b08\n+377, 0x250753192af93a3\n+378, 0xfba1159d9de2cb8e\n+379, 0xba964497ab05505c\n+380, 0x1329ec5d8a709dca\n+381, 0x32927cacb6cd22bb\n+382, 0x6b4d7db904187d56\n+383, 0xe76adccf8e841e02\n+384, 0x8c4bf4b6a788202\n+385, 0x3013a3b409831651\n+386, 0x7427d125c475412f\n+387, 0x84dcc4bb2bf43202\n+388, 0x117526f1101372a5\n+389, 0xfe95d64b8984bd72\n+390, 0x524e129934cc55c1\n+391, 0xc3db4b0418c36d30\n+392, 0xe1cb2047e9c19f7a\n+393, 0xea43d6c8d8982795\n+394, 0xe80ac8a37df89ed\n+395, 0xfecc2104329ed306\n+396, 0xa5c38aac9c1d51ea\n+397, 0x3abe5d1c01e4fe17\n+398, 0x717a805d97fcc7ac\n+399, 0x94441f8207a1fb78\n+400, 0x22d7869c5f002607\n+401, 0x349e899f28c3a1b9\n+402, 0x5639950cdea92b75\n+403, 0x7e08450497c375b\n+404, 0x94bf898b475d211d\n+405, 0x75c761a402375104\n+406, 0x1930920ec9d2a1e7\n+407, 0xb774ba1bc6f6e4e2\n+408, 0xf715602412e5d900\n+409, 0x87bb995f4a13f0ba\n+410, 0xa3c787868dfa9c8d\n+411, 0xa17fd42a5a4f0987\n+412, 0x4a9f7d435242b86\n+413, 0x240364aff88f8aef\n+414, 0xe7cd4cf4bf39f144\n+415, 0xd030f313ca4c2692\n+416, 0xc46696f4e03ec1e9\n+417, 0x22c60f1ec21060b3\n+418, 0x16c88058fd68986f\n+419, 0x69ca448e8e6bde3f\n+420, 0x3466c2cdec218abd\n+421, 0x837ac4d05e6b117d\n+422, 0x911210e154690191\n+423, 0x9ece851d6fa358b7\n+424, 0x42f79cb0c45e7897\n+425, 0xbf7583babd7c499b\n+426, 0x2059fe8031c6e0b9\n+427, 0xabbec8fc00f7e51d\n+428, 0x88809d86a3a256e1\n+429, 0xd36056df829fdcb5\n+430, 0x515632b6cb914c64\n+431, 0xba76d06c2558874\n+432, 0x632c54ca4214d253\n+433, 0xadec487adf2cb215\n+434, 0x521e663e1940513d\n+435, 0xb1b638b548806694\n+436, 0xbe2d5bfbe57d2c72\n+437, 0x8b89e7719db02f7\n+438, 0x90ba5281c1d56e63\n+439, 0x899e1b92fceea102\n+440, 0xf90d918e15182fa6\n+441, 0x94a489ce96c948c4\n+442, 0xad34db453517fcd4\n+443, 0xc5264eb2de15930f\n+444, 0x101b4e6603a21cee\n+445, 0xef9b6258d6e85fff\n+446, 0x6075c7d6c048bd7a\n+447, 0x6f03232c64e438aa\n+448, 0x18c983d7105ee469\n+449, 0x3ffc23f5c1375879\n+450, 0xbc1b4a00afb1f9f\n+451, 0x5afa6b2bb8c6b46e\n+452, 0xe7fce4af2f2c152a\n+453, 0x5b00ab5c4b3982c7\n+454, 0x2d4b0c9c0eb4bd0c\n+455, 0x61d926270642f1f2\n+456, 0x7219c485c23a2377\n+457, 0x7e471c752fecd895\n+458, 0x23c4d30a4d17ba1f\n+459, 0x65cb277fe565ca22\n+460, 0xcbb56ed9c701363b\n+461, 0xfd04ab3a6eba8282\n+462, 0x19c9e5c8bab38500\n+463, 0xea4c15227676b65b\n+464, 0x20f3412606c8da6f\n+465, 0xb06782d3bf61a239\n+466, 0xf96e02d5276a9a31\n+467, 0x835d256b42aa52a6\n+468, 0x25b09151747f39c1\n+469, 0x64507386e1103eda\n+470, 0x51cbc05716ef88e4\n+471, 0x998cd9b7989e81cc\n+472, 0x9d7115416bec28d1\n+473, 0xc992ca39de97906b\n+474, 0xd571e6f7ca598214\n+475, 0xafc7fb6ccd9abbf8\n+476, 0x88ef456febff7bf4\n+477, 0xdbe87ccc55b157d2\n+478, 0xaab95e405f8a4f6d\n+479, 0xad586a385e74af4f\n+480, 0x23cd15225c8485aa\n+481, 0x370940bf47900ac7\n+482, 0xefd6afda1a4b0ead\n+483, 0x9cb1a4c90993dd7a\n+484, 0xff7893e8b2f70b11\n+485, 0xb09e1807c0638e8e\n+486, 0xb10915dcb4978f74\n+487, 0x88212ab0051a85eb\n+488, 0x7af41b76e1ec793f\n+489, 0x2e5c486406d3fefd\n+490, 0xebe54eff67f513cc\n+491, 0xab6c90d0876a79b8\n+492, 0x224df82f93fe9089\n+493, 0xc51c1ce053dc9cd2\n+494, 0x5ef35a4d8a633ee7\n+495, 0x4aca033459c2585f\n+496, 0xd066932c6eefb23d\n+497, 0x5309768aab9a7591\n+498, 0xa2a3e33823df37f9\n+499, 0xcec77ff6a359ee9\n+500, 0x784dc62d999d3483\n+501, 0x84e789fb8acc985d\n+502, 0xd590237e86aa60f\n+503, 0x737e2ffe1c8ad600\n+504, 0xc019c3a39a99eab8\n+505, 0x6a39e9836964c516\n+506, 0xe0fe43129535d9da\n+507, 0xdfc5f603d639d4de\n+508, 0x7b9a7d048a9c03b6\n+509, 0xbb5aa520faa27fdd\n+510, 0x2a09b4200f398fa2\n+511, 0x38cc88107904064e\n+512, 0xa9a90d0b2d92bb25\n+513, 0x9419762f87e987e3\n+514, 0x1a52c525153dedcd\n+515, 0xc26d9973dd65ae99\n+516, 0x8e89bd9d0dc6e6a1\n+517, 0x2f30868dc01bfb53\n+518, 0x20f09d99b46501c4\n+519, 0x78b468a563b8f1e9\n+520, 0xcccf34b0b6c380c7\n+521, 0xf554e7dc815297e6\n+522, 0x332a585cfb4a50ef\n+523, 0xa9fb64a2b6da41d7\n+524, 0xdcd2a5a337391ce0\n+525, 0x8a9bd3e324c6463d\n+526, 0x9f4487d725503bdd\n+527, 0xf72282d82f1d0ff\n+528, 0x308f4160abb72d42\n+529, 0x648de1db3a601b08\n+530, 0x36cab5192e7ebd39\n+531, 0x7975fbe4ab6a1c66\n+532, 0xd515b4d72243864e\n+533, 0x43a568f8b915e895\n+534, 0x15fa9f2057bdb91d\n+535, 0x7a43858ef7a222dc\n+536, 0x17b4a9175ac074fe\n+537, 0xa932c833b8d0f8f8\n+538, 0x1d2db93a9a587678\n+539, 0x98abd1d146124d27\n+540, 0xf0ab0431671740aa\n+541, 0xa9d182467540ad33\n+542, 0x41c8a6cfc331b7fc\n+543, 0xa52c6bd0fcd1d228\n+544, 0x2773c29a34dc6fa3\n+545, 0x3098230746fc1f37\n+546, 0xd63311bb4f23fabe\n+547, 0x6712bf530cd2faec\n+548, 0x342e8f342e42c4dd\n+549, 0xfbd83331851cdcad\n+550, 0xe903be1361bbc34d\n+551, 0xd94372e5077e3ef9\n+552, 0x95aaa234f194bd8\n+553, 0x20c0c8fb11e27538\n+554, 0xfaf47dc90462b30b\n+555, 0x8ddc6d144147682a\n+556, 0xf626833fd926af55\n+557, 0x5df93c34290d1793\n+558, 0xb06a903e6e9fca5e\n+559, 0x10c792dc851d77ca\n+560, 0xd9b1b817b18e56cb\n+561, 0x3a81730c408eb408\n+562, 0x65052c04a8d4b63c\n+563, 0x3328546598e33742\n+564, 0xeca44a13f62d156d\n+565, 0x69f83d1d86b20170\n+566, 0x937764200412027d\n+567, 0xc57eb1b58df0f191\n+568, 0xa1c7d67dce81bc41\n+569, 0x8e709c59a6a579ce\n+570, 0x776a2f5155d46c70\n+571, 0xd92906fbbc373aa5\n+572, 0xe97ad478a2a98bf6\n+573, 0xc296c8819ac815f\n+574, 0x613ede67ba70e93e\n+575, 0xe145222498f99cde\n+576, 0xafcdfa7a3c1cf9bf\n+577, 0x1c89252176db670d\n+578, 0xad245eda5c0865ff\n+579, 0x249463d3053eb917\n+580, 0xc9be16d337517c0b\n+581, 0xefcc82bf67b8f731\n+582, 0x1e01577d029e0d00\n+583, 0xad9c24b2a4f3d418\n+584, 0xed2cceb510db4d0f\n+585, 0xbddadcdb92400c70\n+586, 0x67d6b0476ef82186\n+587, 0xbc7662ff7bf19f73\n+588, 0x9d94452a729e6e92\n+589, 0x6b278d8594f55428\n+590, 0x6c4b31cceb1b2109\n+591, 0xccc6c3a726701e9\n+592, 0x6bc28ece07df8925\n+593, 0xc0422b7bf150ccc4\n+594, 0xab7158f044e73479\n+595, 0xdf3347546d9ed83f\n+596, 0x3b3235a02c70dff4\n+597, 0x2551c49c14ea8d77\n+598, 0xee2f7f5bb3cc228e\n+599, 0x39b87bfe8c882d39\n+600, 0x7dd420fad380b51c\n+601, 0xffe64976af093f96\n+602, 0x4a4f48dc6e7eaa5f\n+603, 0x85f2514d32fdc8cc\n+604, 0x1ab1215fd7f94801\n+605, 0x4cd1200fc795b774\n+606, 0xcf8af463a38942ee\n+607, 0x319caa7ce3022721\n+608, 0x8cd9798a76d1aea4\n+609, 0x2bd3933ac7afd34e\n+610, 0x85d4c323403cf811\n+611, 0xd7b956d3064efa30\n+612, 0x67a078dbf1f13068\n+613, 0x665fa6c83e87c290\n+614, 0x9333ac2416d2469b\n+615, 0xdfb1fd21a0094977\n+616, 0xa1962a6e2c25f8ff\n+617, 0x1f3b10a7ed5287cf\n+618, 0x70641efb3d362713\n+619, 0xe527a2cf85d00918\n+620, 0x9741e45d3f9890a3\n+621, 0x6cb74b5d4d36db4b\n+622, 0xf24734d622bd2209\n+623, 0xadd6d94f78e9d378\n+624, 0xc3bbdb59225cca7f\n+625, 0x5ad36614275b30cd\n+626, 0x495568dd74eea434\n+627, 0xf35de47e0ffe1f2d\n+628, 0xefa209dca719ab18\n+629, 0x844ddcaeb5b99ae8\n+630, 0x37449670a1dc7b19\n+631, 0x5a4612c166f845c1\n+632, 0xe70f7782f2087947\n+633, 0x98d484deac365721\n+634, 0x705302198cf52457\n+635, 0x7135ae0f5b77df41\n+636, 0x342ac6e44a9b6fc3\n+637, 0x2713fd2a59af5826\n+638, 0x6e1a3f90f84efa75\n+639, 0x9fb3b4dd446ca040\n+640, 0x530044ae91e6bd49\n+641, 0xe984c4183974dc3e\n+642, 0x40c1fa961997d066\n+643, 0xb7868250d8c21559\n+644, 0x8bc929fa085fd1de\n+645, 0x7bdb63288dc8733e\n+646, 0xac4faad24326a468\n+647, 0x1c6e799833aea0b1\n+648, 0xcc8a749e94f20f36\n+649, 0x4e7abfd0443547c5\n+650, 0xb661c73bb8caa358\n+651, 0x4a800f5728ff2351\n+652, 0x8c15e15189b9f7ed\n+653, 0xab367846b811362c\n+654, 0x4ba7508f0851ca2a\n+655, 0xe9af891acbafc356\n+656, 0xbdebe183989601f8\n+657, 0x4c665ea496afc061\n+658, 0x3ca1d14a5f2ed7c\n+659, 0xfbdff10a1027dd21\n+660, 0xdfd28f77c8cff968\n+661, 0xc4fbaadf8a3e9c77\n+662, 0xdac7e448b218c589\n+663, 0xb26390b5befd19e2\n+664, 0xd2ef14916c66dba9\n+665, 0xfab600284b0ff86b\n+666, 0xf04a1c229b58dabb\n+667, 0xc21c45637e452476\n+668, 0xd1435966f75e0791\n+669, 0xc1f28522eda4a2d0\n+670, 0x52332ae8f1222185\n+671, 0x81c6c0790c0bf47e\n+672, 0xfebd215e7d8ffb86\n+673, 0x68c5dce55dbe962b\n+674, 0x231d09cb0d2531d1\n+675, 0x3218fba199dbbc6b\n+676, 0x8f23c535f8ea0bf6\n+677, 0x6c228963e1df8bd9\n+678, 0x9843c7722ed153e3\n+679, 0xd032d99e419bddec\n+680, 0xe2dca88aa7814cab\n+681, 0x4d53fb8c6a59cdc2\n+682, 0x8fb3abc46157b68b\n+683, 0xa3e733087e09b8e\n+684, 0x6bdc1aee029d6b96\n+685, 0x4089667a8906d65b\n+686, 0x8f3026a52d39dd03\n+687, 0x6d2e0ccb567bae84\n+688, 0x74bad450199e464\n+689, 0xf114fb68a8f300d5\n+690, 0xc7a5cc7b374c7d10\n+691, 0xf0e93da639b279d1\n+692, 0xb9943841ad493166\n+693, 0x77a69290455a3664\n+694, 0x41530da2ebea054b\n+695, 0xe8f9fab03ea24abf\n+696, 0xaa931f0c9f55a57a\n+697, 0xb4d68a75d56f97ae\n+698, 0x3d58ff898b6ba297\n+699, 0x49d81e08faf5a3f5\n+700, 0xfc5207b9f3697f3b\n+701, 0xa25911abb3cf19b7\n+702, 0x6b8908eb67c3a41\n+703, 0xd63ef402e2e3fa33\n+704, 0x728e75d3f33b14c5\n+705, 0x248cb1b8bc6f379a\n+706, 0x3aa3d6d2b8c72996\n+707, 0x49cc50bd2d3d2860\n+708, 0xb4e1387647c72075\n+709, 0x435a1630a4a81ed3\n+710, 0xa5ea13005d2460cf\n+711, 0xc7a613df37d159ec\n+712, 0x95721ccc218b857e\n+713, 0xd4b70d8c86b124d3\n+714, 0x2b82bcc4b612d494\n+715, 0xaf13062885276050\n+716, 0xcbd8fcf571a33d9c\n+717, 0x3f7f67ca1125fc15\n+718, 0xddf4bb45aac81b4c\n+719, 0x23606da62de9c040\n+720, 0xa3a172375666b636\n+721, 0x292f87387a6c6c3c\n+722, 0xd1d10d00c5496fe1\n+723, 0x86b0411ce8a25550\n+724, 0x38e0487872e33976\n+725, 0x363e49f88ddfd42c\n+726, 0x45bdf1e9f6b66b0a\n+727, 0x8a6fff3de394f9b5\n+728, 0x8502158bb03f6209\n+729, 0x22e24d16dba42907\n+730, 0x3fe3ba427cc2b779\n+731, 0x77144793f66b3d7e\n+732, 0xcf8912ccb29b8af9\n+733, 0xdc856caff2abd670\n+734, 0xe6d3ae0b0d9d4c8b\n+735, 0xb8f5d40e454c539f\n+736, 0x79ca953114fbc6b7\n+737, 0x478d6f4bbfa38837\n+738, 0x9babae1a3ffdc340\n+739, 0x40edd56802bae613\n+740, 0x97a56c2dcccf0641\n+741, 0xafc250257f027f8e\n+742, 0x8da41ef1edf69125\n+743, 0x6574b0280ff9d309\n+744, 0x197c776151b8f820\n+745, 0x6b03e077c9dac3b6\n+746, 0x24a40ebbc5c341c5\n+747, 0x50e585169a6a1c4b\n+748, 0x37783a5a6a3e4e02\n+749, 0xb3de81ee6fbad647\n+750, 0xf4f292f57ca4591e\n+751, 0x6214e9e7d44d30a\n+752, 0x5920190c56d21c12\n+753, 0x9ac163419b5e0c9b\n+754, 0xfc2328761ae8ed93\n+755, 0xc68f945b545508c6\n+756, 0x687c49a17ce0a5e2\n+757, 0x276d8f53d30d4ab4\n+758, 0x8201804970343ce1\n+759, 0x1b5d323cc2e7fb7e\n+760, 0x6f351ef04fd904b\n+761, 0x6c793a7d455d5198\n+762, 0x46f5d108430ae91f\n+763, 0xac16a15b2a0cf77f\n+764, 0xa0d479d9e4122b9d\n+765, 0x3afd94604307f19\n+766, 0x2573ed6d39d38dbf\n+767, 0xa58e14ba60b4294b\n+768, 0xe69c1aed5840d156\n+769, 0x4cf6fda7f04855c2\n+770, 0x2fb65a56ef5f22da\n+771, 0xf95819434d5dc220\n+772, 0x29c65133623dafba\n+773, 0x8e997bd018467523\n+774, 0xfd08ba9d498461a7\n+775, 0xdd52243bc78a5592\n+776, 0x39c30108f6db88b3\n+777, 0x38af8e1894f259b9\n+778, 0x97eedf3b4ae5f6de\n+779, 0x757825add80c5ece\n+780, 0xf0fdd90ac14edb14\n+781, 0xbbb19d4cc8cac6d4\n+782, 0x9a82234edfae05e3\n+783, 0x704401c61d1edf1c\n+784, 0x8b0eb481fb3a1fb2\n+785, 0xef6f36e7cc06c002\n+786, 0x7a208b17e04b8cd7\n+787, 0xf20e33d498838fe9\n+788, 0xc2bdb22117058326\n+789, 0x6ec31939eb4ca543\n+790, 0x6f1654838f507a21\n+791, 0xc65ab81a955d2b93\n+792, 0x40b1420fdd9531b8\n+793, 0xe31f221cab9f4f40\n+794, 0x798cdd414c1deb7a\n+795, 0x9c84e9c7d41cd983\n+796, 0x63d6b1ae3b60b7fa\n+797, 0xb42bfdd1a2f78ffa\n+798, 0x37e431eaccaaa8e9\n+799, 0x7508142a0f73eac9\n+800, 0x91662a023df5893a\n+801, 0x59782070e2fe3031\n+802, 0xb2acd589a8ce7961\n+803, 0xa224743fa877b292\n+804, 0xaa5362aa27e6ed9e\n+805, 0xa394a4e520c0c1c7\n+806, 0xe49b16d2018ffb6f\n+807, 0xb8074b9f2f1e762b\n+808, 0xcf5f86143d5c23a7\n+809, 0xfd838785db987087\n+810, 0x31b1889df389aff8\n+811, 0x30aaca876a4383b\n+812, 0x1731bb71c4c38d4f\n+813, 0x9a83a65395e05458\n+814, 0x99cd0c8d67c8f4fc\n+815, 0xfbd9fdc849b761a5\n+816, 0x82c04834fc466889\n+817, 0xdeef9d6e715e8c97\n+818, 0x549c281c16da6078\n+819, 0x2d70661254ad599d\n+820, 0x57995793a72acac\n+821, 0xf1727005116183ba\n+822, 0xa22bb38945285de3\n+823, 0x4f2d687fe45131ff\n+824, 0x5666c87ddbbc981f\n+825, 0xbcb4b2d4e7a517d0\n+826, 0x5e794dd2e20b785d\n+827, 0x449ad020149e093c\n+828, 0x7704ee0412d106f5\n+829, 0x83cbdf257b072ac1\n+830, 0xae5c4fc9f638b0da\n+831, 0x7b9e5a64e372ed47\n+832, 0x7eddbbb22c2cdf57\n+833, 0x3f19ebfa155b08e\n+834, 0x91d991154dfd7177\n+835, 0x611ae74b952d387f\n+836, 0x3fdf7a335bda36ee\n+837, 0xdf182433fc7a7c05\n+838, 0x62c78598d1f8db0a\n+839, 0xc3750c69d2c5c1f0\n+840, 0xf1318024709efdee\n+841, 0xaa3fd360d224dc29\n+842, 0x62af53b2f307c19\n+843, 0xdf527683c58120c2\n+844, 0x3281deecc496f93d\n+845, 0x4f704ad31527ef08\n+846, 0x127a14a5e07cfdfc\n+847, 0x90d0b1f549255c92\n+848, 0xbc3406b212c5e1fc\n+849, 0x4e89f39379dba91d\n+850, 0x1290ef43c4998e6e\n+851, 0xecfeb1a1cb1c6e1b\n+852, 0x2067e90403003bf1\n+853, 0x38ae04be30bdbeba\n+854, 0x8a3537f298baedda\n+855, 0xd07f3b825cdb2936\n+856, 0xea020b5aebae8b45\n+857, 0xfcd614ab031132b0\n+858, 0x5fb682a4ff2268f5\n+859, 0xd1c4662ce65596f4\n+860, 0x7026b8270dd0b8dc\n+861, 0x8101ec4b4beae45a\n+862, 0xa0e9dc87940610a6\n+863, 0x83ec33679d83165b\n+864, 0x981847ca82e86d41\n+865, 0xda84c188a304a0b7\n+866, 0x3c37529c5a5bbbb8\n+867, 0x34a8491ce3e19a5a\n+868, 0xd36ad716a2fa6cb8\n+869, 0xfd1d1d6a5189a15c\n+870, 0x9716eb47851e8d8d\n+871, 0x7dfb13ea3b15c5aa\n+872, 0xbdf6e707f45113a5\n+873, 0xb8118261b04bd097\n+874, 0x6191f9895881bec6\n+875, 0x7aac257ae11acf9b\n+876, 0x35a491e1537ff120\n+877, 0xe078943432efa71c\n+878, 0xb3338485dd3dc2b9\n+879, 0x456060975d2bb3b5\n+880, 0xaddc4c451bdfc44c\n+881, 0x18bfa7beacf96430\n+882, 0x8802ebcaf0f67498\n+883, 0xad922a5a825bd780\n+884, 0x9fb4587d748f4efa\n+885, 0xdb2a445136cd5e7\n+886, 0xb98b3676ea8e96ac\n+887, 0xb02d8d244d784878\n+888, 0xa1a8442b18860abb\n+889, 0x6a3029ba1361e5d1\n+890, 0xf426d5fac161eb1\n+891, 0xfa5ac2b87acecb23\n+892, 0xaa659896e50535df\n+893, 0xf40dd7a3d3c5c8ed\n+894, 0x3f8367abecb705bc\n+895, 0x2d60e7525873358f\n+896, 0xc4a9d3948a0c3937\n+897, 0x5ecc04fef6003909\n+898, 0x7a865004918cba2\n+899, 0x47ae110a678ec10b\n+900, 0xa0f02f629d91aa67\n+901, 0x4848b99e7fac9347\n+902, 0xaa858346d63b80ac\n+903, 0xeb5bf42ee161eeef\n+904, 0x4d35d723d3c6ba37\n+905, 0xdf22ca6ca93b64a7\n+906, 0x9d198520f97b25b1\n+907, 0x3068415350778efe\n+908, 0xf3709f2e8793c2fe\n+909, 0xd1517bac8dd9f16f\n+910, 0xfb99bccaa15861dc\n+911, 0xa9ad607d796a2521\n+912, 0x55d3793d36bd22e4\n+913, 0xf99270d891ff7401\n+914, 0x401750a5c4aa8238\n+915, 0xd84b3003e6f28309\n+916, 0x8a23798b5fa7c98b\n+917, 0xadd58bbc8f43e399\n+918, 0xbd8c741ada62c6a8\n+919, 0xbdc6937bc55b49fa\n+920, 0x4aefa82201b8502\n+921, 0x17adf29a717b303\n+922, 0xa6ed2197be168f6c\n+923, 0x1ba47543f4359a95\n+924, 0xe34299949ac01ae9\n+925, 0x711c76cffc9b62f3\n+926, 0xbac259895508a4b7\n+927, 0x3c8b3b3626b0d900\n+928, 0x1a8d23fbe2ae71bf\n+929, 0xca984fa3b5a5c3a1\n+930, 0xb1986ab7521a9c93\n+931, 0xd6b5b2c8d47a75b5\n+932, 0xc7f1c4a88afb4957\n+933, 0xdeb58033a3acd6cc\n+934, 0xabe49ddfe1167e67\n+935, 0x8d559c10205c06e3\n+936, 0xea07a1a7de67a651\n+937, 0xcbef60db15b6fef8\n+938, 0xbfca142cff280e7\n+939, 0x362693eba0732221\n+940, 0x7463237e134db103\n+941, 0x45574ddb5035e17a\n+942, 0xfc65e0cb9b94a1aa\n+943, 0x3154c55f1d86b36d\n+944, 0x2d93a96dd6ab2d8b\n+945, 0xbe3bc1d1f2542a25\n+946, 0xdd4b541f7385bdaa\n+947, 0x3b56b919d914e3f8\n+948, 0x82fd51468a21895f\n+949, 0x8988cf120731b916\n+950, 0xa06a61db5fb93e32\n+951, 0x6ed66c1b36f68623\n+952, 0x875ae844d2f01c59\n+953, 0x17ccd7ac912e5925\n+954, 0x12fe2a66b8e40cb1\n+955, 0xf843e5e3923ad791\n+956, 0xa17560f2fd4ef48\n+957, 0x27a2968191a8ee07\n+958, 0xa9aab4d22ff44a3c\n+959, 0x63cd0dcc3bb083ae\n+960, 0x7a30b48c6160bf85\n+961, 0x956160fb572503b3\n+962, 0xc47f6b7546640257\n+963, 0xaf4b625f7f49153\n+964, 0x2f5c86a790e0c7e8\n+965, 0xb52e0610ae07f0b8\n+966, 0x38a589292c3d849e\n+967, 0xc3e9ef655d30b4ef\n+968, 0xb5695f765cda998a\n+969, 0xde5d5e692a028e91\n+970, 0x839476721555f72e\n+971, 0x48b20679b17d9ebf\n+972, 0xe3d4c6b2c26fb0df\n+973, 0xce5a9834f0b4e71f\n+974, 0x533abb253d5d420e\n+975, 0x9eac5ad9aed34627\n+976, 0xc0f2a01ab3c90dbb\n+977, 0x6528eda93f6a066c\n+978, 0xc16a1b625e467ade\n+979, 0x1a4a320fb5e8b098\n+980, 0x8819cccd8b4ab32f\n+981, 0x42daa88531fd0bfd\n+982, 0xcf732226409be17c\n+983, 0xfddcdb25ccbf378c\n+984, 0x9b15b603bf589fc1\n+985, 0x2436066b95d366fe\n+986, 0x8d42eff2e9cbda90\n+987, 0x694b2fc8a4e8303c\n+988, 0x8e207f98aaea3ccd\n+989, 0x4730d7a620f822d9\n+990, 0x468dc9ca30fe2fd4\n+991, 0x74b36d8a1c0f031b\n+992, 0x3c1aac1c488c1a94\n+993, 0x19d0101042444585\n+994, 0x8ec50c56d0c8adf4\n+995, 0x721ec629e4d66394\n+996, 0x3ca5ad93abeac4a4\n+997, 0xaaebc76e71592623\n+998, 0x969cc319e3ed6058\n+999, 0xc0a277e3b2bfc3de"
            },
            {
                "filename": "numpy/random/tests/test_direct.py",
                "patch": "@@ -8,8 +8,8 @@\n import pytest\n \n from numpy.random import (\n-    Generator, MT19937, PCG64, Philox, RandomState, SeedSequence, SFC64,\n-    default_rng\n+    Generator, MT19937, PCG64, PCG64DXSM, Philox, RandomState, SeedSequence,\n+    SFC64, default_rng\n )\n from numpy.random._common import interface\n \n@@ -359,6 +359,34 @@ def test_advance_symmetry(self):\n         assert val_big == val_pos\n \n \n+class TestPCG64DXSM(Base):\n+    @classmethod\n+    def setup_class(cls):\n+        cls.bit_generator = PCG64DXSM\n+        cls.bits = 64\n+        cls.dtype = np.uint64\n+        cls.data1 = cls._read_csv(join(pwd, './data/pcg64dxsm-testset-1.csv'))\n+        cls.data2 = cls._read_csv(join(pwd, './data/pcg64dxsm-testset-2.csv'))\n+        cls.seed_error_type = (ValueError, TypeError)\n+        cls.invalid_init_types = [(3.2,), ([None],), (1, None)]\n+        cls.invalid_init_values = [(-1,)]\n+\n+    def test_advance_symmetry(self):\n+        rs = Generator(self.bit_generator(*self.data1['seed']))\n+        state = rs.bit_generator.state\n+        step = -0x9e3779b97f4a7c150000000000000000\n+        rs.bit_generator.advance(step)\n+        val_neg = rs.integers(10)\n+        rs.bit_generator.state = state\n+        rs.bit_generator.advance(2**128 + step)\n+        val_pos = rs.integers(10)\n+        rs.bit_generator.state = state\n+        rs.bit_generator.advance(10 * 2**128 + step)\n+        val_big = rs.integers(10)\n+        assert val_neg == val_pos\n+        assert val_big == val_pos\n+\n+\n class TestMT19937(Base):\n     @classmethod\n     def setup_class(cls):"
            },
            {
                "filename": "numpy/random/tests/test_smoke.py",
                "patch": "@@ -4,7 +4,7 @@\n import numpy as np\n import pytest\n from numpy.testing import assert_equal, assert_, assert_array_equal\n-from numpy.random import (Generator, MT19937, PCG64, Philox, SFC64)\n+from numpy.random import (Generator, MT19937, PCG64, PCG64DXSM, Philox, SFC64)\n \n @pytest.fixture(scope='module',\n                 params=(np.bool_, np.int8, np.int16, np.int32, np.int64,\n@@ -774,6 +774,18 @@ def setup_class(cls):\n         cls._extra_setup()\n \n \n+class TestPCG64DXSM(RNG):\n+    @classmethod\n+    def setup_class(cls):\n+        cls.bit_generator = PCG64DXSM\n+        cls.advance = 2**63 + 2**31 + 2**15 + 1\n+        cls.seed = [12345]\n+        cls.rg = Generator(cls.bit_generator(*cls.seed))\n+        cls.initial_state = cls.rg.bit_generator.state\n+        cls.seed_vector_bits = 64\n+        cls._extra_setup()\n+\n+\n class TestDefaultRNG(RNG):\n     @classmethod\n     def setup_class(cls):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23573,
        "body": "Bumps [actions/checkout](https://github.com/actions/checkout) from 3.5.0 to 3.5.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/checkout/releases\">actions/checkout's releases</a>.</em></p>\n<blockquote>\n<h2>v3.5.1</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Improve checkout performance on Windows runners by upgrading <code>@\u200bactions/github</code> dependency by <a href=\"https://github.com/BrettDong\"><code>@\u200bBrettDong</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1246\">actions/checkout#1246</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/BrettDong\"><code>@\u200bBrettDong</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1246\">actions/checkout#1246</a></li>\n<li><a href=\"https://github.com/fhammerl\"><code>@\u200bfhammerl</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1284\">actions/checkout#1284</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/checkout/compare/v3.5.0...v3.5.1\">https://github.com/actions/checkout/compare/v3.5.0...v3.5.1</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/checkout/blob/main/CHANGELOG.md\">actions/checkout's changelog</a>.</em></p>\n<blockquote>\n<h1>Changelog</h1>\n<h2>v3.5.1</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1246\">Fix slow checkout on Windows</a></li>\n</ul>\n<h2>v3.5.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1237\">Add new public key for known_hosts</a></li>\n</ul>\n<h2>v3.4.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1209\">Upgrade codeql actions to v2</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1210\">Upgrade dependencies</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1225\">Upgrade <code>@\u200bactions/io</code></a></li>\n</ul>\n<h2>v3.3.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1045\">Implement branch list using callbacks from exec function</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1050\">Add in explicit reference to private checkout options</a></li>\n<li>[Fix comment typos (that got added in <a href=\"https://redirect.github.com/actions/checkout/issues/770\">#770</a>)](<a href=\"https://redirect.github.com/actions/checkout/pull/1057\">actions/checkout#1057</a>)</li>\n</ul>\n<h2>v3.2.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/942\">Add GitHub Action to perform release</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/967\">Fix status badge</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1002\">Replace datadog/squid with ubuntu/squid Docker image</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/964\">Wrap pipeline commands for submoduleForeach in quotes</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1029\">Update <code>@\u200bactions/io</code> to 1.1.2</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1039\">Upgrading version to 3.2.0</a></li>\n</ul>\n<h2>v3.1.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/939\">Use <code>@\u200bactions/core</code> <code>saveState</code> and <code>getState</code></a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/922\">Add <code>github-server-url</code> input</a></li>\n</ul>\n<h2>v3.0.2</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/770\">Add input <code>set-safe-directory</code></a></li>\n</ul>\n<h2>v3.0.1</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/762\">Fixed an issue where checkout failed to run in container jobs due to the new git setting <code>safe.directory</code></a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/744\">Bumped various npm package versions</a></li>\n</ul>\n<h2>v3.0.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/689\">Update to node 16</a></li>\n</ul>\n<h2>v2.3.1</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/284\">Fix default branch resolution for .wiki and when using SSH</a></li>\n</ul>\n<h2>v2.3.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/278\">Fallback to the default branch</a></li>\n</ul>\n<h2>v2.2.0</h2>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/checkout/commit/83b7061638ee4956cf7545a6f7efe594e5ad0247\"><code>83b7061</code></a> Release v3.5.1 (<a href=\"https://redirect.github.com/actions/checkout/issues/1284\">#1284</a>)</li>\n<li><a href=\"https://github.com/actions/checkout/commit/40a16ebeed7da831425b665e600750cb36b38d06\"><code>40a16eb</code></a> Improve checkout performance on Windows runners by upgrading <code>@\u200bactions/github</code> ...</li>\n<li>See full diff in <a href=\"https://github.com/actions/checkout/compare/v3.5.0...83b7061638ee4956cf7545a6f7efe594e5ad0247\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/checkout&package-manager=github_actions&previous-version=3.5.0&new-version=3.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "changed_files": [
            {
                "filename": ".github/workflows/build_test.yml",
                "patch": "@@ -31,7 +31,7 @@ jobs:\n     runs-on: ubuntu-latest\n     continue-on-error: true\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -51,7 +51,7 @@ jobs:\n     env:\n       WITHOUT_SIMD: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -70,7 +70,7 @@ jobs:\n     env:\n       EXPECT_CPU_FEATURES: \"SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL\"\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -85,7 +85,7 @@ jobs:\n     runs-on: ubuntu-20.04\n     if: github.event_name != 'push'\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -124,7 +124,7 @@ jobs:\n     env:\n       WITHOUT_OPTIMIZATIONS: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -140,7 +140,7 @@ jobs:\n     env:\n       CPU_DISPATCH: \"none\"\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -156,7 +156,7 @@ jobs:\n     env:\n       CPU_DISPATCH: \"max -xop -fma4 -avx512f -avx512cd -avx512_knl -avx512_knm -avx512_skx -avx512_clx -avx512_cnl -avx512_icl\"\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -172,7 +172,7 @@ jobs:\n     env:\n       CPU_DISPATCH: \"SSSE3 SSE41 POPCNT SSE42 AVX F16C\"\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -188,7 +188,7 @@ jobs:\n     env:\n       USE_DEBUG: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -204,7 +204,7 @@ jobs:\n     env:\n       NPY_USE_BLAS_ILP64: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -222,7 +222,7 @@ jobs:\n       RUN_COVERAGE: 1\n       INSTALL_PICKLE5: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -244,7 +244,7 @@ jobs:\n       NPY_LAPACK_ORDER: MKL,OPENBLAS,ATLAS,LAPACK\n       USE_ASV: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -262,7 +262,7 @@ jobs:\n       NPY_USE_BLAS_ILP64: 1\n       NPY_RELAXED_STRIDES_DEBUG: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -278,7 +278,7 @@ jobs:\n     env:\n       USE_WHEEL: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -298,7 +298,7 @@ jobs:\n       # currently unfortunately\n       NPY_PROMOTION_STATE: legacy\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -317,7 +317,7 @@ jobs:\n       ATLAS: None\n       DOWNLOAD_OPENBLAS: ''\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -333,7 +333,7 @@ jobs:\n     env:\n       USE_SDIST: 1\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -348,7 +348,7 @@ jobs:\n     runs-on: ubuntu-22.04\n     if: github.event_name != 'push'\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -403,7 +403,7 @@ jobs:\n     needs: [smoke_test]\n     runs-on: ubuntu-latest\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0\n@@ -433,7 +433,7 @@ jobs:\n     needs: [smoke_test]\n     runs-on: ubuntu-latest\n     steps:\n-    - uses: actions/checkout@24cb9080177205b6e8c946b17badbe402adc938f # v3.4.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.4.0\n       with:\n         submodules: recursive\n         fetch-depth: 0"
            },
            {
                "filename": ".github/workflows/codeql.yml",
                "patch": "@@ -41,7 +41,7 @@ jobs:\n \n     steps:\n       - name: Checkout repository\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n \n       # Initializes the CodeQL tools for scanning.\n       - name: Initialize CodeQL"
            },
            {
                "filename": ".github/workflows/cygwin.yml",
                "patch": "@@ -20,7 +20,7 @@ jobs:\n     runs-on: windows-latest\n     if: \"github.repository == 'numpy/numpy'\"\n     steps:\n-      - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+      - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n         with:\n           submodules: recursive\n           fetch-depth: 0"
            },
            {
                "filename": ".github/workflows/dependency-review.yml",
                "patch": "@@ -15,6 +15,6 @@ jobs:\n     runs-on: ubuntu-latest\n     steps:\n       - name: 'Checkout Repository'\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       - name: 'Dependency Review'\n         uses: actions/dependency-review-action@f46c48ed6d4f1227fb2d9ea62bf6bcbed315589e # v3.0.4"
            },
            {
                "filename": ".github/workflows/emscripten.yml",
                "patch": "@@ -31,7 +31,7 @@ jobs:\n       NODE_VERSION: 18\n     steps:\n       - name: Checkout numpy\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n         with:\n           submodules: true\n           # versioneer.py requires the latest tag to be reachable. Here we"
            },
            {
                "filename": ".github/workflows/linux_meson.yml",
                "patch": "@@ -25,7 +25,7 @@ jobs:\n     if: \"github.repository == 'numpy/numpy'\"\n     runs-on: ubuntu-latest\n     steps:\n-    - uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+    - uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0"
            },
            {
                "filename": ".github/workflows/scorecards.yml",
                "patch": "@@ -25,7 +25,7 @@ jobs:\n \n     steps:\n       - name: \"Checkout code\"\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.1.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.1.0\n         with:\n           persist-credentials: false\n "
            },
            {
                "filename": ".github/workflows/wheels.yml",
                "patch": "@@ -43,7 +43,7 @@ jobs:\n       message: ${{ steps.commit_message.outputs.message }}\n     steps:\n       - name: Checkout numpy\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n         # Gets the correct commit message for pull request\n         with:\n           ref: ${{ github.event.pull_request.head.sha }}\n@@ -92,7 +92,7 @@ jobs:\n       IS_SCHEDULE_DISPATCH: ${{ github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' }}\n     steps:\n       - name: Checkout numpy\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n         with:\n           submodules: true\n           # versioneer.py requires the latest tag to be reachable. Here we\n@@ -171,7 +171,7 @@ jobs:\n       # IS_SCHEDULE_DISPATCH: ${{ github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' }}\n     steps:\n       - name: Checkout numpy\n-        uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+        uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n         with:\n           submodules: true\n           # versioneer.py requires the latest tag to be reachable. Here we"
            },
            {
                "filename": ".github/workflows/windows_meson.yml",
                "patch": "@@ -23,7 +23,7 @@ jobs:\n     # if: \"github.repository == 'numpy/numpy'\"\n     steps:\n     - name: Checkout\n-      uses: actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3 # v3.5.0\n+      uses: actions/checkout@83b7061638ee4956cf7545a6f7efe594e5ad0247 # v3.5.1\n       with:\n         submodules: recursive\n         fetch-depth: 0"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23484,
        "body": "This fixes the issue with `fill` for `stringdtype` pointed out in https://github.com/numpy/numpy-user-dtypes/issues/49.\r\n\r\nI tried a benchmark filling a tiny object array and found the extra indirection made a negligible impact on performance. Before:\r\n\r\n```\r\nIn [2]: %%timeit\r\n   ...: for _ in range(100000):\r\n   ...:     arr = np.empty(5, dtype=object)\r\n   ...:     arr.fill('hello')\r\n   ...: \r\n234 ms \u00b1 1.38 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nAfter:\r\n\r\n```\r\nIn [2]: %%timeit\r\n   ...: for _ in range(100000):\r\n   ...:     arr = np.empty(5, dtype=object)\r\n   ...:     arr.fill('hello')\r\n   ...: \r\n241 ms \u00b1 3.01 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/convert.c",
                "patch": "@@ -21,6 +21,7 @@\n \n #include \"convert.h\"\n #include \"array_coercion.h\"\n+#include \"refcount.h\"\n \n int\n fallocate(int fd, int mode, off_t offset, off_t len);\n@@ -416,7 +417,7 @@ PyArray_FillWithScalar(PyArrayObject *arr, PyObject *obj)\n             descr, value);\n \n     if (PyDataType_REFCHK(descr)) {\n-        PyArray_Item_XDECREF(value, descr);\n+        PyArray_ClearBuffer(descr, value, 0, 1, 1);\n     }\n     PyMem_FREE(value_buffer_heap);\n     return retcode;"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23380,
        "body": "Update submodule to point to latest commit. There are no changes in performance or features.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/npysort/x86-simd-sort",
                "patch": "@@ -1 +1 @@\n-Subproject commit 7d7591cf5927e83e4a1e7c4b6f2c4dc91a97889f\n+Subproject commit 58501d026a390895f7fd7ebbe0fb7aea55055ad7"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23269,
        "body": "Fixes #22556\r\nFixes #21022\r\n\r\n* add regression test and fix for gh-22556, where we were relying on the array `copy` arg to deepcopy a compound object type; I thought about performance issues here, but if you are already in the land of `object` and you are explicitly opting in to `deepcopy`, it seems like performance might be wishful thinking anyway\r\n\r\n* add regression test and fix for gh-21022--this one was weirder but seems possible to sidestep by not trying to assign a shape of `()` to something that already has shape `()` and a non-writeable `shape` attribute",
        "changed_files": [
            {
                "filename": "numpy/ma/core.py",
                "patch": "@@ -2870,7 +2870,13 @@ def __new__(cls, data=None, mask=nomask, dtype=None, copy=False,\n                     _data._mask = _data._mask.copy()\n                     # Reset the shape of the original mask\n                     if getmask(data) is not nomask:\n-                        data._mask.shape = data.shape\n+                        # gh-21022 encounters an issue here\n+                        # because data._mask.shape is not writeable, but\n+                        # the op was also pointless in that case, because\n+                        # the shapes were the same, so we can at least\n+                        # avoid that path\n+                        if data._mask.shape != data.shape:\n+                            data._mask.shape = data.shape\n         else:\n             # Case 2. : With a mask in input.\n             # If mask is boolean, create an array of True or False\n@@ -6300,6 +6306,12 @@ def __deepcopy__(self, memo=None):\n         memo[id(self)] = copied\n         for (k, v) in self.__dict__.items():\n             copied.__dict__[k] = deepcopy(v, memo)\n+        # as clearly documented for np.copy(), you need to use\n+        # deepcopy() directly for arrays of object type that may\n+        # contain compound types--you cannot depend on normal\n+        # copy semantics to do the right thing here\n+        if self.dtype.hasobject:\n+            copied._data[...] = deepcopy(copied._data)\n         return copied\n \n "
            },
            {
                "filename": "numpy/ma/tests/test_core.py",
                "patch": "@@ -8,6 +8,7 @@\n \n import sys\n import warnings\n+import copy\n import operator\n import itertools\n import textwrap\n@@ -5570,3 +5571,47 @@ def method(self):\n original note\"\"\"\n \n     assert_equal(np.ma.core.doc_note(method.__doc__, \"note\"), expected_doc)\n+\n+\n+def test_gh_22556():\n+    source = np.ma.array([0, [0, 1, 2]], dtype=object)\n+    deepcopy = copy.deepcopy(source)\n+    deepcopy[1].append('this should not appear in source')\n+    assert len(source[1]) == 3\n+\n+\n+def test_gh_21022():\n+    # testing for absence of reported error\n+    source = np.ma.masked_array(data=[-1, -1], mask=True, dtype=np.float64)\n+    axis = np.array(0)\n+    result = np.prod(source, axis=axis, keepdims=False)\n+    result = np.ma.masked_array(result,\n+                                mask=np.ones(result.shape, dtype=np.bool_))\n+    array = np.ma.masked_array(data=-1, mask=True, dtype=np.float64)\n+    copy.deepcopy(array)\n+    copy.deepcopy(result)\n+\n+\n+def test_deepcopy_2d_obj():\n+    source = np.ma.array([[0, \"dog\"],\n+                          [1, 1],\n+                          [[1, 2], \"cat\"]],\n+                        mask=[[0, 1],\n+                              [0, 0],\n+                              [0, 0]],\n+                        dtype=object)\n+    deepcopy = copy.deepcopy(source)\n+    deepcopy[2, 0].extend(['this should not appear in source', 3])\n+    assert len(source[2, 0]) == 2\n+    assert len(deepcopy[2, 0]) == 4\n+    assert_equal(deepcopy._mask, source._mask)\n+    deepcopy._mask[0, 0] = 1\n+    assert source._mask[0, 0] == 0\n+\n+\n+def test_deepcopy_0d_obj():\n+    source = np.ma.array(0, mask=[0], dtype=object)\n+    deepcopy = copy.deepcopy(source)\n+    deepcopy[...] = 17\n+    assert_equal(source, 0)\n+    assert_equal(deepcopy, 17)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23153,
        "body": "Attribute-based CPU dispatching is no longer needed, we are entirely counting on dispatchable sources.\r\n\r\n\r\nTODO:\r\n- [x] benchmark to check if there's a necessity to auto-vectorize AVX512_SKX\r\n\r\nAbandon the idea of providing binary objects for AVX512_SKX due to the massive increase\r\n in binary size over +400k (striped) with no vast change in performance except\r\n for reciprocal on some data types, tested against gcc/12.2.1 build.\r\n\r\n<details>\r\n<summary>Benchmark for AVX512_SKX VS AVX2</summary>\r\n\r\n```Bash\r\n\r\n    AVX512_SKX         AVX2                    ratio\r\n     [e0e4acb70c]       [0a56560e]\r\n     <removes_old_cpu_dispatcher~1>       <removes_old_cpu_dispatcher>\r\n+     26.0\u00b10.07\u03bcs          125\u00b17\u03bcs     4.83  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'reciprocal'>, 1, 1, 'L')\r\n+     26.0\u00b10.04\u03bcs          122\u00b14\u03bcs     4.71  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'reciprocal'>, 1, 1, 'Q')\r\n+     25.8\u00b10.01\u03bcs        110\u00b10.2\u03bcs     4.24  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'reciprocal'>, 1, 1, 'q')\r\n+     25.9\u00b10.08\u03bcs        108\u00b10.6\u03bcs     4.18  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'reciprocal'>, 1, 1, 'l')\r\n+     26.3\u00b10.04\u03bcs      43.7\u00b10.04\u03bcs     1.66  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'reciprocal'>, 1, 1, 'I')\r\n+     1.63\u00b10.03\u03bcs      2.39\u00b10.02\u03bcs     1.47  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'l')\r\n+     1.64\u00b10.05\u03bcs         2.39\u00b10\u03bcs     1.46  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'Q')\r\n+     1.63\u00b10.05\u03bcs      2.37\u00b10.01\u03bcs     1.45  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'L')\r\n+     1.64\u00b10.04\u03bcs      2.37\u00b10.01\u03bcs     1.45  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'q')\r\n+     1.63\u00b10.01\u03bcs      2.36\u00b10.01\u03bcs     1.44  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_and'>, 1, 1, 1, 'q')\r\n+     1.63\u00b10.01\u03bcs         2.35\u00b10\u03bcs     1.44  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_and'>, 1, 1, 1, 'L')\r\n+     1.64\u00b10.02\u03bcs      2.36\u00b10.02\u03bcs     1.44  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_and'>, 1, 1, 1, 'Q')\r\n+     1.64\u00b10.01\u03bcs         2.35\u00b10\u03bcs     1.43  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_and'>, 1, 1, 1, 'l')\r\n+     8.69\u00b10.07\u03bcs      12.5\u00b10.03\u03bcs     1.43  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'l')\r\n+     8.75\u00b10.09\u03bcs      12.4\u00b10.09\u03bcs     1.42  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'L')\r\n+     8.65\u00b10.09\u03bcs       12.3\u00b10.1\u03bcs     1.42  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'Q')\r\n+     8.63\u00b10.02\u03bcs       12.2\u00b10.2\u03bcs     1.42  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'q')\r\n+        1.63\u00b10\u03bcs      2.28\u00b10.01\u03bcs     1.39  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'l')\r\n+        1.63\u00b10\u03bcs         2.27\u00b10\u03bcs     1.39  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'L')\r\n+        1.63\u00b10\u03bcs         2.26\u00b10\u03bcs     1.39  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'Q')\r\n+     1.64\u00b10.01\u03bcs         2.26\u00b10\u03bcs     1.38  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_and'>, 1, 1, 1, 'l')\r\n+     1.64\u00b10.01\u03bcs         2.26\u00b10\u03bcs     1.38  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_and'>, 1, 1, 1, 'Q')\r\n+     1.64\u00b10.01\u03bcs         2.27\u00b10\u03bcs     1.38  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_and'>, 1, 1, 1, 'L')\r\n+     1.72\u00b10.06\u03bcs      2.37\u00b10.01\u03bcs     1.38  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'L')\r\n+     1.64\u00b10.01\u03bcs         2.26\u00b10\u03bcs     1.38  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_and'>, 1, 1, 1, 'q')\r\n+     1.64\u00b10.01\u03bcs         2.26\u00b10\u03bcs     1.38  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'q')\r\n+     1.74\u00b10.05\u03bcs         2.39\u00b10\u03bcs     1.37  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'q')\r\n+     1.75\u00b10.08\u03bcs      2.37\u00b10.01\u03bcs     1.36  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'Q')\r\n+     1.74\u00b10.03\u03bcs      2.35\u00b10.01\u03bcs     1.35  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'Q')\r\n+     1.75\u00b10.02\u03bcs      2.35\u00b10.01\u03bcs     1.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'L')\r\n+     1.75\u00b10.04\u03bcs         2.36\u00b10\u03bcs     1.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'l')\r\n+     1.77\u00b10.05\u03bcs      2.37\u00b10.01\u03bcs     1.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'l')\r\n+     1.77\u00b10.02\u03bcs         2.35\u00b10\u03bcs     1.33  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'q')\r\n+     1.69\u00b10.01\u03bcs      2.20\u00b10.01\u03bcs     1.31  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'l')\r\n+     1.69\u00b10.02\u03bcs      2.20\u00b10.01\u03bcs     1.30  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'L')\r\n+     1.69\u00b10.02\u03bcs      2.21\u00b10.01\u03bcs     1.30  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'Q')\r\n+     1.70\u00b10.02\u03bcs      2.21\u00b10.01\u03bcs     1.30  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'q')\r\n+     1.70\u00b10.01\u03bcs      2.18\u00b10.01\u03bcs     1.28  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_or'>, 1, 1, 1, 'l')\r\n+        1.69\u00b10\u03bcs      2.15\u00b10.01\u03bcs     1.27  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_or'>, 1, 1, 1, 'L')\r\n+     1.70\u00b10.01\u03bcs      2.16\u00b10.01\u03bcs     1.27  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_or'>, 1, 1, 1, 'Q')\r\n+     1.71\u00b10.01\u03bcs      2.17\u00b10.01\u03bcs     1.27  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_or'>, 1, 1, 1, 'q')\r\n+         886\u00b17ns         1.10\u00b10\u03bcs     1.24  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'H')\r\n+         895\u00b16ns         1.10\u00b10\u03bcs     1.23  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'H')\r\n+         897\u00b17ns         1.10\u00b10\u03bcs     1.23  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'h')\r\n+     4.25\u00b10.02\u03bcs      5.03\u00b10.01\u03bcs     1.18  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'I')\r\n+     1.31\u00b10.02\u03bcs         1.54\u00b10\u03bcs     1.18  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'I')\r\n+     1.31\u00b10.01\u03bcs         1.54\u00b10\u03bcs     1.17  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'i')\r\n+     4.27\u00b10.08\u03bcs         4.95\u00b10\u03bcs     1.16  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'i')\r\n+        996\u00b120ns         1.15\u00b10\u03bcs     1.16  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'h')\r\n+        1.21\u00b10\u03bcs         1.40\u00b10\u03bcs     1.15  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'b')\r\n+     1.33\u00b10.03\u03bcs      1.54\u00b10.01\u03bcs     1.15  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'I')\r\n+        1.19\u00b10\u03bcs         1.37\u00b10\u03bcs     1.15  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'B')\r\n+     1.34\u00b10.01\u03bcs         1.53\u00b10\u03bcs     1.14  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'i')\r\n+     1.32\u00b10.02\u03bcs         1.51\u00b10\u03bcs     1.14  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_and'>, 1, 1, 1, 'I')\r\n+        1.19\u00b10\u03bcs         1.36\u00b10\u03bcs     1.14  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'b')\r\n+     1.32\u00b10.01\u03bcs         1.50\u00b10\u03bcs     1.14  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_and'>, 1, 1, 1, 'i')\r\n+     1.36\u00b10.01\u03bcs      1.54\u00b10.01\u03bcs     1.14  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'h')\r\n+        1.20\u00b10\u03bcs         1.36\u00b10\u03bcs     1.13  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'B')\r\n+        1.43\u00b10\u03bcs      1.61\u00b10.01\u03bcs     1.13  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'H')\r\n+        1.43\u00b10\u03bcs         1.61\u00b10\u03bcs     1.12  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'H')\r\n+        1.38\u00b10\u03bcs         1.55\u00b10\u03bcs     1.12  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'h')\r\n+     1.34\u00b10.01\u03bcs         1.51\u00b10\u03bcs     1.12  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'I')\r\n+     1.35\u00b10.02\u03bcs         1.51\u00b10\u03bcs     1.12  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'i')\r\n+      15.3\u00b10.5\u03bcs      17.1\u00b10.03\u03bcs     1.11  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'l')\r\n+      15.3\u00b10.4\u03bcs      17.1\u00b10.06\u03bcs     1.11  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'q')\r\n+        1.05\u00b10\u03bcs         1.16\u00b10\u03bcs     1.11  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_and'>, 1, 1, 1, 'i')\r\n+     1.41\u00b10.01\u03bcs         1.57\u00b10\u03bcs     1.11  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'b')\r\n+     1.34\u00b10.01\u03bcs         1.49\u00b10\u03bcs     1.11  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'I')\r\n+        1.34\u00b10\u03bcs      1.48\u00b10.01\u03bcs     1.11  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'i')\r\n+     1.05\u00b10.01\u03bcs         1.16\u00b10\u03bcs     1.11  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_and'>, 1, 1, 1, 'I')\r\n+     1.05\u00b10.01\u03bcs         1.16\u00b10\u03bcs     1.10  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'I')\r\n+     1.43\u00b10.02\u03bcs      1.58\u00b10.01\u03bcs     1.10  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'b')\r\n+      15.0\u00b10.3\u03bcs      16.6\u00b10.05\u03bcs     1.10  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'Q')\r\n+      15.1\u00b10.2\u03bcs       16.5\u00b10.1\u03bcs     1.10  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'l')\r\n+      15.0\u00b10.3\u03bcs      16.5\u00b10.02\u03bcs     1.10  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'l')\r\n+        1.06\u00b10\u03bcs         1.16\u00b10\u03bcs     1.10  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'i')\r\n+     1.50\u00b10.01\u03bcs         1.64\u00b10\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'B')\r\n+     1.51\u00b10.01\u03bcs         1.64\u00b10\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'B')\r\n+        1.60\u00b10\u03bcs      1.74\u00b10.01\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_or'>, 1, 1, 1, 'L')\r\n+      15.1\u00b10.5\u03bcs      16.4\u00b10.03\u03bcs     1.09  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'q')\r\n+     1.34\u00b10.01\u03bcs         1.45\u00b10\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_or'>, 1, 1, 1, 'I')\r\n+        1.60\u00b10\u03bcs         1.74\u00b10\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_or'>, 1, 1, 1, 'Q')\r\n+        1.60\u00b10\u03bcs         1.74\u00b10\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_or'>, 1, 1, 1, 'q')\r\n+        1.61\u00b10\u03bcs         1.74\u00b10\u03bcs     1.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_or'>, 1, 1, 1, 'l')\r\n+      7.23\u00b10.3\u03bcs      7.84\u00b10.06\u03bcs     1.08  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'i')\r\n+      15.1\u00b10.3\u03bcs      16.4\u00b10.05\u03bcs     1.08  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'L')\r\n+     3.28\u00b10.04\u03bcs      3.54\u00b10.01\u03bcs     1.08  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'h')\r\n+      15.1\u00b10.2\u03bcs      16.2\u00b10.02\u03bcs     1.07  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'invert'>, 1, 1, 'q')\r\n+     1.14\u00b10.01\u03bcs      1.23\u00b10.01\u03bcs     1.07  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'h')\r\n+     1.35\u00b10.01\u03bcs      1.44\u00b10.01\u03bcs     1.07  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_or'>, 1, 1, 1, 'i')\r\n+      15.1\u00b10.1\u03bcs      16.2\u00b10.04\u03bcs     1.07  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'q')\r\n+      7.09\u00b10.1\u03bcs       7.59\u00b10.1\u03bcs     1.07  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'positive'>, 1, 1, 'i')\r\n+     2.60\u00b10.03\u03bcs      2.78\u00b10.04\u03bcs     1.07  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'h')\r\n+     3.29\u00b10.07\u03bcs      3.50\u00b10.07\u03bcs     1.06  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'conjugate'>, 1, 1, 'H')\r\n+        1.16\u00b10\u03bcs      1.23\u00b10.01\u03bcs     1.06  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_and'>, 1, 1, 1, 'H')\r\n+     2.57\u00b10.02\u03bcs      2.73\u00b10.01\u03bcs     1.06  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'logical_not'>, 1, 1, 'H')\r\n+     3.29\u00b10.07\u03bcs      3.49\u00b10.03\u03bcs     1.06  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'H')\r\n+        1.18\u00b10\u03bcs      1.24\u00b10.01\u03bcs     1.06  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'H')\r\n+     6.94\u00b10.04\u03bcs       7.32\u00b10.1\u03bcs     1.06  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'conjugate'>, 1, 1, 'I')\r\n+     1.16\u00b10.01\u03bcs         1.23\u00b10\u03bcs     1.05  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'h')\r\n+     3.17\u00b10.03\u03bcs      3.33\u00b10.06\u03bcs     1.05  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'positive'>, 1, 1, 'h')\r\n+        1.15\u00b10\u03bcs      1.21\u00b10.01\u03bcs     1.05  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'h')\r\n+     1.15\u00b10.01\u03bcs         1.21\u00b10\u03bcs     1.05  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_or'>, 1, 1, 1, 'H')\r\n-     1.04\u00b10.02\u03bcs          992\u00b13ns     0.95  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'bitwise_and'>, 1, 1, 1, 'b')\r\n-     1.00\u00b10.01\u03bcs          950\u00b11ns     0.95  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'right_shift'>, 1, 1, 1, 'b')\r\n-     2.34\u00b10.03\u03bcs      2.22\u00b10.01\u03bcs     0.95  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'bitwise_xor'>, 1, 1, 1, 'l')\r\n-     2.09\u00b10.05\u03bcs         1.98\u00b10\u03bcs     0.95  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'B')\r\n-      7.29\u00b10.1\u03bcs      6.90\u00b10.01\u03bcs     0.95  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'i')\r\n-     2.10\u00b10.06\u03bcs         1.98\u00b10\u03bcs     0.94  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'square'>, 1, 1, 'b')\r\n-     2.34\u00b10.03\u03bcs      2.20\u00b10.01\u03bcs     0.94  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'right_shift'>, 1, 1, 1, 'l')\r\n-     2.24\u00b10.02\u03bcs      2.10\u00b10.01\u03bcs     0.94  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'subtract'>, 1, 1, 1, 'Q')\r\n-         639\u00b12ns        600\u00b10.9ns     0.94  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'multiply'>, 1, 1, 1, 'b')\r\n-         640\u00b17ns        596\u00b10.6ns     0.93  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'multiply'>, 1, 1, 1, 'B')\r\n-     2.27\u00b10.08\u03bcs         2.10\u00b10\u03bcs     0.93  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'subtract'>, 1, 1, 1, 'l')\r\n```\r\n</details>\r\n\r\n\r\n------\r\n\r\n### X86\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         48 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 7700X 8-Core Processor\r\n    CPU family:          25\r\n    Model:               97\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            2\r\n    Frequency boost:     disabled\r\n    CPU(s) scaling MHz:  67%\r\n    CPU max MHz:         5572.2651\r\n    CPU min MHz:         3000.0000\r\n    BogoMIPS:            8986.45\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep\r\n                         mtrr pge mca cmov pat pse36 clflush mmx fxsr\r\n                          sse sse2 ht syscall nx mmxext fxsr_opt pdpe\r\n                         1gb rdtscp lm constant_tsc rep_good amd_lbr_\r\n                         v2 nopl nonstop_tsc cpuid extd_apicid aperfm\r\n                         perf rapl pni pclmulqdq monitor ssse3 fma cx\r\n                         16 sse4_1 sse4_2 x2apic movbe popcnt aes xsa\r\n                         ve avx f16c rdrand lahf_lm cmp_legacy svm ex\r\n                         tapic cr8_legacy abm sse4a misalignsse 3dnow\r\n                         prefetch osvw ibs skinit wdt tce topoext per\r\n                         fctr_core perfctr_nb bpext perfctr_llc mwait\r\n                         x cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfm\r\n                         on_v2 ibrs ibpb stibp vmmcall fsgsbase bmi1\r\n                         avx2 smep bmi2 erms invpcid cqm rdt_a avx512\r\n                         f avx512dq rdseed adx smap avx512ifma clflus\r\n                         hopt clwb avx512cd sha_ni avx512bw avx512vl\r\n                         xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_o\r\n                         ccup_llc cqm_mbm_total cqm_mbm_local avx512_\r\n                         bf16 clzero irperf xsaveerptr rdpru wbnoinvd\r\n                          cppc arat npt lbrv svm_lock nrip_save tsc_s\r\n                         cale vmcb_clean flushbyasid decodeassists pa\r\n                         usefilter pfthreshold avic v_vmsave_vmload v\r\n                         gif x2avic v_spec_ctrl avx512vbmi umip pku o\r\n                         spke avx512_vbmi2 gfni vaes vpclmulqdq avx51\r\n                         2_vnni avx512_bitalg avx512_vpopcntdq rdpid\r\n                         overflow_recov succor smca fsrm flush_l1d\r\nVirtualization features:\r\n  Virtualization:        AMD-V\r\nCaches (sum of all):\r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    8 MiB (8 instances)\r\n  L3:                    32 MiB (1 instance)\r\nNUMA:\r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disable\r\n                         d via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __u\r\n                         ser pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IB\r\n                         RS_FW, STIBP always-on, RSB filling, PBRSB-e\r\n                         IBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux seiko-pc 6.1.1-arch1-1 #1 SMP PREEMPT_DYNAMIC Wed, 21 Dec 2022 22:27:55 +0000 x86_64 GNU/Linux\r\nPython 3.10.9\r\ngcc (GCC) 12.2.1\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>Before vs After (AVX2)</summary>\r\n\r\n```Bash\r\npython runtests.py -n --bench-compare parent/main IntContig  -- --cpu-affinity=6,7\r\n```\r\n```Bash   before           after         ratio\r\n       before           after         ratio\r\n     [86450a0c]       [52bf5007]\r\n     <removes_old_cpu_dispatcher~11>       <removes_old_cpu_dispatcher>\r\n+         559\u00b12ns         609\u00b130ns     1.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'bitwise_xor'>, 1, 1, 1, 'B')\r\n+         563\u00b12ns         600\u00b110ns     1.07  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'bitwise_xor'>, 1, 1, 1, 'b')\r\n+         561\u00b15ns         597\u00b120ns     1.06  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'bitwise_or'>, 1, 1, 1, 'b')\r\n+         555\u00b13ns         584\u00b120ns     1.05  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'bitwise_and'>, 1, 1, 1, 'b')\r\n-     15.9\u00b10.03\u03bcs       15.1\u00b10.5\u03bcs     0.95  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'L')\r\n-         111\u00b11\u03bcs          106\u00b11\u03bcs     0.95  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'reciprocal'>, 1, 1, 'l')\r\n-     2.39\u00b10.02\u03bcs      2.26\u00b10.09\u03bcs     0.95  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'multiply'>, 1, 1, 1, 'L')\r\n-        2.39\u00b10\u03bcs       2.26\u00b10.1\u03bcs     0.95  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'multiply'>, 1, 1, 1, 'l')\r\n-     2.41\u00b10.02\u03bcs       2.27\u00b10.1\u03bcs     0.94  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'multiply'>, 1, 1, 1, 'q')\r\n-     15.9\u00b10.06\u03bcs       14.9\u00b10.4\u03bcs     0.94  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'Q')\r\n-     1.69\u00b10.01\u03bcs         1.59\u00b10\u03bcs     0.94  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'right_shift'>, 1, 1, 1, 'i')\r\n-     2.42\u00b10.02\u03bcs       2.27\u00b10.1\u03bcs     0.94  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'multiply'>, 1, 1, 1, 'Q')\r\n-     1.07\u00b10.01\u03bcs      1.00\u00b10.01\u03bcs     0.94  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'right_shift'>, 1, 1, 1, 'b')\r\n-     2.03\u00b10.03\u03bcs      1.79\u00b10.02\u03bcs     0.88  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'b')\r\n-     7.74\u00b10.05\u03bcs      6.82\u00b10.02\u03bcs     0.88  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'I')\r\n-     3.80\u00b10.05\u03bcs       3.32\u00b10.1\u03bcs     0.87  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'h')\r\n-     21.4\u00b10.04\u03bcs       16.2\u00b10.2\u03bcs     0.75  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'q')\r\n-     21.5\u00b10.06\u03bcs      16.1\u00b10.09\u03bcs     0.75  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'l')\r\n-     9.19\u00b10.05\u03bcs      6.83\u00b10.07\u03bcs     0.74  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'absolute'>, 1, 1, 'i')\r\n-     3.37\u00b10.02\u03bcs      2.16\u00b10.06\u03bcs     0.64  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'Q')\r\n-        3.42\u00b10\u03bcs      2.17\u00b10.06\u03bcs     0.63  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'q')\r\n-     3.51\u00b10.02\u03bcs      2.16\u00b10.06\u03bcs     0.61  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'L')\r\n-        3.63\u00b10\u03bcs      2.22\u00b10.07\u03bcs     0.61  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'l')\r\n-     12.8\u00b10.08\u03bcs      7.76\u00b10.03\u03bcs     0.61  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'i')\r\n-     4.00\u00b10.03\u03bcs      2.38\u00b10.02\u03bcs     0.59  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'right_shift'>, 1, 1, 1, 'q')\r\n-     3.88\u00b10.01\u03bcs      2.30\u00b10.06\u03bcs     0.59  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'right_shift'>, 1, 1, 1, 'l')\r\n-        3.13\u00b10\u03bcs      1.82\u00b10.04\u03bcs     0.58  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'b')\r\n-     4.47\u00b10.01\u03bcs      2.57\u00b10.05\u03bcs     0.57  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'Q')\r\n-     4.37\u00b10.02\u03bcs      2.51\u00b10.03\u03bcs     0.57  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'L')\r\n-        5.94\u00b10\u03bcs       3.35\u00b10.1\u03bcs     0.56  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'h')\r\n-      35.4\u00b10.8\u03bcs       16.6\u00b10.3\u03bcs     0.47  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'l')\r\n-      35.4\u00b10.7\u03bcs       16.5\u00b10.4\u03bcs     0.47  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'q')\r\n-     4.03\u00b10.01\u03bcs      1.81\u00b10.01\u03bcs     0.45  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'I')\r\n-      4.91\u00b10.1\u03bcs      2.18\u00b10.06\u03bcs     0.44  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'q')\r\n-     3.74\u00b10.01\u03bcs         1.66\u00b10\u03bcs     0.44  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'B')\r\n-     6.00\u00b10.02\u03bcs      2.64\u00b10.02\u03bcs     0.44  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'q')\r\n-     4.02\u00b10.01\u03bcs      1.72\u00b10.02\u03bcs     0.43  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'i')\r\n-      34.9\u00b10.4\u03bcs       14.9\u00b10.3\u03bcs     0.43  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'Q')\r\n-     5.90\u00b10.03\u03bcs      2.51\u00b10.03\u03bcs     0.43  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'l')\r\n-        5.14\u00b10\u03bcs      2.16\u00b10.06\u03bcs     0.42  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'Q')\r\n-     3.76\u00b10.01\u03bcs         1.56\u00b10\u03bcs     0.42  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'b')\r\n-      35.5\u00b10.4\u03bcs       14.7\u00b10.4\u03bcs     0.41  bench_ufunc_strides.UnaryIntContig.time_unary(<ufunc 'sign'>, 1, 1, 'L')\r\n-        5.34\u00b10\u03bcs      2.16\u00b10.06\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'l')\r\n-        5.38\u00b10\u03bcs      2.16\u00b10.06\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'L')\r\n-        3.41\u00b10\u03bcs      1.37\u00b10.01\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'B')\r\n-     6.29\u00b10.03\u03bcs      2.52\u00b10.01\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'q')\r\n-     6.42\u00b10.02\u03bcs      2.57\u00b10.04\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'Q')\r\n-     6.31\u00b10.01\u03bcs      2.52\u00b10.03\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'L')\r\n-     4.11\u00b10.02\u03bcs         1.63\u00b10\u03bcs     0.40  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'H')\r\n-     4.33\u00b10.01\u03bcs      1.70\u00b10.02\u03bcs     0.39  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'i')\r\n-     6.18\u00b10.01\u03bcs      2.41\u00b10.04\u03bcs     0.39  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'l')\r\n-     4.14\u00b10.01\u03bcs         1.58\u00b10\u03bcs     0.38  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'b')\r\n-        3.37\u00b10\u03bcs      1.27\u00b10.02\u03bcs     0.38  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'i')\r\n-        3.43\u00b10\u03bcs      1.27\u00b10.02\u03bcs     0.37  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'I')\r\n-     6.38\u00b10.01\u03bcs      2.28\u00b10.01\u03bcs     0.36  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'L')\r\n-        6.41\u00b10\u03bcs         2.28\u00b10\u03bcs     0.36  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'q')\r\n-     6.40\u00b10.01\u03bcs         2.28\u00b10\u03bcs     0.36  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'l')\r\n-        6.41\u00b10\u03bcs         2.27\u00b10\u03bcs     0.35  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'Q')\r\n-        3.98\u00b10\u03bcs      1.36\u00b10.01\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'b')\r\n-        6.96\u00b10\u03bcs      2.37\u00b10.01\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'L')\r\n-     6.99\u00b10.01\u03bcs      2.37\u00b10.02\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'Q')\r\n-        6.98\u00b10\u03bcs      2.36\u00b10.01\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'l')\r\n-     7.00\u00b10.01\u03bcs      2.37\u00b10.02\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'q')\r\n-     6.98\u00b10.01\u03bcs      2.36\u00b10.01\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'L')\r\n-        6.98\u00b10\u03bcs      2.36\u00b10.03\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'q')\r\n-     6.99\u00b10.02\u03bcs      2.36\u00b10.02\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'Q')\r\n-        6.98\u00b10\u03bcs      2.35\u00b10.02\u03bcs     0.34  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'l')\r\n-     5.29\u00b10.02\u03bcs      1.64\u00b10.02\u03bcs     0.31  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'H')\r\n-        3.68\u00b10\u03bcs         1.12\u00b10\u03bcs     0.31  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'H')\r\n-        3.71\u00b10\u03bcs         1.13\u00b10\u03bcs     0.30  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'H')\r\n-     5.96\u00b10.02\u03bcs      1.79\u00b10.01\u03bcs     0.30  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'I')\r\n-     5.71\u00b10.01\u03bcs         1.66\u00b10\u03bcs     0.29  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'B')\r\n-        5.53\u00b10\u03bcs         1.56\u00b10\u03bcs     0.28  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'left_shift'>, 1, 1, 1, 'h')\r\n-     5.68\u00b10.01\u03bcs         1.57\u00b10\u03bcs     0.28  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'right_shift'>, 1, 1, 1, 'h')\r\n-        5.16\u00b10\u03bcs         1.37\u00b10\u03bcs     0.27  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'B')\r\n-     5.38\u00b10.01\u03bcs         1.40\u00b10\u03bcs     0.26  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'b')\r\n-        5.14\u00b10\u03bcs      1.26\u00b10.02\u03bcs     0.25  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'I')\r\n-        5.22\u00b10\u03bcs      1.27\u00b10.02\u03bcs     0.24  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'i')\r\n-        4.94\u00b10\u03bcs         1.12\u00b10\u03bcs     0.23  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'left_shift'>, 1, 1, 1, 'h')\r\n-     6.96\u00b10.01\u03bcs      1.53\u00b10.01\u03bcs     0.22  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'i')\r\n-     6.96\u00b10.01\u03bcs      1.52\u00b10.01\u03bcs     0.22  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'I')\r\n-     6.94\u00b10.01\u03bcs      1.52\u00b10.02\u03bcs     0.22  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'i')\r\n-     6.94\u00b10.01\u03bcs      1.51\u00b10.03\u03bcs     0.22  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'I')\r\n-     5.38\u00b10.02\u03bcs         1.16\u00b10\u03bcs     0.22  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'right_shift'>, 1, 1, 1, 'h')\r\n-        6.37\u00b10\u03bcs      1.18\u00b10.01\u03bcs     0.19  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'i')\r\n-     6.36\u00b10.01\u03bcs      1.17\u00b10.01\u03bcs     0.18  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'I')\r\n-        6.91\u00b10\u03bcs         1.22\u00b10\u03bcs     0.18  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'h')\r\n-        6.91\u00b10\u03bcs      1.22\u00b10.01\u03bcs     0.18  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'H')\r\n-        6.91\u00b10\u03bcs      1.22\u00b10.02\u03bcs     0.18  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'H')\r\n-     6.90\u00b10.02\u03bcs      1.20\u00b10.03\u03bcs     0.17  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'h')\r\n-     6.84\u00b10.02\u03bcs      1.05\u00b10.01\u03bcs     0.15  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'B')\r\n-     6.83\u00b10.01\u03bcs      1.05\u00b10.01\u03bcs     0.15  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'B')\r\n-     6.84\u00b10.02\u03bcs      1.04\u00b10.01\u03bcs     0.15  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in1(<ufunc 'logical_xor'>, 1, 1, 1, 'b')\r\n-     6.83\u00b10.01\u03bcs      1.03\u00b10.01\u03bcs     0.15  bench_ufunc_strides.BinaryIntContig.time_binary_scalar_in0(<ufunc 'logical_xor'>, 1, 1, 1, 'b')\r\n-        6.34\u00b10\u03bcs          769\u00b17ns     0.12  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'h')\r\n-     6.33\u00b10.01\u03bcs          766\u00b14ns     0.12  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'H')\r\n-        6.40\u00b10\u03bcs          600\u00b16ns     0.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'B')\r\n-        6.40\u00b10\u03bcs          598\u00b15ns     0.09  bench_ufunc_strides.BinaryIntContig.time_binary(<ufunc 'logical_xor'>, 1, 1, 1, 'b')\r\n\r\n```\r\n</details>\r\n\r\n\r\n#### Binary size(striped)\r\n\r\n| LIB         | Before(KBytes) | After(KBytes) | Diff(KBytes) |\r\n| ----------- | ------------- | ------------ | ---------- |\r\n| _multiarray_umath.cpython-38-x86_64-linux-gnu.so | 6124  | 6296 | 172 |\r\n",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -229,6 +229,7 @@ numpy/core/src/umath/loops_hyperbolic.dispatch.c\n numpy/core/src/umath/loops_modulo.dispatch.c\n numpy/core/src/umath/loops_comparison.dispatch.c\n numpy/core/src/umath/loops_unary_complex.dispatch.c\n+numpy/core/src/umath/loops_autovec.dispatch.c\n # multiarray module\n numpy/core/src/multiarray/argfunc.dispatch.c\n numpy/core/src/multiarray/arraytypes.h"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc_strides.py",
                "patch": "@@ -1,167 +1,178 @@\n-from .common import Benchmark\n+from .common import Benchmark, get_data\n \n import numpy as np\n \n-UNARY_UFUNCS = [obj for obj in np.core.umath.__dict__.values() if\n-        isinstance(obj, np.ufunc)]\n-UNARY_OBJECT_UFUNCS = [uf for uf in UNARY_UFUNCS if \"O->O\" in uf.types]\n-UNARY_OBJECT_UFUNCS.remove(getattr(np, 'invert'))\n+UFUNCS = [obj for obj in np.core.umath.__dict__.values() if\n+          isinstance(obj, np.ufunc)]\n+UFUNCS_UNARY = [uf for uf in UFUNCS if \"O->O\" in uf.types]\n \n-stride = [1, 2, 4]\n-stride_out = [1, 2, 4]\n-dtype = ['e', 'f', 'd']\n-\n-class Unary(Benchmark):\n-    params = [UNARY_OBJECT_UFUNCS, stride, stride_out, dtype]\n-    param_names = ['ufunc', 'stride_in', 'stride_out', 'dtype']\n-    timeout = 10\n-\n-    def setup(self, ufuncname, stride, stride_out, dtype):\n-        np.seterr(all='ignore')\n-        try:\n-            self.f = ufuncname\n-        except AttributeError:\n-            raise NotImplementedError(f\"No ufunc {ufuncname} found\") from None\n-        N = 100000\n-        self.arr_out = np.empty(stride_out*N, dtype)\n-        self.arr = np.random.rand(stride*N).astype(dtype)\n-        if (ufuncname.__name__ == 'arccosh'):\n-            self.arr = 1.0 + self.arr\n-\n-    def time_ufunc(self, ufuncname, stride, stride_out, dtype):\n-        self.f(self.arr[::stride], self.arr_out[::stride_out])\n-\n-class AVX_UFunc_log(Benchmark):\n-    params = [stride, dtype]\n-    param_names = ['stride', 'dtype']\n-    timeout = 10\n-\n-    def setup(self, stride, dtype):\n-        np.seterr(all='ignore')\n-        N = 10000\n-        self.arr = np.array(np.random.random_sample(stride*N), dtype=dtype)\n-\n-    def time_log(self, stride, dtype):\n-        np.log(self.arr[::stride])\n-\n-\n-binary_ufuncs = [\n-    'maximum', 'minimum', 'fmax', 'fmin'\n-]\n-binary_dtype = ['f', 'd']\n-\n-class Binary(Benchmark):\n-    param_names = ['ufunc', 'stride_in0', 'stride_in1', 'stride_out', 'dtype']\n-    params = [binary_ufuncs, stride, stride, stride_out, binary_dtype]\n+class _AbstractBinary(Benchmark):\n+    params = []\n+    param_names = ['ufunc', 'stride_in0', 'stride_in1' 'stride_out', 'dtype']\n     timeout = 10\n+    arrlen = 10000\n+    data_finite = True\n+    data_denormal = False\n+    data_zeros = False\n+\n+    def setup(self, ufunc, stride_in0, stride_in1, stride_out, dtype):\n+        ufunc_insig = f'{dtype}{dtype}->'\n+        if ufunc_insig+dtype not in ufunc.types:\n+            for st_sig in (ufunc_insig, dtype):\n+                test = [sig for sig in ufunc.types if sig.startswith(st_sig)]\n+                if test:\n+                    break\n+            if not test:\n+                raise NotImplementedError(\n+                    f\"Ufunc {ufunc} doesn't support \"\n+                    f\"binary input of dtype {dtype}\"\n+                ) from None\n+            tin, tout = test[0].split('->')\n+        else:\n+            tin = dtype + dtype\n+            tout = dtype\n+\n+        self.ufunc_args = []\n+        for i, (dt, stride) in enumerate(zip(tin, (stride_in0, stride_in1))):\n+            self.ufunc_args += [get_data(\n+                self.arrlen*stride, dt, i,\n+                zeros=self.data_zeros,\n+                finite=self.data_finite,\n+                denormal=self.data_denormal,\n+            )[::stride]]\n+        for dt in tout:\n+            self.ufunc_args += [\n+                np.empty(stride_out*self.arrlen, dt)[::stride_out]\n+            ]\n \n-    def setup(self, ufuncname, stride_in0, stride_in1, stride_out, dtype):\n         np.seterr(all='ignore')\n-        try:\n-            self.f = getattr(np, ufuncname)\n-        except AttributeError:\n-            raise NotImplementedError(f\"No ufunc {ufuncname} found\") from None\n-        N = 100000\n-        self.arr1 = np.array(np.random.rand(stride_in0*N), dtype=dtype)\n-        self.arr2 = np.array(np.random.rand(stride_in1*N), dtype=dtype)\n-        self.arr_out = np.empty(stride_out*N, dtype)\n-\n-    def time_ufunc(self, ufuncname, stride_in0, stride_in1, stride_out, dtype):\n-        self.f(self.arr1[::stride_in0], self.arr2[::stride_in1],\n-               self.arr_out[::stride_out])\n-\n \n-binary_int_ufuncs = ['maximum', 'minimum']\n-binary_int_dtype = ['b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q']\n+    def time_binary(self, ufunc, stride_in0, stride_in1, stride_out,\n+             dtype):\n+        ufunc(*self.ufunc_args)\n \n-class BinaryInt(Binary):\n+    def time_binary_scalar_in0(self, ufunc, stride_in0, stride_in1,\n+                        stride_out, dtype):\n+        ufunc(self.ufunc_args[0][0], *self.ufunc_args[1:])\n \n-    param_names = ['ufunc', 'stride_in0', 'stride_in1', 'stride_out', 'dtype']\n-    params = [binary_int_ufuncs, stride, stride, stride_out, binary_int_dtype]\n-\n-class AVX_ldexp(Benchmark):\n-\n-    params = [dtype, stride]\n-    param_names = ['dtype', 'stride']\n-    timeout = 10\n+    def time_binary_scalar_in1(self, ufunc, stride_in0, stride_in1,\n+                        stride_out, dtype):\n+        ufunc(self.ufunc_args[0], self.ufunc_args[1][0], *self.ufunc_args[2:])\n \n-    def setup(self, dtype, stride):\n-        np.seterr(all='ignore')\n-        self.f = getattr(np, 'ldexp')\n-        N = 10000\n-        self.arr1 = np.array(np.random.rand(stride*N), dtype=dtype)\n-        self.arr2 = np.array(np.random.rand(stride*N), dtype='i')\n-\n-    def time_ufunc(self, dtype, stride):\n-        self.f(self.arr1[::stride], self.arr2[::stride])\n-\n-cmplx_bfuncs = ['add',\n-                'subtract',\n-                'multiply',\n-                'divide']\n-cmplxstride = [1, 2, 4]\n-cmplxdtype  = ['F', 'D']\n-\n-class BinaryComplex(Benchmark):\n-    params = [cmplx_bfuncs, cmplxstride, cmplxstride, cmplxstride, cmplxdtype]\n-    param_names = ['bfunc', 'stride_in0', 'stride_in1' 'stride_out', 'dtype']\n-    timeout = 10\n-\n-    def setup(self, bfuncname, stride_in0, stride_in1, stride_out, dtype):\n-        np.seterr(all='ignore')\n-        try:\n-            self.f = getattr(np, bfuncname)\n-        except AttributeError:\n-            raise NotImplementedError(f\"No bfunc {bfuncname} found\") from None\n-        N = 10000\n-        self.arr1 = np.ones(stride_in0*N, dtype)\n-        self.arr2 = np.ones(stride_in1*N, dtype)\n-        self.arr_out = np.empty(stride_out*N, dtype)\n-\n-    def time_ufunc(self, bfuncname, stride_in0, stride_in1, stride_out,\n-                   dtype):\n-        self.f(self.arr1[::stride_in0], self.arr2[::stride_in1],\n-               self.arr_out[::stride_out])\n-\n-    def time_ufunc_scalar_in0(self, bfuncname, stride_in0, stride_in1,\n-                              stride_out, dtype):\n-        self.f(self.arr1[0], self.arr2[::stride_in1],\n-               self.arr_out[::stride_out])\n-\n-    def time_ufunc_scalar_in1(self, bfuncname, stride_in0, stride_in1,\n-                              stride_out, dtype):\n-        self.f(self.arr1[::stride_in0], self.arr2[0],\n-               self.arr_out[::stride_out])\n-\n-cmplx_ufuncs = ['reciprocal',\n-                'absolute',\n-                'square',\n-                'conjugate']\n-\n-class UnaryComplex(Benchmark):\n-    params = [cmplx_ufuncs, cmplxstride, cmplxstride, cmplxdtype]\n-    param_names = ['bfunc', 'stride_in', 'stride_out', 'dtype']\n+class _AbstractUnary(Benchmark):\n+    params = []\n+    param_names = ['ufunc', 'stride_in', 'stride_out', 'dtype']\n     timeout = 10\n+    arrlen = 10000\n+    data_finite = True\n+    data_denormal = False\n+    data_zeros = False\n+\n+    def setup(self, ufunc, stride_in, stride_out, dtype):\n+        arr_in = get_data(\n+            stride_in*self.arrlen, dtype,\n+            zeros=self.data_zeros,\n+            finite=self.data_finite,\n+            denormal=self.data_denormal,\n+        )\n+        self.ufunc_args = [arr_in[::stride_in]]\n+\n+        ufunc_insig = f'{dtype}->'\n+        if ufunc_insig+dtype not in ufunc.types:\n+            test = [sig for sig in ufunc.types if sig.startswith(ufunc_insig)]\n+            if not test:\n+                raise NotImplementedError(\n+                    f\"Ufunc {ufunc} doesn't support \"\n+                    f\"unary input of dtype {dtype}\"\n+                ) from None\n+            tout = test[0].split('->')[1]\n+        else:\n+            tout = dtype\n+\n+        for dt in tout:\n+            self.ufunc_args += [\n+                np.empty(stride_out*self.arrlen, dt)[::stride_out]\n+            ]\n \n-    def setup(self, bfuncname, stride_in, stride_out, dtype):\n         np.seterr(all='ignore')\n-        try:\n-            self.f = getattr(np, bfuncname)\n-        except AttributeError:\n-            raise NotImplementedError(f\"No bfunc {bfuncname} found\") from None\n-        N = 10000\n-        self.arr1 = np.ones(stride_in*N, dtype)\n-        self.arr_out = np.empty(stride_out*N, dtype)\n \n-    def time_ufunc(self, bfuncname, stride_in, stride_out, dtype):\n-        self.f(self.arr1[::stride_in], self.arr_out[::stride_out])\n+    def time_unary(self, ufunc, stride_in, stride_out, dtype):\n+        ufunc(*self.ufunc_args)\n+\n+class UnaryFP(_AbstractUnary):\n+    params = [UFUNCS_UNARY, [1, 2, 4], [1, 2, 4], ['e', 'f', 'd']]\n+\n+    def setup(self, ufunc, stride_in, stride_out, dtype):\n+        _AbstractUnary.setup(self, ufunc, stride_in, stride_out, dtype)\n+        if (ufunc.__name__ == 'arccosh'):\n+            self.ufunc_args[0] += 1.0\n+\n+class UnaryFPSpecial(UnaryFP):\n+    data_finite = False\n+    data_denormal = True\n+    data_zeros = True\n+\n+class BinaryFP(_AbstractBinary):\n+    params = [\n+        [np.maximum, np.minimum, np.fmax, np.fmin, np.ldexp],\n+        [1, 2, 4], [1, 2, 4], [1, 2, 4], ['f', 'd']\n+    ]\n+\n+class BinaryFPSpecial(BinaryFP):\n+    data_finite = False\n+    data_denormal = True\n+    data_zeros = True\n+\n+class BinaryComplex(_AbstractBinary):\n+    params = [\n+        [np.add, np.subtract, np.multiply, np.divide],\n+        [1, 2, 4], [1, 2, 4], [1, 2, 4],\n+        ['F', 'D']\n+    ]\n+\n+class UnaryComplex(_AbstractUnary):\n+    params = [\n+        [np.reciprocal, np.absolute, np.square, np.conjugate],\n+        [1, 2, 4], [1, 2, 4], ['F', 'D']\n+    ]\n+\n+class BinaryInt(_AbstractBinary):\n+    arrlen = 100000\n+    params = [\n+        [np.maximum, np.minimum],\n+        [1, 2], [1, 2], [1, 2],\n+        ['b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q']\n+    ]\n+\n+class BinaryIntContig(_AbstractBinary):\n+    params = [\n+        [getattr(np, uf) for uf in (\n+            'add', 'subtract', 'multiply', 'bitwise_and', 'bitwise_or',\n+            'bitwise_xor', 'logical_and', 'logical_or', 'logical_xor',\n+            'right_shift', 'left_shift',\n+        )],\n+        [1], [1], [1],\n+        ['b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q']\n+    ]\n+\n+class UnaryIntContig(_AbstractUnary):\n+    arrlen = 100000\n+    params = [\n+        [getattr(np, uf) for uf in (\n+            'positive', 'square', 'reciprocal', 'conjugate', 'logical_not',\n+            'invert', 'isnan', 'isinf', 'isfinite',\n+            'absolute', 'sign'\n+        )],\n+        [1], [1],\n+        ['b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'Q']\n+    ]\n \n class Mandelbrot(Benchmark):\n     def f(self,z):\n         return np.abs(z) < 4.0\n \n     def g(self,z,c):\n-        return np.sum(np.multiply(z,z) + c)\n+        return np.sum(np.multiply(z, z) + c)\n \n     def mandelbrot_numpy(self, c, maxiter):\n         output = np.zeros(c.shape, np.int32)"
            },
            {
                "filename": "benchmarks/benchmarks/common.py",
                "patch": "@@ -1,5 +1,7 @@\n import numpy\n import random\n+import os\n+import functools\n \n # Various pre-crafted datasets/variables for testing\n # !!! Must not be changed -- only appended !!!\n@@ -110,5 +112,112 @@ def get_indexes_rand_():\n     return indexes_rand_\n \n \n+CACHE_ROOT = os.path.dirname(__file__)\n+CACHE_ROOT = os.path.abspath(\n+    os.path.join(CACHE_ROOT, '..', 'env', 'numpy_benchdata')\n+)\n+\n+\n+@functools.cache\n+def get_data(size, dtype, ip_num=0, zeros=False, finite=True, denormal=False):\n+    \"\"\"\n+    Generates a cached random array that covers several scenarios that\n+    may affect the benchmark for fairness and to stabilize the benchmark.\n+\n+    Parameters\n+    ----------\n+    size: int\n+        Array length.\n+\n+    dtype: dtype or dtype specifier\n+\n+    ip_num: int\n+        Input number, to avoid memory overload\n+        and to provide unique data for each operand.\n+\n+    zeros: bool\n+        Spreading zeros along with generated data.\n+\n+    finite: bool\n+        Avoid spreading fp special cases nan/inf.\n+\n+    denormal:\n+        Spreading subnormal numbers along with generated data.\n+    \"\"\"\n+    np = numpy\n+    dtype = np.dtype(dtype)\n+    dname = dtype.name\n+    cache_name = f'{dname}_{size}_{ip_num}_{int(zeros)}'\n+    if dtype.kind in 'fc':\n+        cache_name += f'{int(finite)}{int(denormal)}'\n+    cache_name += '.bin'\n+    cache_path = os.path.join(CACHE_ROOT, cache_name)\n+    if os.path.exists(cache_path):\n+        return np.fromfile(cache_path, dtype)\n+\n+    array = np.ones(size, dtype)\n+    rands = []\n+    if dtype.kind == 'i':\n+        dinfo = np.iinfo(dtype)\n+        scale = 8\n+        if zeros:\n+            scale += 1\n+        lsize = size // scale\n+        for low, high in (\n+            (-0x80, -1),\n+            (1, 0x7f),\n+            (-0x8000, -1),\n+            (1, 0x7fff),\n+            (-0x80000000, -1),\n+            (1, 0x7fffffff),\n+            (-0x8000000000000000, -1),\n+            (1, 0x7fffffffffffffff),\n+        ):\n+            rands += [np.random.randint(\n+                max(low, dinfo.min),\n+                min(high, dinfo.max),\n+                lsize, dtype\n+            )]\n+    elif dtype.kind == 'u':\n+        dinfo = np.iinfo(dtype)\n+        scale = 4\n+        if zeros:\n+            scale += 1\n+        lsize = size // scale\n+        for high in (0xff, 0xffff, 0xffffffff, 0xffffffffffffffff):\n+            rands += [np.random.randint(1, min(high, dinfo.max), lsize, dtype)]\n+    elif dtype.kind in 'fc':\n+        scale = 1\n+        if zeros:\n+            scale += 1\n+        if not finite:\n+            scale += 2\n+        if denormal:\n+            scale += 1\n+        dinfo = np.finfo(dtype)\n+        lsize = size // scale\n+        rands = [np.random.rand(lsize).astype(dtype)]\n+        if not finite:\n+            rands += [\n+                np.empty(lsize, dtype=dtype), np.empty(lsize, dtype=dtype)\n+            ]\n+            rands[1].fill(float('nan'))\n+            rands[2].fill(float('inf'))\n+        if denormal:\n+            rands += [np.empty(lsize, dtype=dtype)]\n+            rands[-1].fill(dinfo.smallest_subnormal)\n+\n+    if rands:\n+        if zeros:\n+            rands += [np.zeros(lsize, dtype)]\n+        stride = len(rands)\n+        for start, r in enumerate(rands):\n+            array[start:len(r)*stride:stride] = r\n+\n+    if not os.path.exists(CACHE_ROOT):\n+        os.mkdir(CACHE_ROOT)\n+    array.tofile(cache_path)\n+    return array\n+\n class Benchmark:\n     pass"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -61,17 +61,14 @@ class TypeDescription:\n     cfunc_alias : str or none, optional\n         Appended to inner loop C function name, e.g., FLOAT_{cfunc_alias}. See make_arrays.\n         NOTE: it doesn't support 'astype'\n-    simd : list\n-        Available SIMD ufunc loops, dispatched at runtime in specified order\n-        Currently only supported for simples types (see make_arrays)\n     dispatch : str or None, optional\n         Dispatch-able source name without its extension '.dispatch.c' that\n         contains the definition of ufunc, dispatched at runtime depending on the\n         specified targets of the dispatch-able source.\n         NOTE: it doesn't support 'astype'\n     \"\"\"\n     def __init__(self, type, f=None, in_=None, out=None, astype=None, cfunc_alias=None,\n-                 simd=None, dispatch=None):\n+                 dispatch=None):\n         self.type = type\n         self.func_data = f\n         if astype is None:\n@@ -84,7 +81,6 @@ def __init__(self, type, f=None, in_=None, out=None, astype=None, cfunc_alias=No\n             out = out.replace('P', type)\n         self.out = out\n         self.cfunc_alias = cfunc_alias\n-        self.simd = simd\n         self.dispatch = dispatch\n \n     def finish_signature(self, nin, nout):\n@@ -146,8 +142,9 @@ def build_func_data(types, f):\n     return func_data\n \n def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,\n-       simd=None, dispatch=None):\n-    \"\"\"Generate a TypeDescription instance for each item in types\n+       dispatch=None):\n+    \"\"\"\n+    Generate a TypeDescription instance for each item in types\n     \"\"\"\n     if f is not None:\n         if isinstance(f, str):\n@@ -172,20 +169,14 @@ def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,\n         raise ValueError(\"Number of types and outputs do not match\")\n     tds = []\n     for t, fd, i, o in zip(types, func_data, in_, out):\n-        # [(simd-name, list of types)]\n-        if simd is not None:\n-            simdt = [k for k, v in simd if t in v]\n-        else:\n-            simdt = []\n-\n         # [(dispatch file name without extension '.dispatch.c*', list of types)]\n         if dispatch:\n             dispt = ([k for k, v in dispatch if t in v]+[None])[0]\n         else:\n             dispt = None\n         tds.append(TypeDescription(\n             t, f=fd, in_=i, out=o, astype=astype, cfunc_alias=cfunc_alias,\n-            simd=simdt, dispatch=dispt\n+            dispatch=dispt\n         ))\n     return tds\n \n@@ -352,8 +343,10 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.add'),\n           'PyUFunc_AdditionTypeResolver',\n           TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n-          TD(no_bool_times_obj, simd=[('avx2', ints)],\n-                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, dispatch=[\n+              ('loops_arithm_fp', 'fdFD'),\n+              ('loops_autovec', ints),\n+          ]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'mM', 'M'),\n@@ -365,8 +358,10 @@ def english_upper(s):\n     Ufunc(2, 1, None, # Zero is only a unit to the right, not the left\n           docstrings.get('numpy.core.umath.subtract'),\n           'PyUFunc_SubtractionTypeResolver',\n-          TD(no_bool_times_obj, simd=[('avx2', ints)],\n-                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, dispatch=[\n+              ('loops_arithm_fp', 'fdFD'),\n+              ('loops_autovec', ints),\n+          ]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'MM', 'm'),\n@@ -380,8 +375,10 @@ def english_upper(s):\n           'PyUFunc_MultiplicationTypeResolver',\n           TD('?', cfunc_alias='logical_and',\n                   dispatch=[('loops_logical', '?')]),\n-          TD(no_bool_times_obj, simd=[('avx2', ints)],\n-                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, dispatch=[\n+              ('loops_arithm_fp', 'fdFD'),\n+              ('loops_autovec', ints),\n+          ]),\n           [TypeDescription('m', FullTypeDescr, 'mq', 'm'),\n            TypeDescription('m', FullTypeDescr, 'qm', 'm'),\n            TypeDescription('m', FullTypeDescr, 'md', 'm'),\n@@ -421,8 +418,10 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.conjugate'),\n           None,\n-          TD(ints+flts+cmplx, simd=[('avx2', ints)],\n-            dispatch=[('loops_arithm_fp', 'FD')]),\n+          TD(ints+flts+cmplx, dispatch=[\n+              ('loops_arithm_fp', 'FD'),\n+              ('loops_autovec', ints),\n+          ]),\n           TD(P, f='conjugate'),\n           ),\n 'fmod':\n@@ -437,15 +436,21 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.square'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints)],\n-             dispatch=[('loops_unary_fp', 'fd'), ('loops_arithm_fp', 'FD')]),\n+          TD(ints+inexact, dispatch=[\n+              ('loops_unary_fp', 'fd'),\n+              ('loops_arithm_fp', 'FD'),\n+              ('loops_autovec', ints),\n+          ]),\n           TD(O, f='Py_square'),\n           ),\n 'reciprocal':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.reciprocal'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints)], dispatch=[('loops_unary_fp', 'fd')]),\n+          TD(ints+inexact, dispatch=[\n+              ('loops_unary_fp', 'fd'),\n+              ('loops_autovec', ints),\n+          ]),\n           TD(O, f='Py_reciprocal'),\n           ),\n # This is no longer used as numpy.ones_like, however it is\n@@ -477,8 +482,11 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.absolute'),\n           'PyUFunc_AbsoluteTypeResolver',\n-          TD(bints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd'),\n-                                                 ('loops_logical', '?')]),\n+          TD(bints+flts+timedeltaonly, dispatch=[\n+              ('loops_unary_fp', 'fd'),\n+              ('loops_logical', '?'),\n+              ('loops_autovec', ints + 'e'),\n+          ]),\n           TD(cmplx, dispatch=[('loops_unary_complex', 'FD')],\n              out=('f', 'd', 'g')),\n           TD(O, f='PyNumber_Absolute'),\n@@ -509,7 +517,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sign'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(nobool_or_datetime),\n+          TD(nobool_or_datetime, dispatch=[('loops_autovec', ints)]),\n           ),\n 'greater':\n     Ufunc(2, 1, None,\n@@ -563,24 +571,30 @@ def english_upper(s):\n     Ufunc(2, 1, True_,\n           docstrings.get('numpy.core.umath.logical_and'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n-                                dispatch=[('loops_logical', '?')]),\n+          TD(nodatetime_or_obj, out='?', dispatch=[\n+              ('loops_logical', '?'),\n+              ('loops_autovec', ints),\n+          ]),\n           TD(O, f='npy_ObjectLogicalAnd'),\n           ),\n 'logical_not':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.logical_not'),\n           None,\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n-                                dispatch=[('loops_logical', '?')]),\n+          TD(nodatetime_or_obj, out='?', dispatch=[\n+              ('loops_logical', '?'),\n+              ('loops_autovec', ints),\n+          ]),\n           TD(O, f='npy_ObjectLogicalNot'),\n           ),\n 'logical_or':\n     Ufunc(2, 1, False_,\n           docstrings.get('numpy.core.umath.logical_or'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n-                                dispatch=[('loops_logical', '?')]),\n+          TD(nodatetime_or_obj, out='?', dispatch=[\n+              ('loops_logical', '?'),\n+              ('loops_autovec', ints),\n+          ]),\n           TD(O, f='npy_ObjectLogicalOr'),\n           ),\n 'logical_xor':\n@@ -589,7 +603,9 @@ def english_upper(s):\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n           TD('?', out='?', cfunc_alias='not_equal',\n                            dispatch=[('loops_comparison', '?')]),\n-          TD(no_bool_times_obj, out='?'),\n+          TD(no_bool_times_obj, out='?', dispatch=[\n+              ('loops_autovec', ints),\n+          ]),\n           # TODO: using obj.logical_xor() seems pretty much useless:\n           TD(P, f='logical_xor'),\n           ),\n@@ -656,15 +672,15 @@ def english_upper(s):\n           None,\n           TD('?', cfunc_alias='logical_and',\n                   dispatch=[('loops_logical', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec', ints)]),\n           TD(O, f='PyNumber_And'),\n           ),\n 'bitwise_or':\n     Ufunc(2, 1, Zero,\n           docstrings.get('numpy.core.umath.bitwise_or'),\n           None,\n           TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec', ints)]),\n           TD(O, f='PyNumber_Or'),\n           ),\n 'bitwise_xor':\n@@ -673,7 +689,7 @@ def english_upper(s):\n           None,\n           TD('?', cfunc_alias='not_equal',\n                   dispatch=[('loops_comparison', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec', ints)]),\n           TD(O, f='PyNumber_Xor'),\n           ),\n 'invert':\n@@ -682,21 +698,21 @@ def english_upper(s):\n           None,\n           TD('?', cfunc_alias='logical_not',\n                   dispatch=[('loops_logical', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec', ints)]),\n           TD(O, f='PyNumber_Invert'),\n           ),\n 'left_shift':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.left_shift'),\n           None,\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec', ints)]),\n           TD(O, f='PyNumber_Lshift'),\n           ),\n 'right_shift':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.right_shift'),\n           None,\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec', ints)]),\n           TD(O, f='PyNumber_Rshift'),\n           ),\n 'heaviside':\n@@ -986,7 +1002,10 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.isnan'),\n           'PyUFunc_IsFiniteTypeResolver',\n-          TD(noobj, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n+          TD(noobj, out='?', dispatch=[\n+              ('loops_unary_fp_le', inexactvec),\n+              ('loops_autovec', bints),\n+          ]),\n           ),\n 'isnat':\n     Ufunc(1, 1, None,\n@@ -998,13 +1017,19 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.isinf'),\n           'PyUFunc_IsFiniteTypeResolver',\n-          TD(noobj, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n+          TD(noobj, out='?', dispatch=[\n+              ('loops_unary_fp_le', inexactvec),\n+              ('loops_autovec', bints + 'mM'),\n+          ]),\n           ),\n 'isfinite':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.isfinite'),\n           'PyUFunc_IsFiniteTypeResolver',\n-          TD(noobj, out='?', dispatch=[('loops_unary_fp_le', inexactvec)]),\n+          TD(noobj, out='?', dispatch=[\n+              ('loops_unary_fp_le', inexactvec),\n+              ('loops_autovec', bints),\n+          ]),\n           ),\n 'signbit':\n     Ufunc(1, 1, None,\n@@ -1156,18 +1181,6 @@ def make_arrays(funcdict):\n                 datalist.append('(void *)NULL')\n                 tname = english_upper(chartoname[t.type])\n                 cfunc_fname = f\"{tname}_{cfunc_alias}\"\n-                if t.simd is not None:\n-                    for vt in t.simd:\n-                        code2list.append(textwrap.dedent(\"\"\"\\\n-                        #ifdef HAVE_ATTRIBUTE_TARGET_{ISA}\n-                        if (NPY_CPU_HAVE({ISA})) {{\n-                            {fname}_functions[{idx}] = {cname}_{isa};\n-                        }}\n-                        #endif\n-                        \"\"\").format(\n-                            ISA=vt.upper(), isa=vt,\n-                            fname=name, cname=cfunc_fname, idx=k\n-                        ))\n             else:\n                 try:\n                     thedict = arity_lookup[uf.nin, uf.nout]"
            },
            {
                "filename": "numpy/core/config.h.in",
                "patch": "@@ -29,28 +29,12 @@\n #mesondefine HAVE___BUILTIN_BSWAP64\n #mesondefine HAVE___BUILTIN_EXPECT\n #mesondefine HAVE___BUILTIN_MUL_OVERFLOW\n-#mesondefine HAVE__M_FROM_INT64\n-#mesondefine HAVE__MM_LOAD_PS\n-#mesondefine HAVE__MM_PREFETCH\n-#mesondefine HAVE__MM_LOAD_PD\n #mesondefine HAVE___BUILTIN_PREFETCH\n-#mesondefine HAVE_LINK_AVX\n-#mesondefine HAVE_LINK_AVX2\n-#mesondefine HAVE_LINK_AVX512F\n-#mesondefine HAVE_LINK_AVX512_SKX\n-#mesondefine HAVE_XGETBV\n \n #mesondefine HAVE_ATTRIBUTE_OPTIMIZE_UNROLL_LOOPS\n #mesondefine HAVE_ATTRIBUTE_OPTIMIZE_OPT_3\n #mesondefine HAVE_ATTRIBUTE_OPTIMIZE_OPT_2\n #mesondefine HAVE_ATTRIBUTE_NONNULL\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX2\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX512F\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX512_SKX\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS\n-#mesondefine HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS\n \n /* C99 complex support and complex.h are not universal */\n #mesondefine HAVE_COMPLEX_H"
            },
            {
                "filename": "numpy/core/include/numpy/npy_common.h",
                "patch": "@@ -40,39 +40,6 @@\n #define NPY_GCC_OPT_3\n #endif\n \n-/* compile target attributes */\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX && defined HAVE_LINK_AVX\n-#define NPY_GCC_TARGET_AVX __attribute__((target(\"avx\")))\n-#else\n-#define NPY_GCC_TARGET_AVX\n-#endif\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n-#define HAVE_ATTRIBUTE_TARGET_FMA\n-#define NPY_GCC_TARGET_FMA __attribute__((target(\"avx2,fma\")))\n-#endif\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX2 && defined HAVE_LINK_AVX2\n-#define NPY_GCC_TARGET_AVX2 __attribute__((target(\"avx2\")))\n-#else\n-#define NPY_GCC_TARGET_AVX2\n-#endif\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512F && defined HAVE_LINK_AVX512F\n-#define NPY_GCC_TARGET_AVX512F __attribute__((target(\"avx512f\")))\n-#elif defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS\n-#define NPY_GCC_TARGET_AVX512F __attribute__((target(\"avx512f\")))\n-#else\n-#define NPY_GCC_TARGET_AVX512F\n-#endif\n-\n-#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX && defined HAVE_LINK_AVX512_SKX\n-#define NPY_GCC_TARGET_AVX512_SKX __attribute__((target(\"avx512f,avx512dq,avx512vl,avx512bw,avx512cd\")))\n-#elif defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS\n-#define NPY_GCC_TARGET_AVX512_SKX __attribute__((target(\"avx512f,avx512dq,avx512vl,avx512bw,avx512cd\")))\n-#else\n-#define NPY_GCC_TARGET_AVX512_SKX\n-#endif\n /*\n  * mark an argument (starting from 1) that must not be NULL and is not checked\n  * DO NOT USE IF FUNCTION CHECKS FOR NULL!! the compiler will remove the check\n@@ -83,21 +50,6 @@\n #define NPY_GCC_NONNULL(n)\n #endif\n \n-#if defined HAVE_XMMINTRIN_H && defined HAVE__MM_LOAD_PS\n-#define NPY_HAVE_SSE_INTRINSICS\n-#endif\n-\n-#if defined HAVE_EMMINTRIN_H && defined HAVE__MM_LOAD_PD\n-#define NPY_HAVE_SSE2_INTRINSICS\n-#endif\n-\n-#if defined HAVE_IMMINTRIN_H && defined HAVE_LINK_AVX2\n-#define NPY_HAVE_AVX2_INTRINSICS\n-#endif\n-\n-#if defined HAVE_IMMINTRIN_H && defined HAVE_LINK_AVX512F\n-#define NPY_HAVE_AVX512F_INTRINSICS\n-#endif\n /*\n  * give a hint to the compiler which branch is more likely or unlikely\n  * to occur, e.g. rare error cases:\n@@ -120,7 +72,7 @@\n /* unlike _mm_prefetch also works on non-x86 */\n #define NPY_PREFETCH(x, rw, loc) __builtin_prefetch((x), (rw), (loc))\n #else\n-#ifdef HAVE__MM_PREFETCH\n+#ifdef NPY_HAVE_SSE\n /* _MM_HINT_ET[01] (rw = 1) unsupported, only available in gcc >= 4.9 */\n #define NPY_PREFETCH(x, rw, loc) _mm_prefetch((x), loc == 0 ? _MM_HINT_NTA : \\\n                                              (loc == 1 ? _MM_HINT_T2 : \\"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -293,16 +293,7 @@ optional_function_attributes = [\n   ['optimize(\"O3\")', 'OPTIMIZE_OPT_3'],\n   ['optimize(\"O2\")', 'OPTIMIZE_OPT_2'],\n   ['optimize(\"nonnull (1)\")', 'NONNULL'],\n-  ]\n-if host_machine.cpu_family() in ['x86', 'x86_64']\n-  optional_function_attributes += [\n-    ['target(\"avx\")', 'TARGET_AVX'],\n-    ['target(\"avx2\")', 'TARGET_AVX2'],\n-    ['target(\"avx512f\")', 'TARGET_AVX512F'],\n-    ['target(\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")', 'TARGET_AVX512_SKX'],\n-  ]\n-  # TODO: add the _WITH_INTRINSICS_AVX list\n-endif\n+]\n #foreach attr: optional_function_attributes\n #  if cc.has_function_attribute(attr[0])\n #    cdata.set10('HAVE_ATTRIBUTE_' + attr[1], true)\n@@ -321,26 +312,8 @@ optional_intrinsics = [\n   ['__builtin_expect', '5, 0', [], []],\n   # Test `long long` for arm+clang 13 (gh-22811, but we use all versions):\n   ['__builtin_mul_overflow', '(long long)5, 5, (int*)5', [], []],\n+  ['__builtin_prefetch', '(float*)0, 0, 3', [], []],\n ]\n-if host_machine.cpu_family() in ['x86', 'x86_64']\n- optional_intrinsics += [\n-    # MMX only needed for icc, but some clang's don't have it\n-    ['_m_from_int64', '0', ['emmintrin.h'], []],\n-    ['_mm_load_ps', '(float*)0', ['xmmintrin.h'], []],  # SSE\n-    ['_mm_prefetch', '(float*)0, _MM_HINT_NTA', ['xmmintrin.h'], []],  # SSE\n-    ['_mm_load_pd', '(double*)0', ['emmintrin.h'], []],  # SSE2\n-    ['__builtin_prefetch', '(float*)0, 0, 3', [], []],\n-    # Check that the linker can handle AVX\n-    ['__asm__ volatile', '\"vpand %xmm1, %xmm2, %xmm3\"', ['stdio.h'], ['HAVE_LINK_AVX']],\n-    ['__asm__ volatile', '\"vpand %ymm1, %ymm2, %ymm3\"', ['stdio.h'], ['HAVE_LINK_AVX2']],\n-    ['__asm__ volatile', '\"vpaddd %zmm1, %zmm2, %zmm3\"', ['stdio.h'], ['HAVE_LINK_AVX512F']],\n-    ['__asm__ volatile',\n-     '\"vfpclasspd $0x40, %zmm15, %k6\\\\n vmovdqu8 %xmm0, %xmm1\\\\n vpbroadcastmb2q %k0, %xmm0\"',\n-     ['stdio.h'], ['HAVE_LINK_AVX512_SKX']\n-    ],\n-    ['__asm__ volatile', '\"xgetbv\"', ['stdio.h'], ['HAVE_XGETBV']],\n-  ]\n-endif\n foreach intrin: optional_intrinsics\n   func = intrin[0]\n   func_args = intrin[1]\n@@ -767,6 +740,7 @@ src_umath = [\n   src_file.process('src/umath/loops_unary_fp.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary_fp_le.dispatch.c.src'),\n   src_file.process('src/umath/loops_unary_complex.dispatch.c.src'),\n+  src_file.process('src/umath/loops_autovec.dispatch.c.src'),\n   src_file.process('src/umath/matmul.c.src'),\n   src_file.process('src/umath/matmul.h.src'),\n   'src/umath/ufunc_type_resolution.c',"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -171,18 +171,6 @@ def check_funcs(\n         else:\n             return 1\n \n-    # NOTE: not needed in Meson build, we set the minimum\n-    #       compiler version to 8.4 to avoid this bug\n-    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n-    # support on Windows-based platforms\n-    def check_gh14787(fn):\n-        if fn == 'attribute_target_avx512f':\n-            if (sys.platform in ('win32', 'cygwin') and\n-                    config.check_compiler_gcc() and\n-                    not config.check_gcc_version_at_least(8, 4)):\n-                ext.extra_compile_args.extend(\n-                        ['-ffixed-xmm%s' % n for n in range(16, 32)])\n-\n     #use_msvc = config.check_decl(\"_MSC_VER\")\n     if not check_funcs_once(MANDATORY_FUNCS, add_to_moredefs=False):\n         raise SystemError(\"One of the required function to build numpy is not\"\n@@ -233,20 +221,8 @@ def check_gh14787(fn):\n     for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES:\n         if config.check_gcc_function_attribute(dec, fn):\n             moredefs.append((fname2def(fn), 1))\n-            check_gh14787(fn)\n \n     platform = sysconfig.get_platform()\n-    if (\"x86_64\" in platform):\n-        for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES_AVX:\n-            if config.check_gcc_function_attribute(dec, fn):\n-                moredefs.append((fname2def(fn), 1))\n-                check_gh14787(fn)\n-        for dec, fn, code, header in (\n-        OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX):\n-            if config.check_gcc_function_attribute_with_intrinsics(\n-                    dec, fn, code, header):\n-                moredefs.append((fname2def(fn), 1))\n-\n     for fn in OPTIONAL_VARIABLE_ATTRIBUTES:\n         if config.check_gcc_variable_attribute(fn):\n             m = fn.replace(\"(\", \"_\").replace(\")\", \"_\")\n@@ -1019,6 +995,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops_modulo.dispatch.c.src'),\n             join('src', 'umath', 'loops_comparison.dispatch.c.src'),\n             join('src', 'umath', 'loops_unary_complex.dispatch.c.src'),\n+            join('src', 'umath', 'loops_autovec.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),\n             join('src', 'umath', 'matmul.c.src'),\n             join('src', 'umath', 'clip.h'),"
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -183,25 +183,7 @@ def set_sig(sig):\n                        # Test `long long` for arm+clang 13 (gh-22811,\n                        # but we use all versions of __builtin_mul_overflow):\n                        (\"__builtin_mul_overflow\", '(long long)5, 5, (int*)5'),\n-                       # MMX only needed for icc, but some clangs don't have it\n-                       (\"_m_from_int64\", '0', \"emmintrin.h\"),\n-                       (\"_mm_load_ps\", '(float*)0', \"xmmintrin.h\"),  # SSE\n-                       (\"_mm_prefetch\", '(float*)0, _MM_HINT_NTA',\n-                        \"xmmintrin.h\"),  # SSE\n-                       (\"_mm_load_pd\", '(double*)0', \"emmintrin.h\"),  # SSE2\n                        (\"__builtin_prefetch\", \"(float*)0, 0, 3\"),\n-                       # check that the linker can handle avx\n-                       (\"__asm__ volatile\", '\"vpand %xmm1, %xmm2, %xmm3\"',\n-                        \"stdio.h\", \"LINK_AVX\"),\n-                       (\"__asm__ volatile\", '\"vpand %ymm1, %ymm2, %ymm3\"',\n-                        \"stdio.h\", \"LINK_AVX2\"),\n-                       (\"__asm__ volatile\", '\"vpaddd %zmm1, %zmm2, %zmm3\"',\n-                        \"stdio.h\", \"LINK_AVX512F\"),\n-                       (\"__asm__ volatile\", '\"vfpclasspd $0x40, %zmm15, %k6\\\\n\"\\\n-                                             \"vmovdqu8 %xmm0, %xmm1\\\\n\"\\\n-                                             \"vpbroadcastmb2q %k0, %xmm0\\\\n\"',\n-                        \"stdio.h\", \"LINK_AVX512_SKX\"),\n-                       (\"__asm__ volatile\", '\"xgetbv\"', \"stdio.h\", \"XGETBV\"),\n                        ]\n \n # function attributes\n@@ -216,44 +198,6 @@ def set_sig(sig):\n                                 ('__attribute__((nonnull (1)))',\n                                  'attribute_nonnull'),\n                                 ]\n-\n-OPTIONAL_FUNCTION_ATTRIBUTES_AVX = [('__attribute__((target (\"avx\")))',\n-    'attribute_target_avx'),\n-    ('__attribute__((target (\"avx2\")))',\n-    'attribute_target_avx2'),\n-    ('__attribute__((target (\"avx512f\")))',\n-    'attribute_target_avx512f'),\n-    ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n-    'attribute_target_avx512_skx'),\n-    ]\n-\n-# function attributes with intrinsics\n-# To ensure your compiler can compile avx intrinsics with just the attributes\n-# gcc 4.8.4 support attributes but not with intrisics\n-# tested via \"#include<%s> int %s %s(void *){code; return 0;};\" % (header, attribute, name, code)\n-# function name will be converted to HAVE_<upper-case-name> preprocessor macro\n-# The _mm512_castps_si512 instruction is specific check for AVX-512F support\n-# in gcc-4.9 which is missing a subset of intrinsics. See\n-# https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61878\n-OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX = [\n-    ('__attribute__((target(\"avx2,fma\")))',\n-    'attribute_target_avx2_with_intrinsics',\n-    '__m256 temp = _mm256_set1_ps(1.0); temp = \\\n-    _mm256_fmadd_ps(temp, temp, temp)',\n-    'immintrin.h'),\n-    ('__attribute__((target(\"avx512f\")))',\n-    'attribute_target_avx512f_with_intrinsics',\n-    '__m512i temp = _mm512_castps_si512(_mm512_set1_ps(1.0))',\n-    'immintrin.h'),\n-    ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n-    'attribute_target_avx512_skx_with_intrinsics',\n-    '__mmask8 temp = _mm512_fpclass_pd_mask(_mm512_set1_pd(1.0), 0x01);\\\n-    __m512i unused_temp = \\\n-        _mm512_castps_si512(_mm512_set1_ps(1.0));\\\n-    _mm_mask_storeu_epi8(NULL, 0xFF, _mm_broadcastmb_epi64(temp))',\n-    'immintrin.h'),\n-    ]\n-\n def fname2def(name):\n     return \"HAVE_%s\" % name.upper()\n "
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/memory.h",
                "patch": "@@ -215,7 +215,7 @@ NPY_FINLINE npyv_s64 npyv_load_till_s64(const npy_int64 *ptr, npy_uintp nlane, n\n     const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n     __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n     __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n-    __m256i payload = _mm256_maskload_epi64((const void*)ptr, mask);\n+    __m256i payload = _mm256_maskload_epi64((const long long*)ptr, mask);\n     return _mm256_blendv_epi8(vfill, payload, mask);\n }\n // fill zero to rest lanes\n@@ -225,7 +225,7 @@ NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n     const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n     __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n     __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n-    return _mm256_maskload_epi64((const void*)ptr, mask);\n+    return _mm256_maskload_epi64((const long long*)ptr, mask);\n }\n \n //// 64-bit nlane\n@@ -240,7 +240,7 @@ NPY_FINLINE npyv_s32 npyv_load2_till_s32(const npy_int32 *ptr, npy_uintp nlane,\n     const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n     __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n     __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n-    __m256i payload = _mm256_maskload_epi64((const void*)ptr, mask);\n+    __m256i payload = _mm256_maskload_epi64((const long long*)ptr, mask);\n     return _mm256_blendv_epi8(vfill, payload, mask);\n }\n // fill zero to rest lanes\n@@ -253,7 +253,7 @@ NPY_FINLINE npyv_u64 npyv_load2_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n     assert(nlane > 0);\n     npy_int64 m = -((npy_int64)(nlane > 1));\n     __m256i mask = npyv_set_s64(-1, -1, m, m);\n-    return _mm256_maskload_epi64((const void*)ptr, mask);\n+    return _mm256_maskload_epi64((const long long*)ptr, mask);\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_u64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n@@ -262,7 +262,7 @@ NPY_FINLINE npyv_u64 npyv_load2_till_s64(const npy_int64 *ptr, npy_uintp nlane,\n     const __m256i vfill = npyv_set_s64(0, 0, fill_lo, fill_hi);\n     npy_int64 m = -((npy_int64)(nlane > 1));\n     __m256i mask = npyv_set_s64(-1, -1, m, m);\n-    __m256i payload = _mm256_maskload_epi64((const void*)ptr, mask);\n+    __m256i payload = _mm256_maskload_epi64((const long long*)ptr, mask);\n     return _mm256_blendv_epi8(vfill, payload, mask);\n }\n /*********************************\n@@ -295,7 +295,7 @@ npyv_loadn_till_s64(const npy_int64 *ptr, npy_intp stride, npy_uintp nlane, npy_\n     const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n     __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n     __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n-    return _mm256_mask_i64gather_epi64(vfill, (const void*)ptr, idx, mask, 8);\n+    return _mm256_mask_i64gather_epi64(vfill, (const long long*)ptr, idx, mask, 8);\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s64\n@@ -315,7 +315,7 @@ NPY_FINLINE npyv_s64 npyv_loadn2_till_s32(const npy_int32 *ptr, npy_intp stride,\n     const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n     __m256i vnlane  = npyv_setall_s64(nlane > 4 ? 4 : (int)nlane);\n     __m256i mask    = _mm256_cmpgt_epi64(vnlane, steps);\n-    return _mm256_mask_i64gather_epi64(vfill, (const void*)ptr, idx, mask, 4);\n+    return _mm256_mask_i64gather_epi64(vfill, (const long long*)ptr, idx, mask, 4);\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s32 npyv_loadn2_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n@@ -361,7 +361,7 @@ NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a\n     const __m256i steps = npyv_set_s64(0, 1, 2, 3);\n     __m256i vnlane = npyv_setall_s64(nlane > 8 ? 8 : (int)nlane);\n     __m256i mask   = _mm256_cmpgt_epi64(vnlane, steps);\n-    _mm256_maskstore_epi64((void*)ptr, mask, a);\n+    _mm256_maskstore_epi64((long long*)ptr, mask, a);\n }\n \n //// 64-bit nlane"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/memory.h",
                "patch": "@@ -244,13 +244,14 @@ NPY_FINLINE npyv_s32 npyv_load_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n         return _mm_cvtsi32_si128(*ptr);\n     case 2:\n         return _mm_loadl_epi64((const __m128i*)ptr);\n-    case 3:;\n-        npyv_s32 a = _mm_loadl_epi64((const __m128i*)ptr);\n-    #ifdef NPY_HAVE_SSE41\n-        return _mm_insert_epi32(a, ptr[2], 2);\n-    #else\n-        return _mm_unpacklo_epi64(a, _mm_cvtsi32_si128(ptr[2]));\n-    #endif\n+    case 3: {\n+            npyv_s32 a = _mm_loadl_epi64((const __m128i*)ptr);\n+        #ifdef NPY_HAVE_SSE41\n+            return _mm_insert_epi32(a, ptr[2], 2);\n+        #else\n+            return _mm_unpacklo_epi64(a, _mm_cvtsi32_si128(ptr[2]));\n+        #endif\n+        }\n     default:\n         return npyv_load_s32(ptr);\n     }\n@@ -371,23 +372,27 @@ npyv_loadn_tillz_s32(const npy_int32 *ptr, npy_intp stride, npy_uintp nlane)\n     case 1:\n         return _mm_cvtsi32_si128(ptr[0]);\n     case 2:;\n-        npyv_s32 a = _mm_cvtsi32_si128(ptr[0]);\n-#ifdef NPY_HAVE_SSE41\n-        return _mm_insert_epi32(a, ptr[stride], 1);\n-#else\n-        return _mm_unpacklo_epi32(a, _mm_cvtsi32_si128(ptr[stride]));\n-#endif // NPY_HAVE_SSE41\n-    case 3:;\n-        a = _mm_cvtsi32_si128(ptr[0]);\n-#ifdef NPY_HAVE_SSE41\n-        a = _mm_insert_epi32(a, ptr[stride], 1);\n-        a = _mm_insert_epi32(a, ptr[stride*2], 2);\n-        return a;\n-#else\n-        a = _mm_unpacklo_epi32(a, _mm_cvtsi32_si128(ptr[stride]));\n-        a = _mm_unpacklo_epi64(a, _mm_cvtsi32_si128(ptr[stride*2]));\n-        return a;\n-#endif // NPY_HAVE_SSE41\n+        {\n+            npyv_s32 a = _mm_cvtsi32_si128(ptr[0]);\n+    #ifdef NPY_HAVE_SSE41\n+            return _mm_insert_epi32(a, ptr[stride], 1);\n+    #else\n+            return _mm_unpacklo_epi32(a, _mm_cvtsi32_si128(ptr[stride]));\n+    #endif // NPY_HAVE_SSE41\n+        }\n+    case 3:\n+        {\n+            npyv_s32 a = _mm_cvtsi32_si128(ptr[0]);\n+    #ifdef NPY_HAVE_SSE41\n+            a = _mm_insert_epi32(a, ptr[stride], 1);\n+            a = _mm_insert_epi32(a, ptr[stride*2], 2);\n+            return a;\n+    #else\n+            a = _mm_unpacklo_epi32(a, _mm_cvtsi32_si128(ptr[stride]));\n+            a = _mm_unpacklo_epi64(a, _mm_cvtsi32_si128(ptr[stride*2]));\n+            return a;\n+    #endif // NPY_HAVE_SSE41\n+        }\n     default:\n         return npyv_loadn_s32(ptr, stride);\n     }"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/arithmetic.h",
                "patch": "@@ -366,6 +366,7 @@ NPY_FINLINE float npyv_sum_f32(npyv_f32 a)\n {\n     npyv_f32 sum = vec_add(a, npyv_combineh_f32(a, a));\n     return vec_extract(sum, 0) + vec_extract(sum, 1);\n+    (void)sum;\n }\n #endif\n \n@@ -386,6 +387,7 @@ NPY_FINLINE npy_uint16 npyv_sumup_u8(npyv_u8 a)\n     npyv_u32 four = vec_sum4s(a, zero);\n     npyv_s32 one  = vec_sums((npyv_s32)four, (npyv_s32)zero);\n     return (npy_uint16)vec_extract(one, 3);\n+    (void)one;\n #endif\n }\n \n@@ -400,6 +402,7 @@ NPY_FINLINE npy_uint32 npyv_sumup_u16(npyv_u16 a)\n     npyv_u32   four  = vec_add(eight.val[0], eight.val[1]);\n     npyv_s32   one   = vec_sums((npyv_s32)four, zero);\n     return (npy_uint32)vec_extract(one, 3);\n+    (void)one;\n #endif\n }\n "
            },
            {
                "filename": "numpy/core/src/common/simd/vec/conversion.h",
                "patch": "@@ -96,6 +96,8 @@ npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n     #else\n         return vec_extract(r, 4);\n     #endif\n+        // to suppress ambiguous warning: variable `r` but not used [-Wunused-but-set-variable]\n+\t(void)r;\n     }\n     NPY_FINLINE npy_uint64 npyv_tobits_b16(npyv_b16 a)\n     {\n@@ -106,6 +108,8 @@ npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n     #else\n         return vec_extract(r, 8);\n     #endif\n+\t// to suppress ambiguous warning: variable `r` but not used [-Wunused-but-set-variable]\n+        (void)r;\n     }\n     NPY_FINLINE npy_uint64 npyv_tobits_b32(npyv_b32 a)\n     {\n@@ -120,6 +124,8 @@ npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n     #else\n         return vec_extract(r, 8);\n     #endif\n+\t// to suppress ambiguous warning: variable `r` but not used [-Wunused-but-set-variable]\n+        (void)r;\n     }\n     NPY_FINLINE npy_uint64 npyv_tobits_b64(npyv_b64 a)\n     {\n@@ -134,6 +140,8 @@ npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n     #else\n         return vec_extract(r, 8);\n     #endif\n+\t// to suppress ambiguous warning: variable `r` but not used [-Wunused-but-set-variable]\n+        (void)r;\n     }\n #else\n     NPY_FINLINE npy_uint64 npyv_tobits_b8(npyv_b8 a)"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/math.h",
                "patch": "@@ -186,6 +186,7 @@ NPY_IMPL_VEC_REDUCE_MINMAX(max, int32, s32)\n     {                                                                   \\\n         npyv_##SFX r = vec_##INTRIN(a, vec_sld(a, a, 8));               \\\n         return (npy_##STYPE)vec_extract(r, 0);                          \\\n+    \t(void)r;\t\t\t\t\t \t                                \\\n     }\n NPY_IMPL_VEC_REDUCE_MINMAX(min, uint64, u64)\n NPY_IMPL_VEC_REDUCE_MINMAX(max, uint64, u64)\n@@ -225,6 +226,7 @@ NPY_IMPL_VEC_REDUCE_MINMAX(max, int64, s64)\n     {                                                             \\\n         npyv_f64 r = vec_##INTRIN(a, vec_sld(a, a, 8));           \\\n         return vec_extract(r, 0);                                 \\\n+        (void)r;                                                  \\\n     }                                                             \\\n     NPY_FINLINE double npyv_reduce_##INTRIN##n_f64(npyv_f64 a)    \\\n     {                                                             \\"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/memory.h",
                "patch": "@@ -46,14 +46,8 @@\n #endif\n \n // avoid aliasing rules\n-#ifdef __cplusplus\n-    template<typename T_PTR>\n-    NPY_FINLINE npy_uint64 *npyv__ptr2u64(const T_PTR *ptr)\n-    { npy_uint64 *ptr64 = (npy_uint64*)ptr; return ptr64; }\n-#else\n-    NPY_FINLINE npy_uint64 *npyv__ptr2u64(const void *ptr)\n-    { npy_uint64 *ptr64 = (npy_uint64*)ptr; return ptr64; }\n-#endif // __cplusplus\n+NPY_FINLINE npy_uint64 *npyv__ptr2u64(const void *ptr)\n+{ npy_uint64 *ptr64 = (npy_uint64*)ptr; return ptr64; }\n \n // load lower part\n NPY_FINLINE npyv_u64 npyv__loadl(const void *ptr)"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/misc.h",
                "patch": "@@ -26,28 +26,28 @@\n #define NPYV_IMPL_VEC_SPLTW(T_VEC, V) ((T_VEC){V, V, V, V})\n #define NPYV_IMPL_VEC_SPLTD(T_VEC, V) ((T_VEC){V, V})\n \n-#define npyv_setall_u8(VAL)  NPYV_IMPL_VEC_SPLTB(npyv_u8,  (unsigned char)VAL)\n-#define npyv_setall_s8(VAL)  NPYV_IMPL_VEC_SPLTB(npyv_s8,  (signed char)VAL)\n-#define npyv_setall_u16(VAL) NPYV_IMPL_VEC_SPLTH(npyv_u16, (unsigned short)VAL)\n-#define npyv_setall_s16(VAL) NPYV_IMPL_VEC_SPLTH(npyv_s16, (short)VAL)\n-#define npyv_setall_u32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_u32, (unsigned int)VAL)\n-#define npyv_setall_s32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_s32, (int)VAL)\n+#define npyv_setall_u8(VAL)  NPYV_IMPL_VEC_SPLTB(npyv_u8,  (unsigned char)(VAL))\n+#define npyv_setall_s8(VAL)  NPYV_IMPL_VEC_SPLTB(npyv_s8,  (signed char)(VAL))\n+#define npyv_setall_u16(VAL) NPYV_IMPL_VEC_SPLTH(npyv_u16, (unsigned short)(VAL))\n+#define npyv_setall_s16(VAL) NPYV_IMPL_VEC_SPLTH(npyv_s16, (short)(VAL))\n+#define npyv_setall_u32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_u32, (unsigned int)(VAL))\n+#define npyv_setall_s32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_s32, (int)(VAL))\n #if NPY_SIMD_F32\n-    #define npyv_setall_f32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_f32, VAL)\n+    #define npyv_setall_f32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_f32, (VAL))\n #endif\n-#define npyv_setall_u64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_u64, (npy_uint64)VAL)\n-#define npyv_setall_s64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_s64, (npy_int64)VAL)\n+#define npyv_setall_u64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_u64, (npy_uint64)(VAL))\n+#define npyv_setall_s64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_s64, (npy_int64)(VAL))\n #define npyv_setall_f64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_f64, VAL)\n \n // vector with specific values set to each lane and\n // set a specific value to all remained lanes\n-#define npyv_setf_u8(FILL, ...)  ((npyv_u8){NPYV__SET_FILL_16(char, FILL, __VA_ARGS__)})\n-#define npyv_setf_s8(FILL, ...)  ((npyv_s8){NPYV__SET_FILL_16(char, FILL, __VA_ARGS__)})\n-#define npyv_setf_u16(FILL, ...) ((npyv_u16){NPYV__SET_FILL_8(short, FILL, __VA_ARGS__)})\n+#define npyv_setf_u8(FILL, ...)  ((npyv_u8){NPYV__SET_FILL_16(unsigned char, FILL, __VA_ARGS__)})\n+#define npyv_setf_s8(FILL, ...)  ((npyv_s8){NPYV__SET_FILL_16(signed char, FILL, __VA_ARGS__)})\n+#define npyv_setf_u16(FILL, ...) ((npyv_u16){NPYV__SET_FILL_8(unsigned short, FILL, __VA_ARGS__)})\n #define npyv_setf_s16(FILL, ...) ((npyv_s16){NPYV__SET_FILL_8(short, FILL, __VA_ARGS__)})\n-#define npyv_setf_u32(FILL, ...) ((npyv_u32){NPYV__SET_FILL_4(int, FILL, __VA_ARGS__)})\n+#define npyv_setf_u32(FILL, ...) ((npyv_u32){NPYV__SET_FILL_4(unsigned int, FILL, __VA_ARGS__)})\n #define npyv_setf_s32(FILL, ...) ((npyv_s32){NPYV__SET_FILL_4(int, FILL, __VA_ARGS__)})\n-#define npyv_setf_u64(FILL, ...) ((npyv_u64){NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__)})\n+#define npyv_setf_u64(FILL, ...) ((npyv_u64){NPYV__SET_FILL_2(npy_uint64, FILL, __VA_ARGS__)})\n #define npyv_setf_s64(FILL, ...) ((npyv_s64){NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__)})\n #if NPY_SIMD_F32\n     #define npyv_setf_f32(FILL, ...) ((npyv_f32){NPYV__SET_FILL_4(float, FILL, __VA_ARGS__)})"
            },
            {
                "filename": "numpy/core/src/npysort/mergesort.cpp",
                "patch": "@@ -171,6 +171,7 @@ amergesort_(type *v, npy_intp *tosort, npy_intp num)\n }\n \n /*\n+ \n  *****************************************************************************\n  **                             STRING SORTS                                **\n  *****************************************************************************"
            },
            {
                "filename": "numpy/core/src/umath/fast_loop_macros.h",
                "patch": "@@ -12,6 +12,19 @@\n \n #include <assert.h>\n \n+#include \"simd/simd.h\"\n+\n+/*\n+ * largest simd vector size in bytes numpy supports\n+ * it is currently a extremely large value as it is only used for memory\n+ * overlap checks\n+ */\n+#if NPY_SIMD > 0\n+    // Enough for compiler unroll\n+    #define AUTOVEC_OVERLAP_SIZE NPY_SIMD_WIDTH*4\n+#else\n+    #define AUTOVEC_OVERLAP_SIZE 1024\n+#endif\n /*\n  * MAX_STEP_SIZE is used to determine if we need to use SIMD version of the ufunc.\n  * Very large step size can be as slow as processing it using scalar. The\n@@ -219,11 +232,11 @@ abs_ptrdiff(char *a, char *b)\n         /* condition allows compiler to optimize the generic macro */ \\\n         if (IS_BINARY_CONT(tin, tout)) { \\\n             if (abs_ptrdiff(args[2], args[0]) == 0 && \\\n-                    abs_ptrdiff(args[2], args[1]) >= NPY_MAX_SIMD_SIZE) { \\\n+                    abs_ptrdiff(args[2], args[1]) >= AUTOVEC_OVERLAP_SIZE) { \\\n                 BASE_BINARY_LOOP_INP(tin, tout, op) \\\n             } \\\n             else if (abs_ptrdiff(args[2], args[1]) == 0 && \\\n-                         abs_ptrdiff(args[2], args[0]) >= NPY_MAX_SIMD_SIZE) { \\\n+                         abs_ptrdiff(args[2], args[0]) >= AUTOVEC_OVERLAP_SIZE) { \\\n                 BASE_BINARY_LOOP_INP(tin, tout, op) \\\n             } \\\n             else { \\"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -32,16 +32,6 @@\n  */\n #define PW_BLOCKSIZE    128\n \n-\n-/*\n- * largest simd vector size in bytes numpy supports\n- * it is currently a extremely large value as it is only used for memory\n- * overlap checks\n- */\n-#ifndef NPY_MAX_SIMD_SIZE\n-#define NPY_MAX_SIMD_SIZE 1024\n-#endif\n-\n /** Provides the various *_LOOP macros */\n #include \"fast_loop_macros.h\"\n \n@@ -417,24 +407,6 @@ BOOL__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps,\n     }\n }\n \n-\n-/**begin repeat\n- * #kind = isnan, isinf, isfinite#\n- * #func = npy_isnan, npy_isinf, npy_isfinite#\n- * #val = NPY_FALSE, NPY_FALSE, NPY_TRUE#\n- **/\n-NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    /*\n-     * The (void)in; suppresses an unused variable warning raised by gcc and allows\n-     * us to re-use this macro even though we do not depend on in\n-     */\n-    UNARY_LOOP_FAST(npy_bool, npy_bool, (void)in; *out = @val@);\n-}\n-\n-/**end repeat**/\n-\n /*\n  *****************************************************************************\n  **                           INTEGER LOOPS\n@@ -466,82 +438,16 @@ NPY_NO_EXPORT void\n         *((@type@ *)op1) = 1;\n     }\n }\n-\n-NPY_NO_EXPORT void\n-@TYPE@_positive(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = +in);\n-}\n-\n /**begin repeat1\n- * #isa = , _avx2#\n- * #CHK = 1, defined(HAVE_ATTRIBUTE_TARGET_AVX2)#\n- * #ATTR = , NPY_GCC_TARGET_AVX2#\n- */\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_square@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = in * in);\n-}\n-#endif\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_reciprocal@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = 1.0 / in);\n-}\n-#endif\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_conjugate@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = in);\n-}\n-#endif\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_logical_not@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, npy_bool, *out = !in);\n-}\n-#endif\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_invert@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = ~in);\n-}\n-#endif\n-\n-/**begin repeat2\n  * Arithmetic\n  * #kind = add, subtract, multiply, bitwise_and, bitwise_or, bitwise_xor#\n  * #OP = +, -, *, &, |, ^#\n  */\n \n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions,\n-                   npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    if (IS_BINARY_REDUCE) {\n-        BINARY_REDUCE_LOOP_FAST(@type@, io1 @OP@= in2);\n-    }\n-    else {\n-        BINARY_LOOP_FAST(@type@, @type@, *out = in1 @OP@ in2);\n-    }\n-}\n-\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ int\n-@TYPE@_@kind@@isa@_indexed(PyArrayMethod_Context *NPY_UNUSED(context),\n-         char **args, npy_intp const *dimensions, npy_intp const *steps,\n-         void *NPY_UNUSED(func))\n+NPY_NO_EXPORT NPY_GCC_OPT_3 int\n+@TYPE@_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context),\n+                      char **args, npy_intp const *dimensions, npy_intp const *steps,\n+                      void *NPY_UNUSED(func))\n {\n     char *ip1 = args[0];\n     char *indx = args[1];\n@@ -556,86 +462,6 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ int\n     }\n     return 0;\n }\n-\n-#endif\n-\n-/**end repeat2**/\n-\n-/*\n- * Arithmetic bit shift operations.\n- *\n- * Intel hardware masks bit shift values, so large shifts wrap around\n- * and can produce surprising results. The special handling ensures that\n- * behavior is independent of compiler or hardware.\n- * TODO: We could implement consistent behavior for negative shifts,\n- *       which is undefined in C.\n- */\n-\n-#define INT_left_shift_needs_clear_floatstatus\n-#define UINT_left_shift_needs_clear_floatstatus\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 void\n-@TYPE@_left_shift@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps,\n-                  void *NPY_UNUSED(func))\n-{\n-    BINARY_LOOP_FAST(@type@, @type@, *out = npy_lshift@c@(in1, in2));\n-\n-#ifdef @TYPE@_left_shift_needs_clear_floatstatus\n-    // For some reason, our macOS CI sets an \"invalid\" flag here, but only\n-    // for some types.\n-    npy_clear_floatstatus_barrier((char*)dimensions);\n-#endif\n-}\n-#endif\n-\n-#undef INT_left_shift_needs_clear_floatstatus\n-#undef UINT_left_shift_needs_clear_floatstatus\n-\n-#if @CHK@\n-NPY_NO_EXPORT\n-#ifndef NPY_DO_NOT_OPTIMIZE_@TYPE@_right_shift\n-NPY_GCC_OPT_3\n-#endif\n-void\n-@TYPE@_right_shift@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps,\n-                   void *NPY_UNUSED(func))\n-{\n-    BINARY_LOOP_FAST(@type@, @type@, *out = npy_rshift@c@(in1, in2));\n-}\n-#endif\n-\n-/**begin repeat2\n- * #kind = logical_and, logical_or#\n- * #OP = &&, ||#\n- */\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    /*\n-     * gcc vectorization of this is not good (PR60575) but manual integer\n-     * vectorization is too tedious to be worthwhile\n-     */\n-    BINARY_LOOP_FAST(@type@, npy_bool, *out = in1 @OP@ in2);\n-}\n-#endif\n-\n-/**end repeat2**/\n-\n-#if @CHK@\n-NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n-@TYPE@_logical_xor@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    BINARY_LOOP {\n-        const int t1 = !!*(@type@ *)ip1;\n-        const int t2 = !!*(@type@ *)ip2;\n-        *((npy_bool *)op1) = (t1 != t2);\n-    }\n-}\n-#endif\n-\n /**end repeat1**/\n \n NPY_NO_EXPORT void\n@@ -677,43 +503,13 @@ NPY_NO_EXPORT void\n         *((@type@ *) op1) = out;\n     }\n }\n-\n-/**begin repeat1\n- * #kind = isnan, isinf, isfinite#\n- * #func = npy_isnan, npy_isinf, npy_isfinite#\n- * #val = NPY_FALSE, NPY_FALSE, NPY_TRUE#\n- **/\n-NPY_NO_EXPORT void\n-@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    /*\n-     * The (void)in; suppresses an unused variable warning raised by gcc and allows\n-     * us to re-use this macro even though we do not depend on in\n-     */\n-    UNARY_LOOP_FAST(@type@, npy_bool, (void)in; *out = @val@);\n-}\n-/**end repeat1**/\n-\n /**end repeat**/\n \n /**begin repeat\n  * #TYPE = BYTE, SHORT, INT, LONG, LONGLONG#\n  * #type = npy_byte, npy_short, npy_int, npy_long, npy_longlong#\n  * #c    = ,,,l,ll#\n  */\n-\n-NPY_NO_EXPORT NPY_GCC_OPT_3 void\n-@TYPE@_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = (in >= 0) ? in : -in);\n-}\n-\n-NPY_NO_EXPORT NPY_GCC_OPT_3 void\n-@TYPE@_sign(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = in > 0 ? 1 : (in < 0 ? -1 : 0));\n-}\n-\n /**begin repeat1\n  * #kind = gcd, lcm#\n  **/\n@@ -727,27 +523,13 @@ NPY_NO_EXPORT void\n     }\n }\n /**end repeat1**/\n-\n /**end repeat**/\n \n /**begin repeat\n  * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG#\n  * #type = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong#\n  * #c    = u,u,u,ul,ull#\n  */\n-\n-NPY_NO_EXPORT NPY_GCC_OPT_3 void\n-@TYPE@_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = in);\n-}\n-\n-NPY_NO_EXPORT NPY_GCC_OPT_3 void\n-@TYPE@_sign(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(@type@, @type@, *out = in > 0 ? 1 : 0);\n-}\n-\n /**begin repeat1\n  * #kind = gcd, lcm#\n  **/\n@@ -761,7 +543,6 @@ NPY_NO_EXPORT void\n     }\n }\n /**end repeat1**/\n-\n /**end repeat**/\n \n /*\n@@ -839,12 +620,6 @@ NPY_NO_EXPORT void\n     }\n }\n \n-NPY_NO_EXPORT void\n-@TYPE@_isinf(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(npy_bool, npy_bool, (void)in; *out = NPY_FALSE);\n-}\n-\n NPY_NO_EXPORT void\n @TYPE@__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {\n@@ -1714,7 +1489,7 @@ HALF_@kind@_indexed(void *NPY_UNUSED(context),\n         const float v = npy_half_to_float(*(npy_half *)value);\n         *indexed = npy_float_to_half(npy_half_to_float(*indexed) @OP@ v);\n     }\n-    return 0; \n+    return 0;\n }\n /**end repeat**/\n \n@@ -1974,12 +1749,6 @@ HALF_conjugate(char **args, npy_intp const *dimensions, npy_intp const *steps, v\n     }\n }\n \n-NPY_NO_EXPORT NPY_GCC_OPT_3 void\n-HALF_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    UNARY_LOOP_FAST(npy_half, npy_half, *out = in&0x7fffu);\n-}\n-\n NPY_NO_EXPORT void\n HALF_negative(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -68,6 +68,17 @@ NPY_NO_EXPORT void\n BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat**/\n \n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_autovec.dispatch.h\"\n+#endif\n+/**begin repeat\n+ * #kind = isnan, isinf, isfinite#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void BOOL_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat**/\n+\n /*\n  *****************************************************************************\n  **                           INTEGER LOOPS\n@@ -123,16 +134,34 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n /**end repeat1**/\n /**end repeat**/\n \n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_autovec.dispatch.h\"\n+#endif\n /**begin repeat\n- * #TYPE = BYTE, SHORT, INT, LONG, LONGLONG#\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n+           BYTE,  SHORT,  INT,  LONG,  LONGLONG#\n+ */\n+/**begin repeat1\n+ * #kind = invert, logical_not, conjugate, reciprocal, square, add,\n+ *         subtract, multiply, bitwise_and, bitwise_or, bitwise_xor,\n+ *         left_shift, right_shift, logical_and, logical_or,\n+ *         logical_xor, isnan, isinf, isfinite,\n+ *         absolute, sign#\n  */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat1**/\n+/**end repeat**/\n \n+/**begin repeat\n+ * #TYPE = BYTE, SHORT, INT, LONG, LONGLONG#\n+ */\n /**begin repeat1\n  * both signed and unsigned integer types\n  * #s = , u#\n  * #S = , U#\n  */\n-\n #define @S@@TYPE@_floor_divide @S@@TYPE@_divide\n #define @S@@TYPE@_floor_divide_indexed @S@@TYPE@_divide_indexed\n #define @S@@TYPE@_fmax @S@@TYPE@_maximum\n@@ -147,49 +176,15 @@ NPY_NO_EXPORT void\n @S@@TYPE@_positive(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n /**begin repeat2\n- * #isa = , _avx2#\n- */\n-\n-NPY_NO_EXPORT void\n-@S@@TYPE@_square@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n-\n-NPY_NO_EXPORT void\n-@S@@TYPE@_reciprocal@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n-\n-NPY_NO_EXPORT void\n-@S@@TYPE@_conjugate@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-NPY_NO_EXPORT void\n-@S@@TYPE@_logical_not@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-NPY_NO_EXPORT void\n-@S@@TYPE@_invert@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-/**begin repeat3\n  * Arithmetic\n  * #kind = add, subtract, multiply, bitwise_and, bitwise_or, bitwise_xor,\n  *          left_shift, right_shift#\n  * #OP = +, -,*, &, |, ^, <<, >>#\n  */\n-NPY_NO_EXPORT void\n-@S@@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n NPY_NO_EXPORT int\n-@S@@TYPE@_@kind@@isa@_indexed(PyArrayMethod_Context *NPY_UNUSED(context), char *const *args, npy_intp const *dimensions, npy_intp const *steps, NpyAuxData *NPY_UNUSED(func));\n-\n-/**end repeat3**/\n-\n-/**begin repeat3\n- * #kind = logical_and, logical_or#\n- * #OP = &&, ||#\n- */\n-NPY_NO_EXPORT void\n-@S@@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-/**end repeat3**/\n+@S@@TYPE@_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context), char *const *args,\n+                         npy_intp const *dimensions, npy_intp const *steps, NpyAuxData *NPY_UNUSED(func));\n \n-NPY_NO_EXPORT void\n-@S@@TYPE@_logical_xor@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat2**/\n \n /**begin repeat2\n@@ -206,27 +201,14 @@ NPY_NO_EXPORT int\n NPY_NO_EXPORT void\n @S@@TYPE@_power(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n-NPY_NO_EXPORT void\n-@S@@TYPE@_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-NPY_NO_EXPORT void\n-@S@@TYPE@_sign(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n NPY_NO_EXPORT void\n @S@@TYPE@_gcd(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n NPY_NO_EXPORT void\n @S@@TYPE@_lcm(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-/**begin repeat2\n- * #kind = isnan, isinf, isfinite#\n- **/\n-NPY_NO_EXPORT void\n-@S@@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat2**/\n \n /**end repeat1**/\n-\n /**end repeat**/\n \n \n@@ -375,18 +357,6 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@func@,\n /**end repeat1**/\n /**end repeat**/\n \n-/**begin repeat\n- *  #TYPE = FLOAT, DOUBLE#\n- */\n-/**begin repeat1\n- * #func = maximum, minimum#\n- */\n-NPY_NO_EXPORT void\n-@TYPE@_@func@_avx512f(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n-/**end repeat1**/\n-/**end repeat**/\n-\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_trigonometric.dispatch.h\"\n #endif\n@@ -562,6 +532,19 @@ NPY_NO_EXPORT void\n /**end repeat1**/\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_autovec.dispatch.h\"\n+#endif\n+/**begin repeat\n+ * #TYPE = HALF#\n+ */\n+/**begin repeat1\n+ * #kind = absolute#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat1**/\n+/**end repeat**/\n /*\n  *****************************************************************************\n  **                           COMPLEX LOOPS                                 **\n@@ -728,9 +711,6 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @TYPE@_isfinite(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n-NPY_NO_EXPORT void\n-@TYPE@_isinf(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n #define @TYPE@_isnan @TYPE@_isnat\n \n NPY_NO_EXPORT void\n@@ -806,6 +786,21 @@ TIMEDELTA_mm_qm_divmod(char **args, npy_intp const *dimensions, npy_intp const *\n #define TIMEDELTA_md_m_floor_divide TIMEDELTA_md_m_divide\n /* #define TIMEDELTA_mm_d_floor_divide TIMEDELTA_mm_d_divide */\n \n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_autovec.dispatch.h\"\n+#endif\n+/**begin repeat\n+ * #TYPE = TIMEDELTA, DATETIME#\n+ */\n+/**begin repeat1\n+ * #kind = isinf#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat1**/\n+/**end repeat**/\n+\n /*\n  *****************************************************************************\n  **                            OBJECT LOOPS                                 **"
            },
            {
                "filename": "numpy/core/src/umath/loops_autovec.dispatch.c.src",
                "patch": "@@ -0,0 +1,287 @@\n+/*@targets\n+ ** $maxopt $autovec baseline\n+ ** sse2 avx2\n+ ** neon\n+ ** vsx2\n+ ** vx\n+ **/\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+/*\n+ *****************************************************************************\n+ **                           INTEGER LOOPS\n+ *****************************************************************************\n+ */\n+/*\n+ * Arithmetic bit shift operations.\n+ *\n+ * Intel hardware masks bit shift values, so large shifts wrap around\n+ * and can produce surprising results. The special handling ensures that\n+ * behavior is independent of compiler or hardware.\n+ * TODO: We could implement consistent behavior for negative shifts,\n+ *       which is undefined in C.\n+ */\n+#define INT_left_shift_needs_clear_floatstatus\n+#define UINT_left_shift_needs_clear_floatstatus\n+\n+/**begin repeat\n+ * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n+ *         LONG, ULONG, LONGLONG, ULONGLONG#\n+ * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n+ *         npy_long, npy_ulong, npy_longlong, npy_ulonglong#\n+ * #ftype = npy_float, npy_float, npy_float, npy_float, npy_double, npy_double,\n+ *          npy_double, npy_double, npy_double, npy_double#\n+ * #SIGNED = 1, 0, 1, 0, 1, 0, 1, 0, 1, 0#\n+ * #c = hh,uhh,h,uh,,u,l,ul,ll,ull#\n+ */\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_positive)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = +in);\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_square)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = in * in);\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_reciprocal)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = 1.0 / in);\n+}\n+\n+/**begin repeat1\n+ * Arithmetic\n+ * #kind = add, subtract, multiply#\n+ * #OP = +, -, *#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    if (IS_BINARY_REDUCE) {\n+        BINARY_REDUCE_LOOP_FAST(@type@, io1 @OP@= in2);\n+    }\n+    else {\n+        BINARY_LOOP_FAST(@type@, @type@, *out = in1 @OP@ in2);\n+    }\n+}\n+/**end repeat1**/\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_left_shift)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps,\n+                  void *NPY_UNUSED(func))\n+{\n+    BINARY_LOOP_FAST(@type@, @type@, *out = npy_lshift@c@(in1, in2));\n+#ifdef @TYPE@_left_shift_needs_clear_floatstatus\n+    // For some reason, our macOS CI sets an \"invalid\" flag here, but only\n+    // for some types.\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+#endif\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_right_shift)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+#ifndef NPY_DO_NOT_OPTIMIZE_@TYPE@_right_shift\n+    BINARY_LOOP_FAST(@type@, @type@, *out = npy_rshift@c@(in1, in2));\n+#else\n+    BINARY_LOOP {\n+        @type@ in1 = *(@type@ *)ip1;\n+        @type@ in2 = *(@type@ *)ip2;\n+        *(@type@ *)op1 = npy_rshift@c@(in1, in2);\n+    }\n+#endif\n+}\n+/**end repeat**/\n+\n+/*\n+ *****************************************************************************\n+ **                         UNSIGNED INTEGER LOOPS\n+ *****************************************************************************\n+ */\n+/**begin repeat\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG#\n+ * #type = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong#\n+ * #c    = u,u,u,ul,ull#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_absolute)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = in);\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_sign)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = in > 0 ? 1 : 0);\n+}\n+\n+/**begin repeat1\n+ * Arithmetic\n+ * #kind = bitwise_and, bitwise_or, bitwise_xor#\n+ * #OP = &, |, ^#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    if (IS_BINARY_REDUCE) {\n+        BINARY_REDUCE_LOOP_FAST(@type@, io1 @OP@= in2);\n+    }\n+    else {\n+        BINARY_LOOP_FAST(@type@, @type@, *out = in1 @OP@ in2);\n+    }\n+}\n+/**end repeat1**/\n+\n+/**begin repeat1\n+ * #kind = logical_and, logical_or#\n+ * #OP = &&, ||#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    /*\n+     * gcc vectorization of this is not good (PR60575) but manual integer\n+     * vectorization is too tedious to be worthwhile\n+     */\n+    BINARY_LOOP_FAST(@type@, npy_bool, *out = in1 @OP@ in2);\n+}\n+/**end repeat1**/\n+\n+NPY_FINLINE npy_bool @TYPE@_logical_xor_(@type@ in1, @type@ in2)\n+{ return (!!in1) != (!!in2); }\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_logical_xor)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    BINARY_LOOP_FAST(@type@, npy_bool, *out = @TYPE@_logical_xor_(in1, in2));\n+}\n+\n+/**begin repeat1\n+ * #kind = isnan, isinf, isfinite#\n+ * #func = npy_isnan, npy_isinf, npy_isfinite#\n+ * #val = NPY_FALSE, NPY_FALSE, NPY_TRUE#\n+ **/\n+NPY_NO_EXPORT void  NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    /*\n+     * The (void)in; suppresses an unused variable warning raised by gcc and allows\n+     * us to re-use this macro even though we do not depend on in\n+     */\n+    UNARY_LOOP_FAST(@type@, npy_bool, (void)in; *out = @val@);\n+}\n+/**end repeat1**/\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_conjugate)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = in);\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_logical_not)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, npy_bool, *out = !in);\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_invert)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = ~in);\n+}\n+/**end repeat**/\n+\n+/*\n+ *****************************************************************************\n+ **                         SIGNED! INTEGER LOOPS\n+ *****************************************************************************\n+ */\n+\n+/**begin repeat\n+ * #TYPE = BYTE, SHORT, INT, LONG, LONGLONG#\n+ * #type = npy_byte, npy_short, npy_int, npy_long, npy_longlong#\n+ * #c    = ,,,l,ll#\n+ */\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_absolute)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = (in >= 0) ? in : -in);\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_sign)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(@type@, @type@, *out = in > 0 ? 1 : (in < 0 ? -1 : 0));\n+}\n+\n+/**begin repeat1\n+ * #kind = conjugate, invert, isnan, isinf, isfinite,\n+ *         logical_and, logical_or, logical_xor, logical_not,\n+ *         bitwise_and, bitwise_or, bitwise_xor#\n+ **/\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)\n+{\n+    NPY_CPU_DISPATCH_CURFX(U@TYPE@_@kind@)(args, dimensions, steps, func);\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/*\n+ *****************************************************************************\n+ **                             BOOLEAN LOOPS                               **\n+ *****************************************************************************\n+ */\n+/**begin repeat\n+ * #kind = isnan, isinf, isfinite#\n+ * #func = npy_isnan, npy_isinf, npy_isfinite#\n+ * #val = NPY_FALSE, NPY_FALSE, NPY_TRUE#\n+ **/\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)\n+{\n+    NPY_CPU_DISPATCH_CURFX(UBYTE_@kind@)(args, dimensions, steps, func);\n+}\n+/**end repeat**/\n+\n+/*\n+ *****************************************************************************\n+ **                          HALF-FLOAT LOOPS                               **\n+ *****************************************************************************\n+ */\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(HALF_absolute)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    UNARY_LOOP_FAST(npy_half, npy_half, *out = in&0x7fffu);\n+}\n+\n+/*\n+ *****************************************************************************\n+ **                           DATETIME LOOPS                                **\n+ *****************************************************************************\n+ */\n+\n+/**begin repeat\n+ * #type = npy_datetime, npy_timedelta#\n+ * #TYPE = DATETIME, TIMEDELTA#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_isinf)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)\n+{\n+    NPY_CPU_DISPATCH_CURFX(ULONGLONG_isinf)(args, dimensions, steps, func);\n+}\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/loops_exponent_log.dispatch.c.src",
                "patch": "@@ -239,7 +239,7 @@ fma_scalef_ps(__m256 poly, __m256 quadrant)\n \n #ifdef SIMD_AVX512F\n \n-NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n+NPY_FINLINE __mmask16\n avx512_get_full_load_mask_ps(void)\n {\n     return 0xFFFF;"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23251,
        "body": "This follows the same pattern as gh-23248 for put and putmask.  (There are a few more, but the pattern should be a bit less common.)\r\n\r\nOne benchmark gets slower, because it ends up doing no copy at all, but now has to do the setup work anyway, the \"dense\" is massively faster for the same reason of course:\r\n```\r\n       before           after         ratio\r\n     [b657a75c]       [3a33b4c2]\r\n     <main>           <further-removals>\r\n+        1.40\u00b10\u03bcs      1.69\u00b10.01\u03bcs     1.21  bench_itemselection.PutMask.time_sparse(False, 'i,O')\r\n+     1.40\u00b10.01\u03bcs         1.69\u00b10\u03bcs     1.21  bench_itemselection.PutMask.time_sparse(True, 'i,O')\r\n-        5.54\u00b10\u03bcs      4.36\u00b10.01\u03bcs     0.79  bench_itemselection.PutMask.time_dense(True, 'O')\r\n-     5.63\u00b10.09\u03bcs         4.35\u00b10\u03bcs     0.77  bench_itemselection.PutMask.time_dense(False, 'O')\r\n-     6.08\u00b10.04\u03bcs         4.57\u00b10\u03bcs     0.75  bench_itemselection.Put.time_ordered(False, 'O')\r\n-     6.40\u00b10.08\u03bcs       4.61\u00b10.2\u03bcs     0.72  bench_itemselection.Put.time_ordered(True, 'O')\r\n-         175\u00b14\u03bcs      11.3\u00b10.01\u03bcs     0.06  bench_itemselection.Put.time_ordered(False, 'i,O')\r\n-         183\u00b13\u03bcs       11.7\u00b10.3\u03bcs     0.06  bench_itemselection.Put.time_ordered(True, 'i,O')\r\n-       176\u00b10.2\u03bcs      10.4\u00b10.02\u03bcs     0.06  bench_itemselection.PutMask.time_dense(True, 'i,O')\r\n-         178\u00b13\u03bcs      10.4\u00b10.01\u03bcs     0.06  bench_itemselection.PutMask.time_dense(False, 'i,O')\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE DECREASED.\r\n```",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_itemselection.py",
                "patch": "@@ -21,7 +21,7 @@ def time_contiguous(self, shape, mode, dtype):\n class PutMask(Benchmark):\n     params = [\n         [True, False],\n-        TYPES1]\n+        TYPES1 + [\"O\", \"i,O\"]]\n     param_names = [\"values_is_scalar\", \"dtype\"]\n \n     def setup(self, values_is_scalar, dtype):\n@@ -41,3 +41,21 @@ def time_dense(self, values_is_scalar, dtype):\n     def time_sparse(self, values_is_scalar, dtype):\n         np.putmask(self.arr, self.sparse_mask, self.vals)\n \n+\n+class Put(Benchmark):\n+    params = [\n+        [True, False],\n+        TYPES1 + [\"O\", \"i,O\"]]\n+    param_names = [\"values_is_scalar\", \"dtype\"]\n+\n+    def setup(self, values_is_scalar, dtype):\n+        if values_is_scalar:\n+            self.vals = np.array(1., dtype=dtype)\n+        else:\n+            self.vals = np.ones(1000, dtype=dtype)\n+\n+        self.arr = np.ones(1000, dtype=dtype)\n+        self.indx = np.arange(1000, dtype=np.intp)\n+\n+    def time_ordered(self, values_is_scalar, dtype):\n+        np.put(self.arr, self.indx, self.vals)"
            },
            {
                "filename": "numpy/core/src/multiarray/item_selection.c",
                "patch": "@@ -351,7 +351,7 @@ PyArray_PutTo(PyArrayObject *self, PyObject* values0, PyObject *indices0,\n               NPY_CLIPMODE clipmode)\n {\n     PyArrayObject  *indices, *values;\n-    npy_intp i, chunk, ni, max_item, nv, tmp;\n+    npy_intp i, itemsize, ni, max_item, nv, tmp;\n     char *src, *dest;\n     int copied = 0;\n     int overlap = 0;\n@@ -401,25 +401,56 @@ PyArray_PutTo(PyArrayObject *self, PyObject* values0, PyObject *indices0,\n     }\n     max_item = PyArray_SIZE(self);\n     dest = PyArray_DATA(self);\n-    chunk = PyArray_DESCR(self)->elsize;\n+    itemsize = PyArray_DESCR(self)->elsize;\n \n-    if (PyDataType_REFCHK(PyArray_DESCR(self))) {\n+    NPY_BEGIN_THREADS_DEF;\n+    NPY_cast_info cast_info;\n+    NPY_ARRAYMETHOD_FLAGS flags;\n+    const npy_intp one = 1;\n+    const npy_intp strides[2] = {itemsize, itemsize};\n+\n+    NPY_cast_info_init(&cast_info);\n+\n+    int has_references = PyDataType_REFCHK(PyArray_DESCR(self));\n+\n+    if (!has_references) {\n+        /* if has_references is not needed memcpy is safe for a simple copy  */\n+        NPY_BEGIN_THREADS_THRESHOLDED(ni);\n+    }\n+    else {\n+        PyArray_Descr *dtype = PyArray_DESCR(self);\n+        if (PyArray_GetDTypeTransferFunction(\n+                PyArray_ISALIGNED(self), itemsize, itemsize, dtype, dtype, 0,\n+                &cast_info, &flags) < 0) {\n+            goto fail;\n+        }\n+        if (!(flags & NPY_METH_REQUIRES_PYAPI)) {\n+            NPY_BEGIN_THREADS_THRESHOLDED(ni);\n+        }\n+    }\n+\n+\n+    if (has_references) {\n         switch(clipmode) {\n         case NPY_RAISE:\n             for (i = 0; i < ni; i++) {\n-                src = PyArray_BYTES(values) + chunk*(i % nv);\n+                src = PyArray_BYTES(values) + itemsize*(i % nv);\n                 tmp = ((npy_intp *)(PyArray_DATA(indices)))[i];\n-                if (check_and_adjust_index(&tmp, max_item, 0, NULL) < 0) {\n+                if (check_and_adjust_index(&tmp, max_item, 0, _save) < 0) {\n+                    goto fail;\n+                }\n+                char *data[2] = {src, dest + tmp*itemsize};\n+                if (cast_info.func(\n+                        &cast_info.context, data, &one, strides,\n+                        cast_info.auxdata) < 0) {\n+                    NPY_END_THREADS;\n                     goto fail;\n                 }\n-                PyArray_Item_INCREF(src, PyArray_DESCR(self));\n-                PyArray_Item_XDECREF(dest+tmp*chunk, PyArray_DESCR(self));\n-                memmove(dest + tmp*chunk, src, chunk);\n             }\n             break;\n         case NPY_WRAP:\n             for (i = 0; i < ni; i++) {\n-                src = PyArray_BYTES(values) + chunk * (i % nv);\n+                src = PyArray_BYTES(values) + itemsize * (i % nv);\n                 tmp = ((npy_intp *)(PyArray_DATA(indices)))[i];\n                 if (tmp < 0) {\n                     while (tmp < 0) {\n@@ -431,45 +462,51 @@ PyArray_PutTo(PyArrayObject *self, PyObject* values0, PyObject *indices0,\n                         tmp -= max_item;\n                     }\n                 }\n-                PyArray_Item_INCREF(src, PyArray_DESCR(self));\n-                PyArray_Item_XDECREF(dest+tmp*chunk, PyArray_DESCR(self));\n-                memmove(dest + tmp * chunk, src, chunk);\n+                char *data[2] = {src, dest + tmp*itemsize};\n+                if (cast_info.func(\n+                        &cast_info.context, data, &one, strides,\n+                        cast_info.auxdata) < 0) {\n+                    NPY_END_THREADS;\n+                    goto fail;\n+                }\n             }\n             break;\n         case NPY_CLIP:\n             for (i = 0; i < ni; i++) {\n-                src = PyArray_BYTES(values) + chunk * (i % nv);\n+                src = PyArray_BYTES(values) + itemsize * (i % nv);\n                 tmp = ((npy_intp *)(PyArray_DATA(indices)))[i];\n                 if (tmp < 0) {\n                     tmp = 0;\n                 }\n                 else if (tmp >= max_item) {\n                     tmp = max_item - 1;\n                 }\n-                PyArray_Item_INCREF(src, PyArray_DESCR(self));\n-                PyArray_Item_XDECREF(dest+tmp*chunk, PyArray_DESCR(self));\n-                memmove(dest + tmp * chunk, src, chunk);\n+                char *data[2] = {src, dest + tmp*itemsize};\n+                if (cast_info.func(\n+                        &cast_info.context, data, &one, strides,\n+                        cast_info.auxdata) < 0) {\n+                    NPY_END_THREADS;\n+                    goto fail;\n+                }\n             }\n             break;\n         }\n     }\n     else {\n-        NPY_BEGIN_THREADS_DEF;\n-        NPY_BEGIN_THREADS_THRESHOLDED(ni);\n         switch(clipmode) {\n         case NPY_RAISE:\n             for (i = 0; i < ni; i++) {\n-                src = PyArray_BYTES(values) + chunk * (i % nv);\n+                src = PyArray_BYTES(values) + itemsize * (i % nv);\n                 tmp = ((npy_intp *)(PyArray_DATA(indices)))[i];\n                 if (check_and_adjust_index(&tmp, max_item, 0, _save) < 0) {\n                     goto fail;\n                 }\n-                memmove(dest + tmp * chunk, src, chunk);\n+                memmove(dest + tmp * itemsize, src, itemsize);\n             }\n             break;\n         case NPY_WRAP:\n             for (i = 0; i < ni; i++) {\n-                src = PyArray_BYTES(values) + chunk * (i % nv);\n+                src = PyArray_BYTES(values) + itemsize * (i % nv);\n                 tmp = ((npy_intp *)(PyArray_DATA(indices)))[i];\n                 if (tmp < 0) {\n                     while (tmp < 0) {\n@@ -481,27 +518,29 @@ PyArray_PutTo(PyArrayObject *self, PyObject* values0, PyObject *indices0,\n                         tmp -= max_item;\n                     }\n                 }\n-                memmove(dest + tmp * chunk, src, chunk);\n+                memmove(dest + tmp * itemsize, src, itemsize);\n             }\n             break;\n         case NPY_CLIP:\n             for (i = 0; i < ni; i++) {\n-                src = PyArray_BYTES(values) + chunk * (i % nv);\n+                src = PyArray_BYTES(values) + itemsize * (i % nv);\n                 tmp = ((npy_intp *)(PyArray_DATA(indices)))[i];\n                 if (tmp < 0) {\n                     tmp = 0;\n                 }\n                 else if (tmp >= max_item) {\n                     tmp = max_item - 1;\n                 }\n-                memmove(dest + tmp * chunk, src, chunk);\n+                memmove(dest + tmp * itemsize, src, itemsize);\n             }\n             break;\n         }\n-        NPY_END_THREADS;\n     }\n+    NPY_END_THREADS;\n \n  finish:\n+    NPY_cast_info_xfree(&cast_info);\n+\n     Py_XDECREF(values);\n     Py_XDECREF(indices);\n     if (copied) {\n@@ -511,6 +550,8 @@ PyArray_PutTo(PyArrayObject *self, PyObject* values0, PyObject *indices0,\n     Py_RETURN_NONE;\n \n  fail:\n+    NPY_cast_info_xfree(&cast_info);\n+\n     Py_XDECREF(indices);\n     Py_XDECREF(values);\n     if (copied) {\n@@ -591,11 +632,12 @@ PyArray_PutMask(PyArrayObject *self, PyObject* values0, PyObject* mask0)\n {\n     PyArrayObject *mask, *values;\n     PyArray_Descr *dtype;\n-    npy_intp chunk, ni, nv;\n+    npy_intp itemsize, ni, nv;\n     char *src, *dest;\n     npy_bool *mask_data;\n     int copied = 0;\n     int overlap = 0;\n+    NPY_BEGIN_THREADS_DEF;\n \n     mask = NULL;\n     values = NULL;\n@@ -656,31 +698,47 @@ PyArray_PutMask(PyArrayObject *self, PyObject* values0, PyObject* mask0)\n         self = obj;\n     }\n \n-    chunk = PyArray_DESCR(self)->elsize;\n+    itemsize = PyArray_DESCR(self)->elsize;\n     dest = PyArray_DATA(self);\n \n     if (PyDataType_REFCHK(PyArray_DESCR(self))) {\n+        NPY_cast_info cast_info;\n+        NPY_ARRAYMETHOD_FLAGS flags;\n+        const npy_intp one = 1;\n+        const npy_intp strides[2] = {itemsize, itemsize};\n+\n+        NPY_cast_info_init(&cast_info);\n+        if (PyArray_GetDTypeTransferFunction(\n+                PyArray_ISALIGNED(self), itemsize, itemsize, dtype, dtype, 0,\n+                &cast_info, &flags) < 0) {\n+            return NULL;\n+        }\n+        if (!(flags & NPY_METH_REQUIRES_PYAPI)) {\n+            NPY_BEGIN_THREADS;\n+        }\n+\n         for (npy_intp i = 0, j = 0; i < ni; i++, j++) {\n             if (j >= nv) {\n                 j = 0;\n             }\n             if (mask_data[i]) {\n-                char *src_ptr = src + j*chunk;\n-                char *dest_ptr = dest + i*chunk;\n-\n-                PyArray_Item_INCREF(src_ptr, PyArray_DESCR(self));\n-                PyArray_Item_XDECREF(dest_ptr, PyArray_DESCR(self));\n-                memmove(dest_ptr, src_ptr, chunk);\n+                char *data[2] = {src + j*itemsize, dest + i*itemsize};\n+                if (cast_info.func(\n+                        &cast_info.context, data, &one, strides,\n+                        cast_info.auxdata) < 0) {\n+                    NPY_END_THREADS;\n+                    goto fail;\n+                }\n             }\n         }\n     }\n     else {\n-        NPY_BEGIN_THREADS_DEF;\n-        NPY_BEGIN_THREADS_DESCR(PyArray_DESCR(self));\n-        npy_fastputmask(dest, src, mask_data, ni, nv, chunk);\n-        NPY_END_THREADS;\n+        NPY_BEGIN_THREADS;\n+        npy_fastputmask(dest, src, mask_data, ni, nv, itemsize);\n     }\n \n+    NPY_END_THREADS;\n+\n     Py_XDECREF(values);\n     Py_XDECREF(mask);\n     if (copied) {"
            },
            {
                "filename": "numpy/core/tests/test_item_selection.py",
                "patch": "@@ -1,5 +1,7 @@\n import sys\n \n+import pytest\n+\n import numpy as np\n from numpy.testing import (\n     assert_, assert_raises, assert_array_equal, HAS_REFCOUNT\n@@ -84,3 +86,59 @@ def test_empty_argpartition(self):\n \n         b = np.array([0, 1, 2, 3, 4, 5])\n         assert_array_equal(a, b)\n+\n+\n+class TestPutMask:\n+    @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"]) + [\"i,O\"])\n+    def test_simple(self, dtype):\n+        if dtype.lower() == \"m\":\n+            dtype += \"8[ns]\"\n+\n+        # putmask is weird and doesn't care about value length (even shorter)\n+        vals = np.arange(1001).astype(dtype=dtype)\n+\n+        mask = np.random.randint(2, size=1000).astype(bool)\n+        # Use vals.dtype in case of flexible dtype (i.e. string)\n+        arr = np.zeros(1000, dtype=vals.dtype)\n+        zeros = arr.copy()\n+\n+        np.putmask(arr, mask, vals)\n+        assert_array_equal(arr[mask], vals[:len(mask)][mask])\n+        assert_array_equal(arr[~mask], zeros[~mask])\n+\n+\n+class TestPut:\n+    @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"])[1:] + [\"i,O\"])\n+    @pytest.mark.parametrize(\"mode\", [\"raise\", \"wrap\", \"clip\"])\n+    def test_simple(self, dtype, mode):\n+        if dtype.lower() == \"m\":\n+            dtype += \"8[ns]\"\n+\n+        # put is weird and doesn't care about value length (even shorter)\n+        vals = np.arange(1001).astype(dtype=dtype)\n+\n+        # Use vals.dtype in case of flexible dtype (i.e. string)\n+        arr = np.zeros(1000, dtype=vals.dtype)\n+        zeros = arr.copy()\n+\n+        if mode == \"clip\":\n+            # Special because 0 and -1 value are \"reserved\" for clip test\n+            indx = np.random.permutation(len(arr) - 2)[:-500] + 1\n+\n+            indx[-1] = 0\n+            indx[-2] = len(arr) - 1\n+            indx_put = indx.copy()\n+            indx_put[-1] = -1389\n+            indx_put[-2] = 1321\n+        else:\n+            # Avoid duplicates (for simplicity) and fill half only\n+            indx = np.random.permutation(len(arr) - 3)[:-500]\n+            indx_put = indx\n+            if mode == \"wrap\":\n+                indx_put = indx_put + len(arr)\n+\n+        np.put(arr, indx_put, vals, mode=mode)\n+        assert_array_equal(arr[indx], vals[:len(indx)])\n+        untouched = np.ones(len(arr), dtype=bool)\n+        untouched[indx] = False\n+        assert_array_equal(arr[untouched], zeros[:untouched.sum()])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23177,
        "body": "Continuation of the `ufunc.at` optimizations:\r\n- add indexed loops for maximum, minimum, fmax, fmin ufuncs and a benchmark for `maximum.at` (performance increased by ~13x)\r\n- add a ContextVariable `explain_chain` which can gather information about code paths taken in the ufuncs. \r\nI would like the `explain_chain` functionality to be expanded through the ufuncs so that you can do something like\r\n```\r\n>>> with explain_chain as e:\r\n>>>     <do something>\r\n>>> print(e)\r\n['called ufunc.add', 'using trivial_at_loop', 'calling FLOAT_add_avx512_indexed']\r\n```\r\n\r\nSo far it collects which `ufunc_at` iteration loop is used, and cannot be used as context manager. It seems to cost ~2% in speed. Help with deciding if this is a desired feature, how to document it, and how to expand it are welcome. In the mean time, I used it in the tests to prove I am using the `trivial_at_loop` where I expected, and to find out that `divide` does not actually have  an indexed loop, only `true_divide` has one.\r\n\r\n$ python runtests.py --bench-compare main -- -b bench_ufunc.At --no-only-changed\r\n```\r\nAll benchmarks:\r\n\r\n       before           after         ratio\r\n     [c4c0bbd3]       [70e8f541]\r\n     <main>           <indexed-loops2>\r\n      1.09\u00b10.01ms      1.11\u00b10.03ms     1.02  bench_ufunc.At.time_sum_at\r\n-     15.5\u00b10.04ms         1.21\u00b10ms     0.08  bench_ufunc.At.time_maximum_at\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -36,13 +36,17 @@ def time_broadcast(self):\n \n class At(Benchmark):\n     def setup(self):\n-        self.vals = np.random.rand(1_000_000)\n-        self.idx = np.random.randint(1000, size=1_000_000).astype(np.intp)\n+        rng = np.random.default_rng(1)\n+        self.vals = rng.random(10_000_000, dtype=np.float64)\n+        self.idx = rng.integers(1000, size=10_000_000).astype(np.intp)\n         self.res = np.zeros(1000, dtype=self.vals.dtype)\n \n     def time_sum_at(self):\n         np.add.at(self.res, self.idx, self.vals)\n \n+    def time_maximum_at(self):\n+        np.maximum.at(self.res, self.idx, self.vals)\n+\n \n class UFunc(Benchmark):\n     params = [ufuncs]"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -599,7 +599,8 @@ def english_upper(s):\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n           TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n           TD(no_obj_bool, dispatch=[('loops_minmax', ints+'fdg')]),\n-          TD(O, f='npy_ObjectMax')\n+          TD(O, f='npy_ObjectMax'),\n+          indexed=flts + ints,\n           ),\n 'minimum':\n     Ufunc(2, 1, ReorderableNone,\n@@ -608,7 +609,8 @@ def english_upper(s):\n           TD('?', cfunc_alias='logical_and',\n                   dispatch=[('loops_logical', '?')]),\n           TD(no_obj_bool, dispatch=[('loops_minmax', ints+'fdg')]),\n-          TD(O, f='npy_ObjectMin')\n+          TD(O, f='npy_ObjectMin'),\n+          indexed=flts + ints,\n           ),\n 'clip':\n     Ufunc(3, 1, ReorderableNone,\n@@ -623,7 +625,8 @@ def english_upper(s):\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n           TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n           TD(no_obj_bool, dispatch=[('loops_minmax', 'fdg')]),\n-          TD(O, f='npy_ObjectMax')\n+          TD(O, f='npy_ObjectMax'),\n+          indexed=flts + ints,\n           ),\n 'fmin':\n     Ufunc(2, 1, ReorderableNone,\n@@ -632,7 +635,8 @@ def english_upper(s):\n           TD('?', cfunc_alias='logical_and',\n                   dispatch=[('loops_logical', '?')]),\n           TD(no_obj_bool, dispatch=[('loops_minmax', 'fdg')]),\n-          TD(O, f='npy_ObjectMin')\n+          TD(O, f='npy_ObjectMin'),\n+          indexed=flts + ints,\n           ),\n 'logaddexp':\n     Ufunc(2, 1, MinusInfinity,"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -455,7 +455,9 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n #define @TYPE@_floor_divide @TYPE@_divide\n #define @TYPE@_floor_divide_indexed @TYPE@_divide_indexed\n #define @TYPE@_fmax @TYPE@_maximum\n+#define @TYPE@_fmax_indexed @TYPE@_maximum_indexed\n #define @TYPE@_fmin @TYPE@_minimum\n+#define @TYPE@_fmin_indexed @TYPE@_minimum_indexed\n \n NPY_NO_EXPORT void\n @TYPE@__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n@@ -538,8 +540,8 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n \n NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ int\n @TYPE@_@kind@@isa@_indexed(PyArrayMethod_Context *NPY_UNUSED(context),\n-                           char * const*args, npy_intp const *dimensions,\n-                           npy_intp const *steps, NpyAuxData *NPY_UNUSED(func))\n+         char **args, npy_intp const *dimensions, npy_intp const *steps,\n+         void *NPY_UNUSED(func))\n {\n     char *ip1 = args[0];\n     char *indx = args[1];\n@@ -902,6 +904,7 @@ NPY_NO_EXPORT void\n         }\n     }\n }\n+\n /**end repeat1**/\n \n /**begin repeat1\n@@ -1695,7 +1698,9 @@ HALF_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n }\n \n NPY_NO_EXPORT int\n-HALF_@kind@_indexed(void *NPY_UNUSED(context), char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+HALF_@kind@_indexed(void *NPY_UNUSED(context),\n+         char **args, npy_intp const *dimensions, npy_intp const *steps,\n+         void *NPY_UNUSED(func))\n {\n     char *ip1 = args[0];\n     char *indx = args[1];\n@@ -1812,6 +1817,27 @@ HALF_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n     }\n     /* npy_half_isnan will never set floatstatus_invalid, so do not clear */\n }\n+\n+NPY_NO_EXPORT int\n+HALF_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context),\n+         char **args, npy_intp const *dimensions, npy_intp const *steps,\n+         void *NPY_UNUSED(func))\n+{\n+    char *ip1 = args[0];\n+    char *indx = args[1];\n+    char *value = args[2];\n+    npy_intp is1 = steps[0], isindex = steps[1], isb = steps[2];\n+    npy_intp n = dimensions[0];\n+    npy_intp i;\n+    npy_half *indexed;\n+    for(i = 0; i < n; i++, indx += isindex, value += isb) {\n+        indexed = (npy_half *)(ip1 + is1 * *(npy_intp *)indx);\n+        npy_half v = *(npy_half *)value;\n+        *indexed = (@OP@(*indexed, v) || npy_half_isnan(*indexed)) ? *indexed : v;\n+    }\n+    return 0;\n+}\n+\n /**end repeat**/\n \n /**begin repeat\n@@ -1827,8 +1853,29 @@ HALF_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n         const npy_half in2 = *(npy_half *)ip2;\n         *((npy_half *)op1) = (@OP@(in1, in2) || npy_half_isnan(in2)) ? in1 : in2;\n     }\n-    /* npy_half_isnan will never set floatstatus_invalid, so do not clear */\n+    /* no need to clear floatstatus_invalid */\n+}\n+\n+NPY_NO_EXPORT int\n+HALF_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context),\n+         char **args, npy_intp const *dimensions, npy_intp const *steps,\n+         void *NPY_UNUSED(func))\n+{\n+    char *ip1 = args[0];\n+    char *indx = args[1];\n+    char *value = args[2];\n+    npy_intp is1 = steps[0], isindex = steps[1], isb = steps[2];\n+    npy_intp n = dimensions[0];\n+    npy_intp i;\n+    npy_half *indexed;\n+    for (i = 0; i < n; i++, indx += isindex, value += isb) {\n+        indexed = (npy_half *)(ip1 + is1 * *(npy_intp *)indx);\n+        npy_half v = *(npy_half *)value;\n+        *indexed = (@OP@(*indexed, v) || npy_half_isnan(v)) ? *indexed: v;\n+    }\n+    return 0;\n }\n+\n /**end repeat**/\n \n NPY_NO_EXPORT void"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -137,6 +137,8 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n #define @S@@TYPE@_floor_divide_indexed @S@@TYPE@_divide_indexed\n #define @S@@TYPE@_fmax @S@@TYPE@_maximum\n #define @S@@TYPE@_fmin @S@@TYPE@_minimum\n+#define @S@@TYPE@_fmax_indexed @S@@TYPE@_maximum_indexed\n+#define @S@@TYPE@_fmin_indexed @S@@TYPE@_minimum_indexed\n \n NPY_NO_EXPORT void\n @S@@TYPE@__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n@@ -192,10 +194,13 @@ NPY_NO_EXPORT void\n \n /**begin repeat2\n  * #kind = maximum, minimum#\n- * #OP =  >, <#\n  **/\n NPY_NO_EXPORT void\n @S@@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+\n+NPY_NO_EXPORT int\n+@S@@TYPE@_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context), char *const *args, npy_intp const *dimensions, npy_intp const *steps, NpyAuxData *NPY_UNUSED(func));\n+\n /**end repeat2**/\n \n NPY_NO_EXPORT void\n@@ -475,18 +480,24 @@ NPY_NO_EXPORT void\n \n /**begin repeat1\n  * #kind = maximum, minimum#\n- * #OP =  >=, <=#\n  **/\n NPY_NO_EXPORT void\n @TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+\n+NPY_NO_EXPORT int\n+@TYPE@_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context), char *const *args, npy_intp const *dimensions, npy_intp const *steps, NpyAuxData *NPY_UNUSED(func));\n+\n /**end repeat1**/\n \n /**begin repeat1\n  * #kind = fmax, fmin#\n- * #OP =  >=, <=#\n  **/\n NPY_NO_EXPORT void\n @TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+\n+NPY_NO_EXPORT int\n+@TYPE@_@kind@_indexed(PyArrayMethod_Context *NPY_UNUSED(context), char *const *args, npy_intp const *dimensions, npy_intp const *steps, NpyAuxData *NPY_UNUSED(func));\n+\n /**end repeat1**/\n \n NPY_NO_EXPORT void"
            },
            {
                "filename": "numpy/core/src/umath/loops_minmax.dispatch.c.src",
                "patch": "@@ -451,6 +451,24 @@ clear_fp:\n #endif\n }\n \n+\n+NPY_NO_EXPORT int NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@_indexed)\n+(PyArrayMethod_Context *NPY_UNUSED(context), char *const *args, npy_intp const *dimensions, npy_intp const *steps, NpyAuxData *NPY_UNUSED(func))\n+{\n+    char *ip1 = args[0];\n+    char *indx = args[1];\n+    char *value = args[2];\n+    npy_intp is1 = steps[0], isindex = steps[1], isb = steps[2];\n+    npy_intp n = dimensions[0];\n+    npy_intp i;\n+    @type@ *indexed;\n+    for(i = 0; i < n; i++, indx += isindex, value += isb) {\n+        indexed = (@type@ *)(ip1 + is1 * *(npy_intp *)indx);\n+        *indexed = SCALAR_OP(*indexed, *(@type@ *)value);\n+    }\n+    return 0;\n+}\n+\n #undef SCALAR_OP\n \n #endif // !fp_only || (is_fp && fp_only)"
            },
            {
                "filename": "numpy/core/tests/test_ufunc.py",
                "patch": "@@ -1980,16 +1980,16 @@ def test_ufunc_at_basic(self, a):\n         assert_equal(aa, [0, 1, 202, 3, 4, 105, 6, 7, 8, 9])\n \n         with pytest.raises(ValueError):\n-            # extraneous second operand \n+            # extraneous second operand\n             np.negative.at(a, [2, 5, 3], [1, 2, 3])\n \n         with pytest.raises(ValueError):\n             # second operand cannot be converted to an array\n             np.add.at(a, [2, 5, 3], [[1, 2], 1])\n-    \n+\n     # ufuncs with indexed loops for perfomance in ufunc.at\n-    indexed_ufuncs = [np.add, np.subtract, np.multiply,\n-                      np.floor_divide, np.divide]\n+    indexed_ufuncs = [np.add, np.subtract, np.multiply, np.floor_divide,\n+                      np.maximum, np.minimum, np.fmax, np.fmin]\n \n     @pytest.mark.parametrize(\n                 \"typecode\", np.typecodes['AllInteger'] + np.typecodes['Float'])\n@@ -2195,7 +2195,7 @@ def test_at_no_loop_for_op(self):\n     def test_at_output_casting(self):\n         arr = np.array([-1])\n         np.equal.at(arr, [0], [0])\n-        assert arr[0] == 0 \n+        assert arr[0] == 0\n \n     def test_at_broadcast_failure(self):\n         arr = np.arange(5)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 23018,
        "body": "Convert several methods to the [vectorcall](https://docs.python.org/3/c-api/call.html#the-vectorcall-protocol) convention. The conversions give a performance improvement, see https://github.com/numpy/numpy/issues/20790#issuecomment-1383262957\r\n\r\n**Notes**\r\n- For `vdot` the `METH_KEYWORDS` was removed, as the C vdot method was positional only.\r\n- The `add_docstring` is converted with an additional check. It was parsed as `if (!PyArg_ParseTuple(args, \"OO!:add_docstring\", &obj, &PyUnicode_Type, &str))`, but there is no support for the `!` in the `npy_parse_arguments`\r\n- CI was complaining about coverage of `_get_ndarray_c_version`. A test was added, but only to provide coverage\r\n- In `function_base.py` a redundant check in `def place` was removed\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/multiarray.py",
                "patch": "@@ -14,7 +14,7 @@\n # do not change them. issue gh-15518\n # _get_ndarray_c_version is semi-public, on purpose not added to __all__\n from ._multiarray_umath import (\n-    fastCopyAndTranspose, _flagdict, from_dlpack, _insert, _reconstruct,\n+    fastCopyAndTranspose, _flagdict, from_dlpack, _place, _reconstruct,\n     _vec_string, _ARRAY_API, _monotonicity, _get_ndarray_c_version,\n     _get_madvise_hugepage, _set_madvise_hugepage,\n     _get_promotion_state, _set_promotion_state,\n@@ -25,7 +25,7 @@\n     'ITEM_HASOBJECT', 'ITEM_IS_POINTER', 'LIST_PICKLE', 'MAXDIMS',\n     'MAY_SHARE_BOUNDS', 'MAY_SHARE_EXACT', 'NEEDS_INIT', 'NEEDS_PYAPI',\n     'RAISE', 'USE_GETITEM', 'USE_SETITEM', 'WRAP',\n-    '_flagdict', 'from_dlpack', '_insert', '_reconstruct', '_vec_string',\n+    '_flagdict', 'from_dlpack', '_place', '_reconstruct', '_vec_string',\n     '_monotonicity', 'add_docstring', 'arange', 'array', 'asarray',\n     'asanyarray', 'ascontiguousarray', 'asfortranarray', 'bincount',\n     'broadcast', 'busday_count', 'busday_offset', 'busdaycalendar', 'can_cast',\n@@ -1128,7 +1128,7 @@ def copyto(dst, src, casting=None, where=None):\n \n \n @array_function_from_c_func_and_dispatcher(_multiarray_umath.putmask)\n-def putmask(a, mask, values):\n+def putmask(a, /, mask, values):\n     \"\"\"\n     putmask(a, mask, values)\n "
            },
            {
                "filename": "numpy/core/multiarray.pyi",
                "patch": "@@ -428,6 +428,7 @@ def copyto(\n \n def putmask(\n     a: NDArray[Any],\n+    /,\n     mask: _ArrayLikeBool_co,\n     values: ArrayLike,\n ) -> None: ..."
            },
            {
                "filename": "numpy/core/src/multiarray/compiled_base.c",
                "patch": "@@ -8,6 +8,7 @@\n #include \"numpy/arrayobject.h\"\n #include \"numpy/npy_3kcompat.h\"\n #include \"numpy/npy_math.h\"\n+#include \"npy_argparse.h\"\n #include \"npy_config.h\"\n #include \"templ_common.h\" /* for npy_mul_sizes_with_overflow */\n #include \"lowlevel_strided_loops.h\" /* for npy_bswap8 */\n@@ -109,19 +110,23 @@ minmax(const npy_intp *data, npy_intp data_len, npy_intp *mn, npy_intp *mx)\n  * output array.\n  */\n NPY_NO_EXPORT PyObject *\n-arr_bincount(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwds)\n+arr_bincount(PyObject *NPY_UNUSED(self), PyObject *const *args,\n+                            Py_ssize_t len_args, PyObject *kwnames)\n {\n     PyObject *list = NULL, *weight = Py_None, *mlength = NULL;\n     PyArrayObject *lst = NULL, *ans = NULL, *wts = NULL;\n     npy_intp *numbers, *ians, len, mx, mn, ans_size;\n     npy_intp minlength = 0;\n     npy_intp i;\n     double *weights , *dans;\n-    static char *kwlist[] = {\"list\", \"weights\", \"minlength\", NULL};\n \n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O|OO:bincount\",\n-                kwlist, &list, &weight, &mlength)) {\n-            goto fail;\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"bincount\", args, len_args, kwnames,\n+                \"list\", NULL, &list,\n+                \"|weights\", NULL, &weight,\n+                \"|minlength\", NULL, &mlength,\n+                NULL, NULL, NULL) < 0) {\n+        return NULL;\n     }\n \n     lst = (PyArrayObject *)PyArray_ContiguousFromAny(list, NPY_INTP, 1, 1);\n@@ -265,7 +270,7 @@ arr__monotonicity(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwds)\n  * indicated by the mask\n  */\n NPY_NO_EXPORT PyObject *\n-arr_insert(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n+arr_place(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n {\n     char *src, *dest;\n     npy_bool *mask_data;\n@@ -489,7 +494,8 @@ binary_search_with_guess(const npy_double key, const npy_double *arr,\n #undef LIKELY_IN_CACHE_SIZE\n \n NPY_NO_EXPORT PyObject *\n-arr_interp(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n+arr_interp(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args,\n+                             PyObject *kwnames)\n {\n \n     PyObject *fp, *xp, *x;\n@@ -500,12 +506,16 @@ arr_interp(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n     const npy_double *dy, *dx, *dz;\n     npy_double *dres, *slopes = NULL;\n \n-    static char *kwlist[] = {\"x\", \"xp\", \"fp\", \"left\", \"right\", NULL};\n-\n     NPY_BEGIN_THREADS_DEF;\n \n-    if (!PyArg_ParseTupleAndKeywords(args, kwdict, \"OOO|OO:interp\", kwlist,\n-                                     &x, &xp, &fp, &left, &right)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"interp\", args, len_args, kwnames,\n+                \"x\", NULL, &x,\n+                \"xp\", NULL, &xp,\n+                \"fp\", NULL, &fp,\n+                \"|left\", NULL, &left,\n+                \"|right\", NULL, &right,\n+                NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n \n@@ -654,7 +664,8 @@ arr_interp(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n \n /* As for arr_interp but for complex fp values */\n NPY_NO_EXPORT PyObject *\n-arr_interp_complex(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n+arr_interp_complex(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args,\n+                             PyObject *kwnames)\n {\n \n     PyObject *fp, *xp, *x;\n@@ -667,12 +678,16 @@ arr_interp_complex(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwdict)\n     npy_cdouble lval, rval;\n     npy_cdouble *dres, *slopes = NULL;\n \n-    static char *kwlist[] = {\"x\", \"xp\", \"fp\", \"left\", \"right\", NULL};\n-\n     NPY_BEGIN_THREADS_DEF;\n \n-    if (!PyArg_ParseTupleAndKeywords(args, kwdict, \"OOO|OO:interp_complex\",\n-                                     kwlist, &x, &xp, &fp, &left, &right)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"interp_complex\", args, len_args, kwnames,\n+                \"x\", NULL, &x,\n+                \"xp\", NULL, &xp,\n+                \"fp\", NULL, &fp,\n+                \"|left\", NULL, &left,\n+                \"|right\", NULL, &right,\n+                NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n \n@@ -1389,7 +1404,7 @@ arr_unravel_index(PyObject *self, PyObject *args, PyObject *kwds)\n \n /* Can only be called if doc is currently NULL */\n NPY_NO_EXPORT PyObject *\n-arr_add_docstring(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+arr_add_docstring(PyObject *NPY_UNUSED(dummy), PyObject *const *args, Py_ssize_t len_args)\n {\n     PyObject *obj;\n     PyObject *str;\n@@ -1401,7 +1416,16 @@ arr_add_docstring(PyObject *NPY_UNUSED(dummy), PyObject *args)\n         Py_RETURN_NONE;\n     }\n \n-    if (!PyArg_ParseTuple(args, \"OO!:add_docstring\", &obj, &PyUnicode_Type, &str)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"add_docstring\", args, len_args, NULL,\n+            \"\", NULL, &obj,\n+            \"\", NULL, &str,\n+            NULL, NULL, NULL) < 0) {\n+        return NULL;\n+    }\n+    if (!PyUnicode_Check(str)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"argument docstring of add_docstring should be a str\");\n         return NULL;\n     }\n "
            },
            {
                "filename": "numpy/core/src/multiarray/compiled_base.h",
                "patch": "@@ -4,21 +4,21 @@\n #include \"numpy/ndarraytypes.h\"\n \n NPY_NO_EXPORT PyObject *\n-arr_insert(PyObject *, PyObject *, PyObject *);\n+arr_place(PyObject *, PyObject *, PyObject *);\n NPY_NO_EXPORT PyObject *\n-arr_bincount(PyObject *, PyObject *, PyObject *);\n+arr_bincount(PyObject *, PyObject *const *, Py_ssize_t, PyObject *);\n NPY_NO_EXPORT PyObject *\n arr__monotonicity(PyObject *, PyObject *, PyObject *kwds);\n NPY_NO_EXPORT PyObject *\n-arr_interp(PyObject *, PyObject *, PyObject *);\n+arr_interp(PyObject *, PyObject *const *, Py_ssize_t, PyObject *, PyObject *);\n NPY_NO_EXPORT PyObject *\n-arr_interp_complex(PyObject *, PyObject *, PyObject *);\n+arr_interp_complex(PyObject *, PyObject *const *, Py_ssize_t, PyObject *, PyObject *);\n NPY_NO_EXPORT PyObject *\n arr_ravel_multi_index(PyObject *, PyObject *, PyObject *);\n NPY_NO_EXPORT PyObject *\n arr_unravel_index(PyObject *, PyObject *, PyObject *);\n NPY_NO_EXPORT PyObject *\n-arr_add_docstring(PyObject *, PyObject *);\n+arr_add_docstring(PyObject *, PyObject *const *, Py_ssize_t);\n NPY_NO_EXPORT PyObject *\n io_pack(PyObject *, PyObject *, PyObject *);\n NPY_NO_EXPORT PyObject *"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -1492,19 +1492,26 @@ PyArray_Correlate(PyObject *op1, PyObject *op2, int mode)\n     return NULL;\n }\n \n-\n static PyObject *\n-array_putmask(PyObject *NPY_UNUSED(module), PyObject *args, PyObject *kwds)\n+array_putmask(PyObject *NPY_UNUSED(module), PyObject *const *args,\n+                Py_ssize_t len_args, PyObject *kwnames )\n {\n     PyObject *mask, *values;\n     PyObject *array;\n \n-    static char *kwlist[] = {\"arr\", \"mask\", \"values\", NULL};\n-\n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O!OO:putmask\", kwlist,\n-                &PyArray_Type, &array, &mask, &values)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"putmask\", args, len_args, kwnames,\n+            \"\", NULL, &array,\n+            \"mask\", NULL, &mask,\n+            \"values\", NULL, &values,\n+            NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n+    if (!PyArray_Check(array)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"argument a of putmask must be a numpy array\");\n+    }\n+\n     return PyArray_PutMask((PyArrayObject *)array, values, mask);\n }\n \n@@ -2251,12 +2258,15 @@ array_zeros(PyObject *NPY_UNUSED(ignored),\n }\n \n static PyObject *\n-array_count_nonzero(PyObject *NPY_UNUSED(self), PyObject *args, PyObject *kwds)\n+array_count_nonzero(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args)\n {\n     PyArrayObject *array;\n     npy_intp count;\n \n-    if (!PyArg_ParseTuple(args, \"O&:count_nonzero\", PyArray_Converter, &array)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"count_nonzero\", args, len_args, NULL,\n+            \"\", PyArray_Converter, &array,\n+            NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n \n@@ -2512,13 +2522,18 @@ array_concatenate(PyObject *NPY_UNUSED(dummy),\n }\n \n static PyObject *\n-array_innerproduct(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+array_innerproduct(PyObject *NPY_UNUSED(dummy), PyObject *const *args, Py_ssize_t len_args)\n {\n     PyObject *b0, *a0;\n \n-    if (!PyArg_ParseTuple(args, \"OO:innerproduct\", &a0, &b0)) {\n-        return NULL;\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"innerproduct\", args, len_args, NULL,\n+            \"\", NULL, &a0,\n+            \"\", NULL, &b0,\n+            NULL, NULL, NULL) < 0) {\n+    return NULL;\n     }\n+\n     return PyArray_Return((PyArrayObject *)PyArray_InnerProduct(a0, b0));\n }\n \n@@ -2552,7 +2567,7 @@ array_matrixproduct(PyObject *NPY_UNUSED(dummy),\n \n \n static PyObject *\n-array_vdot(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+array_vdot(PyObject *NPY_UNUSED(dummy), PyObject *const *args, Py_ssize_t len_args)\n {\n     int typenum;\n     char *ip1, *ip2, *op;\n@@ -2565,7 +2580,11 @@ array_vdot(PyObject *NPY_UNUSED(dummy), PyObject *args)\n     PyArray_DotFunc *vdot;\n     NPY_BEGIN_THREADS_DEF;\n \n-    if (!PyArg_ParseTuple(args, \"OO:vdot\", &op1, &op2)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"vdot\", args, len_args, NULL,\n+            \"\", NULL, &op1,\n+            \"\", NULL, &op2,\n+            NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n \n@@ -3142,13 +3161,9 @@ PyArray_GetNDArrayCFeatureVersion(void)\n }\n \n static PyObject *\n-array__get_ndarray_c_version(PyObject *NPY_UNUSED(dummy), PyObject *args, PyObject *kwds)\n+array__get_ndarray_c_version(\n+        PyObject *NPY_UNUSED(dummy), PyObject *NPY_UNUSED(arg))\n {\n-    static char *kwlist[] = {NULL};\n-\n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"\", kwlist )) {\n-        return NULL;\n-    }\n     return PyLong_FromLong( (long) PyArray_GetNDArrayCVersion() );\n }\n \n@@ -3443,24 +3458,34 @@ PyArray_Where(PyObject *condition, PyObject *x, PyObject *y)\n #undef INNER_WHERE_LOOP\n \n static PyObject *\n-array_where(PyObject *NPY_UNUSED(ignored), PyObject *args)\n+array_where(PyObject *NPY_UNUSED(ignored), PyObject *const *args, Py_ssize_t len_args)\n {\n     PyObject *obj = NULL, *x = NULL, *y = NULL;\n \n-    if (!PyArg_ParseTuple(args, \"O|OO:where\", &obj, &x, &y)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"where\", args, len_args, NULL,\n+            \"\", NULL, &obj,\n+            \"|x\", NULL, &x,\n+            \"|y\", NULL, &y,\n+            NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n+\n     return PyArray_Where(obj, x, y);\n }\n \n static PyObject *\n-array_lexsort(PyObject *NPY_UNUSED(ignored), PyObject *args, PyObject *kwds)\n+array_lexsort(PyObject *NPY_UNUSED(ignored), PyObject *const *args, Py_ssize_t len_args,\n+                             PyObject *kwnames)\n {\n     int axis = -1;\n     PyObject *obj;\n-    static char *kwlist[] = {\"keys\", \"axis\", NULL};\n \n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O|i:lexsort\", kwlist, &obj, &axis)) {\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"lexsort\", args, len_args, kwnames,\n+            \"keys\", NULL, &obj,\n+            \"|axis\", PyArray_PythonPyIntFromInt, &axis,\n+            NULL, NULL, NULL) < 0) {\n         return NULL;\n     }\n     return PyArray_Return((PyArrayObject *)PyArray_LexSort(obj, axis));\n@@ -3525,13 +3550,17 @@ array_can_cast_safely(PyObject *NPY_UNUSED(self),\n }\n \n static PyObject *\n-array_promote_types(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+array_promote_types(PyObject *NPY_UNUSED(dummy), PyObject *const *args, Py_ssize_t len_args)\n {\n     PyArray_Descr *d1 = NULL;\n     PyArray_Descr *d2 = NULL;\n     PyObject *ret = NULL;\n-    if (!PyArg_ParseTuple(args, \"O&O&:promote_types\",\n-                PyArray_DescrConverter2, &d1, PyArray_DescrConverter2, &d2)) {\n+\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"promote_types\", args, len_args, NULL,\n+            \"\", PyArray_DescrConverter2, &d1,\n+            \"\", PyArray_DescrConverter2, &d2,\n+            NULL, NULL, NULL) < 0) {\n         goto finish;\n     }\n \n@@ -3571,14 +3600,13 @@ array_min_scalar_type(PyObject *NPY_UNUSED(dummy), PyObject *args)\n }\n \n static PyObject *\n-array_result_type(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+array_result_type(PyObject *NPY_UNUSED(dummy), PyObject *const *args, Py_ssize_t len)\n {\n-    npy_intp i, len, narr = 0, ndtypes = 0;\n+    npy_intp i, narr = 0, ndtypes = 0;\n     PyArrayObject **arr = NULL;\n     PyArray_Descr **dtypes = NULL;\n     PyObject *ret = NULL;\n \n-    len = PyTuple_GET_SIZE(args);\n     if (len == 0) {\n         PyErr_SetString(PyExc_ValueError,\n                         \"at least one array or dtype is required\");\n@@ -3592,7 +3620,7 @@ array_result_type(PyObject *NPY_UNUSED(dummy), PyObject *args)\n     dtypes = (PyArray_Descr**)&arr[len];\n \n     for (i = 0; i < len; ++i) {\n-        PyObject *obj = PyTuple_GET_ITEM(args, i);\n+        PyObject *obj = args[i];\n         if (PyArray_Check(obj)) {\n             Py_INCREF(obj);\n             arr[narr] = (PyArrayObject *)obj;\n@@ -4394,7 +4422,7 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_VARARGS, NULL},\n     {\"_get_ndarray_c_version\",\n         (PyCFunction)array__get_ndarray_c_version,\n-        METH_VARARGS|METH_KEYWORDS, NULL},\n+        METH_NOARGS, NULL},\n     {\"_reconstruct\",\n         (PyCFunction)array__reconstruct,\n         METH_VARARGS, NULL},\n@@ -4439,7 +4467,7 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"count_nonzero\",\n         (PyCFunction)array_count_nonzero,\n-        METH_VARARGS|METH_KEYWORDS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"empty\",\n         (PyCFunction)array_empty,\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n@@ -4451,13 +4479,13 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_VARARGS|METH_KEYWORDS, NULL},\n     {\"where\",\n         (PyCFunction)array_where,\n-        METH_VARARGS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"lexsort\",\n         (PyCFunction)array_lexsort,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"putmask\",\n         (PyCFunction)array_putmask,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"fromstring\",\n         (PyCFunction)array_fromstring,\n         METH_VARARGS|METH_KEYWORDS, NULL},\n@@ -4469,13 +4497,13 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_FASTCALL|METH_KEYWORDS, NULL},\n     {\"inner\",\n         (PyCFunction)array_innerproduct,\n-        METH_VARARGS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"dot\",\n         (PyCFunction)array_matrixproduct,\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"vdot\",\n         (PyCFunction)array_vdot,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"c_einsum\",\n         (PyCFunction)array_einsum,\n         METH_VARARGS|METH_KEYWORDS, NULL},\n@@ -4499,13 +4527,13 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"promote_types\",\n         (PyCFunction)array_promote_types,\n-        METH_VARARGS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"min_scalar_type\",\n         (PyCFunction)array_min_scalar_type,\n         METH_VARARGS, NULL},\n     {\"result_type\",\n         (PyCFunction)array_result_type,\n-        METH_VARARGS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"shares_memory\",\n         (PyCFunction)array_shares_memory,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n@@ -4544,24 +4572,24 @@ static struct PyMethodDef array_module_methods[] = {\n     {\"_vec_string\",\n         (PyCFunction)_vec_string,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n-    {\"_insert\", (PyCFunction)arr_insert,\n+    {\"_place\", (PyCFunction)arr_place,\n         METH_VARARGS | METH_KEYWORDS,\n         \"Insert vals sequentially into equivalent 1-d positions \"\n         \"indicated by mask.\"},\n     {\"bincount\", (PyCFunction)arr_bincount,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"_monotonicity\", (PyCFunction)arr__monotonicity,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"interp\", (PyCFunction)arr_interp,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"interp_complex\", (PyCFunction)arr_interp_complex,\n-        METH_VARARGS | METH_KEYWORDS, NULL},\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     {\"ravel_multi_index\", (PyCFunction)arr_ravel_multi_index,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"unravel_index\", (PyCFunction)arr_unravel_index,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"add_docstring\", (PyCFunction)arr_add_docstring,\n-        METH_VARARGS, NULL},\n+        METH_FASTCALL, NULL},\n     {\"packbits\", (PyCFunction)io_pack,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"unpackbits\", (PyCFunction)io_unpack,\n@@ -4584,10 +4612,10 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"seterrobj\",\n         (PyCFunction) ufunc_seterr,\n-        METH_VARARGS, NULL},\n+        METH_O, NULL},\n     {\"geterrobj\",\n         (PyCFunction) ufunc_geterr,\n-        METH_VARARGS, NULL},\n+        METH_NOARGS, NULL},\n     {\"get_handler_name\",\n         (PyCFunction) get_handler_name,\n         METH_VARARGS, NULL},"
            },
            {
                "filename": "numpy/core/src/umath/ufunc_object.c",
                "patch": "@@ -5079,14 +5079,11 @@ ufunc_generic_vectorcall(PyObject *ufunc,\n \n \n NPY_NO_EXPORT PyObject *\n-ufunc_geterr(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+ufunc_geterr(PyObject *NPY_UNUSED(dummy), PyObject *NPY_UNUSED(arg))\n {\n     PyObject *thedict;\n     PyObject *res;\n-\n-    if (!PyArg_ParseTuple(args, \"\")) {\n-        return NULL;\n-    }\n+    \n     thedict = PyThreadState_GetDict();\n     if (thedict == NULL) {\n         thedict = PyEval_GetBuiltins();\n@@ -5112,16 +5109,13 @@ ufunc_geterr(PyObject *NPY_UNUSED(dummy), PyObject *args)\n \n \n NPY_NO_EXPORT PyObject *\n-ufunc_seterr(PyObject *NPY_UNUSED(dummy), PyObject *args)\n+ufunc_seterr(PyObject *NPY_UNUSED(dummy), PyObject *arg)\n {\n     PyObject *thedict;\n     int res;\n-    PyObject *val;\n+    PyObject *val = arg;\n     static char *msg = \"Error object must be a list of length 3\";\n \n-    if (!PyArg_ParseTuple(args, \"O:seterrobj\", &val)) {\n-        return NULL;\n-    }\n     if (!PyList_CheckExact(val) || PyList_GET_SIZE(val) != 3) {\n         PyErr_SetString(PyExc_ValueError, msg);\n         return NULL;"
            },
            {
                "filename": "numpy/core/src/umath/ufunc_object.h",
                "patch": "@@ -4,10 +4,10 @@\n #include <numpy/ufuncobject.h>\n \n NPY_NO_EXPORT PyObject *\n-ufunc_geterr(PyObject *NPY_UNUSED(dummy), PyObject *args);\n+ufunc_geterr(PyObject *NPY_UNUSED(dummy), PyObject *NPY_UNUSED(arg));\n \n NPY_NO_EXPORT PyObject *\n-ufunc_seterr(PyObject *NPY_UNUSED(dummy), PyObject *args);\n+ufunc_seterr(PyObject *NPY_UNUSED(dummy), PyObject *arg);\n \n NPY_NO_EXPORT const char*\n ufunc_get_name_cstr(PyUFuncObject *ufunc);"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -33,6 +33,7 @@\n from numpy.testing._private.utils import requires_memory, _no_tracing\n from numpy.core.tests._locales import CommaDecimalPointLocale\n from numpy.lib.recfunctions import repack_fields\n+from numpy.core.multiarray import _get_ndarray_c_version\n \n # Need to test an object that does not fully implement math interface\n from datetime import timedelta, datetime\n@@ -5084,6 +5085,22 @@ def test_writeable(self):\n         with pytest.raises(ValueError):\n             np.putmask(a, a >= 2, 3)\n \n+    def test_kwargs(self):\n+        x = np.array([0, 0])\n+        np.putmask(x, [0, 1], [-1, -2])\n+        assert_array_equal(x, [0, -2])\n+\n+        x = np.array([0, 0])\n+        np.putmask(x, mask=[0, 1], values=[-1, -2])\n+        assert_array_equal(x, [0, -2])\n+\n+        x = np.array([0, 0])\n+        np.putmask(x, values=[-1, -2],  mask=[0, 1])\n+        assert_array_equal(x, [0, -2])\n+\n+        with pytest.raises(TypeError):\n+            np.putmask(a=x, values=[-1, -2],  mask=[0, 1])\n+\n \n class TestTake:\n     def tst_basic(self, x):\n@@ -8870,6 +8887,11 @@ def test_largedim(self):\n             result = array.nonzero()\n             assert_array_equal(benchmark, result)\n \n+    def test_kwargs(self):\n+        a = np.zeros(1)\n+        with assert_raises(TypeError):\n+            np.where(a, x=a, y=a)\n+\n \n if not IS_PYPY:\n     # sys.getsizeof() is not valid on PyPy\n@@ -9882,3 +9904,6 @@ def test_sort_uint():\n     arr = rng.integers(low=0, high=maxv, size=N).astype('uint32')\n     arr[np.random.choice(arr.shape[0], 10)] = maxv\n     assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))\n+\n+def test_private_get_ndarray_c_version():\n+    assert isinstance(_get_ndarray_c_version(), int)"
            },
            {
                "filename": "numpy/lib/function_base.py",
                "patch": "@@ -24,7 +24,7 @@\n from numpy.core.function_base import add_newdoc\n from numpy.lib.twodim_base import diag\n from numpy.core.multiarray import (\n-    _insert, add_docstring, bincount, normalize_axis_index, _monotonicity,\n+    _place, add_docstring, bincount, normalize_axis_index, _monotonicity,\n     interp as compiled_interp, interp_complex as compiled_interp_complex\n     )\n from numpy.core.umath import _add_newdoc_ufunc as add_newdoc_ufunc\n@@ -1949,11 +1949,7 @@ def place(arr, mask, vals):\n            [44, 55, 44]])\n \n     \"\"\"\n-    if not isinstance(arr, np.ndarray):\n-        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n-                        \"not {name}\".format(name=type(arr).__name__))\n-\n-    return _insert(arr, mask, vals)\n+    return _place(arr, mask, vals)\n \n \n def disp(mesg, device=None, linefeed=True):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20975,
        "body": "Continuation of PR #20960 \r\n\r\nUsing the NonNull class in the API interface emits a gcc-only compiler directive\r\n```\r\n__attribute__((nonnull(n))\r\n```\r\n\r\nwhich will emit a compile-time warning when NULL is passed in. This is problematic since\r\n- people ignore compile-time warnings\r\n- it does not protect runtime use of NULL by non-gcc and non-compiler calls, for instance ctypes or other foreign-function interfaces\r\n\r\nThis PR replaces the use of NonNull with actual parameter sanitation in the functions. I don't think the performance hit by internal calls will be measurable, but if so we could replace internal use with helper functions that avoid the checks. For instance, we could avoid calling `PyArray_NewFromDescr`` with `PyArray_NewFromDescrAndBase`.",
        "changed_files": [
            {
                "filename": "numpy/core/code_generators/genapi.py",
                "patch": "@@ -92,17 +92,6 @@ def __str__(self):\n             return 'NPY_STEALS_REF_TO_ARG(%d)' % self.arg\n \n \n-class NonNull:\n-    def __init__(self, arg):\n-        self.arg = arg # counting from 1\n-\n-    def __str__(self):\n-        try:\n-            return ' '.join('NPY_GCC_NONNULL(%d)' % x for x in self.arg)\n-        except TypeError:\n-            return 'NPY_GCC_NONNULL(%d)' % self.arg\n-\n-\n class Function:\n     def __init__(self, name, return_type, args, doc=''):\n         self.name = name"
            },
            {
                "filename": "numpy/core/code_generators/numpy_api.py",
                "patch": "@@ -13,7 +13,7 @@\n exception, so it should hopefully not get unnoticed).\n \n \"\"\"\n-from code_generators.genapi import StealRef, NonNull\n+from code_generators.genapi import StealRef\n \n # index, type\n multiarray_global_vars = {\n@@ -92,7 +92,7 @@\n     'PyArray_TypeObjectFromType':           (46,),\n     'PyArray_Zero':                         (47,),\n     'PyArray_One':                          (48,),\n-    'PyArray_CastToType':                   (49, StealRef(2), NonNull(2)),\n+    'PyArray_CastToType':                   (49, StealRef(2)),\n     'PyArray_CastTo':                       (50,),\n     'PyArray_CastAnyTo':                    (51,),\n     'PyArray_CanCastSafely':                (52,),\n@@ -120,24 +120,24 @@\n     'PyArray_FromBuffer':                   (74,),\n     'PyArray_FromIter':                     (75, StealRef(2)),\n     'PyArray_Return':                       (76, StealRef(1)),\n-    'PyArray_GetField':                     (77, StealRef(2), NonNull(2)),\n-    'PyArray_SetField':                     (78, StealRef(2), NonNull(2)),\n+    'PyArray_GetField':                     (77, StealRef(2)),\n+    'PyArray_SetField':                     (78, StealRef(2)),\n     'PyArray_Byteswap':                     (79,),\n     'PyArray_Resize':                       (80,),\n     'PyArray_MoveInto':                     (81,),\n     'PyArray_CopyInto':                     (82,),\n     'PyArray_CopyAnyInto':                  (83,),\n     'PyArray_CopyObject':                   (84,),\n-    'PyArray_NewCopy':                      (85, NonNull(1)),\n+    'PyArray_NewCopy':                      (85,),\n     'PyArray_ToList':                       (86,),\n     'PyArray_ToString':                     (87,),\n     'PyArray_ToFile':                       (88,),\n     'PyArray_Dump':                         (89,),\n     'PyArray_Dumps':                        (90,),\n     'PyArray_ValidType':                    (91,),\n     'PyArray_UpdateFlags':                  (92,),\n-    'PyArray_New':                          (93, NonNull(1)),\n-    'PyArray_NewFromDescr':                 (94, StealRef(2), NonNull([1, 2])),\n+    'PyArray_New':                          (93,),\n+    'PyArray_NewFromDescr':                 (94, StealRef(2)),\n     'PyArray_DescrNew':                     (95,),\n     'PyArray_DescrNewFromType':             (96,),\n     'PyArray_GetPriority':                  (97,),\n@@ -318,7 +318,7 @@\n     'PyArray_CanCastArrayTo':               (274,),\n     'PyArray_CanCastTypeTo':                (275,),\n     'PyArray_EinsteinSum':                  (276,),\n-    'PyArray_NewLikeArray':                 (277, StealRef(3), NonNull(1)),\n+    'PyArray_NewLikeArray':                 (277, StealRef(3)),\n     'PyArray_GetArrayParamsFromObject':     (278,),\n     'PyArray_ConvertClipmodeSequence':      (279,),\n     'PyArray_MatrixProduct2':               (280,),\n@@ -344,7 +344,7 @@\n     'PyDataMem_NEW_ZEROED':                 (299,),\n     # End 1.8 API\n     # End 1.9 API\n-    'PyArray_CheckAnyScalarExact':          (300, NonNull(1)),\n+    'PyArray_CheckAnyScalarExact':          (300,),\n     # End 1.10 API\n     'PyArray_MapIterArrayCopyIfOverlap':    (301,),\n     # End 1.13 API"
            },
            {
                "filename": "numpy/core/src/multiarray/convert.c",
                "patch": "@@ -544,6 +544,12 @@ PyArray_NewCopy(PyArrayObject *obj, NPY_ORDER order)\n {\n     PyArrayObject *ret;\n \n+    if (obj == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"obj is NULL in PyArray_NewCopy\");\n+        return NULL;\n+    }\n+\n     ret = (PyArrayObject *)PyArray_NewLikeArray(obj, order, NULL, 1);\n     if (ret == NULL) {\n         return NULL;"
            },
            {
                "filename": "numpy/core/src/multiarray/convert_datatype.c",
                "patch": "@@ -245,6 +245,12 @@ PyArray_CastToType(PyArrayObject *arr, PyArray_Descr *dtype, int is_f_order)\n {\n     PyObject *out;\n \n+    if (dtype == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"dtype is NULL in PyArray_CastToType\");\n+        return NULL;\n+    }\n+\n     Py_SETREF(dtype, PyArray_AdaptDescriptorToArray(arr, (PyObject *)dtype));\n     if (dtype == NULL) {\n         return NULL;"
            },
            {
                "filename": "numpy/core/src/multiarray/ctors.c",
                "patch": "@@ -958,6 +958,18 @@ PyArray_NewFromDescr(\n         int nd, npy_intp const *dims, npy_intp const *strides, void *data,\n         int flags, PyObject *obj)\n {\n+    if (subtype == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"subtype is NULL in PyArray_NewFromDescr\");\n+        return NULL;\n+    }\n+\n+    if (descr == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"descr is NULL in PyArray_NewFromDescr\");\n+        return NULL;\n+    }\n+\n     return PyArray_NewFromDescrAndBase(\n             subtype, descr,\n             nd, dims, strides, data,\n@@ -1110,6 +1122,11 @@ NPY_NO_EXPORT PyObject *\n PyArray_NewLikeArray(PyArrayObject *prototype, NPY_ORDER order,\n                      PyArray_Descr *dtype, int subok)\n {\n+    if (prototype == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"prototype is NULL in PyArray_NewLikeArray\");\n+        return NULL;\n+    }\n     return PyArray_NewLikeArrayWithShape(prototype, order, dtype, -1, NULL, subok);\n }\n \n@@ -1125,6 +1142,12 @@ PyArray_New(\n     PyArray_Descr *descr;\n     PyObject *new;\n \n+    if (subtype == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"subtype is NULL in PyArray_New\");\n+        return NULL;\n+    }\n+\n     descr = PyArray_DescrFromType(type_num);\n     if (descr == NULL) {\n         return NULL;"
            },
            {
                "filename": "numpy/core/src/multiarray/methods.c",
                "patch": "@@ -379,6 +379,18 @@ PyArray_GetField(PyArrayObject *self, PyArray_Descr *typed, int offset)\n     static PyObject *checkfunc = NULL;\n     int self_elsize, typed_elsize;\n \n+    if (self == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"self is NULL in PyArray_GetField\");\n+        return NULL;\n+    }\n+\n+    if (typed == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"typed is NULL in PyArray_GetField\");\n+        return NULL;\n+    }\n+\n     /* check that we are not reinterpreting memory containing Objects. */\n     if (_may_have_objects(PyArray_DESCR(self)) || _may_have_objects(typed)) {\n         npy_cache_import(\"numpy.core._internal\", \"_getfield_is_safe\",\n@@ -457,6 +469,18 @@ PyArray_SetField(PyArrayObject *self, PyArray_Descr *dtype,\n     PyObject *ret = NULL;\n     int retval = 0;\n \n+    if (self == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"self is NULL in PyArray_SetField\");\n+        return -1;\n+    }\n+\n+    if (dtype == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"dtype is NULL in PyArray_SetField\");\n+        return -1;\n+    }\n+\n     if (PyArray_FailUnlessWriteable(self, \"assignment destination\") < 0) {\n         Py_DECREF(dtype);\n         return -1;"
            },
            {
                "filename": "numpy/core/src/multiarray/scalarapi.c",
                "patch": "@@ -182,11 +182,17 @@ scalar_value(PyObject *scalar, PyArray_Descr *descr)\n }\n \n /*NUMPY_API\n- * return true an object is exactly a numpy scalar\n+ * return 1 if an object is exactly a numpy scalar\n  */\n NPY_NO_EXPORT int\n PyArray_CheckAnyScalarExact(PyObject * obj)\n {\n+    if (obj == NULL) {\n+        PyErr_SetString(PyExc_ValueError,\n+            \"obj is NULL in PyArray_CheckAnyScalarExact\");\n+        return 0;\n+    }\n+\n     return is_anyscalar_exact(obj);\n }\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22167,
        "body": "NumPy has SIMD versions of BOOL `logical_and`, `logical_or`, `logical_not`, and `absolute` for SSE2.  The changes here replace that implementation with one that uses universal intrinsics.  This allows other architectures to have SIMD versions of the functions too.\r\n\r\nBOOL `logical_and` and `logical_or` are particularly important for NumPy as that's how  `np.any()` / `np.all()` are implemented.\r\n\r\n\r\nApple M1: **up to 16.5x faster**\r\n```\r\n       before           after         ratio\r\n     [7c143834]       [c49d9dc2]\r\n     <main>           <logical/dev>\r\n-        3.47\u00b10\u03bcs      1.82\u00b10.01\u03bcs     0.52  bench_reduce.AnyAll.time_all_slow\r\n-     4.05\u00b10.01\u03bcs      1.83\u00b10.06\u03bcs     0.45  bench_reduce.AnyAll.time_any_slow\r\n-        6.55\u00b10\u03bcs          532\u00b12ns     0.08  bench_ufunc.Custom.time_not_bool\r\n-        10.2\u00b10\u03bcs          680\u00b17ns     0.07  bench_ufunc.Custom.time_or_bool\r\n-     11.0\u00b10.07\u03bcs          665\u00b13ns     0.06  bench_ufunc.Custom.time_and_bool\r\n                                                                                                                                                                    \r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\n\r\nApple M1 (Rosetta): **up to 1.6x faster**\r\n```\r\n       before           after         ratio\r\n     [7c143834]       [1ad2f2f2]\r\n     <main>           <logical/dev>\r\n-     1.14\u00b10.01\u03bcs         1.03\u00b10\u03bcs     0.90  bench_ufunc.Custom.time_not_bool\r\n-     4.38\u00b10.03\u03bcs      3.06\u00b10.02\u03bcs     0.70  bench_reduce.AnyAll.time_any_slow\r\n-     5.20\u00b10.01\u03bcs      3.17\u00b10.01\u03bcs     0.61  bench_reduce.AnyAll.time_all_slow\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\n\r\niMac Pro (AVX512): **up to 1.2x faster**\r\n```\r\n       before           after         ratio\r\n     [da6297b9]       [c49d9dc2]\r\n     <main>           <logical/dev>\r\n-     1.42\u00b10.02\u03bcs      1.24\u00b10.03\u03bcs     0.87  bench_ufunc.Custom.time_and_bool\r\n-     4.16\u00b10.03\u03bcs       3.60\u00b10.1\u03bcs     0.86  bench_reduce.AnyAll.time_any_slow\r\n-     1.21\u00b10.03\u03bcs      1.03\u00b10.02\u03bcs     0.86  bench_ufunc.Custom.time_not_bool\r\n-      4.30\u00b10.1\u03bcs      3.56\u00b10.07\u03bcs     0.83  bench_reduce.AnyAll.time_all_slow\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -220,6 +220,7 @@ numpy/core/src/umath/loops_unary.dispatch.c\n numpy/core/src/umath/loops_unary_fp.dispatch.c\n numpy/core/src/umath/loops_arithm_fp.dispatch.c\n numpy/core/src/umath/loops_arithmetic.dispatch.c\n+numpy/core/src/umath/loops_logical.dispatch.c\n numpy/core/src/umath/loops_minmax.dispatch.c\n numpy/core/src/umath/loops_trigonometric.dispatch.c\n numpy/core/src/umath/loops_exponent_log.dispatch.c"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -246,7 +246,8 @@ def english_upper(s):\n     'P': 'OBJECT',\n }\n \n-noobj = '?bBhHiIlLqQefdgFDGmM'\n+no_obj_bool = 'bBhHiIlLqQefdgFDGmM'\n+noobj = '?' + no_obj_bool\n all = '?bBhHiIlLqQefdgFDGOmM'\n \n O = 'O'\n@@ -280,6 +281,7 @@ def english_upper(s):\n nocmplxP = nocmplx+P\n notimes_or_obj = bints + inexact\n nodatetime_or_obj = bints + inexact\n+no_bool_times_obj = ints + inexact\n \n # Find which code corresponds to int64.\n int64 = ''\n@@ -299,7 +301,9 @@ def english_upper(s):\n     Ufunc(2, 1, Zero,\n           docstrings.get('numpy.core.umath.add'),\n           'PyUFunc_AdditionTypeResolver',\n-          TD(notimes_or_obj, simd=[('avx2', ints)], dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n+          TD(no_bool_times_obj, simd=[('avx2', ints)],\n+                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'mM', 'M'),\n@@ -310,7 +314,8 @@ def english_upper(s):\n     Ufunc(2, 1, None, # Zero is only a unit to the right, not the left\n           docstrings.get('numpy.core.umath.subtract'),\n           'PyUFunc_SubtractionTypeResolver',\n-          TD(ints + inexact, simd=[('avx2', ints)], dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, simd=[('avx2', ints)],\n+                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'MM', 'm'),\n@@ -321,7 +326,10 @@ def english_upper(s):\n     Ufunc(2, 1, One,\n           docstrings.get('numpy.core.umath.multiply'),\n           'PyUFunc_MultiplicationTypeResolver',\n-          TD(notimes_or_obj, simd=[('avx2', ints)], dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD('?', cfunc_alias='logical_and',\n+                  dispatch=[('loops_logical', '?')]),\n+          TD(no_bool_times_obj, simd=[('avx2', ints)],\n+                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n           [TypeDescription('m', FullTypeDescr, 'mq', 'm'),\n            TypeDescription('m', FullTypeDescr, 'qm', 'm'),\n            TypeDescription('m', FullTypeDescr, 'md', 'm'),\n@@ -412,7 +420,8 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.absolute'),\n           'PyUFunc_AbsoluteTypeResolver',\n-          TD(bints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd')]),\n+          TD(bints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd'),\n+                                                 ('loops_logical', '?')]),\n           TD(cmplx, simd=[('avx512f', cmplxvec)], out=('f', 'd', 'g')),\n           TD(O, f='PyNumber_Absolute'),\n           ),\n@@ -496,43 +505,51 @@ def english_upper(s):\n     Ufunc(2, 1, True_,\n           docstrings.get('numpy.core.umath.logical_and'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),\n+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n+                                dispatch=[('loops_logical', '?')]),\n           TD(O, f='npy_ObjectLogicalAnd'),\n           ),\n 'logical_not':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.logical_not'),\n           None,\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),\n+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n+                                dispatch=[('loops_logical', '?')]),\n           TD(O, f='npy_ObjectLogicalNot'),\n           ),\n 'logical_or':\n     Ufunc(2, 1, False_,\n           docstrings.get('numpy.core.umath.logical_or'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),\n+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n+                                dispatch=[('loops_logical', '?')]),\n           TD(O, f='npy_ObjectLogicalOr'),\n           ),\n 'logical_xor':\n     Ufunc(2, 1, False_,\n           docstrings.get('numpy.core.umath.logical_xor'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?'),\n+          TD('?', out='?', cfunc_alias='not_equal',\n+                           dispatch=[('loops_comparison', '?')]),\n+          TD(no_bool_times_obj, out='?'),\n           # TODO: using obj.logical_xor() seems pretty much useless:\n           TD(P, f='logical_xor'),\n           ),\n 'maximum':\n     Ufunc(2, 1, ReorderableNone,\n           docstrings.get('numpy.core.umath.maximum'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(noobj, dispatch=[('loops_minmax', ints+'fdg')]),\n+          TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n+          TD(no_obj_bool, dispatch=[('loops_minmax', ints+'fdg')]),\n           TD(O, f='npy_ObjectMax')\n           ),\n 'minimum':\n     Ufunc(2, 1, ReorderableNone,\n           docstrings.get('numpy.core.umath.minimum'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(noobj, dispatch=[('loops_minmax', ints+'fdg')]),\n+          TD('?', cfunc_alias='logical_and',\n+                  dispatch=[('loops_logical', '?')]),\n+          TD(no_obj_bool, dispatch=[('loops_minmax', ints+'fdg')]),\n           TD(O, f='npy_ObjectMin')\n           ),\n 'clip':\n@@ -546,14 +563,17 @@ def english_upper(s):\n     Ufunc(2, 1, ReorderableNone,\n           docstrings.get('numpy.core.umath.fmax'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(noobj, dispatch=[('loops_minmax', 'fdg')]),\n+          TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n+          TD(no_obj_bool, dispatch=[('loops_minmax', 'fdg')]),\n           TD(O, f='npy_ObjectMax')\n           ),\n 'fmin':\n     Ufunc(2, 1, ReorderableNone,\n           docstrings.get('numpy.core.umath.fmin'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(noobj, dispatch=[('loops_minmax', 'fdg')]),\n+          TD('?', cfunc_alias='logical_and',\n+                  dispatch=[('loops_logical', '?')]),\n+          TD(no_obj_bool, dispatch=[('loops_minmax', 'fdg')]),\n           TD(O, f='npy_ObjectMin')\n           ),\n 'logaddexp':\n@@ -572,28 +592,35 @@ def english_upper(s):\n     Ufunc(2, 1, AllOnes,\n           docstrings.get('numpy.core.umath.bitwise_and'),\n           None,\n-          TD(bints, simd=[('avx2', ints)]),\n+          TD('?', cfunc_alias='logical_and',\n+                  dispatch=[('loops_logical', '?')]),\n+          TD(ints, simd=[('avx2', ints)]),\n           TD(O, f='PyNumber_And'),\n           ),\n 'bitwise_or':\n     Ufunc(2, 1, Zero,\n           docstrings.get('numpy.core.umath.bitwise_or'),\n           None,\n-          TD(bints, simd=[('avx2', ints)]),\n+          TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n+          TD(ints, simd=[('avx2', ints)]),\n           TD(O, f='PyNumber_Or'),\n           ),\n 'bitwise_xor':\n     Ufunc(2, 1, Zero,\n           docstrings.get('numpy.core.umath.bitwise_xor'),\n           None,\n-          TD(bints, simd=[('avx2', ints)]),\n+          TD('?', cfunc_alias='not_equal',\n+                  dispatch=[('loops_comparison', '?')]),\n+          TD(ints, simd=[('avx2', ints)]),\n           TD(O, f='PyNumber_Xor'),\n           ),\n 'invert':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.invert'),\n           None,\n-          TD(bints, simd=[('avx2', ints)]),\n+          TD('?', cfunc_alias='logical_not',\n+                  dispatch=[('loops_logical', '?')]),\n+          TD(ints, simd=[('avx2', ints)]),\n           TD(O, f='PyNumber_Invert'),\n           ),\n 'left_shift':"
            },
            {
                "filename": "numpy/core/meson.build",
                "patch": "@@ -743,6 +743,7 @@ src_umath = [\n   src_file.process('src/umath/loops_comparison.dispatch.c.src'),\n   src_file.process('src/umath/loops_exponent_log.dispatch.c.src'),\n   src_file.process('src/umath/loops_hyperbolic.dispatch.c.src'),\n+  src_file.process('src/umath/loops_logical.dispatch.c.src'),\n   src_file.process('src/umath/loops_minmax.dispatch.c.src'),\n   src_file.process('src/umath/loops_modulo.dispatch.c.src'),\n   src_file.process('src/umath/loops_trigonometric.dispatch.c.src'),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1009,6 +1009,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops_unary_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),\n+            join('src', 'umath', 'loops_logical.dispatch.c.src'),\n             join('src', 'umath', 'loops_minmax.dispatch.c.src'),\n             join('src', 'umath', 'loops_trigonometric.dispatch.c.src'),\n             join('src', 'umath', 'loops_umath_fp.dispatch.c.src'),"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -416,98 +416,6 @@ PyUFunc_On_Om(char **args, npy_intp const *dimensions, npy_intp const *steps, vo\n  *****************************************************************************\n  */\n \n-/**begin repeat\n- * #kind = logical_and, logical_or#\n- * #OP =  &&, ||#\n- * #SC =  ==, !=#\n- * #and = 1, 0#\n- **/\n-\n-NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    if(IS_BINARY_REDUCE) {\n-#ifdef NPY_HAVE_SSE2_INTRINSICS\n-        /*\n-         * stick with our variant for more reliable performance, only known\n-         * platform which outperforms it by ~20% is an i7 with glibc 2.17\n-         */\n-        if (run_reduce_simd_@kind@_BOOL(args, dimensions, steps)) {\n-            return;\n-        }\n-#else\n-        /* for now only use libc on 32-bit/non-x86 */\n-        if (steps[1] == 1) {\n-            npy_bool * op = (npy_bool *)args[0];\n-#if @and@\n-            /* np.all(), search for a zero (false) */\n-            if (*op) {\n-                *op = memchr(args[1], 0, dimensions[0]) == NULL;\n-            }\n-#else\n-            /*\n-             * np.any(), search for a non-zero (true) via comparing against\n-             * zero blocks, memcmp is faster than memchr on SSE4 machines\n-             * with glibc >= 2.12 and memchr can only check for equal 1\n-             */\n-            static const npy_bool zero[4096]; /* zero by C standard */\n-            npy_uintp i, n = dimensions[0];\n-\n-            for (i = 0; !*op && i < n - (n % sizeof(zero)); i += sizeof(zero)) {\n-                *op = memcmp(&args[1][i], zero, sizeof(zero)) != 0;\n-            }\n-            if (!*op && n - i > 0) {\n-                *op = memcmp(&args[1][i], zero, n - i) != 0;\n-            }\n-#endif\n-            return;\n-        }\n-#endif\n-        else {\n-            BINARY_REDUCE_LOOP(npy_bool) {\n-                const npy_bool in2 = *(npy_bool *)ip2;\n-                io1 = io1 @OP@ in2;\n-                if (io1 @SC@ 0) {\n-                    break;\n-                }\n-            }\n-            *((npy_bool *)iop1) = io1;\n-        }\n-    }\n-    else {\n-        if (run_binary_simd_@kind@_BOOL(args, dimensions, steps)) {\n-            return;\n-        }\n-        else {\n-            BINARY_LOOP {\n-                const npy_bool in1 = *(npy_bool *)ip1;\n-                const npy_bool in2 = *(npy_bool *)ip2;\n-                *((npy_bool *)op1) = in1 @OP@ in2;\n-            }\n-        }\n-    }\n-}\n-/**end repeat**/\n-\n-/**begin repeat\n- * #kind = absolute, logical_not#\n- * #OP =  !=, ==#\n- **/\n-NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    if (run_unary_simd_@kind@_BOOL(args, dimensions, steps)) {\n-        return;\n-    }\n-    else {\n-        UNARY_LOOP {\n-            npy_bool in1 = *(npy_bool *)ip1;\n-            *((npy_bool *)op1) = in1 @OP@ 0;\n-        }\n-    }\n-}\n-/**end repeat**/\n-\n NPY_NO_EXPORT void\n BOOL__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -10,24 +10,29 @@\n     #define NPY_NO_EXPORT NPY_VISIBILITY_HIDDEN\n #endif\n \n-#define BOOL_invert BOOL_logical_not\n-#define BOOL_add BOOL_logical_or\n-#define BOOL_bitwise_and BOOL_logical_and\n-#define BOOL_bitwise_or BOOL_logical_or\n-#define BOOL_logical_xor BOOL_not_equal\n-#define BOOL_bitwise_xor BOOL_logical_xor\n-#define BOOL_multiply BOOL_logical_and\n-#define BOOL_maximum BOOL_logical_or\n-#define BOOL_minimum BOOL_logical_and\n-#define BOOL_fmax BOOL_maximum\n-#define BOOL_fmin BOOL_minimum\n-\n /*\n  *****************************************************************************\n  **                             BOOLEAN LOOPS                               **\n  *****************************************************************************\n  */\n \n+/*\n+ * Following functions are defined by umath generator\n+ * to enable runtime dispatching without the need\n+ * to redefine them within dsipatch-able sources.\n+ */\n+// #define BOOL_invert BOOL_logical_not\n+// #define BOOL_add BOOL_logical_or\n+// #define BOOL_bitwise_and BOOL_logical_and\n+// #define BOOL_bitwise_or BOOL_logical_or\n+// #define BOOL_logical_xor BOOL_not_equal\n+// #define BOOL_bitwise_xor BOOL_logical_xor\n+// #define BOOL_multiply BOOL_logical_and\n+// #define BOOL_maximum BOOL_logical_or\n+// #define BOOL_minimum BOOL_logical_and\n+// #define BOOL_fmax BOOL_maximum\n+// #define BOOL_fmin BOOL_minimum\n+\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_comparison.dispatch.h\"\n #endif\n@@ -39,11 +44,15 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void BOOL_@kind@,\n     (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_logical.dispatch.h\"\n+#endif\n+\n /**begin repeat\n- * #kind = logical_and, logical_or, absolute, logical_not#\n- **/\n-NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+ * #kind = logical_and, logical_or, logical_not, absolute#\n+ */\n+ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void BOOL_@kind@,\n+   (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data)))\n /**end repeat**/\n \n NPY_NO_EXPORT void\n@@ -203,7 +212,7 @@ NPY_NO_EXPORT void\n \n /**end repeat**/\n \n- \n+\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_unary.dispatch.h\"\n #endif"
            },
            {
                "filename": "numpy/core/src/umath/loops_logical.dispatch.c.src",
                "patch": "@@ -0,0 +1,353 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** neon asimd\n+ ** sse2 avx2 avx512_skx\n+ ** vsx2\n+ ** vx\n+ **/\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"lowlevel_strided_loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+/*******************************************************************************\n+ ** Defining the SIMD kernels\n+ ******************************************************************************/\n+\n+#if NPY_SIMD\n+/*\n+ * convert any bit set to boolean true so vectorized and normal operations are\n+ * consistent, should not be required if bool is used correctly everywhere but\n+ * you never know\n+ */\n+NPY_FINLINE npyv_u8 byte_to_true(npyv_u8 v)\n+{\n+    const npyv_u8 zero = npyv_zero_u8();\n+    const npyv_u8 truemask = npyv_setall_u8(1 == 1);\n+    // cmpeq(v, 0) turns 0x00 -> 0xff and non-zero -> 0x00\n+    npyv_u8 tmp = npyv_cvt_u8_b8(npyv_cmpeq_u8(v, zero));\n+    // tmp is filled with 0xff/0x00, negate and mask to boolean true\n+    return npyv_andc_u8(truemask, tmp);\n+}\n+/*\n+ * convert mask vector (0xff/0x00) to boolean true.  similar to byte_to_true(),\n+ * but we've already got a mask and can skip negation.\n+ */\n+NPY_FINLINE npyv_u8 mask_to_true(npyv_b8 v)\n+{\n+    const npyv_u8 truemask = npyv_setall_u8(1 == 1);\n+    return npyv_and_u8(truemask, npyv_cvt_u8_b8(v));\n+}\n+\n+\n+/**begin repeat\n+ * #kind = logical_and, logical_or#\n+ * #and  = 1, 0#\n+ * #scalar_op = &&, ||#\n+ * #intrin = and, or#\n+ * #reduce = min, max#\n+ * #scalar_cmp = ==, !=#\n+ * #anyall = all, any#\n+ */\n+static void\n+simd_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2, npy_intp len)\n+{\n+    #define UNROLL 16\n+\n+    const int vstep = npyv_nlanes_u8;\n+    const int wstep = vstep * UNROLL;\n+\n+    // Unrolled vectors loop\n+    for (; len >= wstep; len -= wstep, ip1 += wstep, ip2 += wstep, op += wstep) {\n+        /**begin repeat1\n+         * #unroll = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+         */\n+        #if UNROLL > @unroll@\n+        npyv_u8 a@unroll@ = npyv_load_u8(ip1 + vstep * @unroll@);\n+        npyv_u8 b@unroll@ = npyv_load_u8(ip2 + vstep * @unroll@);\n+        npyv_u8 r@unroll@ = npyv_@intrin@_u8(a@unroll@, b@unroll@);\n+        npyv_store_u8(op + vstep * @unroll@, byte_to_true(r@unroll@));\n+        #endif\n+        /**end repeat1**/\n+    }\n+    #undef UNROLL\n+\n+    // Single vectors loop\n+    for (; len >= vstep; len -= vstep, ip1 += vstep, ip2 += vstep, op += vstep) {\n+        npyv_u8 a = npyv_load_u8(ip1);\n+        npyv_u8 b = npyv_load_u8(ip2);\n+        npyv_u8 r = npyv_@intrin@_u8(a, b);\n+        npyv_store_u8(op, byte_to_true(r));\n+    }\n+\n+    // Scalar loop to finish off\n+    for (; len > 0; len--, ip1++, ip2++, op++) {\n+        *op = *ip1 @scalar_op@ *ip2;\n+    }\n+}\n+\n+static void\n+simd_reduce_@kind@_BOOL(npy_bool * op, npy_bool * ip, npy_intp len)\n+{\n+    #define UNROLL 8\n+\n+    const int vstep = npyv_nlanes_u8;\n+    const int wstep = vstep * UNROLL;\n+\n+    // Unrolled vectors loop\n+    for (; len >= wstep; len -= wstep, ip += wstep) {\n+    #if defined(NPY_HAVE_SSE2)\n+        NPY_PREFETCH(ip + wstep, 0, 3);\n+    #endif\n+        npyv_u8 v0 = npyv_load_u8(ip + vstep * 0);\n+        npyv_u8 v1 = npyv_load_u8(ip + vstep * 1);\n+        npyv_u8 v2 = npyv_load_u8(ip + vstep * 2);\n+        npyv_u8 v3 = npyv_load_u8(ip + vstep * 3);\n+        npyv_u8 v4 = npyv_load_u8(ip + vstep * 4);\n+        npyv_u8 v5 = npyv_load_u8(ip + vstep * 5);\n+        npyv_u8 v6 = npyv_load_u8(ip + vstep * 6);\n+        npyv_u8 v7 = npyv_load_u8(ip + vstep * 7);\n+\n+        npyv_u8 m01 = npyv_@reduce@_u8(v0, v1);\n+        npyv_u8 m23 = npyv_@reduce@_u8(v2, v3);\n+        npyv_u8 m45 = npyv_@reduce@_u8(v4, v5);\n+        npyv_u8 m67 = npyv_@reduce@_u8(v6, v7);\n+\n+        npyv_u8 m0123 = npyv_@reduce@_u8(m01, m23);\n+        npyv_u8 m4567 = npyv_@reduce@_u8(m45, m67);\n+\n+        npyv_u8 mv = npyv_@reduce@_u8(m0123, m4567);\n+\n+        if(npyv_@anyall@_u8(mv) @scalar_cmp@ 0){\n+            *op = !@and@;\n+            return;\n+        }\n+    }\n+\n+    // Single vectors loop\n+    for (; len >= vstep; len -= vstep, ip += vstep) {\n+        npyv_u8 v0 = npyv_load_u8(ip);\n+        if(npyv_@anyall@_u8(v0) @scalar_cmp@ 0){\n+            *op = !@and@;\n+            return;\n+        }\n+    }\n+\n+    // Scalar loop to finish off\n+    for (; len > 0; --len, ++ip) {\n+        *op = *op @scalar_op@ *ip;\n+        if (*op @scalar_cmp@ 0) {\n+            return;\n+        }\n+    }\n+#undef UNROLL\n+}\n+/**end repeat**/ \n+\n+/**begin repeat\n+ * #kind = logical_not, absolute#\n+ * #op = ==, !=#\n+ * #not = 1, 0#\n+ */\n+static void\n+simd_@kind@_BOOL(npy_bool * op, npy_bool * ip, npy_intp len)\n+{\n+    #define UNROLL 16\n+\n+    const int vstep = npyv_nlanes_u8;\n+    const int wstep = vstep * UNROLL;\n+\n+    #if @not@\n+    const npyv_u8 zero = npyv_zero_u8();\n+    #endif\n+\n+    // Unrolled vectors loop\n+    for (; len >= wstep; len -= wstep, ip += wstep, op += wstep) {\n+        /**begin repeat1\n+         * #unroll = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15#\n+         */\n+        #if UNROLL > @unroll@\n+        npyv_u8 v@unroll@ = npyv_load_u8(ip + vstep * @unroll@);\n+#if @not@\n+        npyv_u8 r@unroll@ = mask_to_true(npyv_cmpeq_u8(v@unroll@, zero));\n+#else\n+        npyv_u8 r@unroll@ = byte_to_true(v@unroll@);\n+#endif\n+        npyv_store_u8(op + vstep * @unroll@, r@unroll@);\n+        #endif\n+        /**end repeat1**/\n+    }\n+    #undef UNROLL\n+\n+    // Single vectors loop\n+    for (; len >= vstep; len -= vstep, ip += vstep, op += vstep) {\n+        npyv_u8 v = npyv_load_u8(ip);\n+#if @not@\n+        npyv_u8 r = mask_to_true(npyv_cmpeq_u8(v, zero));\n+#else\n+        npyv_u8 r = byte_to_true(v);\n+#endif\n+        npyv_store_u8(op, r);\n+    }\n+\n+    // Scalar loop to finish off\n+    for (; len > 0; --len, ++ip, ++op) {\n+        *op = (*ip @op@ 0);\n+    }\n+}\n+/**end repeat**/\n+\n+#endif // NPY_SIMD\n+\n+/*******************************************************************************\n+ ** Defining ufunc inner functions\n+ ******************************************************************************/\n+\n+/**begin repeat\n+ * # kind = logical_or, logical_and#\n+ */\n+static NPY_INLINE int\n+run_binary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+{\n+#if NPY_SIMD\n+    if (sizeof(npy_bool) == 1 &&\n+            IS_BLOCKABLE_BINARY(sizeof(npy_bool), NPY_SIMD_WIDTH)) {\n+        simd_binary_@kind@_BOOL((npy_bool*)args[2], (npy_bool*)args[0],\n+                               (npy_bool*)args[1], dimensions[0]);\n+        return 1;\n+    }\n+#endif\n+    return 0;\n+}\n+\n+\n+static NPY_INLINE int\n+run_reduce_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+{\n+#if NPY_SIMD\n+    if (sizeof(npy_bool) == 1 &&\n+            IS_BLOCKABLE_REDUCE(sizeof(npy_bool), NPY_SIMD_WIDTH)) {\n+        simd_reduce_@kind@_BOOL((npy_bool*)args[0], (npy_bool*)args[1],\n+                                dimensions[0]);\n+        return 1;\n+    }\n+#endif\n+    return 0;\n+}\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #kind = logical_not, absolute#\n+ */\n+static NPY_INLINE int\n+run_unary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+{\n+#if NPY_SIMD\n+    if (sizeof(npy_bool) == 1 &&\n+            IS_BLOCKABLE_UNARY(sizeof(npy_bool), NPY_SIMD_WIDTH)) {\n+        simd_@kind@_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);\n+        return 1;\n+    }\n+#endif\n+    return 0;\n+}\n+/**end repeat**/\n+\n+\n+/**begin repeat\n+ * #kind = logical_and, logical_or#\n+ * #OP =  &&, ||#\n+ * #SC =  ==, !=#\n+ * #and = 1, 0#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    if(IS_BINARY_REDUCE) {\n+#if NPY_SIMD\n+        /*\n+         * stick with our variant for more reliable performance, only known\n+         * platform which outperforms it by ~20% is an i7 with glibc 2.17\n+         */\n+        if (run_reduce_simd_@kind@_BOOL(args, dimensions, steps)) {\n+            return;\n+        }\n+#else\n+        /* for now only use libc on 32-bit/non-x86 */\n+        if (steps[1] == 1) {\n+            npy_bool * op = (npy_bool *)args[0];\n+#if @and@\n+            /* np.all(), search for a zero (false) */\n+            if (*op) {\n+                *op = memchr(args[1], 0, dimensions[0]) == NULL;\n+            }\n+#else\n+            /*\n+             * np.any(), search for a non-zero (true) via comparing against\n+             * zero blocks, memcmp is faster than memchr on SSE4 machines\n+             * with glibc >= 2.12 and memchr can only check for equal 1\n+             */\n+            static const npy_bool zero[4096]; /* zero by C standard */\n+            npy_uintp i, n = dimensions[0];\n+\n+            for (i = 0; !*op && i < n - (n % sizeof(zero)); i += sizeof(zero)) {\n+                *op = memcmp(&args[1][i], zero, sizeof(zero)) != 0;\n+            }\n+            if (!*op && n - i > 0) {\n+                *op = memcmp(&args[1][i], zero, n - i) != 0;\n+            }\n+#endif\n+            return;\n+        }\n+#endif\n+        else {\n+            BINARY_REDUCE_LOOP(npy_bool) {\n+                const npy_bool in2 = *(npy_bool *)ip2;\n+                io1 = io1 @OP@ in2;\n+                if (io1 @SC@ 0) {\n+                    break;\n+                }\n+            }\n+            *((npy_bool *)iop1) = io1;\n+        }\n+    }\n+    else {\n+        if (run_binary_simd_@kind@_BOOL(args, dimensions, steps)) {\n+            return;\n+        }\n+        else {\n+            BINARY_LOOP {\n+                const npy_bool in1 = *(npy_bool *)ip1;\n+                const npy_bool in2 = *(npy_bool *)ip2;\n+                *((npy_bool *)op1) = in1 @OP@ in2;\n+            }\n+        }\n+    }\n+}\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #kind = logical_not, absolute#\n+ * #OP = ==, !=#\n+ **/\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    if (run_unary_simd_@kind@_BOOL(args, dimensions, steps)) {\n+        return;\n+    }\n+    else {\n+        UNARY_LOOP {\n+            npy_bool in1 = *(npy_bool *)ip1;\n+            *((npy_bool *)op1) = in1 @OP@ 0;\n+        }\n+    }\n+}\n+/**end repeat**/\n+"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -154,80 +154,6 @@ run_@kind@_simd_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *\n /**end repeat1**/\n /**end repeat**/\n \n-/*\n- *****************************************************************************\n- **                           BOOL DISPATCHERS\n- *****************************************************************************\n- */\n-\n-/**begin repeat\n- * # kind = logical_or, logical_and#\n- */\n-\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n-static void\n-sse2_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n-                        npy_intp n);\n-\n-static void\n-sse2_reduce_@kind@_BOOL(npy_bool * op, npy_bool * ip, npy_intp n);\n-#endif\n-\n-static inline int\n-run_binary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n-    if (sizeof(npy_bool) == 1 &&\n-            IS_BLOCKABLE_BINARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_@kind@_BOOL((npy_bool*)args[2], (npy_bool*)args[0],\n-                               (npy_bool*)args[1], dimensions[0]);\n-        return 1;\n-    }\n-#endif\n-    return 0;\n-}\n-\n-\n-static inline int\n-run_reduce_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n-    if (sizeof(npy_bool) == 1 &&\n-            IS_BLOCKABLE_REDUCE(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {\n-        sse2_reduce_@kind@_BOOL((npy_bool*)args[0], (npy_bool*)args[1],\n-                                dimensions[0]);\n-        return 1;\n-    }\n-#endif\n-    return 0;\n-}\n-\n-/**end repeat**/\n-\n-/**begin repeat\n- * # kind = absolute, logical_not#\n- */\n-\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n-static void\n-sse2_@kind@_BOOL(npy_bool *, npy_bool *, const npy_intp n);\n-#endif\n-\n-static inline int\n-run_unary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n-    if (sizeof(npy_bool) == 1 &&\n-            IS_BLOCKABLE_UNARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {\n-        sse2_@kind@_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);\n-        return 1;\n-    }\n-#endif\n-    return 0;\n-}\n-\n-/**end repeat**/\n-\n #ifdef NPY_HAVE_SSE2_INTRINSICS\n \n /*\n@@ -1005,143 +931,6 @@ AVX512F_absolute_@TYPE@(@type@ * op,\n #endif\n /**end repeat**/\n \n-/*\n- *****************************************************************************\n- **                           BOOL LOOPS\n- *****************************************************************************\n- */\n-\n-/**begin repeat\n- * # kind = logical_or, logical_and#\n- * # and = 0, 1#\n- * # op = ||, &&#\n- * # sc = !=, ==#\n- * # vpre = _mm*2#\n- * # vsuf = si128*2#\n- * # vtype = __m128i*2#\n- * # type = npy_bool*2#\n- * # vload = _mm_load_si128*2#\n- * # vloadu = _mm_loadu_si128*2#\n- * # vstore = _mm_store_si128*2#\n- */\n-\n-/*\n- * convert any bit set to boolean true so vectorized and normal operations are\n- * consistent, should not be required if bool is used correctly everywhere but\n- * you never know\n- */\n-#if !@and@\n-NPY_FINLINE @vtype@ byte_to_true(@vtype@ v)\n-{\n-    const @vtype@ zero = @vpre@_setzero_@vsuf@();\n-    const @vtype@ truemask = @vpre@_set1_epi8(1 == 1);\n-    /* get 0xFF for zeros */\n-    @vtype@ tmp = @vpre@_cmpeq_epi8(v, zero);\n-    /* filled with 0xFF/0x00, negate and mask to boolean true */\n-    return @vpre@_andnot_@vsuf@(tmp, truemask);\n-}\n-#endif\n-\n-static void\n-sse2_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2, npy_intp n)\n-{\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES)\n-        op[i] = ip1[i] @op@ ip2[i];\n-    LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n-        @vtype@ a = @vloadu@((@vtype@*)&ip1[i]);\n-        @vtype@ b = @vloadu@((@vtype@*)&ip2[i]);\n-#if @and@\n-        const @vtype@ zero = @vpre@_setzero_@vsuf@();\n-        /* get 0xFF for non zeros*/\n-        @vtype@ tmp = @vpre@_cmpeq_epi8(a, zero);\n-        /* andnot -> 0x00 for zeros xFF for non zeros, & with ip2 */\n-        tmp = @vpre@_andnot_@vsuf@(tmp, b);\n-#else\n-        @vtype@ tmp = @vpre@_or_@vsuf@(a, b);\n-#endif\n-\n-        @vstore@((@vtype@*)&op[i], byte_to_true(tmp));\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = (ip1[i] @op@ ip2[i]);\n-    }\n-}\n-\n-\n-static void\n-sse2_reduce_@kind@_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)\n-{\n-    const @vtype@ zero = @vpre@_setzero_@vsuf@();\n-    LOOP_BLOCK_ALIGN_VAR(ip, npy_bool, VECTOR_SIZE_BYTES) {\n-        *op = *op @op@ ip[i];\n-        if (*op @sc@ 0) {\n-            return;\n-        }\n-    }\n-    /* unrolled once to replace a slow movmsk with a fast pmaxb */\n-    LOOP_BLOCKED(npy_bool, 2 * VECTOR_SIZE_BYTES) {\n-        @vtype@ v = @vload@((@vtype@*)&ip[i]);\n-        @vtype@ v2 = @vload@((@vtype@*)&ip[i + VECTOR_SIZE_BYTES]);\n-        v = @vpre@_cmpeq_epi8(v, zero);\n-        v2 = @vpre@_cmpeq_epi8(v2, zero);\n-#if @and@\n-        if ((@vpre@_movemask_epi8(@vpre@_max_epu8(v, v2)) != 0)) {\n-            *op = 0;\n-#else\n-        if ((@vpre@_movemask_epi8(@vpre@_min_epu8(v, v2)) != 0xFFFF)) {\n-            *op = 1;\n-#endif\n-            return;\n-        }\n-    }\n-    LOOP_BLOCKED_END {\n-        *op = *op @op@ ip[i];\n-        if (*op @sc@ 0) {\n-            return;\n-        }\n-    }\n-}\n-\n-/**end repeat**/\n-\n-/**begin repeat\n- * # kind = absolute, logical_not#\n- * # op = !=, ==#\n- * # not = 0, 1#\n- * # vpre = _mm*2#\n- * # vsuf = si128*2#\n- * # vtype = __m128i*2#\n- * # type = npy_bool*2#\n- * # vloadu = _mm_loadu_si128*2#\n- * # vstore = _mm_store_si128*2#\n- */\n-\n-static void\n-sse2_@kind@_BOOL(@type@ * op, @type@ * ip, const npy_intp n)\n-{\n-    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES)\n-        op[i] = (ip[i] @op@ 0);\n-    LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n-        @vtype@ a = @vloadu@((@vtype@*)&ip[i]);\n-#if @not@\n-        const @vtype@ zero = @vpre@_setzero_@vsuf@();\n-        const @vtype@ truemask = @vpre@_set1_epi8(1 == 1);\n-        /* equivalent to byte_to_true but can skip the negation */\n-        a = @vpre@_cmpeq_epi8(a, zero);\n-        a = @vpre@_and_@vsuf@(a, truemask);\n-#else\n-        /* abs is kind of pointless but maybe its used for byte_to_true */\n-        a = byte_to_true(a);\n-#endif\n-        @vstore@((@vtype@*)&op[i], a);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = (ip[i] @op@ 0);\n-    }\n-}\n-\n-/**end repeat**/\n-\n #undef VECTOR_SIZE_BYTES\n #endif  /* NPY_HAVE_SSE2_INTRINSICS */\n #endif"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22168,
        "body": "Apple silicon builds of NumPy have extra functions in them for AVX2/AVX512.  The changes here remove those implementations if we're not building for x86.\r\n\r\nApple silicon:\r\n- original size: 3946035 bytes\r\n- new size: 3657731 bytes\r\n- savings: 288304 bytes (7.31%)\r\n\r\nChanges pass all tests on M1 native, M1 Rosetta, and iMacPro (AVX512). We've verified performance is the same before/after for Rosetta and iMacPro. We've also verified that binaries are exactly the same size and have the same number of symbols in them.",
        "changed_files": [
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -173,6 +173,16 @@ def check_funcs(funcs_name, headers=[\"feature_detection_math.h\"]):\n         else:\n             return 1\n \n+    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n+    # support on Windows-based platforms\n+    def check_gh14787(fn):\n+        if fn == 'attribute_target_avx512f':\n+            if (sys.platform in ('win32', 'cygwin') and\n+                    config.check_compiler_gcc() and\n+                    not config.check_gcc_version_at_least(8, 4)):\n+                ext.extra_compile_args.extend(\n+                        ['-ffixed-xmm%s' % n for n in range(16, 32)])\n+\n     #use_msvc = config.check_decl(\"_MSC_VER\")\n     if not check_funcs_once(MANDATORY_FUNCS, add_to_moredefs=False):\n         raise SystemError(\"One of the required function to build numpy is not\"\n@@ -223,19 +233,19 @@ def check_funcs(funcs_name, headers=[\"feature_detection_math.h\"]):\n     for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES:\n         if config.check_gcc_function_attribute(dec, fn):\n             moredefs.append((fname2def(fn), 1))\n-            if fn == 'attribute_target_avx512f':\n-                # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n-                # support on Windows-based platforms\n-                if (sys.platform in ('win32', 'cygwin') and\n-                        config.check_compiler_gcc() and\n-                        not config.check_gcc_version_at_least(8, 4)):\n-                    ext.extra_compile_args.extend(\n-                            ['-ffixed-xmm%s' % n for n in range(16, 32)])\n-\n-    for dec, fn, code, header in OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS:\n-        if config.check_gcc_function_attribute_with_intrinsics(dec, fn, code,\n-                                                               header):\n-            moredefs.append((fname2def(fn), 1))\n+            check_gh14787(fn)\n+\n+    platform = sysconfig.get_platform()\n+    if (\"x86_64\" in platform):\n+        for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES_AVX:\n+            if config.check_gcc_function_attribute(dec, fn):\n+                moredefs.append((fname2def(fn), 1))\n+                check_gh14787(fn)\n+        for dec, fn, code, header in (\n+        OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX):\n+            if config.check_gcc_function_attribute_with_intrinsics(\n+                    dec, fn, code, header):\n+                moredefs.append((fname2def(fn), 1))\n \n     for fn in OPTIONAL_VARIABLE_ATTRIBUTES:\n         if config.check_gcc_variable_attribute(fn):"
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -204,16 +204,18 @@ def set_sig(sig):\n                                  'attribute_optimize_opt_2'),\n                                 ('__attribute__((nonnull (1)))',\n                                  'attribute_nonnull'),\n-                                ('__attribute__((target (\"avx\")))',\n-                                 'attribute_target_avx'),\n-                                ('__attribute__((target (\"avx2\")))',\n-                                 'attribute_target_avx2'),\n-                                ('__attribute__((target (\"avx512f\")))',\n-                                 'attribute_target_avx512f'),\n-                                ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n-                                 'attribute_target_avx512_skx'),\n                                 ]\n \n+OPTIONAL_FUNCTION_ATTRIBUTES_AVX = [('__attribute__((target (\"avx\")))',\n+    'attribute_target_avx'),\n+    ('__attribute__((target (\"avx2\")))',\n+    'attribute_target_avx2'),\n+    ('__attribute__((target (\"avx512f\")))',\n+    'attribute_target_avx512f'),\n+    ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n+    'attribute_target_avx512_skx'),\n+    ]\n+\n # function attributes with intrinsics\n # To ensure your compiler can compile avx intrinsics with just the attributes\n # gcc 4.8.4 support attributes but not with intrisics\n@@ -222,23 +224,24 @@ def set_sig(sig):\n # The _mm512_castps_si512 instruction is specific check for AVX-512F support\n # in gcc-4.9 which is missing a subset of intrinsics. See\n # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61878\n-OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS = [('__attribute__((target(\"avx2,fma\")))',\n-                                'attribute_target_avx2_with_intrinsics',\n-                                '__m256 temp = _mm256_set1_ps(1.0); temp = \\\n-                                _mm256_fmadd_ps(temp, temp, temp)',\n-                                'immintrin.h'),\n-                                ('__attribute__((target(\"avx512f\")))',\n-                                'attribute_target_avx512f_with_intrinsics',\n-                                '__m512i temp = _mm512_castps_si512(_mm512_set1_ps(1.0))',\n-                                'immintrin.h'),\n-                                ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n-                                'attribute_target_avx512_skx_with_intrinsics',\n-                                '__mmask8 temp = _mm512_fpclass_pd_mask(_mm512_set1_pd(1.0), 0x01);\\\n-                                __m512i unused_temp = \\\n-                                    _mm512_castps_si512(_mm512_set1_ps(1.0));\\\n-                                _mm_mask_storeu_epi8(NULL, 0xFF, _mm_broadcastmb_epi64(temp))',\n-                                'immintrin.h'),\n-                                ]\n+OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX = [\n+    ('__attribute__((target(\"avx2,fma\")))',\n+    'attribute_target_avx2_with_intrinsics',\n+    '__m256 temp = _mm256_set1_ps(1.0); temp = \\\n+    _mm256_fmadd_ps(temp, temp, temp)',\n+    'immintrin.h'),\n+    ('__attribute__((target(\"avx512f\")))',\n+    'attribute_target_avx512f_with_intrinsics',\n+    '__m512i temp = _mm512_castps_si512(_mm512_set1_ps(1.0))',\n+    'immintrin.h'),\n+    ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n+    'attribute_target_avx512_skx_with_intrinsics',\n+    '__mmask8 temp = _mm512_fpclass_pd_mask(_mm512_set1_pd(1.0), 0x01);\\\n+    __m512i unused_temp = \\\n+        _mm512_castps_si512(_mm512_set1_ps(1.0));\\\n+    _mm_mask_storeu_epi8(NULL, 0xFF, _mm_broadcastmb_epi64(temp))',\n+    'immintrin.h'),\n+    ]\n \n def fname2def(name):\n     return \"HAVE_%s\" % name.upper()"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -571,7 +571,6 @@ NPY_NO_EXPORT void\n \n /**begin repeat1\n  * #isa = , _avx2#\n- * #ISA = , AVX2#\n  * #CHK = 1, defined(HAVE_ATTRIBUTE_TARGET_AVX2)#\n  * #ATTR = , NPY_GCC_TARGET_AVX2#\n  */\n@@ -658,6 +657,7 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n #define INT_left_shift_needs_clear_floatstatus\n #define UINT_left_shift_needs_clear_floatstatus\n \n+#if @CHK@\n NPY_NO_EXPORT NPY_GCC_OPT_3 void\n @TYPE@_left_shift@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps,\n                   void *NPY_UNUSED(func))\n@@ -670,10 +670,12 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 void\n     npy_clear_floatstatus_barrier((char*)dimensions);\n #endif\n }\n+#endif\n \n #undef INT_left_shift_needs_clear_floatstatus\n #undef UINT_left_shift_needs_clear_floatstatus\n \n+#if @CHK@\n NPY_NO_EXPORT\n #ifndef NPY_DO_NOT_OPTIMIZE_@TYPE@_right_shift\n NPY_GCC_OPT_3\n@@ -684,7 +686,7 @@ void\n {\n     BINARY_LOOP_FAST(@type@, @type@, *out = npy_rshift@c@(in1, in2));\n }\n-\n+#endif\n \n /**begin repeat2\n  * #kind = logical_and, logical_or#\n@@ -1448,7 +1450,10 @@ NPY_NO_EXPORT void\n /**begin repeat2\n  * #ISA  = , _avx512_skx#\n  * #isa  = simd, avx512_skx#\n+ * #CHK  = 1, defined(HAVE_ATTRIBUTE_TARGET_AVX512_SKX)#\n  **/\n+\n+#if @CHK@\n NPY_NO_EXPORT void\n @TYPE@_@kind@@ISA@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n@@ -1460,6 +1465,7 @@ NPY_NO_EXPORT void\n     }\n     npy_clear_floatstatus_barrier((char*)dimensions);\n }\n+#endif\n /**end repeat2**/\n /**end repeat1**/\n \n@@ -2289,7 +2295,7 @@ NPY_NO_EXPORT void\n     }\n }\n \n-#if @SIMD@\n+#if @SIMD@ && defined(HAVE_ATTRIBUTE_TARGET_AVX512F)\n /**begin repeat1\n  * arithmetic\n  * #kind = conjugate, square, absolute#"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22325,
        "body": "  Using load over stack has a bad impact on the compiler static analysis\r\n  compared with direct compiler initializer via curly brace which\r\n  is supported by both GCC and clang.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/common/simd/neon/misc.h",
                "patch": "@@ -31,86 +31,94 @@\n \n // vector with specific values set to each lane and\n // set a specific value to all remained lanes\n-NPY_FINLINE uint8x16_t npyv__set_u8(npy_uint8 i0, npy_uint8 i1, npy_uint8 i2, npy_uint8 i3,\n-    npy_uint8 i4, npy_uint8 i5, npy_uint8 i6, npy_uint8 i7, npy_uint8 i8, npy_uint8 i9,\n-    npy_uint8 i10, npy_uint8 i11, npy_uint8 i12, npy_uint8 i13, npy_uint8 i14, npy_uint8 i15)\n-{\n-    const uint8_t NPY_DECL_ALIGNED(16) data[16] = {\n-        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15\n-    };\n-    return vld1q_u8(data);\n-}\n-#define npyv_setf_u8(FILL, ...)  npyv__set_u8(NPYV__SET_FILL_16(npy_uint8, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE int8x16_t npyv__set_s8(npy_int8 i0, npy_int8 i1, npy_int8 i2, npy_int8 i3,\n-    npy_int8 i4, npy_int8 i5, npy_int8 i6, npy_int8 i7, npy_int8 i8, npy_int8 i9,\n-    npy_int8 i10, npy_int8 i11, npy_int8 i12, npy_int8 i13, npy_int8 i14, npy_int8 i15)\n-{\n-    const int8_t NPY_DECL_ALIGNED(16) data[16] = {\n-        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15\n-    };\n-    return vld1q_s8(data);\n-}\n-#define npyv_setf_s8(FILL, ...)  npyv__set_s8(NPYV__SET_FILL_16(npy_int8, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE uint16x8_t npyv__set_u16(npy_uint16 i0, npy_uint16 i1, npy_uint16 i2, npy_uint16 i3,\n-    npy_uint16 i4, npy_uint16 i5, npy_uint16 i6, npy_uint16 i7)\n-{\n-    const uint16_t NPY_DECL_ALIGNED(16) data[8] = {i0, i1, i2, i3, i4, i5, i6, i7};\n-    return vld1q_u16(data);\n-}\n-#define npyv_setf_u16(FILL, ...) npyv__set_u16(NPYV__SET_FILL_8(npy_uint16, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE int16x8_t npyv__set_s16(npy_int16 i0, npy_int16 i1, npy_int16 i2, npy_int16 i3,\n-    npy_int16 i4, npy_int16 i5, npy_int16 i6, npy_int16 i7)\n-{\n-    const int16_t NPY_DECL_ALIGNED(16) data[8] = {i0, i1, i2, i3, i4, i5, i6, i7};\n-    return vld1q_s16(data);\n-}\n-#define npyv_setf_s16(FILL, ...) npyv__set_s16(NPYV__SET_FILL_8(npy_int16, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE uint32x4_t npyv__set_u32(npy_uint32 i0, npy_uint32 i1, npy_uint32 i2, npy_uint32 i3)\n-{\n-    const uint32_t NPY_DECL_ALIGNED(16) data[4] = {i0, i1, i2, i3};\n-    return vld1q_u32(data);\n-}\n-#define npyv_setf_u32(FILL, ...) npyv__set_u32(NPYV__SET_FILL_4(npy_uint32, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE int32x4_t npyv__set_s32(npy_int32 i0, npy_int32 i1, npy_int32 i2, npy_int32 i3)\n-{\n-    const int32_t NPY_DECL_ALIGNED(16) data[4] = {i0, i1, i2, i3};\n-    return vld1q_s32(data);\n-}\n-#define npyv_setf_s32(FILL, ...) npyv__set_s32(NPYV__SET_FILL_4(npy_int32, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE uint64x2_t npyv__set_u64(npy_uint64 i0, npy_uint64 i1)\n-{\n-    const uint64_t NPY_DECL_ALIGNED(16) data[2] = {i0, i1};\n-    return vld1q_u64(data);\n-}\n-#define npyv_setf_u64(FILL, ...) npyv__set_u64(NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE int64x2_t npyv__set_s64(npy_int64 i0, npy_int64 i1)\n-{\n-    const int64_t NPY_DECL_ALIGNED(16) data[2] = {i0, i1};\n-    return vld1q_s64(data);\n-}\n-#define npyv_setf_s64(FILL, ...) npyv__set_s64(NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__))\n-\n-NPY_FINLINE float32x4_t npyv__set_f32(float i0, float i1, float i2, float i3)\n-{\n-    const float NPY_DECL_ALIGNED(16) data[4] = {i0, i1, i2, i3};\n-    return vld1q_f32(data);\n-}\n-#define npyv_setf_f32(FILL, ...) npyv__set_f32(NPYV__SET_FILL_4(float, FILL, __VA_ARGS__))\n-\n-#ifdef __aarch64__\n-NPY_FINLINE float64x2_t npyv__set_f64(double i0, double i1)\n-{\n-    const double NPY_DECL_ALIGNED(16) data[2] = {i0, i1};\n-    return vld1q_f64(data);\n-}\n-#define npyv_setf_f64(FILL, ...) npyv__set_f64(NPYV__SET_FILL_2(double, FILL, __VA_ARGS__))\n+#if defined(__clang__) || defined(__GNUC__)\n+    #define npyv_setf_u8(FILL, ...)  ((uint8x16_t){NPYV__SET_FILL_16(uint8_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_s8(FILL, ...)  ((int8x16_t){NPYV__SET_FILL_16(int8_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_u16(FILL, ...) ((uint16x8_t){NPYV__SET_FILL_8(uint16_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_s16(FILL, ...) ((int16x8_t){NPYV__SET_FILL_8(int16_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_u32(FILL, ...) ((uint32x4_t){NPYV__SET_FILL_4(uint32_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_s32(FILL, ...) ((int32x4_t){NPYV__SET_FILL_4(int32_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_u64(FILL, ...) ((uint64x2_t){NPYV__SET_FILL_2(uint64_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_s64(FILL, ...) ((int64x2_t){NPYV__SET_FILL_2(int64_t, FILL, __VA_ARGS__)})\n+    #define npyv_setf_f32(FILL, ...) ((float32x4_t){NPYV__SET_FILL_4(float, FILL, __VA_ARGS__)})\n+    #if NPY_SIMD_F64\n+        #define npyv_setf_f64(FILL, ...) ((float64x2_t){NPYV__SET_FILL_2(double, FILL, __VA_ARGS__)})\n+    #endif\n+#else\n+    NPY_FINLINE uint8x16_t npyv__set_u8(npy_uint8 i0, npy_uint8 i1, npy_uint8 i2, npy_uint8 i3,\n+        npy_uint8 i4, npy_uint8 i5, npy_uint8 i6, npy_uint8 i7, npy_uint8 i8, npy_uint8 i9,\n+        npy_uint8 i10, npy_uint8 i11, npy_uint8 i12, npy_uint8 i13, npy_uint8 i14, npy_uint8 i15)\n+    {\n+        const uint8_t NPY_DECL_ALIGNED(16) data[16] = {\n+            i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15\n+        };\n+        return vld1q_u8(data);\n+    }\n+    NPY_FINLINE int8x16_t npyv__set_s8(npy_int8 i0, npy_int8 i1, npy_int8 i2, npy_int8 i3,\n+        npy_int8 i4, npy_int8 i5, npy_int8 i6, npy_int8 i7, npy_int8 i8, npy_int8 i9,\n+        npy_int8 i10, npy_int8 i11, npy_int8 i12, npy_int8 i13, npy_int8 i14, npy_int8 i15)\n+    {\n+        const int8_t NPY_DECL_ALIGNED(16) data[16] = {\n+            i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15\n+        };\n+        return vld1q_s8(data);\n+    }\n+    NPY_FINLINE uint16x8_t npyv__set_u16(npy_uint16 i0, npy_uint16 i1, npy_uint16 i2, npy_uint16 i3,\n+        npy_uint16 i4, npy_uint16 i5, npy_uint16 i6, npy_uint16 i7)\n+    {\n+        const uint16_t NPY_DECL_ALIGNED(16) data[8] = {i0, i1, i2, i3, i4, i5, i6, i7};\n+        return vld1q_u16(data);\n+    }\n+    NPY_FINLINE int16x8_t npyv__set_s16(npy_int16 i0, npy_int16 i1, npy_int16 i2, npy_int16 i3,\n+        npy_int16 i4, npy_int16 i5, npy_int16 i6, npy_int16 i7)\n+    {\n+        const int16_t NPY_DECL_ALIGNED(16) data[8] = {i0, i1, i2, i3, i4, i5, i6, i7};\n+        return vld1q_s16(data);\n+    }\n+    NPY_FINLINE uint32x4_t npyv__set_u32(npy_uint32 i0, npy_uint32 i1, npy_uint32 i2, npy_uint32 i3)\n+    {\n+        const uint32_t NPY_DECL_ALIGNED(16) data[4] = {i0, i1, i2, i3};\n+        return vld1q_u32(data);\n+    }\n+    NPY_FINLINE int32x4_t npyv__set_s32(npy_int32 i0, npy_int32 i1, npy_int32 i2, npy_int32 i3)\n+    {\n+        const int32_t NPY_DECL_ALIGNED(16) data[4] = {i0, i1, i2, i3};\n+        return vld1q_s32(data);\n+    }\n+    NPY_FINLINE uint64x2_t npyv__set_u64(npy_uint64 i0, npy_uint64 i1)\n+    {\n+        const uint64_t NPY_DECL_ALIGNED(16) data[2] = {i0, i1};\n+        return vld1q_u64(data);\n+    }\n+    NPY_FINLINE int64x2_t npyv__set_s64(npy_int64 i0, npy_int64 i1)\n+    {\n+        const int64_t NPY_DECL_ALIGNED(16) data[2] = {i0, i1};\n+        return vld1q_s64(data);\n+    }\n+    NPY_FINLINE float32x4_t npyv__set_f32(float i0, float i1, float i2, float i3)\n+    {\n+        const float NPY_DECL_ALIGNED(16) data[4] = {i0, i1, i2, i3};\n+        return vld1q_f32(data);\n+    }\n+    #if NPY_SIMD_F64\n+        NPY_FINLINE float64x2_t npyv__set_f64(double i0, double i1)\n+        {\n+            const double NPY_DECL_ALIGNED(16) data[2] = {i0, i1};\n+            return vld1q_f64(data);\n+        }\n+    #endif\n+    #define npyv_setf_u8(FILL, ...)  npyv__set_u8(NPYV__SET_FILL_16(npy_uint8, FILL, __VA_ARGS__))\n+    #define npyv_setf_s8(FILL, ...)  npyv__set_s8(NPYV__SET_FILL_16(npy_int8, FILL, __VA_ARGS__))\n+    #define npyv_setf_u16(FILL, ...) npyv__set_u16(NPYV__SET_FILL_8(npy_uint16, FILL, __VA_ARGS__))\n+    #define npyv_setf_s16(FILL, ...) npyv__set_s16(NPYV__SET_FILL_8(npy_int16, FILL, __VA_ARGS__))\n+    #define npyv_setf_u32(FILL, ...) npyv__set_u32(NPYV__SET_FILL_4(npy_uint32, FILL, __VA_ARGS__))\n+    #define npyv_setf_s32(FILL, ...) npyv__set_s32(NPYV__SET_FILL_4(npy_int32, FILL, __VA_ARGS__))\n+    #define npyv_setf_u64(FILL, ...) npyv__set_u64(NPYV__SET_FILL_2(npy_uint64, FILL, __VA_ARGS__))\n+    #define npyv_setf_s64(FILL, ...) npyv__set_s64(NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__))\n+    #define npyv_setf_f32(FILL, ...) npyv__set_f32(NPYV__SET_FILL_4(float, FILL, __VA_ARGS__))\n+    #if NPY_SIMD_F64\n+        #define npyv_setf_f64(FILL, ...) npyv__set_f64(NPYV__SET_FILL_2(double, FILL, __VA_ARGS__))\n+    #endif\n #endif\n \n // vector with specific values set to each lane and"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 22240,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\nThis adds support for SVML's power and arctan2 for 32 and 64 bit floating point. On a simple benchmark of these functions included in this merge request, this gives a very significant performance improvement. It also adds some tests for power's behavior on infinities and NANs.\r\n```\r\n       before           after         ratio\r\n     [397b6e51]       [c85e17e3]\r\n     <main>           <SVML>\r\n-     14.5\u00b10.05ms      2.29\u00b10.07ms     0.16  bench_ufunc.BinaryBench.time_pow_64\r\n-     6.12\u00b10.02ms         954\u00b110\u03bcs     0.16  bench_ufunc.BinaryBench.time_pow_32\r\n-      16.2\u00b10.2ms      1.93\u00b10.04ms     0.12  bench_ufunc.BinaryBench.time_atan2_64\r\n-     18.3\u00b10.06ms          822\u00b18\u03bcs     0.04  bench_ufunc.BinaryBench.time_atan2_32\r\n\r\n```",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -294,3 +294,23 @@ class ArgParsingReduce(Benchmark):\n \n     def time_add_reduce_arg_parsing(self, arg_pack):\n         np.add.reduce(*arg_pack.args, **arg_pack.kwargs)\n+\n+class BinaryBench(Benchmark):\n+    def setup(self):\n+        N = 1000000\n+        self.a32 = np.random.rand(N).astype(np.float32)\n+        self.b32 = np.random.rand(N).astype(np.float32)\n+        self.a64 = np.random.rand(N).astype(np.float64)\n+        self.b64 = np.random.rand(N).astype(np.float64)\n+    \n+    def time_pow_32(self):\n+        np.power(self.a32, self.b32)\n+\n+    def time_pow_64(self):\n+        np.power(self.a64, self.b64)\n+\n+    def time_atan2_32(self):\n+        np.arctan2(self.a32, self.b32)\n+\n+    def time_atan2_64(self):\n+        np.arctan2(self.a64, self.b64)"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -396,6 +396,8 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.power'),\n           None,\n           TD(ints),\n+          TD('e', f='pow', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n           TD(inexact, f='pow', astype={'e': 'f'}),\n           TD(O, f='npy_ObjectPower'),\n           ),\n@@ -877,6 +879,7 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.arctan2'),\n           None,\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n           TD(flts, f='atan2', astype={'e': 'f'}),\n           TD(P, f='arctan2'),\n           ),"
            },
            {
                "filename": "numpy/core/src/common/npy_svml.h",
                "patch": "@@ -13,13 +13,14 @@ extern __m512 __svml_tanf16(__m512 x);\n extern __m512 __svml_asinf16(__m512 x);\n extern __m512 __svml_acosf16(__m512 x);\n extern __m512 __svml_atanf16(__m512 x);\n-extern __m512 __svml_atan2f16(__m512 x);\n+extern __m512 __svml_atan2f16(__m512 x, __m512 y);\n extern __m512 __svml_sinhf16(__m512 x);\n extern __m512 __svml_coshf16(__m512 x);\n extern __m512 __svml_tanhf16(__m512 x);\n extern __m512 __svml_asinhf16(__m512 x);\n extern __m512 __svml_acoshf16(__m512 x);\n extern __m512 __svml_atanhf16(__m512 x);\n+extern __m512 __svml_powf16(__m512 x, __m512 y);\n \n extern __m512d __svml_exp8(__m512d x);\n extern __m512d __svml_exp28(__m512d x);\n@@ -35,11 +36,12 @@ extern __m512d __svml_tan8(__m512d x);\n extern __m512d __svml_asin8(__m512d x);\n extern __m512d __svml_acos8(__m512d x);\n extern __m512d __svml_atan8(__m512d x);\n-extern __m512d __svml_atan28(__m512d x);\n+extern __m512d __svml_atan28(__m512d x, __m512d y);\n extern __m512d __svml_sinh8(__m512d x);\n extern __m512d __svml_cosh8(__m512d x);\n extern __m512d __svml_tanh8(__m512d x);\n extern __m512d __svml_asinh8(__m512d x);\n extern __m512d __svml_acosh8(__m512d x);\n extern __m512d __svml_atanh8(__m512d x);\n+extern __m512d __svml_pow8(__m512d x, __m512d y);\n #endif"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -282,6 +282,19 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void DOUBLE_@func@,\n \n /**end repeat**/\n \n+/**begin repeat\n+ *  #TYPE = FLOAT, DOUBLE#\n+ */\n+/**begin repeat1\n+ * #func = power, arctan2#\n+ */\n+\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@func@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  *  #TYPE = FLOAT, DOUBLE#\n  */"
            },
            {
                "filename": "numpy/core/src/umath/loops_umath_fp.dispatch.c.src",
                "patch": "@@ -74,6 +74,47 @@ simd_@func@_f64(const double *src, npy_intp ssrc,\n     npyv_cleanup();\n }\n /**end repeat**/\n+\n+/**begin repeat\n+ * #sfx = f32, f64#\n+ * #func_suffix = f16, 8#\n+ */\n+/**begin repeat1\n+ * #func = pow, atan2#\n+ */\n+ \n+static void\n+simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src1, npy_intp ssrc1,\n+                  const npyv_lanetype_@sfx@ *src2, npy_intp ssrc2,\n+                        npyv_lanetype_@sfx@ *dst, npy_intp sdst, npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_@sfx@;\n+    for (; len > 0; len -= vstep, src1 += ssrc1*vstep, src2 += ssrc2*vstep, dst += sdst*vstep) {\n+        npyv_@sfx@ x1;\n+        if (ssrc1 == 1) {\n+            x1 = npyv_load_till_@sfx@(src1, len, 1);\n+        } else {\n+            x1 = npyv_loadn_till_@sfx@(src1, ssrc1, len, 1);\n+        }\n+        \n+        npyv_@sfx@ x2;\n+        if (ssrc2 == 1) {\n+            x2 = npyv_load_till_@sfx@(src2, len, 1);\n+        } else {\n+            x2 = npyv_loadn_till_@sfx@(src2, ssrc2, len, 1);\n+        }\n+        \n+        npyv_@sfx@ out = __svml_@func@@func_suffix@(x1, x2);\n+        if (sdst == 1) {\n+            npyv_store_till_@sfx@(dst, len, out);\n+        } else {\n+            npyv_storen_till_@sfx@(dst, sdst, len, out);\n+        }\n+    }\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n #endif\n \n /**begin repeat\n@@ -112,6 +153,45 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n /**end repeat1**/\n /**end repeat**/\n \n+/**begin repeat\n+ *  #TYPE = DOUBLE, FLOAT#\n+ *  #type = npy_double, npy_float#\n+ *  #vsub = , f#\n+ *  #sfx  = f64, f32#\n+ */\n+/**begin repeat1\n+ *  #func = power, arctan2#\n+ *  #intrin = pow, atan2#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+    const @type@ *src1 = (@type@*)args[0];\n+    const @type@ *src2 = (@type@*)args[1];\n+          @type@ *dst  = (@type@*)args[2];\n+    const int lsize = sizeof(src1[0]);\n+    const npy_intp ssrc1 = steps[0] / lsize;\n+    const npy_intp ssrc2 = steps[1] / lsize;\n+    const npy_intp sdst  = steps[2] / lsize;\n+    const npy_intp len = dimensions[0];\n+    assert(len <= 1 || (steps[0] % lsize == 0 && steps[1] % lsize == 0));\n+    if (!is_mem_overlap(src1, steps[0], dst, steps[2], len) && !is_mem_overlap(src2, steps[1], dst, steps[2], len) &&\n+        npyv_loadable_stride_@sfx@(ssrc1) && npyv_loadable_stride_@sfx@(ssrc2) &&\n+        npyv_storable_stride_@sfx@(sdst)) {\n+        simd_@intrin@_@sfx@(src1, ssrc1, src2, ssrc2, dst, sdst, len);\n+        return;\n+    }\n+#endif\n+    BINARY_LOOP {\n+        const @type@ in1 = *(@type@ *)ip1;\n+        const @type@ in2 = *(@type@ *)ip2;\n+        *(@type@ *)op1 = npy_@intrin@@vsub@(in1, in2);\n+    }\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  *  #func = sin, cos#\n  */"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -1095,6 +1095,14 @@ def test_integer_to_negative_power(self):\n             assert_raises(ValueError, np.power, a, minusone)\n             assert_raises(ValueError, np.power, one, b)\n             assert_raises(ValueError, np.power, one, minusone)\n+    \n+    def test_float_to_inf_power(self):\n+        for dt in [np.float32, np.float64]:\n+            a = np.array([1, 1, 2, 2, -2, -2, np.inf, -np.inf], dt)\n+            b = np.array([np.inf, -np.inf, np.inf, -np.inf,\n+                                np.inf, -np.inf, np.inf, -np.inf], dt)\n+            r = np.array([1, 1, np.inf, 0, np.inf, 0, np.inf, 0], dt)\n+            assert_equal(np.power(a, b), r)\n \n \n class TestFloat_power:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21832,
        "body": "Optimize `np.linspace`:\r\n\r\n* Avoid calculation of `_nx.issubdtype` if no `dtype` is specified\r\n* Avoid calling `np.any` in the scalar case\r\n\r\nBenchmark with `%timeit linspace(0, 10, 2)` and `%timeit np.linspace([1,2], 10, 5)`\r\n\r\nMain:\r\n```\r\n17.1 \u00b5s \u00b1 517 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n28.7 \u00b5s \u00b1 548 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\nPR:\r\n```\r\n8.73 \u00b5s \u00b1 153 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n18.5 \u00b5s \u00b1 265 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n```\r\n\r\n**Details**\r\n\r\n* If no `dtype` is specified, the value for `dtype` is set to `dt` and determined from the input types and `float`. The `float` argument implies the resulting type can never be of integer type, so we skip the call to `_nx.issubdtyp(dtype, _nx.integer)`\r\n* If `_mult_inplace` is `True`, then `delta` is a scalar. We then skip the conversion to a numpy array with `_nx.asarray`\r\n* `np.result_type(..., float)` is almost equivalent to `np.result_type(..., float(num))` (since `num` is of integer type), the former is slightly faster. But for some platforms `test_denormal_numbers` from `test_function_base` fails with this optimization, leaving this out for now. \r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_function_base.py",
                "patch": "@@ -2,6 +2,15 @@\n \n import numpy as np\n \n+class Linspace(Benchmark):\n+    def setup(self):\n+        self.d = np.array([1, 2, 3])\n+\n+    def time_linspace_scalar(self):\n+        np.linspace(0, 10, 2)\n+\n+    def time_linspace_array(self):\n+        np.linspace(self.d, 10, 10)\n \n class Histogram1D(Benchmark):\n     def setup(self):"
            },
            {
                "filename": "numpy/core/function_base.py",
                "patch": "@@ -130,16 +130,21 @@ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,\n     dt = result_type(start, stop, float(num))\n     if dtype is None:\n         dtype = dt\n+        integer_dtype = False\n+    else:\n+        integer_dtype = _nx.issubdtype(dtype, _nx.integer)\n \n     delta = stop - start\n     y = _nx.arange(0, num, dtype=dt).reshape((-1,) + (1,) * ndim(delta))\n     # In-place multiplication y *= delta/div is faster, but prevents the multiplicant\n     # from overriding what class is produced, and thus prevents, e.g. use of Quantities,\n     # see gh-7142. Hence, we multiply in place only for standard scalar types.\n-    _mult_inplace = _nx.isscalar(delta)\n     if div > 0:\n+        _mult_inplace = _nx.isscalar(delta)\n         step = delta / div\n-        if _nx.any(step == 0):\n+        any_step_zero = (\n+            step == 0 if _mult_inplace else _nx.asanyarray(step == 0).any())\n+        if any_step_zero:\n             # Special handling for denormal numbers, gh-5437\n             y /= div\n             if _mult_inplace:\n@@ -166,7 +171,7 @@ def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,\n     if axis != 0:\n         y = _nx.moveaxis(y, 0, axis)\n \n-    if _nx.issubdtype(dtype, _nx.integer):\n+    if integer_dtype:\n         _nx.floor(y, out=y)\n \n     if retstep:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 13520,
        "body": "<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1621,21 +1621,23 @@ FLOAT_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSE\n NPY_NO_EXPORT NPY_GCC_OPT_3 void\n FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n {\n+    if (!run_unary_@isa@_@func@_FLOAT(args, dimensions, steps)) {\n+        UNARY_LOOP {\n+            /*\n+             * We use the AVX function to compute exp/log for scalar elements as well.\n+             * This is needed to ensure the output of strided and non-strided\n+             * cases match. But this worsens the performance of strided arrays.\n+             * There is plan to fix this in a subsequent patch by using gather\n+             * instructions for strided arrays in the AVX function.\n+             */\n #if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n-    @ISA@_@func@_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0]);\n+            @ISA@_@func@_FLOAT((npy_float *)op1, (npy_float *)ip1, 1);\n #else\n-    /*\n-     * This is the path it would take if ISA was runtime detected, but not\n-     * compiled for. It fixes the error on clang6.0 which fails to compile\n-     * AVX512F version. Not sure if I like this idea, if during runtime it\n-     * detects AXV512F, it will end up running the scalar version instead\n-     * of AVX2.\n-     */\n-    UNARY_LOOP {\n-\tconst npy_float in1 = *(npy_float *)ip1;\n-\t*(npy_float *)op1 = @scalarf@(in1);\n-    }\n+            const npy_float in1 = *(npy_float *)ip1;\n+            *(npy_float *)op1 = @scalarf@(in1);\n #endif\n+        }\n+    }\n }\n \n /**end repeat1**/"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -122,20 +122,36 @@ abs_ptrdiff(char *a, char *b)\n \n /**begin repeat\n  * #ISA = AVX2, AVX512F#\n+ * #isa = avx2, avx512f#\n+ * #REGISTER_SIZE = 32, 64#\n  */\n \n /* prototypes */\n-#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n \n /**begin repeat1\n  * #func = exp, log#\n  */\n \n+#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n static NPY_INLINE void\n @ISA@_@func@_FLOAT(npy_float *, npy_float *, const npy_intp n);\n+#endif\n \n-/**end repeat1**/\n+static NPY_INLINE int\n+run_unary_@isa@_@func@_FLOAT(char **args, npy_intp *dimensions, npy_intp *steps)\n+{\n+#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+    if (IS_BLOCKABLE_UNARY(sizeof(npy_float), @REGISTER_SIZE@)) {\n+        @ISA@_@func@_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0]);\n+        return 1;\n+    }\n+    else\n+        return 0;\n #endif\n+    return 0;\n+}\n+\n+/**end repeat1**/\n \n /**end repeat**/\n "
            },
            {
                "filename": "numpy/core/tests/test_ufunc.py",
                "patch": "@@ -1964,3 +1964,28 @@ def test_ufunc_types(ufunc):\n                 assert r.dtype == np.dtype(t)\n         else:\n             assert res.dtype == np.dtype(out)\n+\n+@pytest.mark.parametrize('ufunc', [getattr(np, x) for x in dir(np)\n+                                if isinstance(getattr(np, x), np.ufunc)])\n+def test_ufunc_noncontiguous(ufunc):\n+    '''\n+    Check that contiguous and non-contiguous calls to ufuncs\n+    have the same results for values in range(9)\n+    '''\n+    for typ in ufunc.types:\n+        # types is a list of strings like ii->i\n+        if any(set('O?mM') & set(typ)):\n+            # bool, object, datetime are too irregular for this simple test\n+            continue\n+        inp, out = typ.split('->')\n+        args_c = [np.empty(6, t) for t in inp]\n+        args_n = [np.empty(18, t)[::3] for t in inp]\n+        for a in args_c:\n+            a.flat = range(1,7)\n+        for a in args_n:\n+            a.flat = range(1,7)\n+        with warnings.catch_warnings(record=True):\n+            warnings.filterwarnings(\"always\")\n+            res_c = ufunc(*args_c)\n+            res_n = ufunc(*args_n)\n+        assert_equal(res_c, res_n)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 13368,
        "body": "This commit implements vectorized single precision sine and cosine using\r\nAVX2 and AVX512. Both sine and cosine are computed using a\r\npolynomial approximation which are accurate for values between\r\n[-PI/4,PI/4]. The original input is reduced to this range using a 3-step\r\nCody-Waite's range reduction method. This method is only accurate for\r\nvalues between [-71476.0625f, 71476.0625f] for cosine and [-117435.992f,\r\n117435.992f] for sine. The algorithm identifies elements outside this\r\nrange and calls glibc in a scalar loop to compute their output.\r\n\r\nThe algorithm is a vectorized version of the methods presented\r\nhere: https://stackoverflow.com/questions/30463616/payne-hanek-algorithm-implementation-in-c/30465751#30465751\r\n\r\nAccuracy: maximum ULP error = 1.49\r\n\r\nMax ULP error comparison to GLIBC's implementation: \r\n\r\n| Function | GLIBC scalar | GLIBC vector (libmvec) | this patch |               \r\n|----------|--------------|------------------------|------------|               \r\n| sine     | 1.19         | 2.47                   | 1.49       |               \r\n| cosine   | 1.19         | 2.39                   | 1.49       |               \r\n\r\nPerformance: The speed-up this implementation provides is dependent on\r\nthe values of the input array. AVX512 version performs nearly 22x faster when all the input\r\nvalues are within the range specified above. Details of the performance\r\nbenefits are provided in the commit message. Its worst performance is when all the array\r\nelements are outside the range leading to about 1% reduction in\r\nperformance.\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/gitwash/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/gitwash/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -662,13 +662,17 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cos'),\n           None,\n+          TD('e', f='cos', astype={'e':'f'}),\n+          TD('f', simd=[('fma', 'f'), ('avx512f', 'f')]),\n           TD(inexact, f='cos', astype={'e':'f'}),\n           TD(P, f='cos'),\n           ),\n 'sin':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sin'),\n           None,\n+          TD('e', f='sin', astype={'e':'f'}),\n+          TD('f', simd=[('fma', 'f'), ('avx512f', 'f')]),\n           TD(inexact, f='sin', astype={'e':'f'}),\n           TD(P, f='sin'),\n           ),\n@@ -705,7 +709,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.exp'),\n           None,\n           TD('e', f='exp', astype={'e':'f'}),\n-          TD('f', simd=[('avx2', 'f'), ('avx512f', 'f')]),\n+          TD('f', simd=[('fma', 'f'), ('avx512f', 'f')]),\n           TD(inexact, f='exp', astype={'e':'f'}),\n           TD(P, f='exp'),\n           ),\n@@ -728,7 +732,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.log'),\n           None,\n           TD('e', f='log', astype={'e':'f'}),\n-          TD('f', simd=[('avx2', 'f'), ('avx512f', 'f')]),\n+          TD('f', simd=[('fma', 'f'), ('avx512f', 'f')]),\n           TD(inexact, f='log', astype={'e':'f'}),\n           TD(P, f='log'),\n           ),"
            },
            {
                "filename": "numpy/core/include/numpy/npy_common.h",
                "patch": "@@ -44,10 +44,14 @@\n #else\n #define NPY_GCC_TARGET_AVX\n #endif\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n+#define HAVE_ATTRIBUTE_TARGET_FMA\n+#define NPY_GCC_TARGET_FMA __attribute__((target(\"avx2,fma\")))\n+#endif\n+\n #if defined HAVE_ATTRIBUTE_TARGET_AVX2 && defined HAVE_LINK_AVX2\n #define NPY_GCC_TARGET_AVX2 __attribute__((target(\"avx2\")))\n-#elif defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n-#define NPY_GCC_TARGET_AVX2 __attribute__((target(\"avx2\")))\n #else\n #define NPY_GCC_TARGET_AVX2\n #endif"
            },
            {
                "filename": "numpy/core/include/numpy/npy_math.h",
                "patch": "@@ -144,7 +144,22 @@ NPY_INLINE static float __npy_nzerof(void)\n #define NPY_COEFF_Q3_LOGf 9.864942958519418960339e-01f\n #define NPY_COEFF_Q4_LOGf 1.546476374983906719538e-01f\n #define NPY_COEFF_Q5_LOGf 5.875095403124574342950e-03f\n-\n+/*\n+ * Constants used in vector implementation of sinf/cosf(x)\n+ */\n+#define NPY_TWO_O_PIf 0x1.45f306p-1f\n+#define NPY_CODY_WAITE_PI_O_2_HIGHf -0x1.921fb0p+00f\n+#define NPY_CODY_WAITE_PI_O_2_MEDf -0x1.5110b4p-22f\n+#define NPY_CODY_WAITE_PI_O_2_LOWf -0x1.846988p-48f\n+#define NPY_COEFF_INVF0_COSINEf 0x1.000000p+00f\n+#define NPY_COEFF_INVF2_COSINEf -0x1.000000p-01f\n+#define NPY_COEFF_INVF4_COSINEf 0x1.55553cp-05f\n+#define NPY_COEFF_INVF6_COSINEf -0x1.6c06dcp-10f\n+#define NPY_COEFF_INVF8_COSINEf 0x1.98e616p-16f\n+#define NPY_COEFF_INVF3_SINEf -0x1.555556p-03f\n+#define NPY_COEFF_INVF5_SINEf 0x1.11119ap-07f\n+#define NPY_COEFF_INVF7_SINEf -0x1.a06bbap-13f\n+#define NPY_COEFF_INVF9_SINEf 0x1.7d3bbcp-19f\n /*\n  * Integer functions.\n  */\n@@ -162,6 +177,15 @@ NPY_INPLACE npy_long npy_lcml(npy_long a, npy_long b);\n NPY_INPLACE npy_longlong npy_gcdll(npy_longlong a, npy_longlong b);\n NPY_INPLACE npy_longlong npy_lcmll(npy_longlong a, npy_longlong b);\n \n+/*\n+ * avx function has a common API for both sin & cos. This enum is used to\n+ * distinguish between the two\n+ */\n+typedef enum {\n+    npy_compute_sin,\n+    npy_compute_cos\n+} NPY_TRIG_OP;\n+\n /*\n  * C99 double math funcs\n  */"
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -178,9 +178,10 @@ def check_api_version(apiversion, codegen_dir):\n # gcc 4.8.4 support attributes but not with intrisics\n # tested via \"#include<%s> int %s %s(void *){code; return 0;};\" % (header, attribute, name, code)\n # function name will be converted to HAVE_<upper-case-name> preprocessor macro\n-OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS = [('__attribute__((target(\"avx2\")))',\n+OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS = [('__attribute__((target(\"avx2,fma\")))',\n                                 'attribute_target_avx2_with_intrinsics',\n-                                '__m256 temp = _mm256_set1_ps(1.0)',\n+                                '__m256 temp = _mm256_set1_ps(1.0); temp = \\\n+                                _mm256_fmadd_ps(temp, temp, temp)',\n                                 'immintrin.h'),\n                                 ('__attribute__((target(\"avx512f\")))',\n                                 'attribute_target_avx512f_with_intrinsics',"
            },
            {
                "filename": "numpy/core/src/umath/cpuid.c",
                "patch": "@@ -48,6 +48,25 @@ int os_avx512_support(void)\n #endif\n }\n \n+static NPY_INLINE\n+int cpu_supports_fma(void)\n+{\n+#ifdef __x86_64__\n+    unsigned int feature = 0x01;\n+    unsigned int a, b, c, d;\n+    __asm__ volatile (\n+        \"cpuid\"\t\t\t\t\"\\n\\t\"\n+\t: \"=a\" (a), \"=b\" (b), \"=c\" (c), \"=d\" (d)\n+\t: \"a\" (feature));\n+    /*\n+     * FMA is the 12th bit of ECX\n+     */\n+    return (c >> 12) & 1;\n+#else\n+    return 0;\n+#endif\n+}\n+\n /*\n  * Primitive cpu feature detect function\n  * Currently only supports checking for avx on gcc compatible compilers.\n@@ -63,6 +82,9 @@ npy_cpu_supports(const char * feature)\n         return 0;\n #endif\n     }\n+    else if (strcmp(feature, \"fma\") == 0) {\n+        return cpu_supports_fma() && __builtin_cpu_supports(\"avx2\") && os_avx_support();\n+    }\n     else if (strcmp(feature, \"avx2\") == 0) {\n         return __builtin_cpu_supports(\"avx2\") && os_avx_support();\n     }"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1594,8 +1594,8 @@ NPY_NO_EXPORT void\n /**end repeat**/\n \n /**begin repeat\n- *  #func = exp, log#\n- *  #scalarf = npy_expf, npy_logf#\n+ *  #func = sin, cos, exp, log#\n+ *  #scalarf = npy_sinf, npy_cosf, npy_expf, npy_logf#\n  */\n \n NPY_NO_EXPORT NPY_GCC_OPT_3 void\n@@ -1610,8 +1610,8 @@ FLOAT_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSE\n /**end repeat**/\n \n /**begin repeat\n- * #isa = avx512f, avx2#\n- * #ISA = AVX512F, AVX2#\n+ * #isa = avx512f, fma#\n+ * #ISA = AVX512F, FMA#\n  * #CHK = HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS#\n  */\n \n@@ -1642,6 +1642,31 @@ FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY\n }\n \n /**end repeat1**/\n+\n+/**begin repeat1\n+ *  #func = cos, sin#\n+ *  #enum = npy_compute_cos, npy_compute_sin#\n+ *  #scalarf = npy_cosf, npy_sinf#\n+ */\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    if (!run_unary_@isa@_sincos_FLOAT(args, dimensions, steps, @enum@)) {\n+        UNARY_LOOP {\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n+            @ISA@_sincos_FLOAT((npy_float *)op1, (npy_float *)ip1, 1, steps[0], @enum@);\n+#else\n+\t        const npy_float in1 = *(npy_float *)ip1;\n+\t        *(npy_float *)op1 = @scalarf@(in1);\n+#endif\n+        }\n+    }\n+}\n+\n+/**end repeat1**/\n+\n+\n /**end repeat**/\n \n /**begin repeat"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -178,13 +178,13 @@ NPY_NO_EXPORT void\n /**end repeat**/\n \n /**begin repeat\n- *  #func = exp, log#\n+ *  #func = sin, cos, exp, log#\n  */\n NPY_NO_EXPORT void\n FLOAT_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func));\n \n /**begin repeat1\n- * #isa = avx512f, avx2#\n+ * #isa = avx512f, fma#\n  */\n \n NPY_NO_EXPORT void"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -130,8 +130,9 @@ abs_ptrdiff(char *a, char *b)\n  */\n \n /**begin repeat\n- * #ISA = AVX2, AVX512F#\n- * #isa = avx2, avx512f#\n+ * #ISA = FMA, AVX512F#\n+ * #isa = fma, avx512f#\n+ * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n  * #REGISTER_SIZE = 32, 64#\n  */\n \n@@ -141,15 +142,15 @@ abs_ptrdiff(char *a, char *b)\n  * #func = exp, log#\n  */\n \n-#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n static NPY_INLINE void\n @ISA@_@func@_FLOAT(npy_float *, npy_float *, const npy_intp n, const npy_intp stride);\n #endif\n \n static NPY_INLINE int\n run_unary_@isa@_@func@_FLOAT(char **args, npy_intp *dimensions, npy_intp *steps)\n {\n-#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n     if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), @REGISTER_SIZE@)) {\n         @ISA@_@func@_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);\n         return 1;\n@@ -162,6 +163,25 @@ run_unary_@isa@_@func@_FLOAT(char **args, npy_intp *dimensions, npy_intp *steps)\n \n /**end repeat1**/\n \n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_INLINE void\n+@ISA@_sincos_FLOAT(npy_float *, npy_float *, const npy_intp n, const npy_intp steps, NPY_TRIG_OP);\n+#endif\n+\n+static NPY_INLINE int\n+run_unary_@isa@_sincos_FLOAT(char **args, npy_intp *dimensions, npy_intp *steps, NPY_TRIG_OP my_trig_op)\n+{\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n+    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), @REGISTER_SIZE@)) {\n+        @ISA@_sincos_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0], steps[0], my_trig_op);\n+        return 1;\n+    }\n+    else\n+        return 0;\n+#endif\n+    return 0;\n+}\n+\n /**end repeat**/\n \n \n@@ -1123,56 +1143,63 @@ sse2_@kind@_@TYPE@(@type@ * ip, @type@ * op, const npy_intp n)\n /* bunch of helper functions used in ISA_exp/log_FLOAT*/\n \n #if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_fmadd(__m256 a, __m256 b, __m256 c)\n-{\n-    return _mm256_add_ps(_mm256_mul_ps(a, b), c);\n-}\n-\n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_get_full_load_mask(void)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_get_full_load_mask(void)\n {\n     return _mm256_set1_ps(-1.0);\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_get_partial_load_mask(const npy_int num_lanes, const npy_int total_elem)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_get_partial_load_mask(const npy_int num_lanes, const npy_int total_elem)\n {\n     float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,\n                             1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};\n     float* addr = maskint + total_elem - num_lanes;\n     return _mm256_loadu_ps(addr);\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_masked_gather(__m256 src,\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_masked_gather(__m256 src,\n                    npy_float* addr,\n                    __m256i vindex,\n                    __m256 mask)\n {\n     return _mm256_mask_i32gather_ps(src, addr, vindex, mask, 4);\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_masked_load(__m256 mask, npy_float* addr)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_masked_load(__m256 mask, npy_float* addr)\n {\n     return _mm256_maskload_ps(addr, _mm256_cvtps_epi32(mask));\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_set_masked_lanes(__m256 x, __m256 val, __m256 mask)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_set_masked_lanes(__m256 x, __m256 val, __m256 mask)\n {\n     return _mm256_blendv_ps(x, val, mask);\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_blend(__m256 x, __m256 y, __m256 ymask)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_blend(__m256 x, __m256 y, __m256 ymask)\n {\n     return _mm256_blendv_ps(x, y, ymask);\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_get_exponent(__m256 x)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_should_calculate_sine(__m256i k, __m256i andop, __m256i cmp)\n+{\n+   return _mm256_cvtepi32_ps(\n+                _mm256_cmpeq_epi32(_mm256_and_si256(k, andop), cmp));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_should_negate(__m256i k, __m256i andop, __m256i cmp)\n+{\n+    return fma_should_calculate_sine(k, andop, cmp);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_get_exponent(__m256 x)\n {\n     /*\n      * Special handling of denormals:\n@@ -1198,8 +1225,8 @@ avx2_get_exponent(__m256 x)\n     return _mm256_blendv_ps(exp, denorm_exp, denormal_mask);\n }\n \n-static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_get_mantissa(__m256 x)\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_get_mantissa(__m256 x)\n {\n     /*\n      * Special handling of denormals:\n@@ -1225,7 +1252,7 @@ avx2_get_mantissa(__m256 x)\n }\n \n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_scalef_ps(__m256 poly, __m256 quadrant)\n+fma_scalef_ps(__m256 poly, __m256 quadrant)\n {\n     /*\n      * Handle denormals (which occur when quadrant <= -125):\n@@ -1305,6 +1332,18 @@ avx512_blend(__m512 x, __m512 y, __mmask16 ymask)\n     return _mm512_mask_mov_ps(x, ymask, y);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n+avx512_should_calculate_sine(__m512i k, __m512i andop, __m512i cmp)\n+{\n+    return _mm512_cmpeq_epi32_mask(_mm512_and_epi32(k, andop), cmp);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n+avx512_should_negate(__m512i k, __m512i andop, __m512i cmp)\n+{\n+    return avx512_should_calculate_sine(k, andop, cmp);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n avx512_get_exponent(__m512 x)\n {\n@@ -1325,17 +1364,28 @@ avx512_scalef_ps(__m512 poly, __m512 quadrant)\n #endif\n \n /**begin repeat\n- * #ISA = AVX2, AVX512F#\n- * #isa = avx2, avx512#\n+ * #ISA = FMA, AVX512F#\n+ * #isa = fma, avx512#\n  * #vtype = __m256, __m512#\n  * #vsize = 256, 512#\n  * #or = or_ps, kor#\n  * #vsub = , _mask#\n  * #mask = __m256, __mmask16#\n- * #fmadd = avx2_fmadd,_mm512_fmadd_ps#\n+ * #fmadd = _mm256_fmadd_ps, _mm512_fmadd_ps#\n+ * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n  **/\n \n-#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS\n+#if defined @CHK@\n+\n+/*\n+ * Vectorized Cody-Waite range reduction technique\n+ * Performs the reduction step x* = x - y*C in three steps:\n+ * 1) x* = x - y*c1\n+ * 2) x* = x - y*c2\n+ * 3) x* = x - y*c3\n+ * c1, c2 are exact floating points, c3 = C - c1 - c2 simulates higher precision\n+ */\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n @isa@_range_reduction(@vtype@ x, @vtype@ y, @vtype@ c1, @vtype@ c2, @vtype@ c3)\n {\n@@ -1344,12 +1394,56 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n     reduced_x = @fmadd@(y, c3, reduced_x);\n     return reduced_x;\n }\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @mask@\n+@isa@_in_range_mask(@vtype@ x, npy_float fmax, npy_float fmin)\n+{\n+    @mask@ m1 = _mm@vsize@_cmp_ps@vsub@(\n+                                x, _mm@vsize@_set1_ps(fmax), _CMP_GT_OQ);\n+    @mask@ m2 = _mm@vsize@_cmp_ps@vsub@(\n+                                x, _mm@vsize@_set1_ps(fmin), _CMP_LT_OQ);\n+    return _mm@vsize@_@or@(m1,m2);\n+}\n+\n+/*\n+ * Approximate cosine algorithm for x \\in [-PI/4, PI/4]\n+ * Maximum ULP across all 32-bit floats = 0.875\n+ */\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n+@isa@_cosine(@vtype@ x2, @vtype@ invf8, @vtype@ invf6, @vtype@ invf4,\n+                                                @vtype@ invf2, @vtype@ invf0)\n+{\n+    @vtype@ cos = @fmadd@(invf8, x2, invf6);\n+    cos = @fmadd@(cos, x2, invf4);\n+    cos = @fmadd@(cos, x2, invf2);\n+    cos = @fmadd@(cos, x2, invf0);\n+    return cos;\n+}\n+\n+/*\n+ * Approximate sine algorithm for x \\in [-PI/4, PI/4]\n+ * Maximum ULP across all 32-bit floats = 0.647\n+ */\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n+@isa@_sine(@vtype@ x, @vtype@ x2, @vtype@ invf9, @vtype@ invf7,\n+                                          @vtype@ invf5, @vtype@ invf3,\n+                                          @vtype@ zero)\n+{\n+    @vtype@ sin = @fmadd@(invf9, x2, invf7);\n+    sin = @fmadd@(sin, x2, invf5);\n+    sin = @fmadd@(sin, x2, invf3);\n+    sin = @fmadd@(sin, x2, zero);\n+    sin = @fmadd@(sin, x, x);\n+    return sin;\n+}\n #endif\n /**end repeat**/\n \n /**begin repeat\n- * #ISA = AVX2, AVX512F#\n- * #isa = avx2, avx512#\n+ * #ISA = FMA, AVX512F#\n+ * #isa = fma, avx512#\n  * #vtype = __m256, __m512#\n  * #vsize = 256, 512#\n  * #BYTES = 32, 64#\n@@ -1358,13 +1452,165 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n  * #or_masks =_mm256_or_ps, _mm512_kor#\n  * #and_masks =_mm256_and_ps, _mm512_kand#\n  * #xor_masks =_mm256_xor_ps, _mm512_kxor#\n- * #fmadd = avx2_fmadd,_mm512_fmadd_ps#\n+ * #fmadd = _mm256_fmadd_ps, _mm512_fmadd_ps#\n  * #mask_to_int = _mm256_movemask_ps, #\n  * #full_mask= 0xFF, 0xFFFF#\n  * #masked_store = _mm256_maskstore_ps, _mm512_mask_storeu_ps#\n  * #cvtps_epi32 = _mm256_cvtps_epi32, #\n+ * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n+ */\n+\n+\n+/*\n+ * Vectorized approximate sine/cosine algorithms: The following code is a\n+ * vectorized version of the algorithm presented here:\n+ * https://stackoverflow.com/questions/30463616/payne-hanek-algorithm-implementation-in-c/30465751#30465751\n+ * (1) Load data in ZMM/YMM registers and generate mask for elements that are\n+ * within range [-71476.0625f, 71476.0625f] for cosine and [-117435.992f,\n+ * 117435.992f] for sine.\n+ * (2) For elements within range, perform range reduction using Cody-Waite's\n+ * method: x* = x - y*PI/2, where y = rint(x*2/PI). x* \\in [-PI/4, PI/4].\n+ * (3) Map cos(x) to (+/-)sine or (+/-)cosine of x* based on the quadrant k =\n+ * int(y).\n+ * (4) For elements outside that range, Cody-Waite reduction peforms poorly\n+ * leading to catastrophic cancellation. We compute cosine by calling glibc in\n+ * a scalar fashion.\n+ * (5) Vectorized implementation has a max ULP of 1.49 and performs at least\n+ * 5-7x faster than scalar implementations when magnitude of all elements in\n+ * the array < 71476.0625f (117435.992f for sine). Worst case performance is\n+ * when all the elements are large leading to about 1-2% reduction in\n+ * performance.\n  */\n \n+#if defined @CHK@\n+static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n+@ISA@_sincos_FLOAT(npy_float * op,\n+                   npy_float * ip,\n+                   const npy_intp array_size,\n+                   const npy_intp steps,\n+                   NPY_TRIG_OP my_trig_op)\n+{\n+    const npy_intp stride = steps/sizeof(npy_float);\n+    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    npy_float large_number = 71476.0625f;\n+    if (my_trig_op == npy_compute_sin) {\n+        large_number = 117435.992f;\n+    }\n+\n+    /* Load up frequently used constants */\n+    @vtype@i zeros = _mm@vsize@_set1_epi32(0);\n+    @vtype@i ones = _mm@vsize@_set1_epi32(1);\n+    @vtype@i twos = _mm@vsize@_set1_epi32(2);\n+    @vtype@ two_over_pi = _mm@vsize@_set1_ps(NPY_TWO_O_PIf);\n+    @vtype@ codyw_c1 = _mm@vsize@_set1_ps(NPY_CODY_WAITE_PI_O_2_HIGHf);\n+    @vtype@ codyw_c2 = _mm@vsize@_set1_ps(NPY_CODY_WAITE_PI_O_2_MEDf);\n+    @vtype@ codyw_c3 = _mm@vsize@_set1_ps(NPY_CODY_WAITE_PI_O_2_LOWf);\n+    @vtype@ cos_invf0 = _mm@vsize@_set1_ps(NPY_COEFF_INVF0_COSINEf);\n+    @vtype@ cos_invf2 = _mm@vsize@_set1_ps(NPY_COEFF_INVF2_COSINEf);\n+    @vtype@ cos_invf4 = _mm@vsize@_set1_ps(NPY_COEFF_INVF4_COSINEf);\n+    @vtype@ cos_invf6 = _mm@vsize@_set1_ps(NPY_COEFF_INVF6_COSINEf);\n+    @vtype@ cos_invf8 = _mm@vsize@_set1_ps(NPY_COEFF_INVF8_COSINEf);\n+    @vtype@ sin_invf3 = _mm@vsize@_set1_ps(NPY_COEFF_INVF3_SINEf);\n+    @vtype@ sin_invf5 = _mm@vsize@_set1_ps(NPY_COEFF_INVF5_SINEf);\n+    @vtype@ sin_invf7 = _mm@vsize@_set1_ps(NPY_COEFF_INVF7_SINEf);\n+    @vtype@ sin_invf9 = _mm@vsize@_set1_ps(NPY_COEFF_INVF9_SINEf);\n+    @vtype@ cvt_magic = _mm@vsize@_set1_ps(NPY_RINT_CVT_MAGICf);\n+    @vtype@ zero_f = _mm@vsize@_set1_ps(0.0f);\n+    @vtype@ quadrant, reduced_x, reduced_x2, cos, sin;\n+    @vtype@i iquadrant;\n+    @mask@ nan_mask, glibc_mask, sine_mask, negate_mask;\n+    @mask@ load_mask = @isa@_get_full_load_mask();\n+    npy_intp num_remaining_elements = array_size;\n+    npy_int indexarr[16];\n+    for (npy_int ii = 0; ii < 16; ii++) {\n+        indexarr[ii] = ii*stride;\n+    }\n+    @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);\n+\n+    while (num_remaining_elements > 0) {\n+\n+        if (num_remaining_elements < num_lanes) {\n+            load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n+                                                         num_lanes);\n+        }\n+\n+        @vtype@ x;\n+        if (stride == 1) {\n+            x = @isa@_masked_load(load_mask, ip);\n+        }\n+        else {\n+            x = @isa@_masked_gather(zero_f, ip, vindex, load_mask);\n+        }\n+\n+        /*\n+         * For elements outside of this range, Cody-Waite's range reduction\n+         * becomes inaccurate and we will call glibc to compute cosine for\n+         * these numbers\n+         */\n+\n+        glibc_mask = @isa@_in_range_mask(x, large_number,-large_number);\n+        glibc_mask = @and_masks@(load_mask, glibc_mask);\n+        nan_mask = _mm@vsize@_cmp_ps@vsub@(x, x, _CMP_NEQ_UQ);\n+        x = @isa@_set_masked_lanes(x, zero_f, @or_masks@(nan_mask, glibc_mask));\n+        npy_int iglibc_mask = @mask_to_int@(glibc_mask);\n+\n+        if (iglibc_mask != @full_mask@) {\n+            quadrant = _mm@vsize@_mul_ps(x, two_over_pi);\n+\n+            /* round to nearest */\n+            quadrant = _mm@vsize@_add_ps(quadrant, cvt_magic);\n+            quadrant = _mm@vsize@_sub_ps(quadrant, cvt_magic);\n+\n+            /* Cody-Waite's range reduction algorithm */\n+            reduced_x = @isa@_range_reduction(x, quadrant,\n+                                                   codyw_c1, codyw_c2, codyw_c3);\n+            reduced_x2 = _mm@vsize@_mul_ps(reduced_x, reduced_x);\n+\n+            /* compute cosine and sine */\n+            cos = @isa@_cosine(reduced_x2, cos_invf8, cos_invf6, cos_invf4,\n+                                                           cos_invf2, cos_invf0);\n+            sin = @isa@_sine(reduced_x, reduced_x2, sin_invf9, sin_invf7,\n+                                             sin_invf5, sin_invf3, zero_f);\n+\n+            iquadrant = _mm@vsize@_cvtps_epi32(quadrant);\n+            if (my_trig_op == npy_compute_cos) {\n+                iquadrant = _mm@vsize@_add_epi32(iquadrant, ones);\n+            }\n+\n+            /* blend sin and cos based on the quadrant */\n+            sine_mask = @isa@_should_calculate_sine(iquadrant, ones, zeros);\n+            cos = @isa@_blend(cos, sin, sine_mask);\n+\n+            /* multiply by -1 for appropriate elements */\n+            negate_mask = @isa@_should_negate(iquadrant, twos, twos);\n+            cos = @isa@_blend(cos, _mm@vsize@_sub_ps(zero_f, cos), negate_mask);\n+            cos = @isa@_set_masked_lanes(cos, _mm@vsize@_set1_ps(NPY_NANF), nan_mask);\n+\n+            @masked_store@(op, @cvtps_epi32@(load_mask), cos);\n+        }\n+\n+        /* process elements using glibc for large elements */\n+        if (my_trig_op == npy_compute_cos) {\n+            for (int ii = 0; iglibc_mask != 0; ii++) {\n+                if (iglibc_mask & 0x01) {\n+                    op[ii] = npy_cosf(ip[ii]);\n+                }\n+                iglibc_mask  = iglibc_mask >> 1;\n+            }\n+        }\n+        else {\n+            for (int ii = 0; iglibc_mask != 0; ii++) {\n+                if (iglibc_mask & 0x01) {\n+                    op[ii] = npy_sinf(ip[ii]);\n+                }\n+                iglibc_mask  = iglibc_mask >> 1;\n+            }\n+        }\n+        ip += num_lanes*stride;\n+        op += num_lanes;\n+        num_remaining_elements -= num_lanes;\n+    }\n+}\n \n /*\n  * Vectorized implementation of exp using AVX2 and AVX512:\n@@ -1381,7 +1627,6 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n  * same x = 0xc2781e37)\n  */\n \n-#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS\n static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n @ISA@_exp_FLOAT(npy_float * op,\n                 npy_float * ip,"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -678,30 +678,63 @@ def test_log_values(self):\n             assert_raises(FloatingPointError, np.log, np.float32(-np.inf))\n             assert_raises(FloatingPointError, np.log, np.float32(-1.0))\n \n-class TestExpLogFloat32(object):\n+    def test_sincos_values(self):\n+        with np.errstate(all='ignore'):\n+            x = [np.nan,  np.nan, np.nan, np.nan]\n+            y = [np.nan, -np.nan, np.inf, -np.inf]\n+            for dt in ['f', 'd', 'g']:\n+                xf = np.array(x, dtype=dt)\n+                yf = np.array(y, dtype=dt)\n+                assert_equal(np.sin(yf), xf)\n+                assert_equal(np.cos(yf), xf)\n+\n+        with np.errstate(invalid='raise'):\n+            assert_raises(FloatingPointError, np.sin, np.float32(-np.inf))\n+            assert_raises(FloatingPointError, np.sin, np.float32(np.inf))\n+            assert_raises(FloatingPointError, np.cos, np.float32(-np.inf))\n+            assert_raises(FloatingPointError, np.cos, np.float32(np.inf))\n+\n+\n+class TestSIMDFloat32(object):\n     def test_exp_float32(self):\n         np.random.seed(42)\n         x_f32 = np.float32(np.random.uniform(low=0.0,high=88.1,size=1000000))\n         x_f64 = np.float64(x_f32)\n-        assert_array_max_ulp(np.exp(x_f32), np.float32(np.exp(x_f64)), maxulp=2.6)\n+        assert_array_max_ulp(np.exp(x_f32), np.float32(np.exp(x_f64)), maxulp=3)\n \n     def test_log_float32(self):\n         np.random.seed(42)\n         x_f32 = np.float32(np.random.uniform(low=0.0,high=1000,size=1000000))\n         x_f64 = np.float64(x_f32)\n-        assert_array_max_ulp(np.log(x_f32), np.float32(np.log(x_f64)), maxulp=3.9)\n+        assert_array_max_ulp(np.log(x_f32), np.float32(np.log(x_f64)), maxulp=4)\n+\n+    def test_sincos_float32(self):\n+        np.random.seed(42)\n+        N = 1000000\n+        M = np.int(N/20)\n+        index = np.random.randint(low=0, high=N, size=M)\n+        x_f32 = np.float32(np.random.uniform(low=-100.,high=100.,size=N))\n+        # test coverage for elements > 117435.992f for which glibc is used\n+        x_f32[index] = np.float32(10E+10*np.random.rand(M))\n+        x_f64 = np.float64(x_f32)\n+        assert_array_max_ulp(np.sin(x_f32), np.float32(np.sin(x_f64)), maxulp=2)\n+        assert_array_max_ulp(np.cos(x_f32), np.float32(np.cos(x_f64)), maxulp=2)\n \n-    def test_strided_exp_log_float32(self):\n+    def test_strided_float32(self):\n         np.random.seed(42)\n         strides = np.random.randint(low=-100, high=100, size=100)\n         sizes = np.random.randint(low=1, high=2000, size=100)\n         for ii in sizes:\n             x_f32 = np.float32(np.random.uniform(low=0.01,high=88.1,size=ii))\n             exp_true = np.exp(x_f32)\n             log_true = np.log(x_f32)\n+            sin_true = np.sin(x_f32)\n+            cos_true = np.cos(x_f32)\n             for jj in strides:\n                 assert_array_almost_equal_nulp(np.exp(x_f32[::jj]), exp_true[::jj], nulp=2)\n                 assert_array_almost_equal_nulp(np.log(x_f32[::jj]), log_true[::jj], nulp=2)\n+                assert_array_almost_equal_nulp(np.sin(x_f32[::jj]), sin_true[::jj], nulp=2)\n+                assert_array_almost_equal_nulp(np.cos(x_f32[::jj]), cos_true[::jj], nulp=2)\n \n class TestLogAddExp(_FilterInvalids):\n     def test_logaddexp_values(self):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 13134,
        "body": "This commit implements vectorized single precision exponential and\r\nnatural log using AVX2 and AVX512.\r\n\r\nAccuracy:\r\n\r\n| Function | Max ULP Error | Max Relative Error |\r\n|----------|---------------|--------------------|\r\n| np.exp   | 2.52          | 2.1E-07            |\r\n| np.log   | 3.83          | 2.4E-07            |\r\n\r\nPerformance:\r\n\r\n(1) Micro-benchmarks: measured execution time of np.exp and np.log using\r\ntimeit package in python. Each function is executed 1000 times and this\r\nis repeated 100 times. The standard deviation for all the runs was less\r\nthan 2% of their mean value and hence not included in the data. The\r\nvectorized implementation was upto 7.6x faster than the scalar version.\r\n\r\n| Function | NumPy1.16 | AVX2   | AVX512 | AVX2 speedup | AVX512 speedup |\r\n| -------- | --------- | ------ | ------ | ------------ | -------------- |\r\n| np.exp   | 0.395s    | 0.112s | 0.055s | 3.56x        | 7.25x          |\r\n| np.log   | 0.456s    | 0.147s | 0.059s | 3.10x        | 7.64x          |\r\n\r\n(2) Logistic regression: exp and log are heavily used in training neural\r\nnetworks (as part of sigmoid activation function and loss function\r\nrespectively). This patch significantly speeds up training a logistic\r\nregression model. As an example, we measured how much time it takes to\r\ntrain a model with 15 features using 1000 training data points. We\r\nobserved a 2x speed up to train the model to achieve a loss function\r\nerror < 10E-04.\r\n\r\n| Function       | NumPy1.16  | AVX2   | AVX512 | AVX2 speedup | AVX512 speedup |\r\n| -------------- | ---------- | ------ | ------ | ------------ | -------------- |\r\n| logistic.train | 121.0s     | 75.02s | 60.60s | 1.61x        | 2.02x          |\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/gitwash/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/gitwash/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "doc/release/1.17.0-notes.rst",
                "patch": "@@ -191,6 +191,12 @@ but with this change, you can do::\n \n thereby saving a level of indentation\n \n+`numpy.exp and numpy.log` speed up for float32 implementation\n+-------------------------------------------------------------\n+float32 implementation of numpy.exp and numpy.log now benefit from AVX2/AVX512\n+instruction set which are detected during runtime. numpy.exp has a max ulp\n+error of 2.52 and numpy.log has a max ulp error or 3.83.\n+\n Improve performance of ``np.pad``\n ---------------------------------\n The performance of the function has been improved for most cases by filling in"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -697,6 +697,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.exp'),\n           None,\n+          TD('f', simd=[('avx2', 'f'), ('avx512f', 'f')]),\n           TD(inexact, f='exp', astype={'e':'f'}),\n           TD(P, f='exp'),\n           ),\n@@ -718,6 +719,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log'),\n           None,\n+          TD('f', simd=[('avx2', 'f'), ('avx512f', 'f')]),\n           TD(inexact, f='log', astype={'e':'f'}),\n           TD(P, f='log'),\n           ),"
            },
            {
                "filename": "numpy/core/include/numpy/npy_common.h",
                "patch": "@@ -46,10 +46,20 @@\n #endif\n #if defined HAVE_ATTRIBUTE_TARGET_AVX2 && defined HAVE_LINK_AVX2\n #define NPY_GCC_TARGET_AVX2 __attribute__((target(\"avx2\")))\n+#elif defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n+#define NPY_GCC_TARGET_AVX2 __attribute__((target(\"avx2\")))\n #else\n #define NPY_GCC_TARGET_AVX2\n #endif\n \n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F && defined HAVE_LINK_AVX512F\n+#define NPY_GCC_TARGET_AVX512F __attribute__((target(\"avx512f\")))\n+#elif defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS\n+#define NPY_GCC_TARGET_AVX512F __attribute__((target(\"avx512f\")))\n+#else\n+#define NPY_GCC_TARGET_AVX512F\n+#endif\n+\n /*\n  * mark an argument (starting from 1) that must not be NULL and is not checked\n  * DO NOT USE IF FUNCTION CHECKS FOR NULL!! the compiler will remove the check\n@@ -68,6 +78,13 @@\n #define NPY_HAVE_SSE2_INTRINSICS\n #endif\n \n+#if defined HAVE_IMMINTRIN_H && defined HAVE_LINK_AVX2\n+#define NPY_HAVE_AVX2_INTRINSICS\n+#endif\n+\n+#if defined HAVE_IMMINTRIN_H && defined HAVE_LINK_AVX512F\n+#define NPY_HAVE_AVX512F_INTRINSICS\n+#endif\n /*\n  * give a hint to the compiler which branch is more likely or unlikely\n  * to occur, e.g. rare error cases:"
            },
            {
                "filename": "numpy/core/include/numpy/npy_math.h",
                "patch": "@@ -113,6 +113,38 @@ NPY_INLINE static float __npy_nzerof(void)\n #define NPY_SQRT2l    1.414213562373095048801688724209698079L /* sqrt(2) */\n #define NPY_SQRT1_2l  0.707106781186547524400844362104849039L /* 1/sqrt(2) */\n \n+/* \n+ * Constants used in vector implementation of exp(x) \n+ */\n+#define NPY_RINT_CVT_MAGICf 0x1.800000p+23f\n+#define NPY_CODY_WAITE_LOGE_2_HIGHf -6.93145752e-1f\n+#define NPY_CODY_WAITE_LOGE_2_LOWf -1.42860677e-6f\n+#define NPY_COEFF_P0_EXPf 9.999999999980870924916e-01f                                 \n+#define NPY_COEFF_P1_EXPf 7.257664613233124478488e-01f                                 \n+#define NPY_COEFF_P2_EXPf 2.473615434895520810817e-01f                                 \n+#define NPY_COEFF_P3_EXPf 5.114512081637298353406e-02f                                 \n+#define NPY_COEFF_P4_EXPf 6.757896990527504603057e-03f                                 \n+#define NPY_COEFF_P5_EXPf 5.082762527590693718096e-04f                                 \n+#define NPY_COEFF_Q0_EXPf 1.000000000000000000000e+00f                                 \n+#define NPY_COEFF_Q1_EXPf -2.742335390411667452936e-01f                                \n+#define NPY_COEFF_Q2_EXPf 2.159509375685829852307e-02f  \n+\n+/* \n+ * Constants used in vector implementation of log(x) \n+ */\n+#define NPY_COEFF_P0_LOGf 0.000000000000000000000e+00f                          \n+#define NPY_COEFF_P1_LOGf 9.999999999999998702752e-01f                          \n+#define NPY_COEFF_P2_LOGf 2.112677543073053063722e+00f                          \n+#define NPY_COEFF_P3_LOGf 1.480000633576506585156e+00f                          \n+#define NPY_COEFF_P4_LOGf 3.808837741388407920751e-01f                          \n+#define NPY_COEFF_P5_LOGf 2.589979117907922693523e-02f                          \n+#define NPY_COEFF_Q0_LOGf 1.000000000000000000000e+00f                          \n+#define NPY_COEFF_Q1_LOGf 2.612677543073109236779e+00f                          \n+#define NPY_COEFF_Q2_LOGf 2.453006071784736363091e+00f                          \n+#define NPY_COEFF_Q3_LOGf 9.864942958519418960339e-01f                          \n+#define NPY_COEFF_Q4_LOGf 1.546476374983906719538e-01f                          \n+#define NPY_COEFF_Q5_LOGf 5.875095403124574342950e-03f \n+\n /*\n  * C99 double math funcs\n  */"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -171,6 +171,11 @@ def check_funcs(funcs_name):\n         if config.check_gcc_function_attribute(dec, fn):\n             moredefs.append((fname2def(fn), 1))\n \n+    for dec, fn, code, header in OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS:\n+        if config.check_gcc_function_attribute_with_intrinsics(dec, fn, code,\n+                                                               header):\n+            moredefs.append((fname2def(fn), 1))\n+\n     for fn in OPTIONAL_VARIABLE_ATTRIBUTES:\n         if config.check_gcc_variable_attribute(fn):\n             m = fn.replace(\"(\", \"_\").replace(\")\", \"_\")"
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -118,6 +118,7 @@ def check_api_version(apiversion, codegen_dir):\n # sse headers only enabled automatically on amd64/x32 builds\n                 \"xmmintrin.h\",  # SSE\n                 \"emmintrin.h\",  # SSE2\n+                \"immintrin.h\",  # AVX\n                 \"features.h\",  # for glibc version linux\n                 \"xlocale.h\",  # see GH#8367\n                 \"dlfcn.h\", # dladdr\n@@ -149,6 +150,8 @@ def check_api_version(apiversion, codegen_dir):\n                         \"stdio.h\", \"LINK_AVX\"),\n                        (\"__asm__ volatile\", '\"vpand %ymm1, %ymm2, %ymm3\"',\n                         \"stdio.h\", \"LINK_AVX2\"),\n+                       (\"__asm__ volatile\", '\"vpaddd %zmm1, %zmm2, %zmm3\"',\n+                        \"stdio.h\", \"LINK_AVX512F\"),\n                        (\"__asm__ volatile\", '\"xgetbv\"', \"stdio.h\", \"XGETBV\"),\n                        ]\n \n@@ -165,6 +168,23 @@ def check_api_version(apiversion, codegen_dir):\n                                  'attribute_target_avx'),\n                                 ('__attribute__((target (\"avx2\")))',\n                                  'attribute_target_avx2'),\n+                                ('__attribute__((target (\"avx512f\")))',\n+                                 'attribute_target_avx512f'),\n+                                ]\n+\n+# function attributes with intrinsics\n+# To ensure your compiler can compile avx intrinsics with just the attributes\n+# gcc 4.8.4 support attributes but not with intrisics\n+# tested via \"#include<%s> int %s %s(void *){code; return 0;};\" % (header, attribute, name, code)\n+# function name will be converted to HAVE_<upper-case-name> preprocessor macro\n+OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS = [('__attribute__((target(\"avx2\")))',\n+                                'attribute_target_avx2_with_intrinsics',\n+                                '__m256 temp = _mm256_set1_ps(1.0)',\n+                                'immintrin.h'),\n+                                ('__attribute__((target(\"avx512f\")))',\n+                                'attribute_target_avx512f_with_intrinsics',\n+                                '__m512 temp = _mm512_set1_ps(1.0)',\n+                                'immintrin.h'),\n                                 ]\n \n # variable attributes tested via \"int %s a\" % attribute"
            },
            {
                "filename": "numpy/core/src/umath/cpuid.c",
                "patch": "@@ -11,6 +11,7 @@\n #define XCR_XFEATURE_ENABLED_MASK 0x0\n #define XSTATE_SSE 0x2\n #define XSTATE_YMM 0x4\n+#define XSTATE_ZMM 0x70\n \n /*\n  * verify the OS supports avx instructions\n@@ -33,6 +34,19 @@ int os_avx_support(void)\n #endif\n }\n \n+static NPY_INLINE\n+int os_avx512_support(void)\n+{\n+#if HAVE_XGETBV\n+    unsigned int eax, edx;\n+    unsigned int ecx = XCR_XFEATURE_ENABLED_MASK;\n+    unsigned int xcr0 = XSTATE_ZMM | XSTATE_YMM | XSTATE_SSE;\n+    __asm__(\"xgetbv\" : \"=a\" (eax), \"=d\" (edx) : \"c\" (ecx));\n+    return (eax & xcr0) == xcr0;\n+#else\n+    return 0;\n+#endif\n+}\n \n /*\n  * Primitive cpu feature detect function\n@@ -42,7 +56,14 @@ NPY_NO_EXPORT int\n npy_cpu_supports(const char * feature)\n {\n #ifdef HAVE___BUILTIN_CPU_SUPPORTS\n-    if (strcmp(feature, \"avx2\") == 0) {\n+    if (strcmp(feature, \"avx512f\") == 0) {\n+#if defined(__GNUC__) && (__GNUC__ < 5)\n+        return 0;\n+#else\n+        return __builtin_cpu_supports(\"avx512f\") && os_avx512_support();\n+#endif\n+    }\n+    else if (strcmp(feature, \"avx2\") == 0) {\n         return __builtin_cpu_supports(\"avx2\") && os_avx_support();\n     }\n     else if (strcmp(feature, \"avx\") == 0) {"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1569,6 +1569,55 @@ NPY_NO_EXPORT void\n \n /**end repeat**/\n \n+/**begin repeat\n+ *  #func = exp, log#\n+ *  #scalarf = npy_expf, npy_logf#\n+ */\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+FLOAT_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    UNARY_LOOP {\n+\tconst npy_float in1 = *(npy_float *)ip1;\n+\t*(npy_float *)op1 = @scalarf@(in1);\n+    }\n+}\n+\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #isa = avx512f, avx2#\n+ * #ISA = AVX512F, AVX2#\n+ * #CHK = HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS#\n+ */\n+\n+/**begin repeat1\n+ *  #func = exp, log#\n+ *  #scalarf = npy_expf, npy_logf#\n+ */\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n+    @ISA@_@func@_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0]);\n+#else\n+    /*\n+     * This is the path it would take if ISA was runtime detected, but not\n+     * compiled for. It fixes the error on clang6.0 which fails to compile\n+     * AVX512F version. Not sure if I like this idea, if during runtime it\n+     * detects AXV512F, it will end up running the scalar version instead\n+     * of AVX2.\n+     */\n+    UNARY_LOOP {\n+\tconst npy_float in1 = *(npy_float *)ip1;\n+\t*(npy_float *)op1 = @scalarf@(in1);\n+    }\n+#endif\n+}\n+\n+/**end repeat1**/\n+/**end repeat**/\n \n /**begin repeat\n  * Float types"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -177,6 +177,22 @@ NPY_NO_EXPORT void\n @TYPE@_sqrt(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func));\n /**end repeat**/\n \n+/**begin repeat\n+ *  #func = exp, log#\n+ */\n+NPY_NO_EXPORT void\n+FLOAT_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func));\n+\n+/**begin repeat1\n+ * #isa = avx512f, avx2#\n+ */\n+\n+NPY_NO_EXPORT void\n+FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func));\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  * Float types\n  *  #TYPE = HALF, FLOAT, DOUBLE, LONGDOUBLE#"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -121,6 +121,27 @@ abs_ptrdiff(char *a, char *b)\n  *****************************************************************************\n  */\n \n+/**begin repeat\n+ * #ISA = AVX2, AVX512F#\n+ */\n+\n+/* prototypes */\n+#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+\n+/**begin repeat1\n+ * #func = exp, log#\n+ */\n+\n+static void\n+@ISA@_@func@_FLOAT(npy_float *, npy_float *, const npy_int n);\n+\n+/**end repeat1**/\n+#endif\n+\n+/**end repeat**/\n+\n+\n+\n /**begin repeat\n  * Float types\n  *  #type = npy_float, npy_double, npy_longdouble#\n@@ -1075,6 +1096,381 @@ sse2_@kind@_@TYPE@(@type@ * ip, @type@ * op, const npy_intp n)\n \n /**end repeat**/\n \n+/* bunch of helper functions used in ISA_exp/log_FLOAT*/\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_fmadd(__m256 a, __m256 b, __m256 c)\n+{\n+    return _mm256_add_ps(_mm256_mul_ps(a, b), c);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_get_full_load_mask(void)\n+{\n+    return _mm256_set1_ps(-1.0);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_get_partial_load_mask(const npy_int num_elem, const npy_int total_elem)\n+{\n+    float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,\n+                            1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};\n+    float* addr = maskint + total_elem - num_elem;\n+    return _mm256_loadu_ps(addr);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_masked_load(__m256 mask, npy_float* addr)\n+{\n+    return _mm256_maskload_ps(addr, _mm256_cvtps_epi32(mask));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_set_masked_lanes(__m256 x, __m256 val, __m256 mask)\n+{\n+    return _mm256_blendv_ps(x, val, mask);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_blend(__m256 x, __m256 y, __m256 ymask)\n+{\n+    return _mm256_blendv_ps(x, y, ymask);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_get_exponent(__m256 x)\n+{\n+    /*\n+     * Special handling of denormals:\n+     * 1) Multiply denormal elements with 2**100 (0x71800000)\n+     * 2) Get the 8 bits of unbiased exponent\n+     * 3) Subtract 100 from exponent of denormals\n+     */\n+\n+    __m256 two_power_100 = _mm256_castsi256_ps(_mm256_set1_epi32(0x71800000));\n+    __m256 denormal_mask = _mm256_cmp_ps(x, _mm256_set1_ps(FLT_MIN), _CMP_LT_OQ);\n+    __m256 temp = _mm256_mul_ps(x, two_power_100);\n+    x = _mm256_blendv_ps(x, temp, denormal_mask);\n+\n+    __m256 exp = _mm256_cvtepi32_ps(\n+                    _mm256_sub_epi32(\n+                        _mm256_srli_epi32(\n+                            _mm256_castps_si256(x), 23),_mm256_set1_epi32(0x7E)));\n+\n+    __m256 denorm_exp = _mm256_sub_ps(exp, _mm256_set1_ps(100.0f));\n+    return _mm256_blendv_ps(exp, denorm_exp, denormal_mask);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_get_mantissa(__m256 x)\n+{\n+    /*\n+     * Special handling of denormals:\n+     * 1) Multiply denormal elements with 2**100 (0x71800000)\n+     * 2) Get the 23 bits of mantissa\n+     * 3) Mantissa for denormals is not affected by the multiplication\n+     */\n+\n+    __m256 two_power_100 = _mm256_castsi256_ps(_mm256_set1_epi32(0x71800000));\n+    __m256 denormal_mask = _mm256_cmp_ps(x, _mm256_set1_ps(FLT_MIN), _CMP_LT_OQ);\n+    __m256 temp = _mm256_mul_ps(x, two_power_100);\n+    x = _mm256_blendv_ps(x, temp, denormal_mask);\n+\n+    __m256i mantissa_bits = _mm256_set1_epi32(0x7fffff);\n+    __m256i exp_126_bits  = _mm256_set1_epi32(126 << 23);\n+    return _mm256_castsi256_ps(\n+                _mm256_or_si256(\n+                    _mm256_and_si256(\n+                        _mm256_castps_si256(x), mantissa_bits), exp_126_bits));\n+}\n+#endif\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n+avx512_get_full_load_mask(void)\n+{\n+    return 0xFFFF;\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n+avx512_get_partial_load_mask(const npy_int num_elem, const npy_int total_elem)\n+{\n+    return (0x0001 << num_elem) - 0x0001;\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n+avx512_masked_load(__mmask16 mask, npy_float* addr)\n+{\n+    return _mm512_maskz_loadu_ps(mask, (__m512 *)addr);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n+avx512_set_masked_lanes(__m512 x, __m512 val, __mmask16 mask)\n+{\n+    return _mm512_mask_blend_ps(mask, x, val);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n+avx512_blend(__m512 x, __m512 y, __mmask16 ymask)\n+{\n+    return _mm512_mask_mov_ps(x, ymask, y);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n+avx512_get_exponent(__m512 x)\n+{\n+    return _mm512_add_ps(_mm512_getexp_ps(x), _mm512_set1_ps(1.0f));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n+avx512_get_mantissa(__m512 x)\n+{\n+    return _mm512_getmant_ps(x, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_src);\n+}\n+#endif\n+\n+/**begin repeat\n+ * #ISA = AVX2, AVX512F#\n+ * #isa = avx2, avx512#\n+ * #vtype = __m256, __m512#\n+ * #vsize = 256, 512#\n+ * #or = or_ps, kor#\n+ * #vsub = , _mask#\n+ * #mask = __m256, __mmask16#\n+ * #fmadd = avx2_fmadd,_mm512_fmadd_ps#\n+ **/\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n+@isa@_range_reduction(@vtype@ x, @vtype@ y, @vtype@ c1, @vtype@ c2, @vtype@ c3)\n+{\n+    @vtype@ reduced_x = @fmadd@(y, c1, x);\n+    reduced_x = @fmadd@(y, c2, reduced_x);\n+    reduced_x = @fmadd@(y, c3, reduced_x);\n+    return reduced_x;\n+}\n+#endif\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #ISA = AVX2, AVX512F#\n+ * #isa = avx2, avx512#\n+ * #vtype = __m256, __m512#\n+ * #vsize = 256, 512#\n+ * #BYTES = 32, 64#\n+ * #mask = __m256, __mmask16#\n+ * #vsub = , _mask#\n+ * #and_masks =_mm256_and_ps, _mm512_kand#\n+ * #fmadd = avx2_fmadd,_mm512_fmadd_ps#\n+ * #mask_to_int = _mm256_movemask_ps, #\n+ * #full_mask= 0xFF, 0xFFFF#\n+ * #masked_store = _mm256_maskstore_ps, _mm512_mask_storeu_ps#\n+ * #cvtps_epi32 = _mm256_cvtps_epi32, #\n+ */\n+\n+\n+/*\n+ * Vectorized implementation of exp using AVX2 and AVX512:\n+ * 1) if x >= xmax; return INF (overflow)\n+ * 2) if x <= xmin; return 0.0f (underflow)\n+ * 3) Range reduction (using Coyd-Waite):\n+ *      a) y = x - k*ln(2); k = rint(x/ln(2)); y \\in [0, ln(2)]\n+ * 4) Compute exp(y) = P/Q, ratio of 2 polynomials P and Q\n+ *      b) P = 5th order and Q = 2nd order polynomials obtained from Remez's\n+ *      algorithm (mini-max polynomial approximation)\n+ * 5) Compute exp(x) = exp(y) * 2^k\n+ * 6) Max ULP error measured across all 32-bit FP's = 2.52 (x = 0xc2781e37)\n+ * 7) Max relative error measured across all 32-bit FP's= 2.1264E-07 (for the\n+ * same x = 0xc2781e37)\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS\n+static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n+@ISA@_exp_FLOAT(npy_float * op, npy_float * ip, const npy_int array_size)\n+{\n+    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    npy_float xmax = 88.72283935546875f;\n+    npy_float xmin = -87.3365478515625f;\n+\n+    /* Load up frequently used constants */\n+    @vtype@ codyw_c1 = _mm@vsize@_set1_ps(NPY_CODY_WAITE_LOGE_2_HIGHf);\n+    @vtype@ codyw_c2 = _mm@vsize@_set1_ps(NPY_CODY_WAITE_LOGE_2_LOWf);\n+    @vtype@ exp_p0 = _mm@vsize@_set1_ps(NPY_COEFF_P0_EXPf);\n+    @vtype@ exp_p1 = _mm@vsize@_set1_ps(NPY_COEFF_P1_EXPf);\n+    @vtype@ exp_p2 = _mm@vsize@_set1_ps(NPY_COEFF_P2_EXPf);\n+    @vtype@ exp_p3 = _mm@vsize@_set1_ps(NPY_COEFF_P3_EXPf);\n+    @vtype@ exp_p4 = _mm@vsize@_set1_ps(NPY_COEFF_P4_EXPf);\n+    @vtype@ exp_p5 = _mm@vsize@_set1_ps(NPY_COEFF_P5_EXPf);\n+    @vtype@ exp_q0 = _mm@vsize@_set1_ps(NPY_COEFF_Q0_EXPf);\n+    @vtype@ exp_q1 = _mm@vsize@_set1_ps(NPY_COEFF_Q1_EXPf);\n+    @vtype@ exp_q2 = _mm@vsize@_set1_ps(NPY_COEFF_Q2_EXPf);\n+    @vtype@ cvt_magic = _mm@vsize@_set1_ps(NPY_RINT_CVT_MAGICf);\n+    @vtype@ log2e = _mm@vsize@_set1_ps(NPY_LOG2Ef);\n+    @vtype@ inf = _mm@vsize@_set1_ps(NPY_INFINITYF);\n+    @vtype@ zeros_f = _mm@vsize@_set1_ps(0.0f);\n+    @vtype@ poly, num_poly, denom_poly, quadrant;\n+    @vtype@i exponent;\n+\n+    @mask@ xmax_mask, xmin_mask;\n+    @mask@ load_mask = @isa@_get_full_load_mask();\n+    npy_int num_remaining_elements = array_size;\n+\n+    while (num_remaining_elements > 0) {\n+\n+        if (num_remaining_elements < num_lanes)\n+            load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n+                                                         num_lanes);\n+        @vtype@ x  = @isa@_masked_load(load_mask, ip);\n+        xmax_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(xmax), _CMP_GE_OQ);\n+        xmin_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(xmin), _CMP_LE_OQ);\n+\n+        x = @isa@_set_masked_lanes(x, zeros_f,\n+                                   @and_masks@(xmax_mask,xmin_mask));\n+\n+        quadrant = _mm@vsize@_mul_ps(x, log2e);\n+\n+        /* round to nearest */\n+        quadrant = _mm@vsize@_add_ps(quadrant, cvt_magic);\n+        quadrant = _mm@vsize@_sub_ps(quadrant, cvt_magic);\n+\n+        /* Cody-Waite's range reduction algorithm */\n+        x = @isa@_range_reduction(x, quadrant,\n+                                  codyw_c1, codyw_c2, zeros_f);\n+\n+        num_poly = @fmadd@(exp_p5, x, exp_p4);\n+        num_poly = @fmadd@(num_poly, x, exp_p3);\n+        num_poly = @fmadd@(num_poly, x, exp_p2);\n+        num_poly = @fmadd@(num_poly, x, exp_p1);\n+        num_poly = @fmadd@(num_poly, x, exp_p0);\n+        denom_poly = @fmadd@(exp_q2, x, exp_q1);\n+        denom_poly = @fmadd@(denom_poly, x, exp_q0);\n+        poly = _mm@vsize@_div_ps(num_poly, denom_poly);\n+\n+        /*\n+         * compute val = poly * 2^quadrant; which is same as adding the\n+         * exponent of quadrant to the exponent of poly. quadrant is an int,\n+         * so extracting exponent is simply extracting 8 bits.\n+         */\n+        exponent = _mm@vsize@_slli_epi32(_mm@vsize@_cvtps_epi32(quadrant), 23);\n+        poly = _mm@vsize@_castsi@vsize@_ps(\n+                    _mm@vsize@_add_epi32(\n+                        _mm@vsize@_castps_si@vsize@(poly), exponent));\n+\n+        /* elem > xmax; return inf, elem < xmin; return 0.0f */\n+        poly = @isa@_set_masked_lanes(poly, inf, xmax_mask);\n+        poly = @isa@_set_masked_lanes(poly, zeros_f, xmin_mask);\n+\n+        @masked_store@(op, @cvtps_epi32@(load_mask), poly);\n+\n+        ip += num_lanes;\n+        op += num_lanes;\n+        num_remaining_elements -= num_lanes;\n+    }\n+}\n+\n+/*\n+ * Vectorized implementation of log using AVX2 and AVX512\n+ * 1) if x < 0.0f; return -NAN (invalid input)\n+ * 2) Range reduction: y = x/2^k;\n+ *      a) y = normalized mantissa, k is the exponent (0.5 <= y < 1)\n+ * 3) Compute log(y) = P/Q, ratio of 2 polynomials P and Q\n+ *      b) P = 5th order and Q = 5th order polynomials obtained from Remez's\n+ *      algorithm (mini-max polynomial approximation)\n+ * 5) Compute log(x) = log(y) + k*ln(2)\n+ * 6) Max ULP error measured across all 32-bit FP's = 3.83 (x = 0x3f486945)\n+ * 7) Max relative error measured across all 32-bit FP's = 2.359E-07 (for same\n+ * x = 0x3f486945)\n+ */\n+\n+static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n+@ISA@_log_FLOAT(npy_float * op, npy_float * ip, const npy_int array_size)\n+{\n+    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+\n+    /* Load up frequently used constants */\n+    @vtype@ log_p0 = _mm@vsize@_set1_ps(NPY_COEFF_P0_LOGf);\n+    @vtype@ log_p1 = _mm@vsize@_set1_ps(NPY_COEFF_P1_LOGf);\n+    @vtype@ log_p2 = _mm@vsize@_set1_ps(NPY_COEFF_P2_LOGf);\n+    @vtype@ log_p3 = _mm@vsize@_set1_ps(NPY_COEFF_P3_LOGf);\n+    @vtype@ log_p4 = _mm@vsize@_set1_ps(NPY_COEFF_P4_LOGf);\n+    @vtype@ log_p5 = _mm@vsize@_set1_ps(NPY_COEFF_P5_LOGf);\n+    @vtype@ log_q0 = _mm@vsize@_set1_ps(NPY_COEFF_Q0_LOGf);\n+    @vtype@ log_q1 = _mm@vsize@_set1_ps(NPY_COEFF_Q1_LOGf);\n+    @vtype@ log_q2 = _mm@vsize@_set1_ps(NPY_COEFF_Q2_LOGf);\n+    @vtype@ log_q3 = _mm@vsize@_set1_ps(NPY_COEFF_Q3_LOGf);\n+    @vtype@ log_q4 = _mm@vsize@_set1_ps(NPY_COEFF_Q4_LOGf);\n+    @vtype@ log_q5 = _mm@vsize@_set1_ps(NPY_COEFF_Q5_LOGf);\n+    @vtype@ loge2 = _mm@vsize@_set1_ps(NPY_LOGE2f);\n+    @vtype@ neg_nan = _mm@vsize@_set1_ps(-NPY_NANF);\n+    @vtype@ neg_inf = _mm@vsize@_set1_ps(-NPY_INFINITYF);\n+    @vtype@ zeros_f = _mm@vsize@_set1_ps(0.0f);\n+    @vtype@ ones_f = _mm@vsize@_set1_ps(1.0f);\n+    @vtype@ poly, num_poly, denom_poly, exponent;\n+\n+    @mask@ inf_nan_mask, sqrt2_mask, zero_mask, negx_mask;\n+    @mask@ load_mask = @isa@_get_full_load_mask();\n+    npy_int num_remaining_elements = array_size;\n+\n+    while (num_remaining_elements > 0) {\n+\n+        if (num_remaining_elements < num_lanes)\n+            load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n+                                                         num_lanes);\n+        @vtype@ x_in  = @isa@_masked_load(load_mask, ip);\n+\n+        negx_mask = _mm@vsize@_cmp_ps@vsub@(x_in, zeros_f, _CMP_LT_OQ);\n+        zero_mask = _mm@vsize@_cmp_ps@vsub@(x_in, zeros_f, _CMP_EQ_OQ);\n+        inf_nan_mask = _mm@vsize@_cmp_ps@vsub@(x_in, _mm@vsize@_set1_ps(FLT_MAX), _CMP_GT_OQ);\n+\n+        @vtype@ x = @isa@_set_masked_lanes(x_in, zeros_f, negx_mask);\n+\n+        /* set x = normalized mantissa */\n+        exponent = @isa@_get_exponent(x);\n+        x = @isa@_get_mantissa(x);\n+\n+        /* if x < sqrt(2) {exp = exp-1; x = 2*x} */\n+        sqrt2_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(NPY_SQRT1_2f), _CMP_LE_OQ);\n+        x = @isa@_blend(x, _mm@vsize@_add_ps(x,x), sqrt2_mask);\n+        exponent = @isa@_blend(exponent,\n+                               _mm@vsize@_sub_ps(exponent,ones_f), sqrt2_mask);\n+\n+        /* x = x - 1 */\n+        x = _mm@vsize@_sub_ps(x, ones_f);\n+\n+        /* Polynomial approximation for log(1+x) */\n+        num_poly = @fmadd@(log_p5, x, log_p4);\n+        num_poly = @fmadd@(num_poly, x, log_p3);\n+        num_poly = @fmadd@(num_poly, x, log_p2);\n+        num_poly = @fmadd@(num_poly, x, log_p1);\n+        num_poly = @fmadd@(num_poly, x, log_p0);\n+        denom_poly = @fmadd@(log_q5, x, log_q4);\n+        denom_poly = @fmadd@(denom_poly, x, log_q3);\n+        denom_poly = @fmadd@(denom_poly, x, log_q2);\n+        denom_poly = @fmadd@(denom_poly, x, log_q1);\n+        denom_poly = @fmadd@(denom_poly, x, log_q0);\n+        poly = _mm@vsize@_div_ps(num_poly, denom_poly);\n+        poly = @fmadd@(exponent, loge2, poly);\n+\n+        /*\n+         * x < 0.0f; return -NAN\n+         * x = 0.0f; return -INF\n+         * x > FLT_MAX; return x\n+         */\n+        poly = @isa@_set_masked_lanes(poly, neg_nan, negx_mask);\n+        poly = @isa@_set_masked_lanes(poly, neg_inf, zero_mask);\n+        poly = @isa@_set_masked_lanes(poly, x_in, inf_nan_mask);\n+\n+        @masked_store@(op, @cvtps_epi32@(load_mask), poly);\n+\n+        ip += num_lanes;\n+        op += num_lanes;\n+        num_remaining_elements -= num_lanes;\n+    }\n+}\n+#endif\n+/**end repeat**/\n+\n /*\n  *****************************************************************************\n  **                           BOOL LOOPS"
            },
            {
                "filename": "numpy/distutils/command/autodist.py",
                "patch": "@@ -78,6 +78,26 @@ def check_gcc_function_attribute(cmd, attribute, name):\n \"\"\" % (attribute, name)\n     return cmd.try_compile(body, None, None) != 0\n \n+def check_gcc_function_attribute_with_intrinsics(cmd, attribute, name, code,\n+                                                include):\n+    \"\"\"Return True if the given function attribute is supported with\n+    intrinsics.\"\"\"\n+    cmd._check_compiler()\n+    body = \"\"\"\n+#include<%s>\n+int %s %s(void)\n+{\n+    %s;\n+    return 0;\n+}\n+\n+int\n+main()\n+{\n+    return 0;\n+}\n+\"\"\" % (include, attribute, name, code)\n+    return cmd.try_compile(body, None, None) != 0\n def check_gcc_variable_attribute(cmd, attribute):\n     \"\"\"Return True if the given variable attribute is supported.\"\"\"\n     cmd._check_compiler()"
            },
            {
                "filename": "numpy/distutils/command/config.py",
                "patch": "@@ -18,6 +18,7 @@\n from numpy.distutils.exec_command import filepath_from_subprocess_output\n from numpy.distutils.mingw32ccompiler import generate_manifest\n from numpy.distutils.command.autodist import (check_gcc_function_attribute,\n+                                              check_gcc_function_attribute_with_intrinsics,\n                                               check_gcc_variable_attribute,\n                                               check_inline,\n                                               check_restrict,\n@@ -424,6 +425,11 @@ def check_compiler_gcc4(self):\n     def check_gcc_function_attribute(self, attribute, name):\n         return check_gcc_function_attribute(self, attribute, name)\n \n+    def check_gcc_function_attribute_with_intrinsics(self, attribute, name,\n+                                                     code, include):\n+        return check_gcc_function_attribute_with_intrinsics(self, attribute,\n+                                                            name, code, include)\n+\n     def check_gcc_variable_attribute(self, attribute):\n         return check_gcc_variable_attribute(self, attribute)\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 12459,
        "body": "In microbenchmarks which measured TSC cycles for SSE2, AVX2 and AVX512\r\nversions of the sqrt function, AVX2 and AVX512 showed a 1.99x and 3.6x\r\nperformance improvement over SSE2. The experiments were performed on\r\nIntel(R) Core(TM) i9-7900X CPU @ 3.30GHz. Detailed numbers are listed\r\nbelow:\r\n\r\n| Arraysize | Speed up with AVX2 | Speed up with AVX512 |\r\n|-----------|--------------------|----------------------|\r\n| 5000      | 1.99173            | 3.76                 |\r\n| 20000     | 1.99784            | 3.69                 |\r\n| 40000     | 1.99861            | 3.85                 |\r\n| 80000     | 1.99891            | 3.85                 |\r\n| 160000    | 1.99957            | 3.86                 |\r\n| 320000    | 1.99521            | 3.86                 |\r\n| 640000    | 1.99623            | 3.86                 |\r\n| 1280000   | 1.99966            | 3.81                 |\r\n| 2560000   | 1.96163            | 3.38                 |\r\n| 5120000   | 1.92656            | 3.24                 |\r\n| 10240000  | 1.92680            | 3.17                 |\r\n| 20480000  | 1.92289            | 3.12                 |\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/gitwash/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/gitwash/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -941,6 +941,45 @@ sse2_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy\n static void\n sse2_sqrt_@TYPE@(@type@ * op, @type@ * ip, const npy_intp n)\n {\n+#ifdef __AVX512F__\n+    /* align output to VECTOR_SIZE_BYTES bytes */\n+    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES) {\n+        op[i] = @scalarf@(ip[i]);\n+    }\n+    assert(n < (VECTOR_SIZE_BYTES / sizeof(@type@)) ||\n+\t    npy_is_aligned(&op[i], VECTOR_SIZE_BYTES));\n+    if (npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES)) {\n+        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n+            @vtype512@ d = @vpre512@_load_@vsuf@(&ip[i]);\n+            @vpre512@_store_@vsuf@(&op[i], @vpre512@_sqrt_@vsuf@(d));\n+        }\n+    }\n+    else {\n+        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n+            @vtype512@ d = @vpre512@_loadu_@vsuf@(&ip[i]);\n+            @vpre512@_store_@vsuf@(&op[i], @vpre512@_sqrt_@vsuf@(d));\n+        }\n+    }\n+#elif __AVX2__\n+    /* align output to VECTOR_SIZE_BYTES bytes */\n+    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES) {\n+        op[i] = @scalarf@(ip[i]);\n+    }\n+    assert(n < (VECTOR_SIZE_BYTES / sizeof(@type@)) ||\n+\t    npy_is_aligned(&op[i], VECTOR_SIZE_BYTES));\n+    if (npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES)) {\n+        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n+            @vtype256@ d = @vpre256@_load_@vsuf@(&ip[i]);\n+            @vpre256@_store_@vsuf@(&op[i], @vpre256@_sqrt_@vsuf@(d));\n+        }\n+    }\n+    else {\n+        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {\n+            @vtype256@ d = @vpre256@_loadu_@vsuf@(&ip[i]);\n+            @vpre256@_store_@vsuf@(&op[i], @vpre256@_sqrt_@vsuf@(d));\n+        }\n+    }\n+#else\n     /* align output to VECTOR_SIZE_BYTES bytes */\n     LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES) {\n         op[i] = @scalarf@(ip[i]);\n@@ -959,6 +998,7 @@ sse2_sqrt_@TYPE@(@type@ * op, @type@ * ip, const npy_intp n)\n             @vpre@_store_@vsuf@(&op[i], @vpre@_sqrt_@vsuf@(d));\n         }\n     }\n+#endif\n     LOOP_BLOCKED_END {\n         op[i] = @scalarf@(ip[i]);\n     }"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20133,
        "body": "AVX-512 speeds up quicksort by nearly **14x** for uniformly distributed random numbers. The ideas and code for this patch are based on these two research papers:\r\n\r\n1. Fast and Robust Vectorized In-Place Sorting of Primitive Types https://drops.dagstuhl.de/opus/volltexte/2021/13775/\r\n\r\n2. A Novel Hybrid Quicksort Algorithm Vectorized using AVX-512 on Intel Skylake https://arxiv.org/pdf/1704.08579.pdf\r\n\r\nHigh level idea: Vectorize the quicksort inline partitioning using AVX-512 _compressstore_ instructions. The pivot value is the median of 72 array elements chosen at random. If the array size is < 128, then use Bitonic sorting networks on ZMM registers which can parallelize a lot of the compare and swap required in a sorting algorithm. Good resource for bitonic sorting network: http://mitp-content-server.mit.edu:18180/books/content/sectbyfn?collid=books_pres_0&fn=Chapter%2027.pdf&id=8030\r\n\r\nBenchmarking details: \r\n\r\n```\r\n-      52.6\u00b10.1\u03bcs       36.2\u00b10.4\u03bcs     0.69  bench_function_base.Sort.time_sort('quick', 'uint32', ('ordered',))\r\n-     67.3\u00b10.03\u03bcs       35.4\u00b10.4\u03bcs     0.53  bench_function_base.Sort.time_sort('quick', 'int32', ('ordered',))\r\n-     85.8\u00b10.03\u03bcs       39.9\u00b10.3\u03bcs     0.46  bench_function_base.Sort.time_sort('quick', 'float32', ('ordered',))\r\n-     84.0\u00b10.03\u03bcs       36.5\u00b10.4\u03bcs     0.43  bench_function_base.Sort.time_sort('quick', 'uint32', ('reversed',))\r\n-      105\u00b10.06\u03bcs       35.7\u00b10.3\u03bcs     0.34  bench_function_base.Sort.time_sort('quick', 'int32', ('reversed',))\r\n-       141\u00b10.1\u03bcs       40.4\u00b10.4\u03bcs     0.29  bench_function_base.Sort.time_sort('quick', 'float32', ('reversed',))\r\n-       285\u00b10.3\u03bcs       38.3\u00b10.2\u03bcs     0.13  bench_function_base.Sort.time_sort('quick', 'uint32', ('sorted_block', 1000))\r\n-       377\u00b10.3\u03bcs       42.7\u00b10.1\u03bcs     0.11  bench_function_base.Sort.time_sort('quick', 'uint32', ('sorted_block', 100))\r\n-      325\u00b10.07\u03bcs       36.6\u00b10.2\u03bcs     0.11  bench_function_base.Sort.time_sort('quick', 'int32', ('sorted_block', 1000))\r\n-       370\u00b10.1\u03bcs       40.4\u00b10.2\u03bcs     0.11  bench_function_base.Sort.time_sort('quick', 'float32', ('sorted_block', 1000))\r\n-       384\u00b10.1\u03bcs       38.0\u00b10.2\u03bcs     0.10  bench_function_base.Sort.time_sort('quick', 'uint32', ('sorted_block', 10))\r\n-       416\u00b10.3\u03bcs       38.3\u00b10.1\u03bcs     0.09  bench_function_base.Sort.time_sort('quick', 'int32', ('sorted_block', 100))\r\n-       458\u00b10.2\u03bcs       41.0\u00b10.6\u03bcs     0.09  bench_function_base.Sort.time_sort('quick', 'uint32', ('random',))\r\n-      476\u00b10.09\u03bcs       42.1\u00b10.1\u03bcs     0.09  bench_function_base.Sort.time_sort('quick', 'float32', ('sorted_block', 100))\r\n-       424\u00b10.2\u03bcs      36.1\u00b10.05\u03bcs     0.09  bench_function_base.Sort.time_sort('quick', 'int32', ('sorted_block', 10))\r\n-       487\u00b10.2\u03bcs       40.7\u00b10.2\u03bcs     0.08  bench_function_base.Sort.time_sort('quick', 'float32', ('sorted_block', 10))\r\n-       495\u00b10.2\u03bcs       37.9\u00b10.1\u03bcs     0.08  bench_function_base.Sort.time_sort('quick', 'int32', ('random',))\r\n-     61.5\u00b10.02\u03bcs       4.69\u00b10.2\u03bcs     0.08  bench_function_base.Sort.time_sort('quick', 'uint32', ('uniform',))\r\n-       574\u00b10.2\u03bcs       41.7\u00b10.6\u03bcs     0.07  bench_function_base.Sort.time_sort('quick', 'float32', ('random',))\r\n-      95.0\u00b10.3\u03bcs       4.65\u00b10.1\u03bcs     0.05  bench_function_base.Sort.time_sort('quick', 'int32', ('uniform',))\r\n-       139\u00b10.1\u03bcs       5.27\u00b10.2\u03bcs     0.04  bench_function_base.Sort.time_sort('quick', 'float32', ('uniform',))\r\n```\r\n\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_function_base.py",
                "patch": "@@ -222,7 +222,7 @@ class Sort(Benchmark):\n         # In NumPy 1.17 and newer, 'merge' can be one of several\n         # stable sorts, it isn't necessarily merge sort.\n         ['quick', 'merge', 'heap'],\n-        ['float64', 'int64', 'int16'],\n+        ['float64', 'int64', 'float32', 'uint32', 'int32', 'int16'],\n         [\n             ('random',),\n             ('ordered',),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -946,6 +946,7 @@ def gl_if_msvc(build_cmd):\n             join('src', 'multiarray', 'usertypes.c'),\n             join('src', 'multiarray', 'vdot.c'),\n             join('src', 'common', 'npy_sort.h.src'),\n+            join('src', 'npysort', 'x86-qsort.dispatch.c.src'),\n             join('src', 'npysort', 'quicksort.c.src'),\n             join('src', 'npysort', 'mergesort.c.src'),\n             join('src', 'npysort', 'timsort.c.src'),"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/arithmetic.h",
                "patch": "@@ -371,7 +371,79 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n     #define npyv_sum_u64 _mm512_reduce_add_epi64\n     #define npyv_sum_f32 _mm512_reduce_add_ps\n     #define npyv_sum_f64 _mm512_reduce_add_pd\n+    #define npyv_reducemin_u32 _mm512_reduce_min_epu32\n+    #define npyv_reducemin_s32 _mm512_reduce_min_epi32\n+    #define npyv_reducemin_f32 _mm512_reduce_min_ps\n+    #define npyv_reducemax_u32 _mm512_reduce_max_epu32\n+    #define npyv_reducemax_s32 _mm512_reduce_max_epi32\n+    #define npyv_reducemax_f32 _mm512_reduce_max_ps\n #else\n+    NPY_FINLINE npy_uint32 npyv_reducemax_u32(npyv_u32 a)\n+    {\n+        const npyv_u32 idx1 = _mm512_set_epi32(7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);\n+        const npyv_u32 idx2 = _mm512_set_epi32(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);\n+        npyv_u32 a1 = _mm512_max_epu32(a, _mm512_permutex2var_epi32(a, idx1, a));\n+        npyv_u32 a2 = _mm512_max_epu32(a1, _mm512_permutex2var_epi32(a1, idx2, a1));\n+        npyv_u32 a3 = _mm512_max_epu32(a2, _mm512_shuffle_epi32(a2, (1<<6 | 0<<4 | 3<<2 | 2)));\n+        npyv_u32 a4 = _mm512_max_epu32(a3, _mm512_shuffle_epi32(a3, (2<<6 | 3<<4 | 0<<2 | 1)));\n+        return _mm_cvtsi128_si32(_mm512_extracti32x4_epi32(a4, 0x00));\n+    }\n+\n+    NPY_FINLINE npy_int32 npyv_reducemax_s32(npyv_s32 a)\n+    {\n+        const npyv_u32 idx1 = _mm512_set_epi32(7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);\n+        const npyv_u32 idx2 = _mm512_set_epi32(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);\n+        npyv_s32 a1 = _mm512_max_epi32(a, _mm512_permutex2var_epi32(a, idx1, a));\n+        npyv_s32 a2 = _mm512_max_epi32(a1, _mm512_permutex2var_epi32(a1, idx2, a1));\n+        npyv_s32 a3 = _mm512_max_epi32(a2, _mm512_shuffle_epi32(a2, (1<<6 | 0<<4 | 3<<2 | 2)));\n+        npyv_s32 a4 = _mm512_max_epi32(a3, _mm512_shuffle_epi32(a3, (2<<6 | 3<<4 | 0<<2 | 1)));\n+        return _mm_cvtsi128_si32(_mm512_extracti32x4_epi32(a4, 0x00));\n+    }\n+\n+    NPY_FINLINE npy_float npyv_reducemax_f32(npyv_f32 a)\n+    {\n+        const npyv_u32 idx1 = _mm512_set_epi32(7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);\n+        const npyv_u32 idx2 = _mm512_set_epi32(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);\n+        npyv_f32 a1 = _mm512_max_ps(a, _mm512_permutex2var_ps(a, idx1, a));\n+        npyv_f32 a2 = _mm512_max_ps(a1, _mm512_permutex2var_ps(a1, idx2, a1));\n+        npyv_f32 a3 = _mm512_max_ps(a2, _mm512_shuffle_ps(a2, a2, (1<<6 | 0<<4 | 3<<2 | 2)));\n+        npyv_f32 a4 = _mm512_max_ps(a3, _mm512_shuffle_sp(a3, a3, (2<<6 | 3<<4 | 0<<2 | 1)));\n+        return _mm_cvtss_f32(_mm512_extractf32x4_ps(a4, 0x00));\n+    }\n+\n+    NPY_FINLINE npy_uint32 npyv_reducemin_u32(npyv_u32 a)\n+    {\n+        const npyv_u32 idx1 = _mm512_set_epi32(7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);\n+        const npyv_u32 idx2 = _mm512_set_epi32(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);\n+        npyv_u32 a1 = _mm512_min_epu32(a, _mm512_permutex2var_epi32(a, idx1, a));\n+        npyv_u32 a2 = _mm512_min_epu32(a1, _mm512_permutex2var_epi32(a1, idx2, a1));\n+        npyv_u32 a3 = _mm512_min_epu32(a2, _mm512_shuffle_epi32(a2, (1<<6 | 0<<4 | 3<<2 | 2)));\n+        npyv_u32 a4 = _mm512_min_epu32(a3, _mm512_shuffle_epi32(a3, (2<<6 | 3<<4 | 0<<2 | 1)));\n+        return _mm_cvtsi128_si32(_mm512_extracti32x4_epi32(a4, 0x00));\n+    }\n+\n+    NPY_FINLINE npy_int32 npyv_reducemin_s32(npyv_s32 a)\n+    {\n+        const npyv_u32 idx1 = _mm512_set_epi32(7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);\n+        const npyv_u32 idx2 = _mm512_set_epi32(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);\n+        npyv_s32 a1 = _mm512_min_epi32(a, _mm512_permutex2var_epi32(a, idx1, a));\n+        npyv_s32 a2 = _mm512_min_epi32(a1, _mm512_permutex2var_epi32(a1, idx2, a1));\n+        npyv_s32 a3 = _mm512_min_epi32(a2, _mm512_shuffle_epi32(a2, (1<<6 | 0<<4 | 3<<2 | 2)));\n+        npyv_s32 a4 = _mm512_min_epi32(a3, _mm512_shuffle_epi32(a3, (2<<6 | 3<<4 | 0<<2 | 1)));\n+        return _mm_cvtsi128_si32(_mm512_extracti32x4_epi32(a4, 0x00));\n+    }\n+\n+    NPY_FINLINE npy_float npyv_reducemin_f32(npyv_f32 a)\n+    {\n+        const npyv_u32 idx1 = _mm512_set_epi32(7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);\n+        const npyv_u32 idx2 = _mm512_set_epi32(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);\n+        npyv_f32 a1 = _mm512_min_ps(a, _mm512_permutex2var_ps(a, idx1, a));\n+        npyv_f32 a2 = _mm512_min_ps(a1, _mm512_permutex2var_ps(a1, idx2, a1));\n+        npyv_f32 a3 = _mm512_min_ps(a2, _mm512_shuffle_ps(a2, a2, (1<<6 | 0<<4 | 3<<2 | 2)));\n+        npyv_f32 a4 = _mm512_min_ps(a3, _mm512_shuffle_sp(a3, a3, (2<<6 | 3<<4 | 0<<2 | 1)));\n+        return _mm_cvtss_f32(_mm512_extractf32x4_ps(a4, 0x00));\n+    }\n+\n     NPY_FINLINE npy_uint32 npyv_sum_u32(npyv_u32 a)\n     {\n         __m256i half = _mm256_add_epi32(npyv512_lower_si256(a), npyv512_higher_si256(a));"
            },
            {
                "filename": "numpy/core/src/npysort/quicksort.c.src",
                "patch": "@@ -51,8 +51,14 @@\n \n #include \"npy_sort.h\"\n #include \"npysort_common.h\"\n+#include \"npy_cpu_features.h\"\n+#include \"x86-qsort.h\"\n #include <stdlib.h>\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"x86-qsort.dispatch.h\"\n+#endif\n+\n #define NOT_USED NPY_UNUSED(unused)\n /*\n  * pushing largest partition has upper bound of log2(n) space\n@@ -83,11 +89,22 @@\n  *         npy_uint, npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n  *         npy_ushort, npy_float, npy_double, npy_longdouble, npy_cfloat,\n  *         npy_cdouble, npy_clongdouble, npy_datetime, npy_timedelta#\n+ * #AVX512 = 0*5, 1, 1, 0*5, 1, 0*7#\n  */\n \n NPY_NO_EXPORT int\n quicksort_@suff@(void *start, npy_intp num, void *NOT_USED)\n {\n+\n+#if @AVX512@\n+    void (*dispfunc)(void*, npy_intp) = NULL;\n+    NPY_CPU_DISPATCH_CALL_XB(dispfunc = &x86_quicksort_@suff@);\n+    if (dispfunc) {\n+       (*dispfunc)(start, num);\n+       return 0;\n+    }\n+#endif\n+\n     @type@ vp;\n     @type@ *pl = start;\n     @type@ *pr = pl + num - 1;"
            },
            {
                "filename": "numpy/core/src/npysort/x86-qsort.dispatch.c.src",
                "patch": "@@ -0,0 +1,587 @@\n+/*@targets\n+ * $maxopt $keep_baseline avx512_skx\n+ */\n+// policy $keep_baseline is used to avoid skip building avx512_skx\n+// when its part of baseline features (--cpu-baseline), since\n+// 'baseline' option isn't specified within targets.\n+\n+#include \"x86-qsort.h\"\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#ifdef NPY_HAVE_AVX512_SKX\n+#include <immintrin.h>\n+#include \"numpy/npy_math.h\"\n+#include \"npy_sort.h\"\n+#include \"simd/simd.h\"\n+\n+\n+/*\n+ * Quicksort using AVX-512 for int, uint32 and float. The ideas and code are\n+ * based on these two research papers:\n+ * (1) Fast and Robust Vectorized In-Place Sorting of Primitive Types\n+ *     https://drops.dagstuhl.de/opus/volltexte/2021/13775/\n+ * (2) A Novel Hybrid Quicksort Algorithm Vectorized using AVX-512 on Intel Skylake\n+ *     https://arxiv.org/pdf/1704.08579.pdf\n+ *\n+ * High level idea: Vectorize the quicksort partitioning using AVX-512\n+ * compressstore instructions. The algorithm to pick the pivot is to use median of\n+ * 72 elements picked at random. If the array size is < 128, then use\n+ * Bitonic sorting network. Good resource for bitonic sorting network:\n+ * http://mitp-content-server.mit.edu:18180/books/content/sectbyfn?collid=books_pres_0&fn=Chapter%2027.pdf&id=8030\n+ *\n+ * Refer to https://github.com/numpy/numpy/pull/20133#issuecomment-958110340 for\n+ * potential problems when converting this code to universal intrinsics framework.\n+ */\n+\n+/*\n+ * Constants used in sorting 16 elements in a ZMM registers. Based on Bitonic\n+ * sorting network (see\n+ * https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort.svg)\n+ */\n+#define NETWORK1 14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1\n+#define NETWORK2 12,13,14,15,8,9,10,11,4,5,6,7,0,1,2,3\n+#define NETWORK3 8,9,10,11,12,13,14,15,0,1,2,3,4,5,6,7\n+#define NETWORK4 13,12,15,14,9,8,11,10,5,4,7,6,1,0,3,2\n+#define NETWORK5 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n+#define NETWORK6 11,10,9,8,15,14,13,12,3,2,1,0,7,6,5,4\n+#define NETWORK7 7,6,5,4,3,2,1,0,15,14,13,12,11,10,9,8\n+#define ZMM_MAX_FLOAT _mm512_set1_ps(NPY_INFINITYF)\n+#define ZMM_MAX_UINT _mm512_set1_epi32(NPY_MAX_UINT32)\n+#define ZMM_MAX_INT _mm512_set1_epi32(NPY_MAX_INT32)\n+#define SHUFFLE_MASK(a,b,c,d) (a << 6) | (b << 4) | (c << 2) | d\n+#define SHUFFLE_ps(ZMM, MASK) _mm512_shuffle_ps(zmm, zmm, MASK)\n+#define SHUFFLE_epi32(ZMM, MASK) _mm512_shuffle_epi32(zmm, MASK)\n+\n+#define MAX(x, y) (((x) > (y)) ? (x) : (y))\n+#define MIN(x, y) (((x) < (y)) ? (x) : (y))\n+\n+/*\n+ * Vectorized random number generator xoroshiro128+. Broken into 2 parts:\n+ * (1) vnext generates 2 64-bit random integers\n+ * (2) rnd_epu32 converts this to 4 32-bit random integers and bounds it to\n+ *     the length of the array\n+ */\n+#define VROTL(x, k) /* rotate each uint64_t value in vector */               \\\n+    _mm256_or_si256(_mm256_slli_epi64((x),(k)),_mm256_srli_epi64((x),64-(k)))\n+\n+static NPY_INLINE\n+__m256i vnext(__m256i* s0, __m256i* s1) {\n+    *s1 = _mm256_xor_si256(*s0, *s1); /* modify vectors s1 and s0 */\n+    *s0 = _mm256_xor_si256(_mm256_xor_si256(VROTL(*s0, 24), *s1),\n+                           _mm256_slli_epi64(*s1, 16));\n+    *s1 = VROTL(*s1, 37);\n+    return _mm256_add_epi64(*s0, *s1); /* return random vector */\n+}\n+\n+/* transform random numbers to the range between 0 and bound - 1 */\n+static NPY_INLINE\n+__m256i rnd_epu32(__m256i rnd_vec, __m256i bound) {\n+    __m256i even = _mm256_srli_epi64(_mm256_mul_epu32(rnd_vec, bound), 32);\n+    __m256i odd = _mm256_mul_epu32(_mm256_srli_epi64(rnd_vec, 32), bound);\n+    return _mm256_blend_epi32(odd, even, 0b01010101);\n+}\n+\n+/**begin repeat\n+ *\n+ * #TYPE = INT, UINT, FLOAT#\n+ * #type = int, uint, float#\n+ * #type_t = npy_int, npy_uint, npy_float#\n+ * #zmm_t = __m512i, __m512i, __m512#\n+ * #ymm_t = __m256i, __m256i, __m256#\n+ * #vsuf1 = epi32, epu32, ps#\n+ * #vsuf2 = epi32, epi32, ps#\n+ * #vsuf3 = si512, si512, ps#\n+ * #vsuf4 = s32, u32, f32#\n+ * #CMP_GE_OP = _MM_CMPINT_NLT, _MM_CMPINT_NLT, _CMP_GE_OQ#\n+ * #TYPE_MAX_VAL = NPY_MAX_INT32, NPY_MAX_UINT32, NPY_INFINITYF#\n+ * #TYPE_MIN_VAL = NPY_MIN_INT32, 0, -NPY_INFINITYF#\n+ */\n+\n+/*\n+ * COEX == Compare and Exchange two registers by swapping min and max values\n+ */\n+#define COEX_ZMM_@vsuf1@(a, b) {                                          \\\n+    @zmm_t@ temp = a;                                                     \\\n+    a = _mm512_min_@vsuf1@(a,b);                                          \\\n+    b = _mm512_max_@vsuf1@(temp, b);}                                     \\\n+\n+#define COEX_YMM_@vsuf1@(a, b){                                           \\\n+    @ymm_t@ temp = a;                                                     \\\n+    a = _mm256_min_@vsuf1@(a, b);                                         \\\n+    b = _mm256_max_@vsuf1@(temp, b);}                                     \\\n+\n+static NPY_INLINE\n+@zmm_t@ cmp_merge_@vsuf1@(@zmm_t@ in1, @zmm_t@ in2, __mmask16 mask)\n+{\n+    @zmm_t@ min = _mm512_min_@vsuf1@(in2, in1);\n+    @zmm_t@ max = _mm512_max_@vsuf1@(in2, in1);\n+    return _mm512_mask_mov_@vsuf2@(min, mask, max); // 0 -> min, 1 -> max\n+}\n+\n+/*\n+ * Assumes zmm is random and performs a full sorting network defined in\n+ * https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort.svg\n+ */\n+static NPY_INLINE\n+@zmm_t@ sort_zmm_@vsuf1@(@zmm_t@ zmm)\n+{\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(2,3,0,1)), 0xAAAA);\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(0,1,2,3)), 0xCCCC);\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(2,3,0,1)), 0xAAAA);\n+    zmm = cmp_merge_@vsuf1@(zmm, _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK3),zmm), 0xF0F0);\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(1,0,3,2)), 0xCCCC);\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(2,3,0,1)), 0xAAAA);\n+    zmm = cmp_merge_@vsuf1@(zmm, _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5),zmm), 0xFF00);\n+    zmm = cmp_merge_@vsuf1@(zmm, _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK6),zmm), 0xF0F0);\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(1,0,3,2)), 0xCCCC);\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(2,3,0,1)), 0xAAAA);\n+    return zmm;\n+}\n+\n+// Assumes zmm is bitonic and performs a recursive half cleaner\n+static NPY_INLINE\n+@zmm_t@ bitonic_merge_zmm_@vsuf1@(@zmm_t@ zmm)\n+{\n+    // 1) half_cleaner[16]: compare 1-9, 2-10, 3-11 etc ..\n+    zmm = cmp_merge_@vsuf1@(zmm, _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK7),zmm), 0xFF00);\n+    // 2) half_cleaner[8]: compare 1-5, 2-6, 3-7 etc ..\n+    zmm = cmp_merge_@vsuf1@(zmm, _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK6),zmm), 0xF0F0);\n+    // 3) half_cleaner[4]\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(1,0,3,2)), 0xCCCC);\n+    // 3) half_cleaner[1]\n+    zmm = cmp_merge_@vsuf1@(zmm, SHUFFLE_@vsuf2@(zmm, SHUFFLE_MASK(2,3,0,1)), 0xAAAA);\n+    return zmm;\n+}\n+\n+// Assumes zmm1 and zmm2 are sorted and performs a recursive half cleaner\n+static NPY_INLINE\n+void bitonic_merge_two_zmm_@vsuf1@(@zmm_t@* zmm1, @zmm_t@* zmm2)\n+{\n+    // 1) First step of a merging network: coex of zmm1 and zmm2 reversed\n+    *zmm2 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), *zmm2);\n+    @zmm_t@ zmm3 = _mm512_min_@vsuf1@(*zmm1, *zmm2);\n+    @zmm_t@ zmm4 = _mm512_max_@vsuf1@(*zmm1, *zmm2);\n+    // 2) Recursive half cleaner for each\n+    *zmm1 = bitonic_merge_zmm_@vsuf1@(zmm3);\n+    *zmm2 = bitonic_merge_zmm_@vsuf1@(zmm4);\n+}\n+\n+// Assumes [zmm0, zmm1] and [zmm2, zmm3] are sorted and performs a recursive half cleaner\n+static NPY_INLINE\n+void bitonic_merge_four_zmm_@vsuf1@(@zmm_t@* zmm)\n+{\n+    @zmm_t@ zmm2r = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), zmm[2]);\n+    @zmm_t@ zmm3r = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), zmm[3]);\n+    @zmm_t@ zmm_t1 = _mm512_min_@vsuf1@(zmm[0], zmm3r);\n+    @zmm_t@ zmm_t2 = _mm512_min_@vsuf1@(zmm[1], zmm2r);\n+    @zmm_t@ zmm_t3 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), _mm512_max_@vsuf1@(zmm[1], zmm2r));\n+    @zmm_t@ zmm_t4 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), _mm512_max_@vsuf1@(zmm[0], zmm3r));\n+    @zmm_t@ zmm0 = _mm512_min_@vsuf1@(zmm_t1, zmm_t2);\n+    @zmm_t@ zmm1 = _mm512_max_@vsuf1@(zmm_t1, zmm_t2);\n+    @zmm_t@ zmm2 = _mm512_min_@vsuf1@(zmm_t3, zmm_t4);\n+    @zmm_t@ zmm3 = _mm512_max_@vsuf1@(zmm_t3, zmm_t4);\n+    zmm[0] = bitonic_merge_zmm_@vsuf1@(zmm0);\n+    zmm[1] = bitonic_merge_zmm_@vsuf1@(zmm1);\n+    zmm[2] = bitonic_merge_zmm_@vsuf1@(zmm2);\n+    zmm[3] = bitonic_merge_zmm_@vsuf1@(zmm3);\n+}\n+\n+static NPY_INLINE\n+void bitonic_merge_eight_zmm_@vsuf1@(@zmm_t@* zmm)\n+{\n+    @zmm_t@ zmm4r = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), zmm[4]);\n+    @zmm_t@ zmm5r = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), zmm[5]);\n+    @zmm_t@ zmm6r = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), zmm[6]);\n+    @zmm_t@ zmm7r = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), zmm[7]);\n+    @zmm_t@ zmm_t1 = _mm512_min_@vsuf1@(zmm[0], zmm7r);\n+    @zmm_t@ zmm_t2 = _mm512_min_@vsuf1@(zmm[1], zmm6r);\n+    @zmm_t@ zmm_t3 = _mm512_min_@vsuf1@(zmm[2], zmm5r);\n+    @zmm_t@ zmm_t4 = _mm512_min_@vsuf1@(zmm[3], zmm4r);\n+    @zmm_t@ zmm_t5 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), _mm512_max_@vsuf1@(zmm[3], zmm4r));\n+    @zmm_t@ zmm_t6 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), _mm512_max_@vsuf1@(zmm[2], zmm5r));\n+    @zmm_t@ zmm_t7 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), _mm512_max_@vsuf1@(zmm[1], zmm6r));\n+    @zmm_t@ zmm_t8 = _mm512_permutexvar_@vsuf2@(_mm512_set_epi32(NETWORK5), _mm512_max_@vsuf1@(zmm[0], zmm7r));\n+    COEX_ZMM_@vsuf1@(zmm_t1, zmm_t3);\n+    COEX_ZMM_@vsuf1@(zmm_t2, zmm_t4);\n+    COEX_ZMM_@vsuf1@(zmm_t5, zmm_t7);\n+    COEX_ZMM_@vsuf1@(zmm_t6, zmm_t8);\n+    COEX_ZMM_@vsuf1@(zmm_t1, zmm_t2);\n+    COEX_ZMM_@vsuf1@(zmm_t3, zmm_t4);\n+    COEX_ZMM_@vsuf1@(zmm_t5, zmm_t6);\n+    COEX_ZMM_@vsuf1@(zmm_t7, zmm_t8);\n+    zmm[0] = bitonic_merge_zmm_@vsuf1@(zmm_t1);\n+    zmm[1] = bitonic_merge_zmm_@vsuf1@(zmm_t2);\n+    zmm[2] = bitonic_merge_zmm_@vsuf1@(zmm_t3);\n+    zmm[3] = bitonic_merge_zmm_@vsuf1@(zmm_t4);\n+    zmm[4] = bitonic_merge_zmm_@vsuf1@(zmm_t5);\n+    zmm[5] = bitonic_merge_zmm_@vsuf1@(zmm_t6);\n+    zmm[6] = bitonic_merge_zmm_@vsuf1@(zmm_t7);\n+    zmm[7] = bitonic_merge_zmm_@vsuf1@(zmm_t8);\n+}\n+\n+static NPY_INLINE\n+void sort_16_@vsuf1@(@type_t@* arr, npy_int N)\n+{\n+    __mmask16 load_mask = (0x0001 << N) - 0x0001;\n+    @zmm_t@ zmm = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask, arr);\n+    _mm512_mask_storeu_@vsuf2@(arr, load_mask, sort_zmm_@vsuf1@(zmm));\n+}\n+\n+static NPY_INLINE\n+void sort_32_@vsuf1@(@type_t@* arr, npy_int N)\n+{\n+    if (N <= 16) {\n+        sort_16_@vsuf1@(arr, N);\n+        return;\n+    }\n+    @zmm_t@ zmm1 = _mm512_loadu_@vsuf3@(arr);\n+    __mmask16 load_mask = (0x0001 << (N-16)) - 0x0001;\n+    @zmm_t@ zmm2 = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask, arr + 16);\n+    zmm1 = sort_zmm_@vsuf1@(zmm1);\n+    zmm2 = sort_zmm_@vsuf1@(zmm2);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm1, &zmm2);\n+    _mm512_storeu_@vsuf3@(arr, zmm1);\n+    _mm512_mask_storeu_@vsuf2@(arr + 16, load_mask, zmm2);\n+}\n+\n+static NPY_INLINE\n+void sort_64_@vsuf1@(@type_t@* arr, npy_int N)\n+{\n+    if (N <= 32) {\n+        sort_32_@vsuf1@(arr, N);\n+        return;\n+    }\n+    @zmm_t@ zmm[4];\n+    zmm[0] = _mm512_loadu_@vsuf3@(arr);\n+    zmm[1] = _mm512_loadu_@vsuf3@(arr + 16);\n+    __mmask16 load_mask1 = 0xFFFF, load_mask2 = 0xFFFF;\n+    if (N < 48) {\n+        load_mask1 = (0x0001 << (N-32)) - 0x0001;\n+        load_mask2 = 0x0000;\n+    }\n+    else if (N < 64) {\n+        load_mask2 = (0x0001 << (N-48)) - 0x0001;\n+    }\n+    zmm[2] = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask1, arr + 32);\n+    zmm[3] = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask2, arr + 48);\n+    zmm[0] = sort_zmm_@vsuf1@(zmm[0]);\n+    zmm[1] = sort_zmm_@vsuf1@(zmm[1]);\n+    zmm[2] = sort_zmm_@vsuf1@(zmm[2]);\n+    zmm[3] = sort_zmm_@vsuf1@(zmm[3]);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm[0], &zmm[1]);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm[2], &zmm[3]);\n+    bitonic_merge_four_zmm_@vsuf1@(zmm);\n+    _mm512_storeu_@vsuf3@(arr, zmm[0]);\n+    _mm512_storeu_@vsuf3@(arr + 16, zmm[1]);\n+    _mm512_mask_storeu_@vsuf2@(arr + 32, load_mask1, zmm[2]);\n+    _mm512_mask_storeu_@vsuf2@(arr + 48, load_mask2, zmm[3]);\n+}\n+\n+static NPY_INLINE\n+void sort_128_@vsuf1@(@type_t@* arr, npy_int N)\n+{\n+    if (N <= 64) {\n+        sort_64_@vsuf1@(arr, N);\n+        return;\n+    }\n+    @zmm_t@ zmm[8];\n+    zmm[0] = _mm512_loadu_@vsuf3@(arr);\n+    zmm[1] = _mm512_loadu_@vsuf3@(arr + 16);\n+    zmm[2] = _mm512_loadu_@vsuf3@(arr + 32);\n+    zmm[3] = _mm512_loadu_@vsuf3@(arr + 48);\n+    zmm[0] = sort_zmm_@vsuf1@(zmm[0]);\n+    zmm[1] = sort_zmm_@vsuf1@(zmm[1]);\n+    zmm[2] = sort_zmm_@vsuf1@(zmm[2]);\n+    zmm[3] = sort_zmm_@vsuf1@(zmm[3]);\n+    __mmask16 load_mask1 = 0xFFFF, load_mask2 = 0xFFFF;\n+    __mmask16 load_mask3 = 0xFFFF, load_mask4 = 0xFFFF;\n+    if (N < 80) {\n+        load_mask1 = (0x0001 << (N-64)) - 0x0001;\n+        load_mask2 = 0x0000;\n+        load_mask3 = 0x0000;\n+        load_mask4 = 0x0000;\n+    }\n+    else if (N < 96) {\n+        load_mask2 = (0x0001 << (N-80)) - 0x0001;\n+        load_mask3 = 0x0000;\n+        load_mask4 = 0x0000;\n+    }\n+    else if (N < 112) {\n+        load_mask3 = (0x0001 << (N-96)) - 0x0001;\n+        load_mask4 = 0x0000;\n+    }\n+    else {\n+        load_mask4 = (0x0001 << (N-112)) - 0x0001;\n+    }\n+    zmm[4] = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask1, arr + 64);\n+    zmm[5] = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask2, arr + 80);\n+    zmm[6] = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask3, arr + 96);\n+    zmm[7] = _mm512_mask_loadu_@vsuf2@(ZMM_MAX_@TYPE@, load_mask4, arr + 112);\n+    zmm[4] = sort_zmm_@vsuf1@(zmm[4]);\n+    zmm[5] = sort_zmm_@vsuf1@(zmm[5]);\n+    zmm[6] = sort_zmm_@vsuf1@(zmm[6]);\n+    zmm[7] = sort_zmm_@vsuf1@(zmm[7]);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm[0], &zmm[1]);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm[2], &zmm[3]);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm[4], &zmm[5]);\n+    bitonic_merge_two_zmm_@vsuf1@(&zmm[6], &zmm[7]);\n+    bitonic_merge_four_zmm_@vsuf1@(zmm);\n+    bitonic_merge_four_zmm_@vsuf1@(zmm + 4);\n+    bitonic_merge_eight_zmm_@vsuf1@(zmm);\n+    _mm512_storeu_@vsuf3@(arr, zmm[0]);\n+    _mm512_storeu_@vsuf3@(arr + 16, zmm[1]);\n+    _mm512_storeu_@vsuf3@(arr + 32, zmm[2]);\n+    _mm512_storeu_@vsuf3@(arr + 48, zmm[3]);\n+    _mm512_mask_storeu_@vsuf2@(arr + 64, load_mask1, zmm[4]);\n+    _mm512_mask_storeu_@vsuf2@(arr + 80, load_mask2, zmm[5]);\n+    _mm512_mask_storeu_@vsuf2@(arr + 96, load_mask3, zmm[6]);\n+    _mm512_mask_storeu_@vsuf2@(arr + 112, load_mask4, zmm[7]);\n+}\n+\n+\n+static NPY_INLINE\n+void swap_@TYPE@(@type_t@ *arr, npy_intp ii, npy_intp jj) {\n+    @type_t@ temp = arr[ii];\n+    arr[ii] = arr[jj];\n+    arr[jj] = temp;\n+}\n+\n+// Median of 3 stratergy\n+//static NPY_INLINE\n+//npy_intp get_pivot_index(@type_t@ *arr, const npy_intp left, const npy_intp right) {\n+//    return (rand() % (right + 1 - left)) + left;\n+//    //npy_intp middle = ((right-left)/2) + left;\n+//    //@type_t@ a = arr[left], b = arr[middle], c = arr[right];\n+//    //if ((b >= a && b <= c) || (b <= a && b >= c))\n+//    //    return middle;\n+//    //if ((a >= b && a <= c) || (a <= b && a >= c))\n+//    //    return left;\n+//    //else\n+//    //    return right;\n+//}\n+\n+/*\n+ * Picking the pivot: Median of 72 array elements chosen at random.\n+ */\n+\n+static NPY_INLINE\n+@type_t@ get_pivot_@vsuf1@(@type_t@ *arr, const npy_intp left, const npy_intp right) {\n+    /* seeds for vectorized random number generator */\n+    __m256i s0 = _mm256_setr_epi64x(8265987198341093849, 3762817312854612374,\n+                                    1324281658759788278, 6214952190349879213);\n+    __m256i s1 = _mm256_setr_epi64x(2874178529384792648, 1257248936691237653,\n+                                    7874578921548791257, 1998265912745817298);\n+    s0 = _mm256_add_epi64(s0, _mm256_set1_epi64x(left));\n+    s1 = _mm256_sub_epi64(s1, _mm256_set1_epi64x(right));\n+\n+    npy_intp arrsize = right - left + 1;\n+    __m256i bound = _mm256_set1_epi32(arrsize > INT32_MAX ? INT32_MAX : arrsize);\n+    __m512i left_vec = _mm512_set1_epi64(left);\n+    __m512i right_vec = _mm512_set1_epi64(right);\n+    @ymm_t@ v[9];\n+    /* fill 9 vectors with random numbers */\n+    for (npy_int i = 0; i < 9; ++i) {\n+        __m256i rand_64 = vnext(&s0, &s1); /* vector with 4 random uint64_t */\n+        __m512i rand_32 = _mm512_cvtepi32_epi64(rnd_epu32(rand_64, bound)); /* random numbers between 0 and bound - 1 */\n+        __m512i indices;\n+        if (i < 5)\n+            indices = _mm512_add_epi64(left_vec, rand_32); /* indices for arr */\n+        else \n+            indices = _mm512_sub_epi64(right_vec, rand_32); /* indices for arr */\n+\n+        v[i] = _mm512_i64gather_@vsuf2@(indices, arr, sizeof(@type_t@));\n+    }\n+\n+    /* median network for 9 elements */\n+    COEX_YMM_@vsuf1@(v[0], v[1]); COEX_YMM_@vsuf1@(v[2], v[3]);\n+    COEX_YMM_@vsuf1@(v[4], v[5]); COEX_YMM_@vsuf1@(v[6], v[7]);\n+    COEX_YMM_@vsuf1@(v[0], v[2]); COEX_YMM_@vsuf1@(v[1], v[3]);\n+    COEX_YMM_@vsuf1@(v[4], v[6]); COEX_YMM_@vsuf1@(v[5], v[7]);\n+    COEX_YMM_@vsuf1@(v[0], v[4]); COEX_YMM_@vsuf1@(v[1], v[2]);\n+    COEX_YMM_@vsuf1@(v[5], v[6]); COEX_YMM_@vsuf1@(v[3], v[7]);\n+    COEX_YMM_@vsuf1@(v[1], v[5]); COEX_YMM_@vsuf1@(v[2], v[6]);\n+    COEX_YMM_@vsuf1@(v[3], v[5]); COEX_YMM_@vsuf1@(v[2], v[4]);\n+    COEX_YMM_@vsuf1@(v[3], v[4]);\n+    COEX_YMM_@vsuf1@(v[3], v[8]);\n+    COEX_YMM_@vsuf1@(v[4], v[8]);\n+\n+    // technically v[4] needs to be sorted before we pick the correct median,\n+    // picking the 4th element works just as well for performance\n+    @type_t@* temp = (@type_t@*) &v[4];\n+\n+    return temp[4];\n+}\n+\n+/*\n+ * Parition one ZMM register based on the pivot and returns the index of the\n+ * last element that is less than equal to the pivot.\n+ */\n+static NPY_INLINE\n+npy_int partition_vec_@vsuf1@(@type_t@* arr, npy_intp left, npy_intp right,\n+                                const @zmm_t@ curr_vec, const @zmm_t@ pivot_vec,\n+                                @zmm_t@* smallest_vec, @zmm_t@* biggest_vec)\n+{\n+    /* which elements are larger than the pivot */\n+    __mmask16 gt_mask = _mm512_cmp_@vsuf1@_mask(curr_vec, pivot_vec, @CMP_GE_OP@);\n+    npy_int amount_gt_pivot = _mm_popcnt_u32((npy_int)gt_mask);\n+    _mm512_mask_compressstoreu_@vsuf2@(arr + left, _knot_mask16(gt_mask), curr_vec);\n+    _mm512_mask_compressstoreu_@vsuf2@(arr + right - amount_gt_pivot, gt_mask, curr_vec);\n+    *smallest_vec = _mm512_min_@vsuf1@(curr_vec, *smallest_vec);\n+    *biggest_vec = _mm512_max_@vsuf1@(curr_vec, *biggest_vec);\n+    return amount_gt_pivot;\n+}\n+\n+/*\n+ * Parition an array based on the pivot and returns the index of the\n+ * last element that is less than equal to the pivot.\n+ */\n+static NPY_INLINE\n+npy_intp partition_avx512_@vsuf1@(@type_t@* arr, npy_intp left, npy_intp right,\n+                                    @type_t@ pivot, @type_t@* smallest, @type_t@* biggest)\n+{\n+    /* make array length divisible by 16 , shortening the array */\n+    for (npy_int i = (right - left) % 16; i > 0; --i) {\n+        *smallest = MIN(*smallest, arr[left]);\n+        *biggest = MAX(*biggest, arr[left]);\n+        if (arr[left] > pivot) {\n+            swap_@TYPE@(arr, left, --right);\n+        }\n+        else {\n+            ++left;\n+        }\n+    }\n+\n+    if(left == right)\n+      return left; /* less than 16 elements in the array */\n+\n+    @zmm_t@ pivot_vec = _mm512_set1_@vsuf2@(pivot);\n+    @zmm_t@ min_vec = _mm512_set1_@vsuf2@(*smallest);\n+    @zmm_t@ max_vec = _mm512_set1_@vsuf2@(*biggest);\n+\n+    if(right - left == 16) {\n+        @zmm_t@ vec = _mm512_loadu_@vsuf3@(arr + left);\n+        npy_int amount_gt_pivot = partition_vec_@vsuf1@(arr, left, left + 16, vec, pivot_vec, &min_vec, &max_vec);\n+        *smallest = npyv_reducemin_@vsuf4@(min_vec);\n+        *biggest = npyv_reducemax_@vsuf4@(max_vec);\n+        return left + (16 - amount_gt_pivot);\n+    }\n+\n+    // first and last 16 values are partitioned at the end\n+    @zmm_t@ vec_left = _mm512_loadu_@vsuf3@(arr + left);\n+    @zmm_t@ vec_right = _mm512_loadu_@vsuf3@(arr + (right - 16));\n+    // store points of the vectors\n+    npy_intp r_store = right - 16;\n+    npy_intp l_store = left;\n+    // indices for loading the elements\n+    left += 16;\n+    right -= 16;\n+    while(right - left != 0) {\n+        @zmm_t@ curr_vec;\n+        /*\n+         * if fewer elements are stored on the right side of the array,\n+         * then next elements are loaded from the right side,\n+         * otherwise from the left side\n+         */\n+        if((r_store + 16) - right < left - l_store) {\n+            right -= 16;\n+            curr_vec = _mm512_loadu_@vsuf3@(arr + right);\n+        }\n+        else {\n+            curr_vec = _mm512_loadu_@vsuf3@(arr + left);\n+            left += 16;\n+        }\n+        // partition the current vector and save it on both sides of the array\n+        npy_int amount_gt_pivot = partition_vec_@vsuf1@(arr, l_store, r_store + 16, curr_vec, pivot_vec, &min_vec, &max_vec);;\n+        r_store -= amount_gt_pivot; l_store += (16 - amount_gt_pivot);\n+    }\n+\n+    /* partition and save vec_left and vec_right */\n+    npy_int amount_gt_pivot = partition_vec_@vsuf1@(arr, l_store, r_store + 16, vec_left, pivot_vec, &min_vec, &max_vec);\n+    l_store += (16 - amount_gt_pivot);\n+    amount_gt_pivot = partition_vec_@vsuf1@(arr, l_store, l_store + 16, vec_right, pivot_vec, &min_vec, &max_vec);\n+    l_store += (16 - amount_gt_pivot);\n+    *smallest = npyv_reducemin_@vsuf4@(min_vec);\n+    *biggest = npyv_reducemax_@vsuf4@(max_vec);\n+    return l_store;\n+}\n+\n+static NPY_INLINE\n+void qsort_@type@(@type_t@* arr, npy_intp left, npy_intp right, npy_int max_iters)\n+{\n+    /*\n+     * Resort to heapsort if quicksort isnt making any progress\n+     */\n+    if (max_iters <= 0) {\n+        heapsort_@type@((void*)(arr + left), right + 1 - left, NULL);\n+        return;\n+    }\n+    /*\n+     * Base case: use bitonic networks to sort arrays <= 128\n+     */\n+    if (right + 1 - left <= 128) {\n+        sort_128_@vsuf1@(arr + left, right + 1 -left);\n+        return;\n+    }\n+\n+    @type_t@ pivot = get_pivot_@vsuf1@(arr, left, right);\n+    @type_t@ smallest = @TYPE_MAX_VAL@;\n+    @type_t@ biggest = @TYPE_MIN_VAL@;\n+    npy_intp pivot_index = partition_avx512_@vsuf1@(arr, left, right+1, pivot, &smallest, &biggest);\n+    if (pivot != smallest)\n+        qsort_@type@(arr, left, pivot_index - 1, max_iters - 1);\n+    if (pivot != biggest)\n+        qsort_@type@(arr, pivot_index, right, max_iters - 1);\n+}\n+/**end repeat**/\n+\n+static NPY_INLINE\n+npy_intp replace_nan_with_inf(npy_float* arr, npy_intp arrsize)\n+{\n+    npy_intp nan_count = 0;\n+    __mmask16 loadmask = 0xFFFF;\n+    while (arrsize > 0) {\n+        if (arrsize < 16) {\n+            loadmask = (0x0001 << arrsize) - 0x0001;\n+        }\n+        __m512 in_zmm = _mm512_maskz_loadu_ps(loadmask, arr);\n+        __mmask16 nanmask = _mm512_cmp_ps_mask(in_zmm, in_zmm, _CMP_NEQ_UQ);\n+        nan_count += _mm_popcnt_u32((npy_int) nanmask);\n+        _mm512_mask_storeu_ps(arr, nanmask, ZMM_MAX_FLOAT);\n+        arr += 16;\n+        arrsize -= 16;\n+    }\n+    return nan_count;\n+}\n+\n+static NPY_INLINE\n+void replace_inf_with_nan(npy_float* arr, npy_intp arrsize, npy_intp nan_count)\n+{\n+    for (npy_intp ii = arrsize-1; nan_count > 0; --ii) {\n+        arr[ii] = NPY_NANF;\n+        nan_count -= 1;\n+    }\n+}\n+\n+/**begin repeat\n+ *\n+ * #type = int, uint, float#\n+ * #type_t = npy_int, npy_uint, npy_float#\n+ * #FIXNAN = 0, 0, 1#\n+ */\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(x86_quicksort_@type@)\n+(void* arr, npy_intp arrsize)\n+{\n+    if (arrsize > 1) {\n+#if @FIXNAN@\n+        npy_intp nan_count = replace_nan_with_inf((@type_t@*) arr, arrsize);\n+#endif\n+        qsort_@type@((@type_t@*) arr, 0, arrsize-1, 2*log2(arrsize));\n+#if @FIXNAN@\n+        replace_inf_with_nan((@type_t@*) arr, arrsize, nan_count);\n+#endif\n+    }\n+}\n+/**end repeat**/\n+\n+#endif // NPY_HAVE_AVX512_SKX"
            },
            {
                "filename": "numpy/core/src/npysort/x86-qsort.h",
                "patch": "@@ -0,0 +1,18 @@\n+#include \"numpy/npy_common.h\"\n+#include \"npy_cpu_dispatch.h\"\n+\n+#ifndef NPY_NO_EXPORT\n+    #define NPY_NO_EXPORT NPY_VISIBILITY_HIDDEN\n+#endif\n+\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"x86-qsort.dispatch.h\"\n+#endif\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void x86_quicksort_int,\n+             (void *start, npy_intp num))\n+\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void x86_quicksort_uint,\n+             (void *start, npy_intp num))\n+\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void x86_quicksort_float,\n+             (void *start, npy_intp num))"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -9278,3 +9278,52 @@ def test_non_c_contiguous(self):\n                     [[1284, 1798], [4368, 4882]],\n                     [[2312, 2826], [5396, 5910]]]\n         assert_array_equal(x.view('<i2'), expected)\n+\n+\n+# Test various array sizes that hit different code paths in quicksort-avx512\n+@pytest.mark.parametrize(\"N\", [8, 16, 24, 32, 48, 64, 96, 128, 151, 191,\n+                               256, 383, 512, 1023, 2047])\n+def test_sort_float(N):\n+    # Regular data with nan sprinkled\n+    np.random.seed(42)\n+    arr = -0.5 + np.random.sample(N).astype('f')\n+    arr[np.random.choice(arr.shape[0], 3)] = np.nan\n+    assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))\n+\n+    # (2) with +INF\n+    infarr = np.inf*np.ones(N, dtype='f')\n+    infarr[np.random.choice(infarr.shape[0], 5)] = -1.0\n+    assert_equal(np.sort(infarr, kind='quick'), np.sort(infarr, kind='heap'))\n+\n+    # (3) with -INF\n+    neginfarr = -np.inf*np.ones(N, dtype='f')\n+    neginfarr[np.random.choice(neginfarr.shape[0], 5)] = 1.0\n+    assert_equal(np.sort(neginfarr, kind='quick'),\n+                 np.sort(neginfarr, kind='heap'))\n+\n+    # (4) with +/-INF\n+    infarr = np.inf*np.ones(N, dtype='f')\n+    infarr[np.random.choice(infarr.shape[0], (int)(N/2))] = -np.inf\n+    assert_equal(np.sort(infarr, kind='quick'), np.sort(infarr, kind='heap'))\n+\n+\n+def test_sort_int():\n+    # Random data with NPY_MAX_INT32 and NPY_MIN_INT32 sprinkled\n+    rng = np.random.default_rng(42)\n+    N = 2047\n+    minv = np.iinfo(np.int32).min\n+    maxv = np.iinfo(np.int32).max\n+    arr = rng.integers(low=minv, high=maxv, size=N).astype('int32')\n+    arr[np.random.choice(arr.shape[0], 10)] = minv\n+    arr[np.random.choice(arr.shape[0], 10)] = maxv\n+    assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))\n+\n+\n+def test_sort_uint():\n+    # Random data with NPY_MAX_UINT32 sprinkled\n+    rng = np.random.default_rng(42)\n+    N = 2047\n+    maxv = np.iinfo(np.uint32).max\n+    arr = rng.integers(low=0, high=maxv, size=N).astype('uint32')\n+    arr[np.random.choice(arr.shape[0], 10)] = maxv\n+    assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 19478,
        "body": "This patch integrates Intel Short Vector Math Library (SVML) into NumPy. SVML provides AVX-512 implementations of 44 math functions: `exp, exp2, log, log2, log10, expm1, log1p, cbrt, pow, sin, cos, tan, asin, acos, atan, atan2, sinh, cosh, tanh, asinh, acosh and atanh` (both single and double precisions). Some key points to note: \r\n\r\n1. On a Skylake i9-7900X CPU, the library provides a speed up of up-to **55x** (for cbrt function). The average speed up this patch provides is **32x** and **14x** for single and double precision respectively. More detailed numbers are provided below. \r\n2. The maximum ULP error across all the implementations is **4 ULP**. \r\n3. This patch increases the multiarray shared object file from 29226320 bytes to 30147792 bytes, an increase of **899.875 Kb** (compiled on Ubuntu 20.04 using gcc-11 compiler).\r\n\r\nDetailed benchmarking numbers: \r\n\r\n```\r\n   before           after         ratio\r\n     [fe3d7170]       [6049dc65]\r\n     <main>           <svml>\r\n-      459\u00b10.05\u03bcs         61.2\u00b11\u03bcs     0.13  bench_ufunc_strides.Unary.time_ufunc('exp2', 'd')\r\n-        2.05\u00b10ms          212\u00b14\u03bcs     0.10  bench_ufunc_strides.Unary.time_ufunc('tanh', 'd')\r\n-       871\u00b10.8\u03bcs         86.4\u00b13\u03bcs     0.10  bench_ufunc_strides.Unary.time_ufunc('cos', 'd')\r\n-       856\u00b10.3\u03bcs         76.6\u00b18\u03bcs     0.09  bench_ufunc_strides.Unary.time_ufunc('log2', 'd')\r\n-        2.66\u00b10ms        238\u00b10.1\u03bcs     0.09  bench_ufunc_strides.Unary.time_ufunc('arccosh', 'd')\r\n-        1.33\u00b10ms         107\u00b110\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc('log1p', 'd')\r\n-     2.98\u00b10.01ms         229\u00b110\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc('arcsinh', 'd')\r\n-        1.05\u00b10ms         78.5\u00b15\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc('sin', 'd')\r\n-        1.11\u00b10ms         78.1\u00b16\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc('log10', 'd')\r\n-        1.66\u00b10ms         115\u00b110\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc('arccos', 'd')\r\n-        1.24\u00b10ms         79.6\u00b14\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc('expm1', 'd')\r\n-        1.72\u00b10ms          107\u00b18\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc('arcsin', 'd')\r\n-        1.33\u00b10ms         81.6\u00b16\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc('cosh', 'd')\r\n-        1.94\u00b10ms          118\u00b17\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc('sinh', 'd')\r\n-        2.69\u00b10ms         158\u00b110\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc('arctanh', 'd')\r\n-       366\u00b10.1\u03bcs         20.7\u00b12\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc('exp2', 'f')\r\n-        2.29\u00b10ms         121\u00b110\u03bcs     0.05  bench_ufunc_strides.Unary.time_ufunc('arctan', 'd')\r\n-        2.16\u00b10ms          111\u00b16\u03bcs     0.05  bench_ufunc_strides.Unary.time_ufunc('cbrt', 'd')\r\n-      427\u00b10.08\u03bcs         21.7\u00b13\u03bcs     0.05  bench_ufunc_strides.Unary.time_ufunc('log2', 'f')\r\n-        1.05\u00b10ms         46.4\u00b16\u03bcs     0.04  bench_ufunc_strides.Unary.time_ufunc('arccos', 'f')\r\n-        2.36\u00b10ms          103\u00b17\u03bcs     0.04  bench_ufunc_strides.Unary.time_ufunc('tan', 'd')\r\n-        1.24\u00b10ms         52.9\u00b16\u03bcs     0.04  bench_ufunc_strides.Unary.time_ufunc('log1p', 'f')\r\n-     1.18\u00b10.01ms         48.9\u00b17\u03bcs     0.04  bench_ufunc_strides.Unary.time_ufunc('arctan', 'f')\r\n-         974\u00b13\u03bcs         38.9\u00b16\u03bcs     0.04  bench_ufunc_strides.Unary.time_ufunc('arcsin', 'f')\r\n-        2.32\u00b10ms       89.9\u00b10.5\u03bcs     0.04  bench_ufunc_strides.Unary.time_ufunc('arccosh', 'f')\r\n-        2.58\u00b10ms        86.3\u00b110\u03bcs     0.03  bench_ufunc_strides.Unary.time_ufunc('arcsinh', 'f')\r\n-        1.59\u00b10ms         50.2\u00b17\u03bcs     0.03  bench_ufunc_strides.Unary.time_ufunc('tan', 'f')\r\n-       826\u00b10.1\u03bcs         21.8\u00b13\u03bcs     0.03  bench_ufunc_strides.Unary.time_ufunc('log10', 'f')\r\n-        1.17\u00b10ms         30.8\u00b14\u03bcs     0.03  bench_ufunc_strides.Unary.time_ufunc('expm1', 'f')\r\n-     1.94\u00b10.01ms         49.6\u00b14\u03bcs     0.03  bench_ufunc_strides.Unary.time_ufunc('tanh', 'f')\r\n-        1.33\u00b10ms         33.5\u00b14\u03bcs     0.03  bench_ufunc_strides.Unary.time_ufunc('cosh', 'f')\r\n-        2.45\u00b10ms         57.3\u00b19\u03bcs     0.02  bench_ufunc_strides.Unary.time_ufunc('arctanh', 'f')\r\n-        1.98\u00b10ms         42.1\u00b15\u03bcs     0.02  bench_ufunc_strides.Unary.time_ufunc('sinh', 'f')\r\n-        2.09\u00b10ms         38.3\u00b15\u03bcs     0.02  bench_ufunc_strides.Unary.time_ufunc('cbrt', 'f')\r\n```\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": ".circleci/config.yml",
                "patch": "@@ -19,6 +19,12 @@ jobs:\n           command: |\n             if [[ -v CI_PULL_REQUEST ]] ; then git pull --ff-only origin \"refs/pull/${CI_PULL_REQUEST//*pull\\//}/merge\" ; fi\n \n+      - run:\n+          name: update submodules\n+          command: |\n+            git submodule init\n+            git submodule update\n+\n       - run:\n           name: create virtual environment, install dependencies\n           command: |"
            },
            {
                "filename": ".gitattributes",
                "patch": "@@ -11,6 +11,7 @@ numpy/linalg/lapack_lite/f2c.c linguist-vendored\n numpy/linalg/lapack_lite/f2c.h linguist-vendored\n tools/npy_tempita/* linguist-vendored\n numpy/core/include/numpy/libdivide/* linguist-vendored\n+numpy/core/src/umath/svml/* linguist-vendored\n \n # Mark some files as generated\n numpy/linalg/lapack_lite/f2c_*.c linguist-generated"
            },
            {
                "filename": ".gitmodules",
                "patch": "@@ -1,3 +1,6 @@\n [submodule \"doc/source/_static/scipy-mathjax\"]\n \tpath = doc/source/_static/scipy-mathjax\n \turl = https://github.com/scipy/scipy-mathjax.git\n+[submodule \"numpy/core/src/umath/svml\"]\n+\tpath = numpy/core/src/umath/svml\n+\turl = https://github.com/numpy/SVML.git"
            },
            {
                "filename": "azure-pipelines.yml",
                "patch": "@@ -23,6 +23,9 @@ stages:\n     pool:\n       vmImage: 'ubuntu-20.04'\n     steps:\n+    - script: |\n+        git submodule update --init\n+      displayName: 'Fetch submodules'\n     - script: |\n             if ! `gcc 2>/dev/null`; then\n                 sudo apt install gcc\n@@ -71,6 +74,9 @@ stages:\n     pool:\n       vmImage: 'ubuntu-20.04'\n     steps:\n+    - script: |\n+        git submodule update --init\n+      displayName: 'Fetch submodules'\n     - script: |\n             docker run -v $(pwd):/numpy -e CFLAGS=\"-msse2 -std=c99 -UNDEBUG\" \\\n             -e F77=gfortran-5 -e F90=gfortran-5 quay.io/pypa/manylinux2014_i686 \\\n@@ -258,6 +264,9 @@ stages:\n     pool:\n       vmImage: 'ubuntu-20.04'\n     steps:\n+    - script: |\n+        git submodule update --init\n+      displayName: 'Fetch submodules'\n     - script: |\n             # create and activate conda environment\n             conda env create -f environment.yml"
            },
            {
                "filename": "doc/release/upcoming_changes/19478.performance.rst",
                "patch": "@@ -0,0 +1,11 @@\n+Vectorize umath module using AVX-512\n+-------------------------------------\n+\n+By leveraging Intel Short Vector Math Library (SVML), 18 umath functions\n+(``exp2``, ``log2``, ``log10``, ``expm1``, ``log1p``, ``cbrt``, ``sin``,\n+``cos``, ``tan``, ``arcsin``, ``arccos``, ``arctan``, ``sinh``, ``cosh``,\n+``tanh``, ``arcsinh``, ``arccosh``, ``arctanh``) are vectorized using AVX-512\n+instruction set for both single and double precision implementations.  This\n+change is currently enabled only for Linux users and on processors with\n+AVX-512 instruction set.  It provides an average speed up of 32x and 14x for\n+single and double precision functions respectively."
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -359,7 +359,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.fmod'),\n           None,\n           TD(ints),\n-          TD(flts, f='fmod', astype={'e':'f'}),\n+          TD(flts, f='fmod', astype={'e': 'f'}),\n           TD(P, f='fmod'),\n           ),\n 'square':\n@@ -390,7 +390,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.power'),\n           None,\n           TD(ints),\n-          TD(inexact, f='pow', astype={'e':'f'}),\n+          TD(inexact, f='pow', astype={'e': 'f'}),\n           TD(O, f='npy_ObjectPower'),\n           ),\n 'float_power':\n@@ -551,13 +551,13 @@ def english_upper(s):\n     Ufunc(2, 1, MinusInfinity,\n           docstrings.get('numpy.core.umath.logaddexp'),\n           None,\n-          TD(flts, f=\"logaddexp\", astype={'e':'f'})\n+          TD(flts, f=\"logaddexp\", astype={'e': 'f'})\n           ),\n 'logaddexp2':\n     Ufunc(2, 1, MinusInfinity,\n           docstrings.get('numpy.core.umath.logaddexp2'),\n           None,\n-          TD(flts, f=\"logaddexp2\", astype={'e':'f'})\n+          TD(flts, f=\"logaddexp2\", astype={'e': 'f'})\n           ),\n 'bitwise_and':\n     Ufunc(2, 1, AllOnes,\n@@ -605,125 +605,147 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.heaviside'),\n           None,\n-          TD(flts, f='heaviside', astype={'e':'f'}),\n+          TD(flts, f='heaviside', astype={'e': 'f'}),\n           ),\n 'degrees':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.degrees'),\n           None,\n-          TD(fltsP, f='degrees', astype={'e':'f'}),\n+          TD(fltsP, f='degrees', astype={'e': 'f'}),\n           ),\n 'rad2deg':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.rad2deg'),\n           None,\n-          TD(fltsP, f='rad2deg', astype={'e':'f'}),\n+          TD(fltsP, f='rad2deg', astype={'e': 'f'}),\n           ),\n 'radians':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.radians'),\n           None,\n-          TD(fltsP, f='radians', astype={'e':'f'}),\n+          TD(fltsP, f='radians', astype={'e': 'f'}),\n           ),\n 'deg2rad':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.deg2rad'),\n           None,\n-          TD(fltsP, f='deg2rad', astype={'e':'f'}),\n+          TD(fltsP, f='deg2rad', astype={'e': 'f'}),\n           ),\n 'arccos':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arccos'),\n           None,\n-          TD(inexact, f='acos', astype={'e':'f'}),\n+          TD('e', f='acos', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='acos', astype={'e': 'f'}),\n           TD(P, f='arccos'),\n           ),\n 'arccosh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arccosh'),\n           None,\n-          TD(inexact, f='acosh', astype={'e':'f'}),\n+          TD('e', f='acosh', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='acosh', astype={'e': 'f'}),\n           TD(P, f='arccosh'),\n           ),\n 'arcsin':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arcsin'),\n           None,\n-          TD(inexact, f='asin', astype={'e':'f'}),\n+          TD('e', f='asin', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='asin', astype={'e': 'f'}),\n           TD(P, f='arcsin'),\n           ),\n 'arcsinh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arcsinh'),\n           None,\n-          TD(inexact, f='asinh', astype={'e':'f'}),\n+          TD('e', f='asinh', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='asinh', astype={'e': 'f'}),\n           TD(P, f='arcsinh'),\n           ),\n 'arctan':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arctan'),\n           None,\n-          TD(inexact, f='atan', astype={'e':'f'}),\n+          TD('e', f='atan', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='atan', astype={'e': 'f'}),\n           TD(P, f='arctan'),\n           ),\n 'arctanh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.arctanh'),\n           None,\n-          TD(inexact, f='atanh', astype={'e':'f'}),\n+          TD('e', f='atanh', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='atanh', astype={'e': 'f'}),\n           TD(P, f='arctanh'),\n           ),\n 'cos':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cos'),\n           None,\n-          TD('e', f='cos', astype={'e':'f'}),\n+          TD('e', f='cos', astype={'e': 'f'}),\n           TD('f', dispatch=[('loops_trigonometric', 'f')]),\n+          TD('d', dispatch=[('loops_umath_fp', 'd')]),\n           TD('fdg' + cmplx, f='cos'),\n           TD(P, f='cos'),\n           ),\n 'sin':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sin'),\n           None,\n-          TD('e', f='sin', astype={'e':'f'}),\n+          TD('e', f='sin', astype={'e': 'f'}),\n           TD('f', dispatch=[('loops_trigonometric', 'f')]),\n+          TD('d', dispatch=[('loops_umath_fp', 'd')]),\n           TD('fdg' + cmplx, f='sin'),\n           TD(P, f='sin'),\n           ),\n 'tan':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.tan'),\n           None,\n-          TD(inexact, f='tan', astype={'e':'f'}),\n+          TD('e', f='tan', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='tan', astype={'e': 'f'}),\n           TD(P, f='tan'),\n           ),\n 'cosh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cosh'),\n           None,\n-          TD(inexact, f='cosh', astype={'e':'f'}),\n+          TD('e', f='cosh', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='cosh', astype={'e': 'f'}),\n           TD(P, f='cosh'),\n           ),\n 'sinh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sinh'),\n           None,\n-          TD(inexact, f='sinh', astype={'e':'f'}),\n+          TD('e', f='sinh', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='sinh', astype={'e': 'f'}),\n           TD(P, f='sinh'),\n           ),\n 'tanh':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.tanh'),\n           None,\n-          TD(inexact, f='tanh', astype={'e':'f'}),\n+          TD('e', f='tanh', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='tanh', astype={'e': 'f'}),\n           TD(P, f='tanh'),\n           ),\n 'exp':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.exp'),\n           None,\n-          TD('e', f='exp', astype={'e':'f'}),\n+          TD('e', f='exp', astype={'e': 'f'}),\n           TD('fd', dispatch=[('loops_exponent_log', 'fd')]),\n           TD('fdg' + cmplx, f='exp'),\n           TD(P, f='exp'),\n@@ -732,21 +754,25 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.exp2'),\n           None,\n-          TD(inexact, f='exp2', astype={'e':'f'}),\n+          TD('e', f='exp2', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='exp2', astype={'e': 'f'}),\n           TD(P, f='exp2'),\n           ),\n 'expm1':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.expm1'),\n           None,\n-          TD(inexact, f='expm1', astype={'e':'f'}),\n+          TD('e', f='expm1', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='expm1', astype={'e': 'f'}),\n           TD(P, f='expm1'),\n           ),\n 'log':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log'),\n           None,\n-          TD('e', f='log', astype={'e':'f'}),\n+          TD('e', f='log', astype={'e': 'f'}),\n           TD('fd', dispatch=[('loops_exponent_log', 'fd')]),\n           TD('fdg' + cmplx, f='log'),\n           TD(P, f='log'),\n@@ -755,28 +781,34 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log2'),\n           None,\n-          TD(inexact, f='log2', astype={'e':'f'}),\n+          TD('e', f='log2', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='log2', astype={'e': 'f'}),\n           TD(P, f='log2'),\n           ),\n 'log10':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log10'),\n           None,\n-          TD(inexact, f='log10', astype={'e':'f'}),\n+          TD('e', f='log10', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='log10', astype={'e': 'f'}),\n           TD(P, f='log10'),\n           ),\n 'log1p':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.log1p'),\n           None,\n-          TD(inexact, f='log1p', astype={'e':'f'}),\n+          TD('e', f='log1p', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(inexact, f='log1p', astype={'e': 'f'}),\n           TD(P, f='log1p'),\n           ),\n 'sqrt':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sqrt'),\n           None,\n-          TD('e', f='sqrt', astype={'e':'f'}),\n+          TD('e', f='sqrt', astype={'e': 'f'}),\n           TD(inexactvec, dispatch=[('loops_unary_fp', 'fd')]),\n           TD('fdg' + cmplx, f='sqrt'),\n           TD(P, f='sqrt'),\n@@ -785,14 +817,16 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cbrt'),\n           None,\n-          TD(flts, f='cbrt', astype={'e':'f'}),\n+          TD('e', f='cbrt', astype={'e': 'f'}),\n+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n+          TD(flts, f='cbrt', astype={'e': 'f'}),\n           TD(P, f='cbrt'),\n           ),\n 'ceil':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.ceil'),\n           None,\n-          TD('e', f='ceil', astype={'e':'f'}),\n+          TD('e', f='ceil', astype={'e': 'f'}),\n           TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n           TD('fdg', f='ceil'),\n           TD(O, f='npy_ObjectCeil'),\n@@ -801,7 +835,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.trunc'),\n           None,\n-          TD('e', f='trunc', astype={'e':'f'}),\n+          TD('e', f='trunc', astype={'e': 'f'}),\n           TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n           TD('fdg', f='trunc'),\n           TD(O, f='npy_ObjectTrunc'),\n@@ -810,14 +844,14 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.fabs'),\n           None,\n-          TD(flts, f='fabs', astype={'e':'f'}),\n+          TD(flts, f='fabs', astype={'e': 'f'}),\n           TD(P, f='fabs'),\n        ),\n 'floor':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.floor'),\n           None,\n-          TD('e', f='floor', astype={'e':'f'}),\n+          TD('e', f='floor', astype={'e': 'f'}),\n           TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n           TD('fdg', f='floor'),\n           TD(O, f='npy_ObjectFloor'),\n@@ -826,7 +860,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.rint'),\n           None,\n-          TD('e', f='rint', astype={'e':'f'}),\n+          TD('e', f='rint', astype={'e': 'f'}),\n           TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n           TD('fdg' + cmplx, f='rint'),\n           TD(P, f='rint'),\n@@ -835,7 +869,7 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.arctan2'),\n           None,\n-          TD(flts, f='atan2', astype={'e':'f'}),\n+          TD(flts, f='atan2', astype={'e': 'f'}),\n           TD(P, f='arctan2'),\n           ),\n 'remainder':\n@@ -858,7 +892,7 @@ def english_upper(s):\n     Ufunc(2, 1, Zero,\n           docstrings.get('numpy.core.umath.hypot'),\n           None,\n-          TD(flts, f='hypot', astype={'e':'f'}),\n+          TD(flts, f='hypot', astype={'e': 'f'}),\n           TD(P, f='hypot'),\n           ),\n 'isnan':"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -5,6 +5,7 @@\n import warnings\n import platform\n import textwrap\n+import glob\n from os.path import join\n \n from numpy.distutils import log\n@@ -63,6 +64,20 @@ def check_complex(self, *a, **kw):\n             out = copy.deepcopy(pickle.loads(self._check_complex))\n         return out\n \n+def can_link_svml():\n+    \"\"\"SVML library is supported only on x86_64 architecture and currently\n+    only on linux\n+    \"\"\"\n+    machine = platform.machine()\n+    system = platform.system()\n+    return \"x86_64\" in machine and system == \"Linux\"\n+\n+def check_svml_submodule(svmlpath):\n+    if not os.path.exists(svmlpath + \"/README.md\"):\n+        raise RuntimeError(\"Missing `SVML` submodule! Run `git submodule \"\n+                           \"update --init` to fix this.\")\n+    return True\n+\n def pythonlib_dir():\n     \"\"\"return path where libpython* is.\"\"\"\n     if sys.platform == 'win32':\n@@ -455,6 +470,9 @@ def generate_config_h(ext, build_dir):\n             # Inline check\n             inline = config_cmd.check_inline()\n \n+            if can_link_svml():\n+                moredefs.append(('NPY_CAN_LINK_SVML', 1))\n+\n             # Use relaxed stride checking\n             if NPY_RELAXED_STRIDES_CHECKING:\n                 moredefs.append(('NPY_RELAXED_STRIDES_CHECKING', 1))\n@@ -727,6 +745,7 @@ def get_mathlib_info(*args):\n             join('src', 'common', 'npy_import.h'),\n             join('src', 'common', 'npy_hashtable.h'),\n             join('src', 'common', 'npy_longdouble.h'),\n+            join('src', 'common', 'npy_svml.h'),\n             join('src', 'common', 'templ_common.h.src'),\n             join('src', 'common', 'ucsnarrow.h'),\n             join('src', 'common', 'ufunc_override.h'),\n@@ -923,6 +942,7 @@ def generate_umath_c(ext, build_dir):\n             join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),\n             join('src', 'umath', 'loops_trigonometric.dispatch.c.src'),\n+            join('src', 'umath', 'loops_umath_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_exponent_log.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),\n             join('src', 'umath', 'matmul.c.src'),\n@@ -951,6 +971,11 @@ def generate_umath_c(ext, build_dir):\n             join(codegen_dir, 'generate_ufunc_api.py'),\n             ]\n \n+    svml_path = join('numpy', 'core', 'src', 'umath', 'svml')\n+    svml_objs = []\n+    if can_link_svml() and check_svml_submodule(svml_path):\n+        svml_objs = glob.glob(svml_path + '/**/*.s', recursive=True)\n+\n     config.add_extension('_multiarray_umath',\n                          sources=multiarray_src + umath_src +\n                                  common_src +\n@@ -965,6 +990,7 @@ def generate_umath_c(ext, build_dir):\n                          depends=deps + multiarray_deps + umath_deps +\n                                 common_deps,\n                          libraries=['npymath'],\n+                         extra_objects=svml_objs,\n                          extra_info=extra_info)\n \n     #######################################################################"
            },
            {
                "filename": "numpy/core/src/common/npy_svml.h",
                "patch": "@@ -0,0 +1,41 @@\n+#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+extern __m512 __svml_exp2f16(__m512 x);\n+extern __m512 __svml_log2f16(__m512 x);\n+extern __m512 __svml_log10f16(__m512 x);\n+extern __m512 __svml_expm1f16(__m512 x);\n+extern __m512 __svml_log1pf16(__m512 x);\n+extern __m512 __svml_cbrtf16(__m512 x);\n+extern __m512 __svml_sinf16(__m512 x);\n+extern __m512 __svml_cosf16(__m512 x);\n+extern __m512 __svml_tanf16(__m512 x);\n+extern __m512 __svml_asinf16(__m512 x);\n+extern __m512 __svml_acosf16(__m512 x);\n+extern __m512 __svml_atanf16(__m512 x);\n+extern __m512 __svml_atan2f16(__m512 x);\n+extern __m512 __svml_sinhf16(__m512 x);\n+extern __m512 __svml_coshf16(__m512 x);\n+extern __m512 __svml_tanhf16(__m512 x);\n+extern __m512 __svml_asinhf16(__m512 x);\n+extern __m512 __svml_acoshf16(__m512 x);\n+extern __m512 __svml_atanhf16(__m512 x);\n+\n+extern __m512d __svml_exp28(__m512d x);\n+extern __m512d __svml_log28(__m512d x);\n+extern __m512d __svml_log108(__m512d x);\n+extern __m512d __svml_expm18(__m512d x);\n+extern __m512d __svml_log1p8(__m512d x);\n+extern __m512d __svml_cbrt8(__m512d x);\n+extern __m512d __svml_sin8(__m512d x);\n+extern __m512d __svml_cos8(__m512d x);\n+extern __m512d __svml_tan8(__m512d x);\n+extern __m512d __svml_asin8(__m512d x);\n+extern __m512d __svml_acos8(__m512d x);\n+extern __m512d __svml_atan8(__m512d x);\n+extern __m512d __svml_atan28(__m512d x);\n+extern __m512d __svml_sinh8(__m512d x);\n+extern __m512d __svml_cosh8(__m512d x);\n+extern __m512d __svml_tanh8(__m512d x);\n+extern __m512d __svml_asinh8(__m512d x);\n+extern __m512d __svml_acosh8(__m512d x);\n+extern __m512d __svml_atanh8(__m512d x);\n+#endif"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -210,6 +210,32 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n /**end repeat1**/\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_umath_fp.dispatch.h\"\n+#endif\n+\n+/**begin repeat\n+ *  #TYPE = FLOAT, DOUBLE#\n+ */\n+/**begin repeat1\n+ * #func = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, arcsin, arccos, arctan, sinh, cosh, arcsinh, arccosh, arctanh#\n+ */\n+\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@func@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #func = sin, cos# \n+ */\n+\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void DOUBLE_@func@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+\n+/**end repeat**/\n+\n /**begin repeat\n  *  #TYPE = FLOAT, DOUBLE#\n  */"
            },
            {
                "filename": "numpy/core/src/umath/loops_umath_fp.dispatch.c.src",
                "patch": "@@ -0,0 +1,141 @@\n+/*@targets\n+ ** $maxopt baseline avx512_skx\n+ */\n+#include \"numpy/npy_math.h\"\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"npy_svml.h\"\n+#include \"fast_loop_macros.h\"\n+\n+#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+/**begin repeat\n+ * #sfx = f32, f64#\n+ * #func_suffix = f16, 8#\n+ */\n+/**begin repeat1\n+ * #func = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, asin, acos, atan, sinh, cosh, asinh, acosh, atanh#\n+ * #default_val = 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0#\n+ */\n+static void\n+simd_@func@_@sfx@(const npyv_lanetype_@sfx@ *src, npy_intp ssrc,\n+                        npyv_lanetype_@sfx@ *dst, npy_intp sdst, npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_@sfx@;\n+    for (; len > 0; len -= vstep, src += ssrc*vstep, dst += sdst*vstep) {\n+        npyv_@sfx@ x;\n+        #if @default_val@\n+            if (ssrc == 1) {\n+                x = npyv_load_till_@sfx@(src, len, @default_val@);\n+            } else {\n+                x = npyv_loadn_till_@sfx@(src, ssrc, len, @default_val@);\n+            }\n+        #else\n+            if (ssrc == 1) {\n+                x = npyv_load_tillz_@sfx@(src, len);\n+            } else {\n+                x = npyv_loadn_tillz_@sfx@(src, ssrc, len);\n+            }\n+        #endif\n+        npyv_@sfx@ out = __svml_@func@@func_suffix@(x);\n+        if (sdst == 1) {\n+            npyv_store_till_@sfx@(dst, len, out);\n+        } else {\n+            npyv_storen_till_@sfx@(dst, sdst, len, out);\n+        }\n+    }\n+    npyv_cleanup();\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #func = sin, cos#\n+ */\n+static void\n+simd_@func@_f64(const double *src, npy_intp ssrc,\n+                      double *dst, npy_intp sdst, npy_intp len)\n+{\n+    const int vstep = npyv_nlanes_f64;\n+    for (; len > 0; len -= vstep, src += ssrc*vstep, dst += sdst*vstep) {\n+        npyv_f64 x;\n+        if (ssrc == 1) {\n+            x = npyv_load_tillz_f64(src, len);\n+        } else {\n+            x = npyv_loadn_tillz_f64(src, ssrc, len);\n+        }\n+        npyv_f64 out = __svml_@func@8(x);\n+        if (sdst == 1) {\n+            npyv_store_till_f64(dst, len, out);\n+        } else {\n+            npyv_storen_till_f64(dst, sdst, len, out);\n+        }\n+    }\n+    npyv_cleanup();\n+}\n+/**end repeat**/\n+#endif\n+\n+/**begin repeat\n+ *  #TYPE = DOUBLE, FLOAT#\n+ *  #type = npy_double, npy_float#\n+ *  #vsub = , f#\n+ *  #sfx  = f64, f32#\n+ */\n+/**begin repeat1\n+ *  #func = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, arcsin, arccos, arctan, sinh, cosh, arcsinh, arccosh, arctanh#\n+ *  #intrin = tanh, exp2, log2, log10, expm1, log1p, cbrt, tan, asin, acos, atan, sinh, cosh, asinh, acosh, atanh#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+    const @type@ *src = (@type@*)args[0];\n+          @type@ *dst = (@type@*)args[1];\n+    const int lsize = sizeof(src[0]);\n+    const npy_intp ssrc = steps[0] / lsize;\n+    const npy_intp sdst = steps[1] / lsize;\n+    const npy_intp len = dimensions[0];\n+    assert(steps[0] % lsize == 0 && steps[1] % lsize == 0);\n+    if (!is_mem_overlap(src, steps[0], dst, steps[1], len) &&\n+        npyv_loadable_stride_@sfx@(ssrc) &&\n+        npyv_storable_stride_@sfx@(sdst)) {\n+        simd_@intrin@_@sfx@(src, ssrc, dst, sdst, len);\n+        return;\n+    }\n+#endif\n+    UNARY_LOOP {\n+        const @type@ in1 = *(@type@ *)ip1;\n+        *(@type@ *)op1 = npy_@intrin@@vsub@(in1);\n+    }\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ *  #func = sin, cos#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(DOUBLE_@func@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n+{\n+#if NPY_SIMD && defined(NPY_HAVE_AVX512_SKX) && defined(NPY_CAN_LINK_SVML)\n+    const double *src = (double*)args[0];\n+          double *dst = (double*)args[1];\n+    const int lsize = sizeof(src[0]);\n+    const npy_intp ssrc = steps[0] / lsize;\n+    const npy_intp sdst = steps[1] / lsize;\n+    const npy_intp len = dimensions[0];\n+    assert(steps[0] % lsize == 0 && steps[1] % lsize == 0);\n+    if (!is_mem_overlap(src, steps[0], dst, steps[1], len) &&\n+        npyv_loadable_stride_f64(ssrc) &&\n+        npyv_storable_stride_f64(sdst)) {\n+        simd_@func@_f64(src, ssrc, dst, sdst, len);\n+        return;\n+    }\n+#endif\n+    UNARY_LOOP {\n+        const npy_double in1 = *(npy_double *)ip1;\n+        *(npy_double *)op1 = npy_@func@(in1);\n+    }\n+}\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -116,9 +116,8 @@ run_binary_avx512f_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_in\n #endif\n     return 0;\n }\n-\n-\n /**end repeat1**/\n+\n /**end repeat**/\n \n /**begin repeat\n@@ -1152,6 +1151,7 @@ NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n  * #is_finite = 0, 1, 0, 0#\n  * #is_signbit = 0, 0, 0, 1#\n  */\n+\n #if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void\n AVX512_SKX_@func@_@TYPE@(npy_bool* op, @type@* ip, const npy_intp array_size, const npy_intp steps)"
            },
            {
                "filename": "numpy/core/src/umath/svml",
                "patch": "@@ -0,0 +1 @@\n+Subproject commit 9f8af767ed6c75455d9a382af829048f8dd18067"
            },
            {
                "filename": "numpy/polynomial/tests/test_classes.py",
                "patch": "@@ -597,4 +597,4 @@ def powx(x, p):\n         for deg in range(0, 10):\n             for t in range(0, deg + 1):\n                 p = Chebyshev.interpolate(powx, deg, domain=[0, 2], args=(t,))\n-                assert_almost_equal(p(x), powx(x, t), decimal=12)\n+                assert_almost_equal(p(x), powx(x, t), decimal=11)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 15408,
        "body": "<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n\r\nThis patch implements complex add, subtract, multiply, square, absolute and conjugate using AVX-512F for both CFLOAT and CDOUBLE. This patch not only improves microbenchmarks but also high level workloads. Specifically, it speeds up the program for computing the Mandelbrot set by nearly 3.5x (added to the benchmarks). Detailed benchmark numbers run in Intel i9-7900X CPU are provided below: \r\n\r\n      before           after         ratio\r\n    [11654979]       [14756955]\r\n    <master>         <cmplx-simd>\r\n      16.8\u00b10.2ms       14.2\u00b10.4ms     0.84  bench_avx.AVX_cmplx_arithmetic.time_ufunc('multiply', 4, 'F')\r\n     8.42\u00b10.03ms       6.97\u00b10.2ms     0.83  bench_avx.AVX_cmplx_funcs.time_ufunc('conjugate', 2, 'D')\r\n     15.5\u00b10.04ms      12.6\u00b10.02ms     0.81  bench_avx.AVX_cmplx_arithmetic.time_ufunc('multiply', 2, 'F')\r\n      6.93\u00b10.2ms       5.34\u00b10.3ms     0.77  bench_avx.AVX_cmplx_arithmetic.time_ufunc('subtract', 1, 'D')\r\n      10.0\u00b10.2ms      7.23\u00b10.05ms     0.72  bench_avx.AVX_cmplx_funcs.time_ufunc('square', 2, 'D')\r\n      8.53\u00b10.2ms       5.87\u00b10.1ms     0.69  bench_avx.AVX_cmplx_funcs.time_ufunc('conjugate', 4, 'F')\r\n     8.28\u00b10.02ms      5.47\u00b10.03ms     0.66  bench_avx.AVX_cmplx_funcs.time_ufunc('conjugate', 2, 'F')\r\n      5.25\u00b10.3ms       3.10\u00b10.1ms     0.59  bench_avx.AVX_cmplx_arithmetic.time_ufunc('add', 1, 'F')\r\n      10.0\u00b10.2ms       5.85\u00b10.3ms     0.58  bench_avx.AVX_cmplx_funcs.time_ufunc('square', 1, 'D')\r\n      5.26\u00b10.1ms       3.04\u00b10.1ms     0.58  bench_avx.AVX_cmplx_arithmetic.time_ufunc('subtract', 1, 'F')\r\n      11.8\u00b10.5ms       6.74\u00b10.1ms     0.57  bench_avx.AVX_cmplx_arithmetic.time_ufunc('multiply', 1, 'D')\r\n      10.3\u00b10.2ms      5.52\u00b10.09ms     0.53  bench_avx.AVX_cmplx_funcs.time_ufunc('square', 2, 'F')\r\n        7.75\u00b11ms       4.12\u00b10.1ms     0.53  bench_avx.AVX_cmplx_funcs.time_ufunc('conjugate', 1, 'D')\r\n      10.4\u00b10.2ms       5.36\u00b10.3ms     0.52  bench_avx.AVX_cmplx_arithmetic.time_ufunc('add', 1, 'D')\r\n      45.6\u00b10.3ms      21.2\u00b10.07ms     0.46  bench_avx.AVX_cmplx_funcs.time_ufunc('absolute', 1, 'D')\r\n      48.2\u00b10.2ms       22.1\u00b10.3ms     0.46  bench_avx.AVX_cmplx_funcs.time_ufunc('absolute', 4, 'D')\r\n      46.2\u00b10.8ms       20.7\u00b10.5ms     0.45  bench_avx.AVX_cmplx_funcs.time_ufunc('absolute', 2, 'D')\r\n     10.0\u00b10.07ms       3.32\u00b10.1ms     0.33  bench_avx.AVX_cmplx_funcs.time_ufunc('square', 1, 'F')\r\n      8.30\u00b10.2ms      2.62\u00b10.04ms     0.32  bench_avx.AVX_cmplx_funcs.time_ufunc('conjugate', 1, 'F')\r\n      9.52\u00b10.02ms      2.64\u00b10.02s     0.28  bench_avx.Mandelbrot.time_mandel\r\n      37.8\u00b10.7ms       9.53\u00b10.2ms     0.25  bench_avx.AVX_cmplx_funcs.time_ufunc('absolute', 4, 'F')\r\n      38.4\u00b10.2ms      9.49\u00b10.02ms     0.25  bench_avx.AVX_cmplx_funcs.time_ufunc('absolute', 2, 'F')\r\n      15.7\u00b10.3ms      3.83\u00b10.08ms     0.24  bench_avx.AVX_cmplx_arithmetic.time_ufunc('multiply', 1, 'F')\r\n      39.2\u00b10.4ms       8.21\u00b10.4ms     0.21  bench_avx.AVX_cmplx_funcs.time_ufunc('absolute', 1, 'F')\r\n\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_avx.py",
                "patch": "@@ -51,3 +51,77 @@ def setup(self, ufuncname, dtype, stride):\n \n     def time_ufunc(self, ufuncname, dtype, stride):\n         self.f(self.arr1[::stride], self.arr2[::stride])\n+\n+cmplx_bfuncs = ['add',\n+                'subtract',\n+                'multiply',\n+                'divide']\n+cmplxstride = [1, 2, 4]\n+cmplxdtype  = ['F', 'D']\n+\n+class AVX_cmplx_arithmetic(Benchmark):\n+    params = [cmplx_bfuncs, cmplxstride, cmplxdtype]\n+    param_names = ['bfunc', 'stride', 'dtype']\n+    timeout = 10\n+\n+    def setup(self, bfuncname, stride, dtype):\n+        np.seterr(all='ignore')\n+        try:\n+            self.f = getattr(np, bfuncname)\n+        except AttributeError:\n+            raise NotImplementedError()\n+        N = 10000\n+        self.arr1 = np.ones(stride*N, dtype)\n+        self.arr2 = np.ones(stride*N, dtype)\n+\n+    def time_ufunc(self, bfuncname, stride, dtype):\n+        self.f(self.arr1[::stride], self.arr2[::stride])\n+\n+cmplx_ufuncs = ['reciprocal',\n+                'absolute',\n+                'square',\n+                'conjugate']\n+\n+class AVX_cmplx_funcs(Benchmark):\n+    params = [cmplx_ufuncs, cmplxstride, cmplxdtype]\n+    param_names = ['bfunc', 'stride', 'dtype']\n+    timeout = 10\n+\n+    def setup(self, bfuncname, stride, dtype):\n+        np.seterr(all='ignore')\n+        try:\n+            self.f = getattr(np, bfuncname)\n+        except AttributeError:\n+            raise NotImplementedError()\n+        N = 10000\n+        self.arr1 = np.ones(stride*N, dtype)\n+\n+    def time_ufunc(self, bfuncname, stride, dtype):\n+        self.f(self.arr1[::stride])\n+\n+class Mandelbrot(Benchmark):\n+    def f(self,z):\n+        return np.abs(z) < 4.0\n+\n+    def g(self,z,c):\n+        return np.sum(np.multiply(z,z) + c)\n+\n+    def mandelbrot_numpy(self, c, maxiter):\n+        output = np.zeros(c.shape, np.int)\n+        z = np.empty(c.shape, np.complex64)\n+        for it in range(maxiter):\n+            notdone = self.f(z)\n+            output[notdone] = it\n+            z[notdone] = self.g(z[notdone],c[notdone])\n+        output[output == maxiter-1] = 0\n+        return output\n+\n+    def mandelbrot_set(self,xmin,xmax,ymin,ymax,width,height,maxiter):\n+        r1 = np.linspace(xmin, xmax, width, dtype=np.float32)\n+        r2 = np.linspace(ymin, ymax, height, dtype=np.float32)\n+        c = r1 + r2[:,None]*1j\n+        n3 = self.mandelbrot_numpy(c,maxiter)\n+        return (r1,r2,n3.T)\n+\n+    def time_mandel(self):\n+        self.mandelbrot_set(-0.74877,-0.74872,0.06505,0.06510,1000,1000,2048)"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -233,6 +233,7 @@ def english_upper(s):\n fltsO = flts + O\n fltsP = flts + P\n cmplx = 'FDG'\n+cmplxvec = 'FD'\n cmplxO = cmplx + O\n cmplxP = cmplx + P\n inexact = flts + cmplx\n@@ -268,7 +269,7 @@ def english_upper(s):\n     Ufunc(2, 1, Zero,\n           docstrings.get('numpy.core.umath.add'),\n           'PyUFunc_AdditionTypeResolver',\n-          TD(notimes_or_obj, simd=[('avx2', ints)]),\n+          TD(notimes_or_obj, simd=[('avx512f', cmplxvec),('avx2', ints)]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'mM', 'M'),\n@@ -279,7 +280,7 @@ def english_upper(s):\n     Ufunc(2, 1, None, # Zero is only a unit to the right, not the left\n           docstrings.get('numpy.core.umath.subtract'),\n           'PyUFunc_SubtractionTypeResolver',\n-          TD(ints + inexact, simd=[('avx2', ints)]),\n+          TD(ints + inexact, simd=[('avx512f', cmplxvec),('avx2', ints)]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'MM', 'm'),\n@@ -290,7 +291,7 @@ def english_upper(s):\n     Ufunc(2, 1, One,\n           docstrings.get('numpy.core.umath.multiply'),\n           'PyUFunc_MultiplicationTypeResolver',\n-          TD(notimes_or_obj, simd=[('avx2', ints)]),\n+          TD(notimes_or_obj, simd=[('avx512f', cmplxvec),('avx2', ints)]),\n           [TypeDescription('m', FullTypeDescr, 'mq', 'm'),\n            TypeDescription('m', FullTypeDescr, 'qm', 'm'),\n            TypeDescription('m', FullTypeDescr, 'md', 'm'),\n@@ -325,7 +326,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.conjugate'),\n           None,\n-          TD(ints+flts+cmplx, simd=[('avx2', ints)]),\n+          TD(ints+flts+cmplx, simd=[('avx2', ints), ('avx512f', cmplxvec)]),\n           TD(P, f='conjugate'),\n           ),\n 'fmod':\n@@ -340,7 +341,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.square'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints), ('fma', 'fd'), ('avx512f', 'fd')]),\n+          TD(ints+inexact, simd=[('avx2', ints), ('fma', 'fd'), ('avx512f', 'FDfd')]),\n           TD(O, f='Py_square'),\n           ),\n 'reciprocal':\n@@ -378,7 +379,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.absolute'),\n           'PyUFunc_AbsoluteTypeResolver',\n           TD(bints+flts+timedeltaonly, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n-          TD(cmplx, out=('f', 'd', 'g')),\n+          TD(cmplx, simd=[('avx512f', cmplxvec)], out=('f', 'd', 'g')),\n           TD(O, f='PyNumber_Absolute'),\n           ),\n '_arg':"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -2509,6 +2509,7 @@ HALF_ldexp_long(char **args, npy_intp const *dimensions, npy_intp const *steps,\n  * #ftype = npy_float, npy_double, npy_longdouble#\n  * #c = f, , l#\n  * #C = F, , L#\n+ * #SIMD = 1, 1, 0#\n  */\n \n /* similar to pairwise sum of real floats */\n@@ -2584,6 +2585,7 @@ pairwise_sum_@TYPE@(@ftype@ *rr, @ftype@ * ri, char * a, npy_intp n,\n     }\n }\n \n+\n /**begin repeat1\n  * arithmetic\n  * #kind = add, subtract#\n@@ -2662,6 +2664,32 @@ NPY_NO_EXPORT void\n     }\n }\n \n+#if @SIMD@\n+NPY_NO_EXPORT void\n+@TYPE@_add_avx512f(char **args, const npy_intp *dimensions, const npy_intp *steps, void *func)\n+{\n+    if (IS_BINARY_REDUCE) {\n+        @TYPE@_add(args, dimensions, steps, func);\n+    }\n+    else if (!run_binary_avx512f_add_@TYPE@(args, dimensions, steps)) {\n+        @TYPE@_add(args, dimensions, steps, func);\n+    }\n+}\n+\n+/**begin repeat1\n+ * arithmetic\n+ * #kind = subtract, multiply#\n+ */\n+NPY_NO_EXPORT void\n+@TYPE@_@kind@_avx512f(char **args, const npy_intp *dimensions, const npy_intp *steps, void *func)\n+{\n+    if (!run_binary_avx512f_@kind@_@TYPE@(args, dimensions, steps)) {\n+        @TYPE@_@kind@(args, dimensions, steps, func);\n+    }\n+}\n+/**end repeat1**/\n+#endif\n+\n NPY_NO_EXPORT void\n @TYPE@_floor_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n@@ -2819,6 +2847,21 @@ NPY_NO_EXPORT void\n     }\n }\n \n+#if @SIMD@\n+/**begin repeat1\n+ * arithmetic\n+ * #kind = conjugate, square, absolute#\n+ */\n+NPY_NO_EXPORT void\n+@TYPE@_@kind@_avx512f(char **args, const npy_intp *dimensions, const npy_intp *steps, void *func)\n+{\n+    if (!run_unary_avx512f_@kind@_@TYPE@(args, dimensions, steps)) {\n+        @TYPE@_@kind@(args, dimensions, steps, func);\n+    }\n+}\n+/**end repeat1**/\n+#endif\n+\n NPY_NO_EXPORT void\n @TYPE@__arg(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -356,20 +356,27 @@ NPY_NO_EXPORT void\n  * #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n  * #c = f, , l#\n  * #C = F, , L#\n+ * #IFSIMD = 1, 1, 0#\n  */\n \n /**begin repeat1\n+ * #isa = , _avx512f#\n+ */\n+\n+/**begin repeat2\n  * arithmetic\n  * #kind = add, subtract#\n  * #OP = +, -#\n  */\n+\n NPY_NO_EXPORT void\n-C@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+C@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n-/**end repeat1**/\n+/**end repeat2**/\n \n NPY_NO_EXPORT void\n-C@TYPE@_multiply(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+C@TYPE@_multiply@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+/**end repeat1**/\n \n NPY_NO_EXPORT void\n C@TYPE@_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n@@ -408,20 +415,25 @@ NPY_NO_EXPORT void\n C@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat1**/\n \n-NPY_NO_EXPORT void\n-C@TYPE@_square(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n-\n NPY_NO_EXPORT void\n C@TYPE@_reciprocal(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n \n NPY_NO_EXPORT void\n C@TYPE@__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n \n+/**begin repeat1\n+ * #isa = , _avx512f#\n+ */\n+\n NPY_NO_EXPORT void\n-C@TYPE@_conjugate(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+C@TYPE@_conjugate@isa@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func));\n \n NPY_NO_EXPORT void\n-C@TYPE@_absolute(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+C@TYPE@_absolute@isa@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func));\n+\n+NPY_NO_EXPORT void\n+C@TYPE@_square@isa@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(data));\n+/**end repeat1**/\n \n NPY_NO_EXPORT void\n C@TYPE@__arg(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n@@ -444,7 +456,6 @@ C@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, v\n NPY_NO_EXPORT void\n C@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat1**/\n-\n #define C@TYPE@_true_divide C@TYPE@_divide\n \n /**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -55,6 +55,13 @@ abs_ptrdiff(char *a, char *b)\n     return (a > b) ? (a - b) : (b - a);\n }\n \n+#define IS_BINARY_STRIDE_ONE(esize, vsize) \\\n+    ((steps[0] == esize) && \\\n+     (steps[1] == esize) && \\\n+     (steps[2] == esize) && \\\n+     (abs_ptrdiff(args[2], args[0]) >= vsize) && \\\n+     (abs_ptrdiff(args[2], args[1]) >= vsize))\n+\n /*\n  * stride is equal to element size and input and destination are equal or\n  * don't overlap within one register. The check of the steps against\n@@ -156,6 +163,71 @@ abs_ptrdiff(char *a, char *b)\n  * if it was run returns true and false if nothing was done\n  */\n \n+/*\n+ *****************************************************************************\n+ **                           CMPLX DISPATCHERS\n+ *****************************************************************************\n+ */\n+\n+/**begin repeat\n+ * #TYPE = CFLOAT, CDOUBLE#\n+ * #type= npy_float, npy_double#\n+ * #esize = 8, 16#\n+ */\n+\n+/**begin repeat1\n+ *  #func = add, subtract, multiply#\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps);\n+#endif\n+\n+static NPY_INLINE int\n+run_binary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)\n+{\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+    if (IS_BINARY_STRIDE_ONE(@esize@, 64)) {\n+        AVX512F_@func@_@TYPE@(args, dimensions, steps);\n+        return 1;\n+    }\n+    else\n+        return 0;\n+#endif\n+    return 0;\n+}\n+\n+/**end repeat1**/\n+\n+/**begin repeat1\n+ *  #func = square, absolute, conjugate#\n+ *  #outsize = 1, 2, 1#\n+ *  #max_stride = 2, 8, 8#\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_@func@_@TYPE@(@type@*, @type@*, const npy_intp n, const npy_intp stride);\n+#endif\n+\n+static NPY_INLINE int\n+run_unary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)\n+{\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+    if ((IS_OUTPUT_BLOCKABLE_UNARY((npy_uint)(@esize@/@outsize@), 64)) && (labs(steps[0]) < 2*@max_stride@*@esize@)) {\n+        AVX512F_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0], steps[0]);\n+        return 1;\n+    }\n+    else\n+        return 0;\n+#endif\n+    return 0;\n+}\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n /*\n  *****************************************************************************\n  **                           FLOAT DISPATCHERS\n@@ -1591,9 +1663,17 @@ avx512_scalef_ps(__m512 poly, __m512 quadrant)\n }\n /**begin repeat\n  *  #vsub  = ps, pd#\n+ *  #type= npy_float, npy_double#\n  *  #epi_vsub  = epi32, epi64#\n  *  #vtype = __m512, __m512d#\n+ *  #mask = __mmask16, __mmask8#\n  *  #and_const = 0x7fffffff, 0x7fffffffffffffffLL#\n+ *  #neg_mask = 0x80000000, 0x8000000000000000#\n+ *  #perm_ = 0xb1, 0x55#\n+ *  #cmpx_img_mask = 0xAAAA, 0xAA#\n+ *  #cmpx_re_mask = 0x5555, 0x55#\n+ *  #INF = NPY_INFINITYF, NPY_INFINITY#\n+ *  #NAN = NPY_NANF, NPY_NAN#\n  */\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n avx512_abs_@vsub@(@vtype@ x)\n@@ -1631,6 +1711,96 @@ avx512_trunc_@vsub@(@vtype@ x)\n {\n     return _mm512_roundscale_@vsub@(x, 0x0B);\n }\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_hadd_@vsub@(const @vtype@ x)\n+{\n+    return _mm512_add_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_hsub_@vsub@(const @vtype@ x)\n+{\n+    return _mm512_sub_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_cabsolute_@vsub@(const @vtype@ x1,\n+                        const @vtype@ x2,\n+                        const __m512i re_indices,\n+                        const __m512i im_indices)\n+{\n+    @vtype@ inf = _mm512_set1_@vsub@(@INF@);\n+    @vtype@ nan = _mm512_set1_@vsub@(@NAN@);\n+    @vtype@ x1_abs = avx512_abs_@vsub@(x1);\n+    @vtype@ x2_abs = avx512_abs_@vsub@(x2);\n+    @vtype@ re = _mm512_permutex2var_@vsub@(x1_abs, re_indices, x2_abs);\n+    @vtype@ im = _mm512_permutex2var_@vsub@(x1_abs, im_indices , x2_abs);\n+    /*\n+     * If real or imag = INF, then convert it to inf + j*inf\n+     * Handles: inf + j*nan, nan + j*inf\n+     */\n+    @mask@ re_infmask = _mm512_cmp_@vsub@_mask(re, inf, _CMP_EQ_OQ);\n+    @mask@ im_infmask = _mm512_cmp_@vsub@_mask(im, inf, _CMP_EQ_OQ);\n+    im = _mm512_mask_mov_@vsub@(im, re_infmask, inf);\n+    re = _mm512_mask_mov_@vsub@(re, im_infmask, inf);\n+\n+    /*\n+     * If real or imag = NAN, then convert it to nan + j*nan\n+     * Handles: x + j*nan, nan + j*x\n+     */\n+    @mask@ re_nanmask = _mm512_cmp_@vsub@_mask(re, re, _CMP_NEQ_UQ);\n+    @mask@ im_nanmask = _mm512_cmp_@vsub@_mask(im, im, _CMP_NEQ_UQ);\n+    im = _mm512_mask_mov_@vsub@(im, re_nanmask, nan);\n+    re = _mm512_mask_mov_@vsub@(re, im_nanmask, nan);\n+\n+    @vtype@ larger  = _mm512_max_@vsub@(re, im);\n+    @vtype@ smaller = _mm512_min_@vsub@(im, re);\n+\n+    /*\n+     * Calculate div_mask to prevent 0./0. and inf/inf operations in div\n+     */\n+    @mask@ zeromask = _mm512_cmp_@vsub@_mask(larger, _mm512_setzero_@vsub@(), _CMP_EQ_OQ);\n+    @mask@ infmask = _mm512_cmp_@vsub@_mask(smaller, inf, _CMP_EQ_OQ);\n+    @mask@ div_mask = _mm512_knot(_mm512_kor(zeromask, infmask));\n+    @vtype@ ratio = _mm512_maskz_div_@vsub@(div_mask, smaller, larger);\n+    @vtype@ hypot = _mm512_sqrt_@vsub@(_mm512_fmadd_@vsub@(\n+                                        ratio, ratio, _mm512_set1_@vsub@(1.0f)));\n+    return _mm512_mul_@vsub@(hypot, larger);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_conjugate_@vsub@(const @vtype@ x)\n+{\n+    /*\n+     * __mm512_mask_xor_ps/pd requires AVX512DQ. We cast it to __m512i and\n+     * use the xor_epi32/64 uinstruction instead. Cast is a zero latency instruction\n+     */\n+    __m512i cast_x = _mm512_cast@vsub@_si512(x);\n+    __m512i res = _mm512_mask_xor_@epi_vsub@(cast_x, @cmpx_img_mask@,\n+                                        cast_x, _mm512_set1_@epi_vsub@(@neg_mask@));\n+    return _mm512_castsi512_@vsub@(res);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_cmul_@vsub@(@vtype@ x1, @vtype@ x2)\n+{\n+    // x1 = r1, i1\n+    // x2 = r2, i2\n+    @vtype@ x3  = _mm512_permute_@vsub@(x2, @perm_@);   // i2, r2\n+    @vtype@ x12 = _mm512_mul_@vsub@(x1, x2);            // r1*r2, i1*i2\n+    @vtype@ x13 = _mm512_mul_@vsub@(x1, x3);            // r1*i2, r2*i1\n+    @vtype@ outreal = avx512_hsub_@vsub@(x12);          // r1*r2 - i1*i2, r1*r2 - i1*i2\n+    @vtype@ outimg  = avx512_hadd_@vsub@(x13);          // r1*i2 + i1*r2, r1*i2 + i1*r2\n+    return _mm512_mask_blend_@vsub@(@cmpx_img_mask@, outreal, outimg);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_csquare_@vsub@(@vtype@ x)\n+{\n+    return avx512_cmul_@vsub@(x, x);\n+}\n+\n /**end repeat**/\n #endif\n \n@@ -2450,6 +2620,184 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n #endif\n /**end repeat**/\n \n+/**begin repeat\n+ * #TYPE = CFLOAT, CDOUBLE#\n+ * #type = npy_float, npy_double#\n+ * #num_lanes = 16, 8#\n+ * #vsuffix = ps, pd#\n+ * #epi_vsub  = epi32, epi64#\n+ * #mask = __mmask16, __mmask8#\n+ * #vtype = __m512, __m512d#\n+ * #scale = 4, 8#\n+ * #vindextype = __m512i, __m256i#\n+ * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#\n+ * #storemask = 0xFF, 0xF#\n+ * #IS_FLOAT = 1, 0#\n+ */\n+\n+/**begin repeat1\n+ *  #func = add, subtract, multiply#\n+ *  #vectorf = _mm512_add, _mm512_sub, avx512_cmul#\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)\n+{\n+    const npy_intp array_size = dimensions[0];\n+    npy_intp num_remaining_elements = 2*array_size;\n+    @type@* ip1 = (@type@*) args[0];\n+    @type@* ip2 = (@type@*) args[1];\n+    @type@* op  = (@type@*) args[2];\n+\n+    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();\n+\n+    while (num_remaining_elements > 0) {\n+        if (num_remaining_elements < @num_lanes@) {\n+            load_mask = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements, @num_lanes@);\n+        }\n+        @vtype@ x1, x2;\n+        x1 = avx512_masked_load_@vsuffix@(load_mask, ip1);\n+        x2 = avx512_masked_load_@vsuffix@(load_mask, ip2);\n+\n+        @vtype@ out = @vectorf@_@vsuffix@(x1, x2);\n+\n+        _mm512_mask_storeu_@vsuffix@(op, load_mask, out);\n+\n+        ip1 += @num_lanes@;\n+        ip2 += @num_lanes@;\n+        op += @num_lanes@;\n+        num_remaining_elements -= @num_lanes@;\n+    }\n+}\n+#endif\n+/**end repeat1**/\n+\n+/**begin repeat1\n+ *  #func = square, conjugate#\n+ *  #vectorf = avx512_csquare, avx512_conjugate#\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_@func@_@TYPE@(@type@ * op,\n+                      @type@ * ip,\n+                      const npy_intp array_size,\n+                      const npy_intp steps)\n+{\n+    npy_intp num_remaining_elements = 2*array_size;\n+    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(@type@)/2;\n+\n+     /*\n+      * Note: while generally indices are npy_intp, we ensure that our maximum index\n+      * will fit in an int32 as a precondition for this function via max_stride\n+      */\n+    npy_int32 index_ip1[16];\n+    for (npy_int32 ii = 0; ii < @num_lanes@; ii=ii+2) {\n+        index_ip1[ii] = ii*stride_ip1;\n+        index_ip1[ii+1] = ii*stride_ip1 + 1;\n+    }\n+    @vindextype@ vindex = @vindexload@((@vindextype@*)index_ip1);\n+    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();\n+    @vtype@ zeros = _mm512_setzero_@vsuffix@();\n+\n+    while (num_remaining_elements > 0) {\n+        if (num_remaining_elements < @num_lanes@) {\n+            load_mask = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements, @num_lanes@);\n+        }\n+        @vtype@ x1;\n+        if (stride_ip1 == 1) {\n+            x1 = avx512_masked_load_@vsuffix@(load_mask, ip);\n+        }\n+        else {\n+            x1  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex, load_mask);\n+        }\n+\n+        @vtype@ out = @vectorf@_@vsuffix@(x1);\n+\n+        _mm512_mask_storeu_@vsuffix@(op, load_mask, out);\n+        op += @num_lanes@;\n+        ip += @num_lanes@*stride_ip1;\n+        num_remaining_elements -= @num_lanes@;\n+    }\n+}\n+#endif\n+/**end repeat1**/\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_absolute_@TYPE@(@type@ * op,\n+                        @type@ * ip,\n+                        const npy_intp array_size,\n+                        const npy_intp steps)\n+{\n+    npy_intp num_remaining_elements = 2*array_size;\n+    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(@type@)/2;\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via max_stride\n+     */\n+    npy_int32 index_ip[32];\n+    for (npy_int32 ii = 0; ii < 2*@num_lanes@; ii=ii+2) {\n+        index_ip[ii] = ii*stride_ip1;\n+        index_ip[ii+1] = ii*stride_ip1 + 1;\n+    }\n+    @vindextype@ vindex1 = @vindexload@((@vindextype@*)index_ip);\n+    @vindextype@ vindex2 = @vindexload@((@vindextype@*)(index_ip+@num_lanes@));\n+\n+    @mask@ load_mask1 = avx512_get_full_load_mask_@vsuffix@();\n+    @mask@ load_mask2 = avx512_get_full_load_mask_@vsuffix@();\n+    @mask@ store_mask = avx512_get_full_load_mask_@vsuffix@();\n+    @vtype@ zeros = _mm512_setzero_@vsuffix@();\n+\n+#if @IS_FLOAT@\n+    __m512i re_index = _mm512_set_epi32(30,28,26,24,22,20,18,16,14,12,10,8,6,4,2,0);\n+    __m512i im_index  = _mm512_set_epi32(31,29,27,25,23,21,19,17,15,13,11,9,7,5,3,1);\n+#else\n+    __m512i re_index = _mm512_set_epi64(14,12,10,8,6,4,2,0);\n+    __m512i im_index  = _mm512_set_epi64(15,13,11,9,7,5,3,1);\n+#endif\n+\n+    while (num_remaining_elements > 0) {\n+        if (num_remaining_elements < @num_lanes@) {\n+            load_mask1 = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements, @num_lanes@);\n+            load_mask2 = 0x0000;\n+            store_mask = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements/2, @num_lanes@);\n+        } else if (num_remaining_elements < 2*@num_lanes@) {\n+            load_mask1 = avx512_get_full_load_mask_@vsuffix@();\n+            load_mask2 = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements - @num_lanes@, @num_lanes@);\n+            store_mask = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements/2, @num_lanes@);\n+        }\n+        @vtype@ x1, x2;\n+        if (stride_ip1 == 1) {\n+            x1 = avx512_masked_load_@vsuffix@(load_mask1, ip);\n+            x2 = avx512_masked_load_@vsuffix@(load_mask2, ip+@num_lanes@);\n+        }\n+        else {\n+            x1  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex1, load_mask1);\n+            x2  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex2, load_mask2);\n+        }\n+\n+        @vtype@ out = avx512_cabsolute_@vsuffix@(x1, x2, re_index, im_index);\n+\n+        _mm512_mask_storeu_@vsuffix@(op, store_mask, out);\n+        op += @num_lanes@;\n+        ip += 2*@num_lanes@*stride_ip1;\n+        num_remaining_elements -= 2*@num_lanes@;\n+    }\n+    npy_clear_floatstatus_barrier((char*)op);\n+}\n+\n+#endif\n+/**end repeat**/\n+\n /*\n  *****************************************************************************\n  **                           BOOL LOOPS"
            },
            {
                "filename": "numpy/core/tests/test_umath_complex.py",
                "patch": "@@ -6,7 +6,7 @@\n # import the c-extension module directly since _arg is not exported via umath\n import numpy.core._multiarray_umath as ncu\n from numpy.testing import (\n-    assert_raises, assert_equal, assert_array_equal, assert_almost_equal\n+    assert_raises, assert_equal, assert_array_equal, assert_almost_equal, assert_array_max_ulp\n     )\n \n # TODO: branch cuts (use Pauli code)\n@@ -540,3 +540,40 @@ def check_complex_value(f, x1, y1, x2, y2, exact=True):\n             assert_equal(f(z1), z2)\n         else:\n             assert_almost_equal(f(z1), z2)\n+\n+class TestSpecialComplexAVX(object):\n+    @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n+    @pytest.mark.parametrize(\"astype\", [np.complex64, np.complex128])\n+    def test_array(self, stride, astype):\n+        arr = np.array([np.complex(np.nan , np.nan),\n+                        np.complex(np.nan , np.inf),\n+                        np.complex(np.inf , np.nan),\n+                        np.complex(np.inf , np.inf),\n+                        np.complex(0.     , np.inf),\n+                        np.complex(np.inf , 0.),\n+                        np.complex(0.     , 0.),\n+                        np.complex(0.     , np.nan),\n+                        np.complex(np.nan , 0.)], dtype=astype)\n+        abs_true = np.array([np.nan, np.inf, np.inf, np.inf, np.inf, np.inf, 0., np.nan, np.nan], dtype=arr.real.dtype)\n+        sq_true = np.array([np.complex(np.nan,  np.nan),\n+                            np.complex(np.nan,  np.nan),\n+                            np.complex(np.nan,  np.nan),\n+                            np.complex(np.nan,  np.inf),\n+                            np.complex(-np.inf, np.nan),\n+                            np.complex(np.inf,  np.nan),\n+                            np.complex(0.,     0.),\n+                            np.complex(np.nan, np.nan),\n+                            np.complex(np.nan, np.nan)], dtype=astype)\n+        assert_equal(np.abs(arr[::stride]), abs_true[::stride])\n+        with np.errstate(invalid='ignore'):\n+            assert_equal(np.square(arr[::stride]), sq_true[::stride])\n+\n+class TestComplexAbsoluteAVX(object):\n+    @pytest.mark.parametrize(\"arraysize\", [1,2,3,4,5,6,7,8,9,10,11,13,15,17,18,19])\n+    @pytest.mark.parametrize(\"stride\", [-4,-3,-2,-1,1,2,3,4])\n+    @pytest.mark.parametrize(\"astype\", [np.complex64, np.complex128])\n+    # test to ensure masking and strides work as intended in the AVX implementation\n+    def test_array(self, arraysize, stride, astype):\n+        arr = np.ones(arraysize, dtype=astype)\n+        abs_true = np.ones(arraysize, dtype=arr.real.dtype)\n+        assert_equal(np.abs(arr[::stride]), abs_true[::stride])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 14867,
        "body": "Micro-benchmarks (added in commit 291e0b4cdbe3cad1f2b8f2b36b40f1d918c6a771) with parameters: function (`maximum, minimum`), dtype (`f,d`) and stride (`1,2,4`)\r\n\r\n```\r\n before           after         ratio                                          \r\n[dcc1fc2f]       [42893993]                                                        \r\n<master>         <maximum-avx>                                                     \r\n  47.9\u00b10.4us       13.5\u00b10.4us     0.28  bench_avx.AVX_BFunc.time_ufunc('minimum', 'd', 4)\r\n  48.0\u00b10.2us       13.1\u00b10.3us     0.27  bench_avx.AVX_BFunc.time_ufunc('maximum', 'd', 4)\r\n  46.2\u00b10.2us       11.3\u00b10.2us     0.24  bench_avx.AVX_BFunc.time_ufunc('minimum', 'd', 2)\r\n  46.7\u00b10.2us       11.2\u00b10.1us     0.24  bench_avx.AVX_BFunc.time_ufunc('maximum', 'd', 2)\r\n 45.9\u00b10.05us      9.68\u00b10.03us     0.21  bench_avx.AVX_BFunc.time_ufunc('maximum', 'f', 4)\r\n  46.2\u00b10.2us      9.71\u00b10.08us     0.21  bench_avx.AVX_BFunc.time_ufunc('minimum', 'f', 4)\r\n  45.4\u00b10.1us       9.13\u00b10.1us     0.20  bench_avx.AVX_BFunc.time_ufunc('minimum', 'd', 1)\r\n  45.8\u00b10.1us       9.11\u00b10.1us     0.20  bench_avx.AVX_BFunc.time_ufunc('maximum', 'd', 1)\r\n  45.8\u00b10.2us      9.04\u00b10.08us     0.20  bench_avx.AVX_BFunc.time_ufunc('maximum', 'f', 2)\r\n  45.8\u00b10.3us      9.00\u00b10.07us     0.20  bench_avx.AVX_BFunc.time_ufunc('minimum', 'f', 2)\r\n 45.2\u00b10.06us      5.41\u00b10.05us     0.12  bench_avx.AVX_BFunc.time_ufunc('minimum', 'f', 1)\r\n 44.9\u00b10.07us      5.35\u00b10.04us     0.12  bench_avx.AVX_BFunc.time_ufunc('maximum', 'f', 1)\r\n```\r\n\r\nVectorizing np.maximum also speeds up sklearn's KMeans fit API by 20-30%. (this particular benchmark measures time taken for `KMeans.fit `API in `sklearn's cluster` module). It is not added to this patch since it has a dependency on sklearn.\r\n\r\n       before           after         ratio\r\n     [2be03c8d]       [583d0d2e]\r\n     <master>         <maximum-avx>\r\n     4.56\u00b10.2s        3.50\u00b10.1s     0.77  bench_sklearn.AVX_KMEANS.time_kmeans\r\n\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_avx.py",
                "patch": "@@ -29,3 +29,25 @@ def setup(self, ufuncname, stride, dtype):\n \n     def time_ufunc(self, ufuncname, stride, dtype):\n         self.f(self.arr[::stride])\n+\n+avx_bfuncs = ['maximum',\n+              'minimum']\n+\n+class AVX_BFunc(Benchmark):\n+\n+    params = [avx_bfuncs, dtype, stride]\n+    param_names = ['avx_based_bfunc', 'dtype', 'stride']\n+    timeout = 10\n+\n+    def setup(self, ufuncname, dtype, stride):\n+        np.seterr(all='ignore')\n+        try:\n+            self.f = getattr(np, ufuncname)\n+        except AttributeError:\n+            raise NotImplementedError()\n+        N = 10000\n+        self.arr1 = np.array(np.random.rand(stride*N), dtype=dtype)\n+        self.arr2 = np.array(np.random.rand(stride*N), dtype=dtype)\n+\n+    def time_ufunc(self, ufuncname, dtype, stride):\n+        self.f(self.arr1[::stride], self.arr2[::stride])"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -492,14 +492,14 @@ def english_upper(s):\n     Ufunc(2, 1, ReorderableNone,\n           docstrings.get('numpy.core.umath.maximum'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(noobj),\n+          TD(noobj, simd=[('avx512f', 'fd')]),\n           TD(O, f='npy_ObjectMax')\n           ),\n 'minimum':\n     Ufunc(2, 1, ReorderableNone,\n           docstrings.get('numpy.core.umath.minimum'),\n           'PyUFunc_SimpleUniformOperationTypeResolver',\n-          TD(noobj),\n+          TD(noobj, simd=[('avx512f', 'fd')]),\n           TD(O, f='npy_ObjectMin')\n           ),\n 'clip':"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1897,6 +1897,34 @@ NPY_NO_EXPORT void\n  * #kind = maximum, minimum#\n  * #OP =  >=, <=#\n  **/\n+NPY_NO_EXPORT void\n+@TYPE@_@kind@_avx512f(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    /*  */\n+    if (IS_BINARY_REDUCE) {\n+        if (!run_unary_reduce_simd_@kind@_@TYPE@(args, dimensions, steps)) {\n+            BINARY_REDUCE_LOOP(@type@) {\n+                const @type@ in2 = *(@type@ *)ip2;\n+                /* Order of operations important for MSVC 2015 */\n+                io1 = (io1 @OP@ in2 || npy_isnan(io1)) ? io1 : in2;\n+            }\n+            *((@type@ *)iop1) = io1;\n+        }\n+    }\n+    else {\n+        if (!run_binary_avx512f_@kind@_@TYPE@(args, dimensions, steps)) {\n+            BINARY_LOOP {\n+                @type@ in1 = *(@type@ *)ip1;\n+                const @type@ in2 = *(@type@ *)ip2;\n+                /* Order of operations important for MSVC 2015 */\n+                in1 = (in1 @OP@ in2 || npy_isnan(in1)) ? in1 : in2;\n+                *((@type@ *)op1) = in1;\n+            }\n+        }\n+    }\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+}\n+\n NPY_NO_EXPORT void\n @TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -174,6 +174,14 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @TYPE@_sqrt(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n+/**begin repeat1\n+ * #func = maximum, minimum#\n+ */\n+NPY_NO_EXPORT void\n+@TYPE@_@func@_avx512f(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+\n+/**end repeat1**/\n+\n /**begin repeat1\n  * #isa = avx512f, fma#\n  */"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -34,6 +34,21 @@\n \n #define VECTOR_SIZE_BYTES 16\n \n+/*\n+ * MAX_STEP_SIZE is used to determine if we need to use SIMD version of the ufunc.\n+ * Very large step size can be as slow as processing it using scalar. The\n+ * value of 2097152 ( = 2MB) was chosen using 2 considerations:\n+ * 1) Typical linux kernel page size is 4Kb, but sometimes it could also be 2MB\n+ *    which is == 2097152 Bytes. For a step size as large as this, surely all\n+ *    the loads/stores of gather/scatter instructions falls on 16 different pages\n+ *    which one would think would slow down gather/scatter instructions.\n+ * 2) It additionally satisfies MAX_STEP_SIZE*16/esize < NPY_MAX_INT32 which\n+ *    allows us to use i32 version of gather/scatter (as opposed to the i64 version)\n+ *    without problems (step larger than NPY_MAX_INT32*esize/16 would require use of\n+ *    i64gather/scatter). esize = element size = 4/8 bytes for float/double.\n+ */\n+#define MAX_STEP_SIZE 2097152\n+\n static NPY_INLINE npy_uintp\n abs_ptrdiff(char *a, char *b)\n {\n@@ -51,11 +66,29 @@ abs_ptrdiff(char *a, char *b)\n      ((abs_ptrdiff(args[1], args[0]) >= (vsize)) || \\\n       ((abs_ptrdiff(args[1], args[0]) == 0))))\n \n+/*\n+ * Avoid using SIMD for very large step sizes for several reasons:\n+ * 1) Supporting large step sizes requires use of i64gather/scatter_ps instructions,\n+ *    in which case we need two i64gather instructions and an additional vinsertf32x8\n+ *    instruction to load a single zmm register (since one i64gather instruction\n+ *    loads into a ymm register). This is not ideal for performance.\n+ * 2) Gather and scatter instructions can be slow when the loads/stores\n+ *    cross page boundaries.\n+ *\n+ * We instead rely on i32gather/scatter_ps instructions which use a 32-bit index\n+ * element. The index needs to be < INT_MAX to avoid overflow. MAX_STEP_SIZE ensures this.\n+ */\n+#define IS_BINARY_SMALL_STEPS \\\n+    ((abs(steps[0]) < MAX_STEP_SIZE)  && \\\n+     (abs(steps[1]) < MAX_STEP_SIZE)  && \\\n+     (abs(steps[2]) < MAX_STEP_SIZE))\n+\n /*\n  * output should be contiguous, can handle strided input data\n+ * Input step should be smaller than MAX_STEP_SIZE for performance\n  */\n #define IS_OUTPUT_BLOCKABLE_UNARY(esize, vsize) \\\n-    (steps[1] == (esize) && \\\n+    (steps[1] == (esize) && abs(steps[0]) < MAX_STEP_SIZE && \\\n      (npy_is_aligned(args[0], esize) && npy_is_aligned(args[1], esize)) && \\\n      ((abs_ptrdiff(args[1], args[0]) >= (vsize)) || \\\n       ((abs_ptrdiff(args[1], args[0]) == 0))))\n@@ -129,6 +162,39 @@ abs_ptrdiff(char *a, char *b)\n  *****************************************************************************\n  */\n \n+/**begin repeat\n+ * #type = npy_float, npy_double, npy_longdouble#\n+ * #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n+ * #EXISTS = 1, 1, 0#\n+ */\n+\n+/**begin repeat1\n+ *  #func = maximum, minimum#\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@\n+static NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps);\n+#endif\n+\n+static NPY_INLINE int\n+run_binary_avx512f_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+{\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@\n+    if (IS_BINARY_SMALL_STEPS) {\n+        AVX512F_@func@_@TYPE@(args, dimensions, steps);\n+        return 1;\n+    }\n+    else\n+        return 0;\n+#endif\n+    return 0;\n+}\n+\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  * #ISA = FMA, AVX512F#\n  * #isa = fma, avx512f#\n@@ -1671,6 +1737,101 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n #endif\n /**end repeat**/\n \n+/**begin repeat\n+ * #type = npy_float, npy_double#\n+ * #TYPE = FLOAT, DOUBLE#\n+ * #num_lanes = 16, 8#\n+ * #vsuffix = ps, pd#\n+ * #mask = __mmask16, __mmask8#\n+ * #vtype = __m512, __m512d#\n+ * #scale = 4, 8#\n+ * #vindextype = __m512i, __m256i#\n+ * #vindexsize = 512, 256#\n+ * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#\n+ */\n+\n+/**begin repeat1\n+ *  #func = maximum, minimum#\n+ *  #vectorf = max, min#\n+ */\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_INLINE NPY_GCC_TARGET_AVX512F void\n+AVX512F_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+{\n+    const npy_intp stride_ip1 = steps[0]/(npy_intp)sizeof(@type@);\n+    const npy_intp stride_ip2 = steps[1]/(npy_intp)sizeof(@type@);\n+    const npy_intp stride_op = steps[2]/(npy_intp)sizeof(@type@);\n+    const npy_intp array_size = dimensions[0];\n+    npy_intp num_remaining_elements = array_size;\n+    @type@* ip1 = (@type@*) args[0];\n+    @type@* ip2 = (@type@*) args[1];\n+    @type@* op  = (@type@*) args[2];\n+\n+    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via\n+     * IS_BINARY_SMALL_STEPS\n+     */\n+\n+    npy_int32 index_ip1[@num_lanes@], index_ip2[@num_lanes@], index_op[@num_lanes@];\n+    for (npy_int32 ii = 0; ii < @num_lanes@; ii++) {\n+        index_ip1[ii] = ii*stride_ip1;\n+        index_ip2[ii] = ii*stride_ip2;\n+        index_op[ii] = ii*stride_op;\n+    }\n+    @vindextype@ vindex_ip1 = @vindexload@((@vindextype@*)&index_ip1[0]);\n+    @vindextype@ vindex_ip2 = @vindexload@((@vindextype@*)&index_ip2[0]);\n+    @vindextype@ vindex_op  = @vindexload@((@vindextype@*)&index_op[0]);\n+    @vtype@ zeros_f = _mm512_setzero_@vsuffix@();\n+\n+    while (num_remaining_elements > 0) {\n+        if (num_remaining_elements < @num_lanes@) {\n+            load_mask = avx512_get_partial_load_mask_@vsuffix@(\n+                                    num_remaining_elements, @num_lanes@);\n+        }\n+        @vtype@ x1, x2;\n+        if (stride_ip1 == 1) {\n+            x1 = avx512_masked_load_@vsuffix@(load_mask, ip1);\n+        }\n+        else {\n+            x1 = avx512_masked_gather_@vsuffix@(zeros_f, ip1, vindex_ip1, load_mask);\n+        }\n+        if (stride_ip2 == 1) {\n+            x2 = avx512_masked_load_@vsuffix@(load_mask, ip2);\n+        }\n+        else {\n+            x2 = avx512_masked_gather_@vsuffix@(zeros_f, ip2, vindex_ip2, load_mask);\n+        }\n+\n+        /*\n+         * when only one of the argument is a nan, the maxps/maxpd instruction\n+         * returns the second argument. The additional blend instruction fixes\n+         * this issue to conform with NumPy behaviour.\n+         */\n+        @mask@ nan_mask = _mm512_cmp_@vsuffix@_mask(x1, x1, _CMP_NEQ_UQ);\n+        @vtype@ out = _mm512_@vectorf@_@vsuffix@(x1, x2);\n+        out = _mm512_mask_blend_@vsuffix@(nan_mask, out, x1);\n+\n+        if (stride_op == 1) {\n+            _mm512_mask_storeu_@vsuffix@(op, load_mask, out);\n+        }\n+        else {\n+            /* scatter! */\n+            _mm512_mask_i32scatter_@vsuffix@(op, load_mask, vindex_op, out, @scale@);\n+        }\n+\n+        ip1 += @num_lanes@*stride_ip1;\n+        ip2 += @num_lanes@*stride_ip2;\n+        op += @num_lanes@*stride_op;\n+        num_remaining_elements -= @num_lanes@;\n+    }\n+}\n+#endif\n+/**end repeat**/\n+/**end repeat1**/\n \n /**begin repeat\n  * #ISA = FMA, AVX512F#\n@@ -1699,16 +1860,23 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n                    const npy_intp array_size,\n                    const npy_intp steps)\n {\n-    const npy_intp stride = steps/sizeof(npy_float);\n-    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    const npy_intp stride = steps/(npy_intp)sizeof(npy_float);\n+    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_float);\n     npy_intp num_remaining_elements = array_size;\n     @vtype@ ones_f = _mm@vsize@_set1_ps(1.0f);\n     @mask@ load_mask = @isa@_get_full_load_mask_ps();\n #if @replace_0_with_1@\n     @mask@ inv_load_mask = @isa@_invert_mask_ps(load_mask);\n #endif\n-    npy_int indexarr[16];\n-    for (npy_int ii = 0; ii < 16; ii++) {\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via\n+     * IS_OUTPUT_BLOCKABLE_UNARY\n+     */\n+\n+    npy_int32 indexarr[16];\n+    for (npy_int32 ii = 0; ii < 16; ii++) {\n         indexarr[ii] = ii*stride;\n     }\n     @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);\n@@ -1778,16 +1946,22 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n                     const npy_intp array_size,\n                     const npy_intp steps)\n {\n-    const npy_intp stride = steps/sizeof(npy_double);\n-    const npy_int num_lanes = @BYTES@/sizeof(npy_double);\n+    const npy_intp stride = steps/(npy_intp)sizeof(npy_double);\n+    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_double);\n     npy_intp num_remaining_elements = array_size;\n     @mask@ load_mask = @isa@_get_full_load_mask_pd();\n #if @replace_0_with_1@\n     @mask@ inv_load_mask = @isa@_invert_mask_pd(load_mask);\n #endif\n     @vtype@ ones_d = _mm@vsize@_set1_pd(1.0f);\n-    npy_int indexarr[8];\n-    for (npy_int ii = 0; ii < 8; ii++) {\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via\n+     * IS_OUTPUT_BLOCKABLE_UNARY\n+     */\n+    npy_int32 indexarr[8];\n+    for (npy_int32 ii = 0; ii < 8; ii++) {\n         indexarr[ii] = ii*stride;\n     }\n     @vindextype@ vindex = @vindexload@((@vindextype@*)&indexarr[0]);\n@@ -1874,8 +2048,8 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n                    const npy_intp steps,\n                    NPY_TRIG_OP my_trig_op)\n {\n-    const npy_intp stride = steps/sizeof(npy_float);\n-    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    const npy_intp stride = steps/(npy_intp)sizeof(npy_float);\n+    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_float);\n     npy_float large_number = 71476.0625f;\n     if (my_trig_op == npy_compute_sin) {\n         large_number = 117435.992f;\n@@ -1905,8 +2079,14 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     @mask@ nan_mask, glibc_mask, sine_mask, negate_mask;\n     @mask@ load_mask = @isa@_get_full_load_mask_ps();\n     npy_intp num_remaining_elements = array_size;\n-    npy_int indexarr[16];\n-    for (npy_int ii = 0; ii < 16; ii++) {\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via\n+     * IS_OUTPUT_BLOCKABLE_UNARY\n+     */\n+    npy_int32 indexarr[16];\n+    for (npy_int32 ii = 0; ii < 16; ii++) {\n         indexarr[ii] = ii*stride;\n     }\n     @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);\n@@ -2017,12 +2197,18 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n                 const npy_intp array_size,\n                 const npy_intp steps)\n {\n-    const npy_intp stride = steps/sizeof(npy_float);\n-    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    const npy_intp stride = steps/(npy_intp)sizeof(npy_float);\n+    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_float);\n     npy_float xmax = 88.72283935546875f;\n     npy_float xmin = -103.97208404541015625f;\n-    npy_int indexarr[16];\n-    for (npy_int ii = 0; ii < 16; ii++) {\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via\n+     * IS_OUTPUT_BLOCKABLE_UNARY\n+     */\n+    npy_int32 indexarr[16];\n+    for (npy_int32 ii = 0; ii < 16; ii++) {\n         indexarr[ii] = ii*stride;\n     }\n \n@@ -2143,10 +2329,16 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n                 const npy_intp array_size,\n                 const npy_intp steps)\n {\n-    const npy_intp stride = steps/sizeof(npy_float);\n-    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n-    npy_int indexarr[16];\n-    for (npy_int ii = 0; ii < 16; ii++) {\n+    const npy_intp stride = steps/(npy_intp)sizeof(npy_float);\n+    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_float);\n+\n+    /*\n+     * Note: while generally indices are npy_intp, we ensure that our maximum index\n+     * will fit in an int32 as a precondition for this function via\n+     * IS_OUTPUT_BLOCKABLE_UNARY\n+     */\n+    npy_int32 indexarr[16];\n+    for (npy_int32 ii = 0; ii < 16; ii++) {\n         indexarr[ii] = ii*stride;\n     }\n "
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -1108,6 +1108,19 @@ def test_object_array(self):\n         arg2 = arg1 + 1\n         assert_equal(np.maximum(arg1, arg2), arg2)\n \n+    def test_strided_array(self):\n+        arr1 = np.array([-4.0, 1.0, 10.0,  0.0, np.nan, -np.nan, np.inf, -np.inf])\n+        arr2 = np.array([-2.0,-1.0, np.nan, 1.0, 0.0,    np.nan, 1.0,    -3.0])\n+        maxtrue  = np.array([-2.0, 1.0, np.nan, 1.0, np.nan, np.nan, np.inf, -3.0])\n+        out = np.ones(8)\n+        out_maxtrue = np.array([-2.0, 1.0, 1.0, 10.0, 1.0, 1.0, np.nan, 1.0])\n+        assert_equal(np.maximum(arr1,arr2), maxtrue)\n+        assert_equal(np.maximum(arr1[::2],arr2[::2]), maxtrue[::2])\n+        assert_equal(np.maximum(arr1[:4:], arr2[::2]), np.array([-2.0, np.nan, 10.0, 1.0]))\n+        assert_equal(np.maximum(arr1[::3], arr2[:3:]), np.array([-2.0, 0.0, np.nan]))\n+        assert_equal(np.maximum(arr1[:6:2], arr2[::3], out=out[::3]), np.array([-2.0, 10., np.nan]))\n+        assert_equal(out, out_maxtrue)\n+\n \n class TestMinimum(_FilterInvalids):\n     def test_reduce(self):\n@@ -1166,6 +1179,18 @@ def test_object_array(self):\n         arg2 = arg1 + 1\n         assert_equal(np.minimum(arg1, arg2), arg1)\n \n+    def test_strided_array(self):\n+        arr1 = np.array([-4.0, 1.0, 10.0,  0.0, np.nan, -np.nan, np.inf, -np.inf])\n+        arr2 = np.array([-2.0,-1.0, np.nan, 1.0, 0.0,    np.nan, 1.0,    -3.0])\n+        mintrue  = np.array([-4.0, -1.0, np.nan, 0.0, np.nan, np.nan, 1.0, -np.inf])\n+        out = np.ones(8)\n+        out_mintrue = np.array([-4.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.nan, 1.0])\n+        assert_equal(np.minimum(arr1,arr2), mintrue)\n+        assert_equal(np.minimum(arr1[::2],arr2[::2]), mintrue[::2])\n+        assert_equal(np.minimum(arr1[:4:], arr2[::2]), np.array([-4.0, np.nan, 0.0, 0.0]))\n+        assert_equal(np.minimum(arr1[::3], arr2[:3:]), np.array([-4.0, -1.0, np.nan]))\n+        assert_equal(np.minimum(arr1[:6:2], arr2[::3], out=out[::3]), np.array([-4.0, 1.0, np.nan]))\n+        assert_equal(out, out_mintrue)\n \n class TestFmax(_FilterInvalids):\n     def test_reduce(self):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 13885,
        "body": "By leveraging AVX gather instructions, this patch enables SIMD processing of strided arrays, which is currently handled in a scalar fashion. Performance of functions like sqrt improves by 9x. Detailed performance numbers are presented below (array size = 10000 for every benchmark): \r\n\r\n```\r\n       before           after         ratio\r\n     [a14a8cef]       [de47213c]\r\n     <master>         <sqrt-sq-rcp-abs-avx>\r\n-     2.27\u00b10.02ns      2.05\u00b10.04ns     0.90  bench_avx.Custom.time_square_stride1_float32\r\n-     3.49\u00b10.04ns      3.11\u00b10.06ns     0.89  bench_avx.Custom.time_square_stride1_float64\r\n-     3.38\u00b10.03ns      2.93\u00b10.03ns     0.87  bench_avx.Custom.time_reciprocal_stride1_float32\r\n-     2.04\u00b10.02ns      1.75\u00b10.02ns     0.86  bench_avx.Custom.time_absolute_stride1_float32\r\n-     3.37\u00b10.02ns      2.78\u00b10.05ns     0.83  bench_avx.Custom.time_absolute_stride1_float64\r\n-     7.15\u00b10.03ns      5.52\u00b10.02ns     0.77  bench_avx.Custom.time_square_stride4_float64\r\n-     7.15\u00b10.03ns      5.04\u00b10.03ns     0.71  bench_avx.Custom.time_square_stride2_float64\r\n-     7.56\u00b10.04ns      4.46\u00b10.03ns     0.59  bench_avx.Custom.time_square_stride4_float32\r\n-     8.94\u00b10.03ns      5.15\u00b10.02ns     0.58  bench_avx.Custom.time_absolute_stride4_float64\r\n-     13.6\u00b10.03ns      7.49\u00b10.03ns     0.55  bench_avx.Custom.time_reciprocal_stride4_float64\r\n-     8.52\u00b10.01ns      4.67\u00b10.02ns     0.55  bench_avx.Custom.time_absolute_stride2_float64\r\n-     13.7\u00b10.05ns      7.48\u00b10.02ns     0.55  bench_avx.Custom.time_reciprocal_stride2_float64\r\n-     7.55\u00b10.05ns      4.13\u00b10.02ns     0.55  bench_avx.Custom.time_square_stride2_float32\r\n-     8.41\u00b10.03ns      4.13\u00b10.01ns     0.49  bench_avx.Custom.time_absolute_stride4_float32\r\n-     7.97\u00b10.03ns      3.80\u00b10.01ns     0.48  bench_avx.Custom.time_absolute_stride2_float32\r\n-     10.6\u00b10.04ns      4.46\u00b10.03ns     0.42  bench_avx.Custom.time_reciprocal_stride4_float32\r\n-     10.5\u00b10.03ns      4.26\u00b10.03ns     0.40  bench_avx.Custom.time_reciprocal_stride2_float32\r\n-     40.6\u00b10.02ns      7.97\u00b10.03ns     0.20  bench_avx.Custom.time_sqrt_stride2_float64\r\n-     40.6\u00b10.05ns      7.97\u00b10.05ns     0.20  bench_avx.Custom.time_sqrt_stride4_float64\r\n-     37.5\u00b10.03ns      4.24\u00b10.02ns     0.11  bench_avx.Custom.time_sqrt_stride4_float32\r\n-     37.5\u00b10.02ns      4.06\u00b10.02ns     0.11  bench_avx.Custom.time_sqrt_stride2_float32\r\n\r\n```\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_avx.py",
                "patch": "@@ -0,0 +1,34 @@\n+from __future__ import absolute_import, division, print_function\n+\n+from .common import Benchmark\n+\n+import numpy as np\n+\n+avx_ufuncs = ['sqrt',\n+              'absolute',\n+              'reciprocal',\n+              'square',\n+              'rint',\n+              'floor',\n+              'ceil' ,\n+              'trunc']\n+stride = [1, 2, 4]\n+dtype  = ['f', 'd']\n+\n+class AVX_UFunc(Benchmark):\n+    params = [avx_ufuncs, stride, dtype]\n+    param_names = ['avx_based_ufunc', 'stride', 'dtype']\n+    timeout = 10\n+\n+    def setup(self, ufuncname, stride, dtype):\n+        np.seterr(all='ignore')\n+        try:\n+            self.f = getattr(np, ufuncname)\n+        except AttributeError:\n+            raise NotImplementedError()\n+        N = 10000\n+        self.arr = np.ones(stride*N, dtype)\n+\n+    def time_ufunc(self, ufuncname, stride, dtype):\n+        self.f(self.arr[::stride])\n+"
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -358,14 +358,14 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.square'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints)]),\n+          TD(ints+inexact, simd=[('avx2', ints), ('fma', 'fd'), ('avx512f', 'fd')]),\n           TD(O, f='Py_square'),\n           ),\n 'reciprocal':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.reciprocal'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints)]),\n+          TD(ints+inexact, simd=[('avx2', ints), ('fma', 'fd'), ('avx512f','fd')]),\n           TD(O, f='Py_reciprocal'),\n           ),\n # This is no longer used as numpy.ones_like, however it is\n@@ -395,7 +395,7 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.absolute'),\n           'PyUFunc_AbsoluteTypeResolver',\n-          TD(bints+flts+timedeltaonly),\n+          TD(bints+flts+timedeltaonly, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n           TD(cmplx, out=('f', 'd', 'g')),\n           TD(O, f='PyNumber_Absolute'),\n           ),\n@@ -762,7 +762,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.sqrt'),\n           None,\n           TD('e', f='sqrt', astype={'e':'f'}),\n-          TD(inexactvec),\n+          TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n           TD('fdg' + cmplx, f='sqrt'),\n           TD(P, f='sqrt'),\n           ),\n@@ -777,14 +777,18 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.ceil'),\n           None,\n-          TD(flts, f='ceil', astype={'e':'f'}),\n+          TD('e', f='ceil', astype={'e':'f'}),\n+          TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n+          TD('fdg', f='ceil'),\n           TD(O, f='npy_ObjectCeil'),\n           ),\n 'trunc':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.trunc'),\n           None,\n-          TD(flts, f='trunc', astype={'e':'f'}),\n+          TD('e', f='trunc', astype={'e':'f'}),\n+          TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n+          TD('fdg', f='trunc'),\n           TD(O, f='npy_ObjectTrunc'),\n           ),\n 'fabs':\n@@ -798,14 +802,18 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.floor'),\n           None,\n-          TD(flts, f='floor', astype={'e':'f'}),\n+          TD('e', f='floor', astype={'e':'f'}),\n+          TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n+          TD('fdg', f='floor'),\n           TD(O, f='npy_ObjectFloor'),\n           ),\n 'rint':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.rint'),\n           None,\n-          TD(inexact, f='rint', astype={'e':'f'}),\n+          TD('e', f='rint', astype={'e':'f'}),\n+          TD(inexactvec, simd=[('fma', 'fd'), ('avx512f', 'fd')]),\n+          TD('fdg' + cmplx, f='rint'),\n           TD(P, f='rint'),\n           ),\n 'arctan2':"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1634,6 +1634,30 @@ NPY_NO_EXPORT void\n \n /**end repeat**/\n \n+/**begin repeat\n+ *  #func = rint, ceil, floor, trunc#\n+ *  #scalarf = npy_rint, npy_ceil, npy_floor, npy_trunc#\n+ */\n+\n+/**begin repeat1\n+*  #TYPE = FLOAT, DOUBLE#\n+*  #type = npy_float, npy_double#\n+*  #typesub = f, #\n+*/\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    UNARY_LOOP {\n+        const @type@ in1 = *(@type@ *)ip1;\n+        *(@type@ *)op1 = @scalarf@@typesub@(in1);\n+    }\n+}\n+\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  *  #func = sin, cos, exp, log#\n  *  #scalarf = npy_sinf, npy_cosf, npy_expf, npy_logf#\n@@ -1656,6 +1680,78 @@ FLOAT_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSE\n  * #CHK = HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS#\n  */\n \n+/**begin repeat1\n+ *  #TYPE = FLOAT, DOUBLE#\n+ *  #type = npy_float, npy_double#\n+ *  #typesub = f, #\n+ */\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_sqrt_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    if (!run_unary_@isa@_sqrt_@TYPE@(args, dimensions, steps)) {\n+        UNARY_LOOP {\n+            const @type@ in1 = *(@type@ *)ip1;\n+            *(@type@ *)op1 = npy_sqrt@typesub@(in1);\n+        }\n+    }\n+}\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_absolute_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    if (!run_unary_@isa@_absolute_@TYPE@(args, dimensions, steps)) {\n+        UNARY_LOOP {\n+            const @type@ in1 = *(@type@ *)ip1;\n+            const @type@ tmp = in1 > 0 ? in1 : -in1;\n+            /* add 0 to clear -0.0 */\n+            *((@type@ *)op1) = tmp + 0;\n+        }\n+    }\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+}\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_square_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    if (!run_unary_@isa@_square_@TYPE@(args, dimensions, steps)) {\n+        UNARY_LOOP {\n+            const @type@ in1 = *(@type@ *)ip1;\n+            *(@type@ *)op1 = in1*in1;\n+        }\n+    }\n+}\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_reciprocal_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    if (!run_unary_@isa@_reciprocal_@TYPE@(args, dimensions, steps)) {\n+        UNARY_LOOP {\n+            const @type@ in1 = *(@type@ *)ip1;\n+            *(@type@ *)op1 = 1.0f/in1;\n+        }\n+    }\n+}\n+\n+/**begin repeat2\n+ *  #func = rint, ceil, floor, trunc#\n+ *  #scalarf = npy_rint, npy_ceil, npy_floor, npy_trunc#\n+ */\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data))\n+{\n+    if (!run_unary_@isa@_@func@_@TYPE@(args, dimensions, steps)) {\n+        UNARY_LOOP {\n+            const @type@ in1 = *(@type@ *)ip1;\n+            *(@type@ *)op1 = @scalarf@@typesub@(in1);\n+        }\n+    }\n+}\n+\n+/**end repeat2**/\n+/**end repeat1**/\n+\n /**begin repeat1\n  *  #func = exp, log#\n  *  #scalarf = npy_expf, npy_logf#\n@@ -1706,10 +1802,9 @@ FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY\n }\n \n /**end repeat1**/\n-\n-\n /**end repeat**/\n \n+\n /**begin repeat\n  * Float types\n  *  #type = npy_float, npy_double, npy_longdouble, npy_float#"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -175,6 +175,19 @@ NPY_NO_EXPORT void\n  */\n NPY_NO_EXPORT void\n @TYPE@_sqrt(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func));\n+\n+/**begin repeat1\n+ * #isa = avx512f, fma#\n+ */\n+\n+/**begin repeat2\n+ * #func = sqrt, absolute, square, reciprocal#\n+ */\n+NPY_NO_EXPORT void\n+@TYPE@_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func));\n+\n+/**end repeat2**/\n+/**end repeat1**/\n /**end repeat**/\n \n /**begin repeat\n@@ -193,6 +206,26 @@ FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY\n /**end repeat1**/\n /**end repeat**/\n \n+/**begin repeat\n+ *  #func = rint, ceil, floor, trunc#\n+ */\n+\n+/**begin repeat1\n+*  #TYPE = FLOAT, DOUBLE#\n+*/\n+\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_@func@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data));\n+\n+/**begin repeat2\n+ * #isa = avx512f, fma#\n+ */\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n+@TYPE@_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(data));\n+/**end repeat2**/\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  * Float types\n  *  #TYPE = HALF, FLOAT, DOUBLE, LONGDOUBLE#"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -138,6 +138,37 @@ abs_ptrdiff(char *a, char *b)\n \n /* prototypes */\n \n+/**begin repeat1\n+ * #type = npy_float, npy_double#\n+ * #TYPE = FLOAT, DOUBLE#\n+ */\n+\n+/**begin repeat2\n+ *  #func = sqrt, absolute, square, reciprocal, rint, floor, ceil, trunc#\n+ */\n+\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n+static NPY_INLINE NPY_GCC_TARGET_@ISA@ void\n+@ISA@_@func@_@TYPE@(@type@ *, @type@ *, const npy_intp n, const npy_intp stride);\n+#endif\n+\n+static NPY_INLINE int\n+run_unary_@isa@_@func@_@TYPE@(char **args, npy_intp *dimensions, npy_intp *steps)\n+{\n+#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n+    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(@type@), @REGISTER_SIZE@)) {\n+        @ISA@_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0], steps[0]);\n+        return 1;\n+    }\n+    else\n+        return 0;\n+#endif\n+    return 0;\n+}\n+\n+/**end repeat2**/\n+/**end repeat1**/\n+\n /**begin repeat1\n  * #func = exp, log#\n  */\n@@ -185,7 +216,6 @@ run_unary_@isa@_sincos_FLOAT(char **args, npy_intp *dimensions, npy_intp *steps,\n /**end repeat**/\n \n \n-\n /**begin repeat\n  * Float types\n  *  #type = npy_float, npy_double, npy_longdouble#\n@@ -1144,47 +1174,94 @@ sse2_@kind@_@TYPE@(@type@ * ip, @type@ * op, const npy_intp n)\n \n #if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_get_full_load_mask(void)\n+fma_get_full_load_mask_ps(void)\n {\n     return _mm256_set1_ps(-1.0);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i\n+fma_get_full_load_mask_pd(void)\n+{\n+    return _mm256_castpd_si256(_mm256_set1_pd(-1.0));\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_get_partial_load_mask(const npy_int num_lanes, const npy_int total_elem)\n+fma_get_partial_load_mask_ps(const npy_int num_elem, const npy_int num_lanes)\n {\n     float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,\n                             1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};\n-    float* addr = maskint + total_elem - num_lanes;\n+    float* addr = maskint + num_lanes - num_elem;\n     return _mm256_loadu_ps(addr);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i\n+fma_get_partial_load_mask_pd(const npy_int num_elem, const npy_int num_lanes)\n+{\n+    npy_int maskint[16] = {-1,-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1,1,1,1};\n+    npy_int* addr = maskint + 2*num_lanes - 2*num_elem;\n+    return _mm256_loadu_si256((__m256i*) addr);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_masked_gather(__m256 src,\n-                   npy_float* addr,\n-                   __m256i vindex,\n-                   __m256 mask)\n+fma_masked_gather_ps(__m256 src,\n+                     npy_float* addr,\n+                     __m256i vindex,\n+                     __m256 mask)\n {\n     return _mm256_mask_i32gather_ps(src, addr, vindex, mask, 4);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d\n+fma_masked_gather_pd(__m256d src,\n+                     npy_double* addr,\n+                     __m128i vindex,\n+                     __m256d mask)\n+{\n+    return _mm256_mask_i32gather_pd(src, addr, vindex, mask, 8);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_masked_load(__m256 mask, npy_float* addr)\n+fma_masked_load_ps(__m256 mask, npy_float* addr)\n {\n     return _mm256_maskload_ps(addr, _mm256_cvtps_epi32(mask));\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d\n+fma_masked_load_pd(__m256i mask, npy_double* addr)\n+{\n+    return _mm256_maskload_pd(addr, mask);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n-fma_set_masked_lanes(__m256 x, __m256 val, __m256 mask)\n+fma_set_masked_lanes_ps(__m256 x, __m256 val, __m256 mask)\n {\n     return _mm256_blendv_ps(x, val, mask);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d\n+fma_set_masked_lanes_pd(__m256d x, __m256d val, __m256d mask)\n+{\n+    return _mm256_blendv_pd(x, val, mask);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n fma_blend(__m256 x, __m256 y, __m256 ymask)\n {\n     return _mm256_blendv_ps(x, y, ymask);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n+fma_invert_mask_ps(__m256 ymask)\n+{\n+    return _mm256_andnot_ps(ymask, _mm256_set1_ps(-1.0));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i\n+fma_invert_mask_pd(__m256i ymask)\n+{\n+    return _mm256_andnot_si256(ymask, _mm256_set1_epi32(0xFFFFFFFF));\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256\n fma_should_calculate_sine(__m256i k, __m256i andop, __m256i cmp)\n {\n@@ -1290,48 +1367,133 @@ fma_scalef_ps(__m256 poly, __m256 quadrant)\n      }\n }\n \n+/**begin repeat\n+ *  #vsub = ps, pd#\n+ *  #vtype = __m256, __m256d#\n+ */\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n+fma_abs_@vsub@(@vtype@ x)\n+{\n+    return _mm256_andnot_@vsub@(_mm256_set1_@vsub@(-0.0), x);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n+fma_reciprocal_@vsub@(@vtype@ x)\n+{\n+    return _mm256_div_@vsub@(_mm256_set1_@vsub@(1.0f), x);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n+fma_rint_@vsub@(@vtype@ x)\n+{\n+    return _mm256_round_@vsub@(x, _MM_FROUND_TO_NEAREST_INT);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n+fma_floor_@vsub@(@vtype@ x)\n+{\n+    return _mm256_round_@vsub@(x, _MM_FROUND_TO_NEG_INF);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n+fma_ceil_@vsub@(@vtype@ x)\n+{\n+    return _mm256_round_@vsub@(x, _MM_FROUND_TO_POS_INF);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@\n+fma_trunc_@vsub@(@vtype@ x)\n+{\n+    return _mm256_round_@vsub@(x, _MM_FROUND_TO_ZERO);\n+}\n+/**end repeat**/\n #endif\n \n #if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n-avx512_get_full_load_mask(void)\n+avx512_get_full_load_mask_ps(void)\n {\n     return 0xFFFF;\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n+avx512_get_full_load_mask_pd(void)\n+{\n+    return 0xFF;\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n-avx512_get_partial_load_mask(const npy_int num_elem, const npy_int total_elem)\n+avx512_get_partial_load_mask_ps(const npy_int num_elem, const npy_int total_elem)\n {\n     return (0x0001 << num_elem) - 0x0001;\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n+avx512_get_partial_load_mask_pd(const npy_int num_elem, const npy_int total_elem)\n+{\n+    return (0x01 << num_elem) - 0x01;\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_masked_gather(__m512 src,\n-                     npy_float* addr,\n-                     __m512i vindex,\n-                     __mmask16 kmask)\n+avx512_masked_gather_ps(__m512 src,\n+                        npy_float* addr,\n+                        __m512i vindex,\n+                        __mmask16 kmask)\n {\n     return _mm512_mask_i32gather_ps(src, kmask, vindex, addr, 4);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d\n+avx512_masked_gather_pd(__m512d src,\n+                        npy_double* addr,\n+                        __m256i vindex,\n+                        __mmask8 kmask)\n+{\n+    return _mm512_mask_i32gather_pd(src, kmask, vindex, addr, 8);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_masked_load(__mmask16 mask, npy_float* addr)\n+avx512_masked_load_ps(__mmask16 mask, npy_float* addr)\n {\n     return _mm512_maskz_loadu_ps(mask, (__m512 *)addr);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d\n+avx512_masked_load_pd(__mmask8 mask, npy_double* addr)\n+{\n+    return _mm512_maskz_loadu_pd(mask, (__m512d *)addr);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n-avx512_set_masked_lanes(__m512 x, __m512 val, __mmask16 mask)\n+avx512_set_masked_lanes_ps(__m512 x, __m512 val, __mmask16 mask)\n {\n     return _mm512_mask_blend_ps(mask, x, val);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d\n+avx512_set_masked_lanes_pd(__m512d x, __m512d val, __mmask8 mask)\n+{\n+    return _mm512_mask_blend_pd(mask, x, val);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n avx512_blend(__m512 x, __m512 y, __mmask16 ymask)\n {\n     return _mm512_mask_mov_ps(x, ymask, y);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n+avx512_invert_mask_ps(__mmask16 ymask)\n+{\n+    return _mm512_knot(ymask);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8\n+avx512_invert_mask_pd(__mmask8 ymask)\n+{\n+    return _mm512_knot(ymask);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16\n avx512_should_calculate_sine(__m512i k, __m512i andop, __m512i cmp)\n {\n@@ -1361,6 +1523,49 @@ avx512_scalef_ps(__m512 poly, __m512 quadrant)\n {\n     return _mm512_scalef_ps(poly, quadrant);\n }\n+/**begin repeat\n+ *  #vsub  = ps, pd#\n+ *  #epi_vsub  = epi32, epi64#\n+ *  #vtype = __m512, __m512d#\n+ *  #and_const = 0x7fffffff, 0x7fffffffffffffffLL#\n+ */\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_abs_@vsub@(@vtype@ x)\n+{\n+    return (@vtype@) _mm512_and_@epi_vsub@((__m512i) x,\n+\t\t\t\t    _mm512_set1_@epi_vsub@ (@and_const@));\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_reciprocal_@vsub@(@vtype@ x)\n+{\n+    return _mm512_div_@vsub@(_mm512_set1_@vsub@(1.0f), x);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_rint_@vsub@(@vtype@ x)\n+{\n+    return _mm512_roundscale_@vsub@(x, 0x08);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_floor_@vsub@(@vtype@ x)\n+{\n+    return _mm512_roundscale_@vsub@(x, 0x09);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_ceil_@vsub@(@vtype@ x)\n+{\n+    return _mm512_roundscale_@vsub@(x, 0x0A);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@\n+avx512_trunc_@vsub@(@vtype@ x)\n+{\n+    return _mm512_roundscale_@vsub@(x, 0x0B);\n+}\n+/**end repeat**/\n #endif\n \n /**begin repeat\n@@ -1438,7 +1643,187 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n     sin = @fmadd@(sin, x, x);\n     return sin;\n }\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n+@isa@_sqrt_ps(@vtype@ x)\n+{\n+    return _mm@vsize@_sqrt_ps(x);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n+@isa@_sqrt_pd(@vtype@d x)\n+{\n+    return _mm@vsize@_sqrt_pd(x);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n+@isa@_square_ps(@vtype@ x)\n+{\n+    return _mm@vsize@_mul_ps(x,x);\n+}\n+\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d\n+@isa@_square_pd(@vtype@d x)\n+{\n+    return _mm@vsize@_mul_pd(x,x);\n+}\n+\n+#endif\n+/**end repeat**/\n+\n+\n+/**begin repeat\n+ * #ISA = FMA, AVX512F#\n+ * #isa = fma, avx512#\n+ * #vsize = 256, 512#\n+ * #BYTES = 32, 64#\n+ * #cvtps_epi32 = _mm256_cvtps_epi32, #\n+ * #mask = __m256, __mmask16#\n+ * #vsub = , _mask#\n+ * #vtype = __m256, __m512#\n+ * #cvtps_epi32 = _mm256_cvtps_epi32, #\n+ * #masked_store = _mm256_maskstore_ps, _mm512_mask_storeu_ps#\n+ * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n+ */\n+\n+/**begin repeat1\n+ *  #func = sqrt, absolute, square, reciprocal, rint, ceil, floor, trunc#\n+ *  #vectorf = sqrt, abs, square, reciprocal, rint, ceil, floor, trunc#\n+ *  #replace_0_with_1 = 0, 0, 0, 1, 0, 0, 0, 0#\n+ */\n+\n+#if defined @CHK@\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n+@ISA@_@func@_FLOAT(npy_float* op,\n+                   npy_float* ip,\n+                   const npy_intp array_size,\n+                   const npy_intp steps)\n+{\n+    const npy_intp stride = steps/sizeof(npy_float);\n+    const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    npy_intp num_remaining_elements = array_size;\n+    @vtype@ ones_f = _mm@vsize@_set1_ps(1.0f);\n+    @mask@ load_mask = @isa@_get_full_load_mask_ps();\n+#if @replace_0_with_1@\n+    @mask@ inv_load_mask = @isa@_invert_mask_ps(load_mask);\n+#endif\n+    npy_int indexarr[16];\n+    for (npy_int ii = 0; ii < 16; ii++) {\n+        indexarr[ii] = ii*stride;\n+    }\n+    @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);\n+\n+    while (num_remaining_elements > 0) {\n+        if (num_remaining_elements < num_lanes) {\n+            load_mask = @isa@_get_partial_load_mask_ps(num_remaining_elements,\n+                                                       num_lanes);\n+#if @replace_0_with_1@\n+            inv_load_mask = @isa@_invert_mask_ps(load_mask);\n+#endif\n+        }\n+        @vtype@ x;\n+        if (stride == 1) {\n+            x = @isa@_masked_load_ps(load_mask, ip);\n+#if @replace_0_with_1@\n+            /*\n+             * Replace masked elements with 1.0f to avoid divide by zero fp\n+             * exception in reciprocal\n+             */\n+            x = @isa@_set_masked_lanes_ps(x, ones_f, inv_load_mask);\n+#endif\n+        }\n+        else {\n+            x = @isa@_masked_gather_ps(ones_f, ip, vindex, load_mask);\n+        }\n+        @vtype@ out = @isa@_@vectorf@_ps(x);\n+        @masked_store@(op, @cvtps_epi32@(load_mask), out);\n+\n+        ip += num_lanes*stride;\n+        op += num_lanes;\n+        num_remaining_elements -= num_lanes;\n+    }\n+}\n+#endif\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #ISA = FMA, AVX512F#\n+ * #isa = fma, avx512#\n+ * #vsize = 256, 512#\n+ * #BYTES = 32, 64#\n+ * #cvtps_epi32 = _mm256_cvtps_epi32, #\n+ * #mask = __m256i, __mmask8#\n+ * #vsub = , _mask#\n+ * #vtype = __m256d, __m512d#\n+ * #vindextype = __m128i, __m256i#\n+ * #vindexsize = 128, 256#\n+ * #vindexload = _mm_loadu_si128, _mm256_loadu_si256#\n+ * #cvtps_epi32 = _mm256_cvtpd_epi32, #\n+ * #castmask = _mm256_castsi256_pd, #\n+ * #masked_store = _mm256_maskstore_pd, _mm512_mask_storeu_pd#\n+ * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n+ */\n+\n+/**begin repeat1\n+ *  #func = sqrt, absolute, square, reciprocal, rint, ceil, floor, trunc#\n+ *  #vectorf = sqrt, abs, square, reciprocal, rint, ceil, floor, trunc#\n+ *  #replace_0_with_1 = 0, 0, 0, 1, 0, 0, 0, 0#\n+ */\n+\n+#if defined @CHK@\n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n+@ISA@_@func@_DOUBLE(npy_double* op,\n+                    npy_double* ip,\n+                    const npy_intp array_size,\n+                    const npy_intp steps)\n+{\n+    const npy_intp stride = steps/sizeof(npy_double);\n+    const npy_int num_lanes = @BYTES@/sizeof(npy_double);\n+    npy_intp num_remaining_elements = array_size;\n+    @mask@ load_mask = @isa@_get_full_load_mask_pd();\n+#if @replace_0_with_1@\n+    @mask@ inv_load_mask = @isa@_invert_mask_pd(load_mask);\n+#endif\n+    @vtype@ ones_d = _mm@vsize@_set1_pd(1.0f);\n+    npy_int indexarr[8];\n+    for (npy_int ii = 0; ii < 8; ii++) {\n+        indexarr[ii] = ii*stride;\n+    }\n+    @vindextype@ vindex = @vindexload@((@vindextype@*)&indexarr[0]);\n+\n+    while (num_remaining_elements > 0) {\n+        if (num_remaining_elements < num_lanes) {\n+            load_mask = @isa@_get_partial_load_mask_pd(num_remaining_elements,\n+                                                       num_lanes);\n+#if @replace_0_with_1@\n+            inv_load_mask = @isa@_invert_mask_pd(load_mask);\n #endif\n+        }\n+        @vtype@ x;\n+        if (stride == 1) {\n+            x = @isa@_masked_load_pd(load_mask, ip);\n+#if @replace_0_with_1@\n+            /*\n+             * Replace masked elements with 1.0f to avoid divide by zero fp\n+             * exception in reciprocal\n+             */\n+            x = @isa@_set_masked_lanes_pd(x, ones_d, @castmask@(inv_load_mask));\n+#endif\n+        }\n+        else {\n+            x = @isa@_masked_gather_pd(ones_d, ip, vindex, @castmask@(load_mask));\n+        }\n+        @vtype@ out = @isa@_@vectorf@_pd(x);\n+        @masked_store@(op, load_mask, out);\n+\n+        ip += num_lanes*stride;\n+        op += num_lanes;\n+        num_remaining_elements -= num_lanes;\n+    }\n+}\n+#endif\n+/**end repeat1**/\n /**end repeat**/\n \n /**begin repeat\n@@ -1460,7 +1845,6 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n  * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#\n  */\n \n-\n /*\n  * Vectorized approximate sine/cosine algorithms: The following code is a\n  * vectorized version of the algorithm presented here:\n@@ -1519,7 +1903,7 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     @vtype@ quadrant, reduced_x, reduced_x2, cos, sin;\n     @vtype@i iquadrant;\n     @mask@ nan_mask, glibc_mask, sine_mask, negate_mask;\n-    @mask@ load_mask = @isa@_get_full_load_mask();\n+    @mask@ load_mask = @isa@_get_full_load_mask_ps();\n     npy_intp num_remaining_elements = array_size;\n     npy_int indexarr[16];\n     for (npy_int ii = 0; ii < 16; ii++) {\n@@ -1530,16 +1914,16 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     while (num_remaining_elements > 0) {\n \n         if (num_remaining_elements < num_lanes) {\n-            load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n+            load_mask = @isa@_get_partial_load_mask_ps(num_remaining_elements,\n                                                          num_lanes);\n         }\n \n         @vtype@ x;\n         if (stride == 1) {\n-            x = @isa@_masked_load(load_mask, ip);\n+            x = @isa@_masked_load_ps(load_mask, ip);\n         }\n         else {\n-            x = @isa@_masked_gather(zero_f, ip, vindex, load_mask);\n+            x = @isa@_masked_gather_ps(zero_f, ip, vindex, load_mask);\n         }\n \n         /*\n@@ -1551,7 +1935,7 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n         glibc_mask = @isa@_in_range_mask(x, large_number,-large_number);\n         glibc_mask = @and_masks@(load_mask, glibc_mask);\n         nan_mask = _mm@vsize@_cmp_ps@vsub@(x, x, _CMP_NEQ_UQ);\n-        x = @isa@_set_masked_lanes(x, zero_f, @or_masks@(nan_mask, glibc_mask));\n+        x = @isa@_set_masked_lanes_ps(x, zero_f, @or_masks@(nan_mask, glibc_mask));\n         npy_int iglibc_mask = @mask_to_int@(glibc_mask);\n \n         if (iglibc_mask != @full_mask@) {\n@@ -1584,7 +1968,7 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n             /* multiply by -1 for appropriate elements */\n             negate_mask = @isa@_should_negate(iquadrant, twos, twos);\n             cos = @isa@_blend(cos, _mm@vsize@_sub_ps(zero_f, cos), negate_mask);\n-            cos = @isa@_set_masked_lanes(cos, _mm@vsize@_set1_ps(NPY_NANF), nan_mask);\n+            cos = @isa@_set_masked_lanes_ps(cos, _mm@vsize@_set1_ps(NPY_NANF), nan_mask);\n \n             @masked_store@(op, @cvtps_epi32@(load_mask), cos);\n         }\n@@ -1662,35 +2046,35 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);\n \n     @mask@ xmax_mask, xmin_mask, nan_mask, inf_mask;\n-    @mask@ overflow_mask = @isa@_get_partial_load_mask(0, num_lanes);\n-    @mask@ load_mask = @isa@_get_full_load_mask();\n+    @mask@ overflow_mask = @isa@_get_partial_load_mask_ps(0, num_lanes);\n+    @mask@ load_mask = @isa@_get_full_load_mask_ps();\n     npy_intp num_remaining_elements = array_size;\n \n     while (num_remaining_elements > 0) {\n \n         if (num_remaining_elements < num_lanes) {\n-            load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n-                                                    num_lanes);\n+            load_mask = @isa@_get_partial_load_mask_ps(num_remaining_elements,\n+                                                       num_lanes);\n         }\n \n         @vtype@ x;\n         if (stride == 1) {\n-            x = @isa@_masked_load(load_mask, ip);\n+            x = @isa@_masked_load_ps(load_mask, ip);\n         }\n         else {\n-            x = @isa@_masked_gather(zeros_f, ip, vindex, load_mask);\n+            x = @isa@_masked_gather_ps(zeros_f, ip, vindex, load_mask);\n         }\n \n         nan_mask = _mm@vsize@_cmp_ps@vsub@(x, x, _CMP_NEQ_UQ);\n-        x = @isa@_set_masked_lanes(x, zeros_f, nan_mask);\n+        x = @isa@_set_masked_lanes_ps(x, zeros_f, nan_mask);\n \n         xmax_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(xmax), _CMP_GE_OQ);\n         xmin_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(xmin), _CMP_LE_OQ);\n         inf_mask = _mm@vsize@_cmp_ps@vsub@(x, inf, _CMP_EQ_OQ);\n         overflow_mask = @or_masks@(overflow_mask,\n                                     @xor_masks@(xmax_mask, inf_mask));\n \n-        x = @isa@_set_masked_lanes(x, zeros_f, @or_masks@(\n+        x = @isa@_set_masked_lanes_ps(x, zeros_f, @or_masks@(\n                                     @or_masks@(nan_mask, xmin_mask), xmax_mask));\n \n         quadrant = _mm@vsize@_mul_ps(x, log2e);\n@@ -1723,9 +2107,9 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n          * elem < xmin; return 0.0f\n          * elem = +/- nan, return nan\n          */\n-        poly = @isa@_set_masked_lanes(poly, _mm@vsize@_set1_ps(NPY_NANF), nan_mask);\n-        poly = @isa@_set_masked_lanes(poly, inf, xmax_mask);\n-        poly = @isa@_set_masked_lanes(poly, zeros_f, xmin_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, _mm@vsize@_set1_ps(NPY_NANF), nan_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, inf, xmax_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, zeros_f, xmin_mask);\n \n         @masked_store@(op, @cvtps_epi32@(load_mask), poly);\n \n@@ -1790,24 +2174,24 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     @vtype@ poly, num_poly, denom_poly, exponent;\n \n     @mask@ inf_mask, nan_mask, sqrt2_mask, zero_mask, negx_mask;\n-    @mask@ invalid_mask = @isa@_get_partial_load_mask(0, num_lanes);\n+    @mask@ invalid_mask = @isa@_get_partial_load_mask_ps(0, num_lanes);\n     @mask@ divide_by_zero_mask = invalid_mask;\n-    @mask@ load_mask = @isa@_get_full_load_mask();\n+    @mask@ load_mask = @isa@_get_full_load_mask_ps();\n     npy_intp num_remaining_elements = array_size;\n \n     while (num_remaining_elements > 0) {\n \n         if (num_remaining_elements < num_lanes) {\n-            load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n-                                                    num_lanes);\n+            load_mask = @isa@_get_partial_load_mask_ps(num_remaining_elements,\n+                                                       num_lanes);\n         }\n \n         @vtype@ x_in;\n         if (stride == 1) {\n-            x_in = @isa@_masked_load(load_mask, ip);\n+            x_in = @isa@_masked_load_ps(load_mask, ip);\n         }\n         else {\n-            x_in  = @isa@_masked_gather(zeros_f, ip, vindex, load_mask);\n+            x_in  = @isa@_masked_gather_ps(zeros_f, ip, vindex, load_mask);\n         }\n \n         negx_mask = _mm@vsize@_cmp_ps@vsub@(x_in, zeros_f, _CMP_LT_OQ);\n@@ -1818,7 +2202,7 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n                                         @and_masks@(zero_mask, load_mask));\n         invalid_mask = @or_masks@(invalid_mask, negx_mask);\n \n-        @vtype@ x = @isa@_set_masked_lanes(x_in, zeros_f, negx_mask);\n+        @vtype@ x = @isa@_set_masked_lanes_ps(x_in, zeros_f, negx_mask);\n \n         /* set x = normalized mantissa */\n         exponent = @isa@_get_exponent(x);\n@@ -1852,10 +2236,10 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n          * x = +/- NAN; return NAN\n          * x = 0.0f; return -INF\n          */\n-        poly = @isa@_set_masked_lanes(poly, nan, nan_mask);\n-        poly = @isa@_set_masked_lanes(poly, neg_nan, negx_mask);\n-        poly = @isa@_set_masked_lanes(poly, neg_inf, zero_mask);\n-        poly = @isa@_set_masked_lanes(poly, inf, inf_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, nan, nan_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, neg_nan, negx_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, neg_inf, zero_mask);\n+        poly = @isa@_set_masked_lanes_ps(poly, inf, inf_mask);\n \n         @masked_store@(op, @cvtps_epi32@(load_mask), poly);\n "
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -694,8 +694,96 @@ def test_sincos_values(self):\n             assert_raises(FloatingPointError, np.cos, np.float32(-np.inf))\n             assert_raises(FloatingPointError, np.cos, np.float32(np.inf))\n \n+    def test_sqrt_values(self):\n+        with np.errstate(all='ignore'):\n+            x = [np.nan,  np.nan, np.inf, np.nan, 0.]\n+            y = [np.nan, -np.nan, np.inf, -np.inf, 0.]\n+            for dt in ['f', 'd', 'g']:\n+                xf = np.array(x, dtype=dt)\n+                yf = np.array(y, dtype=dt)\n+                assert_equal(np.sqrt(yf), xf)\n+\n+        #with np.errstate(invalid='raise'):\n+        #    for dt in ['f', 'd', 'g']:\n+        #        assert_raises(FloatingPointError, np.sqrt, np.array(-100., dtype=dt))\n+\n+    def test_abs_values(self):\n+        x = [np.nan,  np.nan, np.inf, np.inf, 0., 0., 1.0, 1.0]\n+        y = [np.nan, -np.nan, np.inf, -np.inf, 0., -0., -1.0, 1.0]\n+        for dt in ['f', 'd', 'g']:\n+            xf = np.array(x, dtype=dt)\n+            yf = np.array(y, dtype=dt)\n+            assert_equal(np.abs(yf), xf)\n+\n+    def test_square_values(self):\n+        x = [np.nan,  np.nan, np.inf, np.inf]\n+        y = [np.nan, -np.nan, np.inf, -np.inf]\n+        with np.errstate(all='ignore'):\n+            for dt in ['f', 'd', 'g']:\n+                xf = np.array(x, dtype=dt)\n+                yf = np.array(y, dtype=dt)\n+                assert_equal(np.square(yf), xf)\n+\n+        with np.errstate(over='raise'):\n+            assert_raises(FloatingPointError, np.square, np.array(1E32,  dtype='f'))\n+            assert_raises(FloatingPointError, np.square, np.array(1E200, dtype='d'))\n \n-class TestSIMDFloat32(object):\n+    def test_reciprocal_values(self):\n+        with np.errstate(all='ignore'):\n+            x = [np.nan,  np.nan, 0.0, -0.0, np.inf, -np.inf]\n+            y = [np.nan, -np.nan, np.inf, -np.inf, 0., -0.]\n+            for dt in ['f', 'd', 'g']:\n+                xf = np.array(x, dtype=dt)\n+                yf = np.array(y, dtype=dt)\n+                assert_equal(np.reciprocal(yf), xf)\n+\n+        with np.errstate(divide='raise'):\n+            for dt in ['f', 'd', 'g']:\n+                assert_raises(FloatingPointError, np.reciprocal, np.array(-0.0, dtype=dt))\n+\n+# func : [maxulperror, low, high]\n+avx_ufuncs = {'sqrt'        :[1,  0.,   100.],\n+              'absolute'    :[0, -100., 100.],\n+              'reciprocal'  :[1,  1.,   100.],\n+              'square'      :[1, -100., 100.],\n+              'rint'        :[0, -100., 100.],\n+              'floor'       :[0, -100., 100.],\n+              'ceil'        :[0, -100., 100.],\n+              'trunc'       :[0, -100., 100.]}\n+\n+class TestAVXUfuncs(object):\n+    def test_avx_based_ufunc(self):\n+        strides = np.array([-4,-3,-2,-1,1,2,3,4])\n+        np.random.seed(42)\n+        for func, prop in avx_ufuncs.items():\n+            maxulperr = prop[0]\n+            minval = prop[1]\n+            maxval = prop[2]\n+            # various array sizes to ensure masking in AVX is tested\n+            for size in range(1,32):\n+                myfunc = getattr(np, func)\n+                x_f32 = np.float32(np.random.uniform(low=minval, high=maxval,\n+                    size=size))\n+                x_f64 = np.float64(x_f32)\n+                x_f128 = np.longdouble(x_f32)\n+                y_true128 = myfunc(x_f128)\n+                if maxulperr == 0:\n+                    assert_equal(myfunc(x_f32), np.float32(y_true128))\n+                    assert_equal(myfunc(x_f64), np.float64(y_true128))\n+                else:\n+                    assert_array_max_ulp(myfunc(x_f32), np.float32(y_true128),\n+                            maxulp=maxulperr)\n+                    assert_array_max_ulp(myfunc(x_f64), np.float64(y_true128),\n+                            maxulp=maxulperr)\n+                # various strides to test gather instruction\n+                if size > 1:\n+                    y_true32 = myfunc(x_f32)\n+                    y_true64 = myfunc(x_f64)\n+                    for jj in strides:\n+                        assert_equal(myfunc(x_f64[::jj]), y_true64[::jj])\n+                        assert_equal(myfunc(x_f32[::jj]), y_true32[::jj])\n+\n+class TestAVXFloat32Transcendental(object):\n     def test_exp_float32(self):\n         np.random.seed(42)\n         x_f32 = np.float32(np.random.uniform(low=0.0,high=88.1,size=1000000))\n@@ -722,8 +810,8 @@ def test_sincos_float32(self):\n \n     def test_strided_float32(self):\n         np.random.seed(42)\n-        strides = np.random.randint(low=-100, high=100, size=100)\n-        sizes = np.random.randint(low=1, high=2000, size=100)\n+        strides = np.array([-4,-3,-2,-1,1,2,3,4])\n+        sizes = np.arange(2,100)\n         for ii in sizes:\n             x_f32 = np.float32(np.random.uniform(low=0.01,high=88.1,size=ii))\n             exp_true = np.exp(x_f32)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 13581,
        "body": "(1) AVX2 and AVX512F supports gather_ps instruction to load strided data\r\ninto a register. Rather than resort to scalar, these provide good\r\nbenefit when the input array is strided.\r\n\r\n(2) Added some tests to validate AVX based algorithms for exp and log.\r\nThe tests compare the output of float32 against glibc's float64\r\nimplementation and verify maxulp error.\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1626,12 +1626,11 @@ FLOAT_@func@_@isa@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY\n             /*\n              * We use the AVX function to compute exp/log for scalar elements as well.\n              * This is needed to ensure the output of strided and non-strided\n-             * cases match. But this worsens the performance of strided arrays.\n-             * There is plan to fix this in a subsequent patch by using gather\n-             * instructions for strided arrays in the AVX function.\n+             * cases match. SIMD code handles strided input cases, but not\n+             * strided output.\n              */\n #if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS\n-            @ISA@_@func@_FLOAT((npy_float *)op1, (npy_float *)ip1, 1);\n+            @ISA@_@func@_FLOAT((npy_float *)op1, (npy_float *)ip1, 1, steps[0]);\n #else\n             const npy_float in1 = *(npy_float *)ip1;\n             *(npy_float *)op1 = @scalarf@(in1);"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -51,6 +51,15 @@ abs_ptrdiff(char *a, char *b)\n      ((abs_ptrdiff(args[1], args[0]) >= (vsize)) || \\\n       ((abs_ptrdiff(args[1], args[0]) == 0))))\n \n+/*\n+ * output should be contiguous, can handle strided input data\n+ */\n+#define IS_OUTPUT_BLOCKABLE_UNARY(esize, vsize) \\\n+    (steps[1] == (esize) && \\\n+     (npy_is_aligned(args[0], esize) && npy_is_aligned(args[1], esize)) && \\\n+     ((abs_ptrdiff(args[1], args[0]) >= (vsize)) || \\\n+      ((abs_ptrdiff(args[1], args[0]) == 0))))\n+\n #define IS_BLOCKABLE_REDUCE(esize, vsize) \\\n     (steps[1] == (esize) && abs_ptrdiff(args[1], args[0]) >= (vsize) && \\\n      npy_is_aligned(args[1], (esize)) && \\\n@@ -134,15 +143,15 @@ abs_ptrdiff(char *a, char *b)\n \n #if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n static NPY_INLINE void\n-@ISA@_@func@_FLOAT(npy_float *, npy_float *, const npy_intp n);\n+@ISA@_@func@_FLOAT(npy_float *, npy_float *, const npy_intp n, const npy_intp stride);\n #endif\n \n static NPY_INLINE int\n run_unary_@isa@_@func@_FLOAT(char **args, npy_intp *dimensions, npy_intp *steps)\n {\n #if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS\n-    if (IS_BLOCKABLE_UNARY(sizeof(npy_float), @REGISTER_SIZE@)) {\n-        @ISA@_@func@_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0]);\n+    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), @REGISTER_SIZE@)) {\n+        @ISA@_@func@_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);\n         return 1;\n     }\n     else\n@@ -1127,14 +1136,23 @@ avx2_get_full_load_mask(void)\n }\n \n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n-avx2_get_partial_load_mask(const npy_int num_elem, const npy_int total_elem)\n+avx2_get_partial_load_mask(const npy_int num_lanes, const npy_int total_elem)\n {\n     float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,\n                             1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};\n-    float* addr = maskint + total_elem - num_elem;\n+    float* addr = maskint + total_elem - num_lanes;\n     return _mm256_loadu_ps(addr);\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n+avx2_masked_gather(__m256 src,\n+                   npy_float* addr,\n+                   __m256i vindex,\n+                   __m256 mask)\n+{\n+    return _mm256_mask_i32gather_ps(src, addr, vindex, mask, 4);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX2 __m256\n avx2_masked_load(__m256 mask, npy_float* addr)\n {\n@@ -1220,6 +1238,15 @@ avx512_get_partial_load_mask(const npy_int num_elem, const npy_int total_elem)\n     return (0x0001 << num_elem) - 0x0001;\n }\n \n+static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n+avx512_masked_gather(__m512 src,\n+                     npy_float* addr,\n+                     __m512i vindex,\n+                     __mmask16 kmask)\n+{\n+    return _mm512_mask_i32gather_ps(src, kmask, vindex, addr, 4);\n+}\n+\n static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512\n avx512_masked_load(__mmask16 mask, npy_float* addr)\n {\n@@ -1310,11 +1337,19 @@ static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@\n \n #if defined HAVE_ATTRIBUTE_TARGET_@ISA@_WITH_INTRINSICS\n static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n-@ISA@_exp_FLOAT(npy_float * op, npy_float * ip, const npy_intp array_size)\n+@ISA@_exp_FLOAT(npy_float * op,\n+                npy_float * ip,\n+                const npy_intp array_size,\n+                const npy_intp steps)\n {\n+    const npy_intp stride = steps/sizeof(npy_float);\n     const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n     npy_float xmax = 88.72283935546875f;\n     npy_float xmin = -87.3365478515625f;\n+    npy_int indexarr[16];\n+    for (npy_int ii = 0; ii < 16; ii++) {\n+        indexarr[ii] = ii*stride;\n+    }\n \n     /* Load up frequently used constants */\n     @vtype@ codyw_c1 = _mm@vsize@_set1_ps(NPY_CODY_WAITE_LOGE_2_HIGHf);\n@@ -1333,6 +1368,7 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     @vtype@ inf = _mm@vsize@_set1_ps(NPY_INFINITYF);\n     @vtype@ zeros_f = _mm@vsize@_set1_ps(0.0f);\n     @vtype@ poly, num_poly, denom_poly, quadrant;\n+    @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);\n     @vtype@i exponent;\n \n     @mask@ xmax_mask, xmin_mask, nan_mask, inf_mask;\n@@ -1342,10 +1378,18 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n \n     while (num_remaining_elements > 0) {\n \n-        if (num_remaining_elements < num_lanes)\n+        if (num_remaining_elements < num_lanes) {\n             load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n-                                                         num_lanes);\n-        @vtype@ x  = @isa@_masked_load(load_mask, ip);\n+                                                    num_lanes);\n+        }\n+\n+        @vtype@ x;\n+        if (stride == 1) {\n+            x = @isa@_masked_load(load_mask, ip);\n+        }\n+        else {\n+            x = @isa@_masked_gather(zeros_f, ip, vindex, load_mask);\n+        }\n \n         xmax_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(xmax), _CMP_GE_OQ);\n         xmin_mask = _mm@vsize@_cmp_ps@vsub@(x, _mm@vsize@_set1_ps(xmin), _CMP_LE_OQ);\n@@ -1396,13 +1440,14 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n \n         @masked_store@(op, @cvtps_epi32@(load_mask), poly);\n \n-        ip += num_lanes;\n+        ip += num_lanes*stride;\n         op += num_lanes;\n         num_remaining_elements -= num_lanes;\n     }\n \n-    if (@mask_to_int@(overflow_mask))\n+    if (@mask_to_int@(overflow_mask)) {\n         npy_set_floatstatus_overflow();\n+    }\n }\n \n /*\n@@ -1420,9 +1465,17 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n  */\n \n static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n-@ISA@_log_FLOAT(npy_float * op, npy_float * ip, const npy_intp array_size)\n+@ISA@_log_FLOAT(npy_float * op,\n+                npy_float * ip,\n+                const npy_intp array_size,\n+                const npy_intp steps)\n {\n+    const npy_intp stride = steps/sizeof(npy_float);\n     const npy_int num_lanes = @BYTES@/sizeof(npy_float);\n+    npy_int indexarr[16];\n+    for (npy_int ii = 0; ii < 16; ii++) {\n+        indexarr[ii] = ii*stride;\n+    }\n \n     /* Load up frequently used constants */\n     @vtype@ log_p0 = _mm@vsize@_set1_ps(NPY_COEFF_P0_LOGf);\n@@ -1443,6 +1496,7 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n     @vtype@ inf = _mm@vsize@_set1_ps(NPY_INFINITYF);\n     @vtype@ zeros_f = _mm@vsize@_set1_ps(0.0f);\n     @vtype@ ones_f = _mm@vsize@_set1_ps(1.0f);\n+    @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)indexarr);\n     @vtype@ poly, num_poly, denom_poly, exponent;\n \n     @mask@ inf_mask, nan_mask, sqrt2_mask, zero_mask, negx_mask;\n@@ -1453,10 +1507,18 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n \n     while (num_remaining_elements > 0) {\n \n-        if (num_remaining_elements < num_lanes)\n+        if (num_remaining_elements < num_lanes) {\n             load_mask = @isa@_get_partial_load_mask(num_remaining_elements,\n-                                                         num_lanes);\n-        @vtype@ x_in  = @isa@_masked_load(load_mask, ip);\n+                                                    num_lanes);\n+        }\n+\n+        @vtype@ x_in;\n+        if (stride == 1) {\n+            x_in = @isa@_masked_load(load_mask, ip);\n+        }\n+        else {\n+            x_in  = @isa@_masked_gather(zeros_f, ip, vindex, load_mask);\n+        }\n \n         negx_mask = _mm@vsize@_cmp_ps@vsub@(x_in, zeros_f, _CMP_LT_OQ);\n         zero_mask = _mm@vsize@_cmp_ps@vsub@(x_in, zeros_f, _CMP_EQ_OQ);\n@@ -1506,15 +1568,17 @@ static NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void\n \n         @masked_store@(op, @cvtps_epi32@(load_mask), poly);\n \n-        ip += num_lanes;\n+        ip += num_lanes*stride;\n         op += num_lanes;\n         num_remaining_elements -= num_lanes;\n     }\n \n-    if (@mask_to_int@(invalid_mask))\n+    if (@mask_to_int@(invalid_mask)) {\n         npy_set_floatstatus_invalid();\n-    if (@mask_to_int@(divide_by_zero_mask))\n+    }\n+    if (@mask_to_int@(divide_by_zero_mask)) {\n         npy_set_floatstatus_divbyzero();\n+    }\n }\n #endif\n /**end repeat**/"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -13,7 +13,7 @@\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_raises_regex,\n     assert_array_equal, assert_almost_equal, assert_array_almost_equal,\n-    assert_allclose, assert_no_warnings, suppress_warnings,\n+    assert_array_max_ulp, assert_allclose, assert_no_warnings, suppress_warnings,\n     _gen_alignment_data\n     )\n \n@@ -678,6 +678,31 @@ def test_log_values(self):\n             assert_raises(FloatingPointError, np.log, np.float32(-np.inf))\n             assert_raises(FloatingPointError, np.log, np.float32(-1.0))\n \n+class TestExpLogFloat32(object):\n+    def test_exp_float32(self):\n+        np.random.seed(42)\n+        x_f32 = np.float32(np.random.uniform(low=0.0,high=88.1,size=1000000))\n+        x_f64 = np.float64(x_f32)\n+        assert_array_max_ulp(np.exp(x_f32), np.float32(np.exp(x_f64)), maxulp=2.6)\n+\n+    def test_log_float32(self):\n+        np.random.seed(42)\n+        x_f32 = np.float32(np.random.uniform(low=0.0,high=1000,size=1000000))\n+        x_f64 = np.float64(x_f32)\n+        assert_array_max_ulp(np.log(x_f32), np.float32(np.log(x_f64)), maxulp=3.9)\n+\n+    def test_strided_exp_log_float32(self):\n+        np.random.seed(42)\n+        strides = np.random.randint(low=-100, high=100, size=100)\n+        sizes = np.random.randint(low=1, high=2000, size=100)\n+        for ii in sizes:\n+            x_f32 = np.float32(np.random.uniform(low=0.01,high=88.1,size=ii))\n+            exp_true = np.exp(x_f32)\n+            log_true = np.log(x_f32)\n+            for jj in strides:\n+                assert_equal(np.exp(x_f32[::jj]), exp_true[::jj])\n+                assert_equal(np.log(x_f32[::jj]), log_true[::jj])\n+\n class TestLogAddExp(_FilterInvalids):\n     def test_logaddexp_values(self):\n         x = [1, 2, 3, 4, 5]"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18766,
        "body": "### Dispatch for signed floor division\r\n1. Added dispatch for signed floor divide.\r\n2. Added floor logic on top of trunc. Based on [Division by Invariant Integers using Multiplication](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=3BD127D7E42FD7FBD59632922E48C394?doi=10.1.1.1.2556&rep=rep1&type=pdf) by T. Granlund and P. L. Montgomery\r\n\r\nContinues work of #18178 and #18075.\r\n\r\nTODO items:\r\n\r\n~- [ ] Modify timedelta code where libdivide is used,~\r\n~- [ ] Purge process of libdivide. Any license related changes needed(?)~\r\n\r\nCC: @seiko2plus , @seberg \r\n\r\n## Benchmarks\r\n\r\n\r\n### X86\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\n\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              8\r\nOn-line CPU(s) list: 0-7\r\nThread(s) per core:  2\r\nCore(s) per socket:  4\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               142\r\nModel name:          Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz\r\nStepping:            10\r\nCPU MHz:             1800.344\r\nCPU max MHz:         4000.0000\r\nCPU min MHz:         400.0000\r\nBogoMIPS:            3984.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            256K\r\nL3 cache:            8192K\r\nNUMA node0 CPU(s):   0-7\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux seiko-pc 5.8.0-48-generic #54-Ubuntu SMP Fri Mar 19 14:25:20 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\ngcc (Ubuntu 10.2.0-13ubuntu1) 10.2.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n<details>\r\n <summary>AVX2</summary>\r\n\r\n```Bash\r\npython runtests.py --bench-compare parent/main time_floor_divide_int\r\n```\r\n```Bash\r\n  before           after         ratio\r\n[3a61a14b]       [f505827a]\r\n<enh_simd_signed_division~10>     <enh_simd_signed_division>\r\n  105\u00b10.07\u03bcs      23.9\u00b10.06\u03bcs     0.23  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n     109\u00b13\u03bcs      23.7\u00b10.04\u03bcs     0.22  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n   135\u00b10.6\u03bcs       23.9\u00b10.8\u03bcs     0.18  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n   136\u00b10.6\u03bcs      23.5\u00b10.05\u03bcs     0.17  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n  97.5\u00b10.5\u03bcs      8.89\u00b10.08\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n  98.1\u00b10.5\u03bcs       8.88\u00b10.2\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n     116\u00b13\u03bcs      8.82\u00b10.01\u03bcs     0.08  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n     118\u00b12\u03bcs      8.90\u00b10.09\u03bcs     0.08  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n   142\u00b10.7\u03bcs      5.04\u00b10.08\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n     142\u00b11\u03bcs       4.92\u00b10.2\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n   154\u00b10.5\u03bcs       4.86\u00b10.2\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n   154\u00b10.4\u03bcs      4.84\u00b10.01\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n   152\u00b10.2\u03bcs       4.75\u00b10.3\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n     149\u00b17\u03bcs      4.62\u00b10.03\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n     146\u00b17\u03bcs       4.52\u00b10.1\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n     152\u00b15\u03bcs       4.45\u00b10.1\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n```\r\n</details>\r\n\r\n<details>\r\n <summary>SSE41</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX2\"\r\npython runtests.py --bench-compare parent/main time_floor_divide_int\r\n```\r\n```Bash\r\n before           after         ratio\r\n[623bc1fa]       [a2c5af9c]\r\n<enh_simd_signed_division~10>       <enh_simd_signed_division>\r\n 106\u00b10.4\u03bcs       57.3\u00b10.2\u03bcs     0.54  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n   106\u00b14\u03bcs       56.5\u00b10.3\u03bcs     0.53  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n 135\u00b10.7\u03bcs         57.9\u00b12\u03bcs     0.43  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n   140\u00b14\u03bcs       57.4\u00b10.6\u03bcs     0.41  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n97.9\u00b10.3\u03bcs         14.7\u00b11\u03bcs     0.15  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n   102\u00b19\u03bcs         14.7\u00b12\u03bcs     0.14  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n   115\u00b12\u03bcs         13.8\u00b11\u03bcs     0.12  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n   118\u00b12\u03bcs       13.6\u00b10.2\u03bcs     0.12  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n   141\u00b11\u03bcs      6.15\u00b10.04\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n   149\u00b16\u03bcs       6.47\u00b10.2\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n   148\u00b16\u03bcs      6.33\u00b10.05\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n   147\u00b16\u03bcs      6.14\u00b10.02\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n   156\u00b12\u03bcs       6.44\u00b10.5\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n   157\u00b12\u03bcs       6.46\u00b10.3\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n 152\u00b10.6\u03bcs       6.19\u00b10.1\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n   152\u00b11\u03bcs      6.15\u00b10.05\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n```\r\n</details>\r\n\r\n<details>\r\n <summary>SSE3</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"SSE41 AVX2\"\r\npython runtests.py --bench-compare parent/main time_floor_divide_int\r\n```\r\n```Bash\r\n before           after         ratio\r\n[623bc1fa]       [a2c5af9c]\r\n 105\u00b10.3\u03bcs       56.2\u00b10.2\u03bcs     0.54  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n   113\u00b19\u03bcs       56.2\u00b10.2\u03bcs     0.50  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n   140\u00b14\u03bcs       56.2\u00b10.2\u03bcs     0.40  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n   143\u00b14\u03bcs       56.5\u00b10.1\u03bcs     0.39  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n   103\u00b15\u03bcs      17.0\u00b10.08\u03bcs     0.17  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n   107\u00b14\u03bcs      17.3\u00b10.07\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n 116\u00b10.7\u03bcs      17.1\u00b10.08\u03bcs     0.15  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n   117\u00b11\u03bcs      17.0\u00b10.08\u03bcs     0.15  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n   156\u00b16\u03bcs         7.13\u00b14\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n   148\u00b17\u03bcs       6.76\u00b10.4\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n   144\u00b12\u03bcs      6.41\u00b10.05\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n   150\u00b14\u03bcs      6.51\u00b10.09\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n   157\u00b12\u03bcs       6.69\u00b10.1\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n  174\u00b130\u03bcs         7.39\u00b12\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n 152\u00b10.8\u03bcs      6.41\u00b10.03\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n   153\u00b12\u03bcs      6.43\u00b10.06\u03bcs     0.04  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43\r\n```\r\n</details>\r\n\r\n-----\r\n\r\n### Power little-endian\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```\r\nArchitecture:                    ppc64le\r\nByte Order:                      Little Endian\r\nCPU(s):                          8\r\nOn-line CPU(s) list:             0-7\r\nThread(s) per core:              1\r\nCore(s) per socket:              1\r\nSocket(s):                       8\r\nNUMA node(s):                    1\r\nModel:                           2.2 (pvr 004e 1202)\r\nModel name:                      POWER9 (architected), altivec supported\r\nL1d cache:                       256 KiB\r\nL1i cache:                       256 KiB\r\nNUMA node0 CPU(s):               0-7\r\nVulnerability L1tf:              Not affected\r\nVulnerability Meltdown:          Mitigation; RFI Flush\r\nVulnerability Spec store bypass: Mitigation; Kernel entry/exit barrier (eieio)\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Vulnerable\r\n\r\nprocessor   : 7\r\ncpu     : POWER9 (architected), altivec supported\r\nclock       : 2200.000000MHz\r\nrevision    : 2.2 (pvr 004e 1202)\r\n\r\ntimebase    : 512000000\r\nplatform    : pSeries\r\nmodel       : IBM pSeries (emulated by qemu)\r\nmachine     : CHRP IBM pSeries (emulated by qemu)\r\nMMU     : Radix\r\n\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux 8b2db3b0dfac 4.19.0-2-powerpc64le\r\ngcc version 9.2.1 20191008 (Ubuntu 9.2.1-9ubuntu2) \r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>VSX2</summary>\r\n\r\n```Bash\r\npython runtests.py --bench-compare parent/main time_floor_divide_int\r\n```\r\n```Bash\r\n  before           after         ratio\r\n[290a0345]       [2c393340]\r\n<main>           <enh_simd_signed_division>\r\n 64.2\u00b10.2\u03bcs       60.8\u00b10.2\u03bcs     0.95  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n 64.3\u00b10.2\u03bcs       60.7\u00b10.2\u03bcs     0.94  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n 64.5\u00b10.2\u03bcs       60.0\u00b10.2\u03bcs     0.93  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n 64.5\u00b10.2\u03bcs       59.9\u00b10.2\u03bcs     0.93  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n 66.3\u00b10.2\u03bcs      15.2\u00b10.01\u03bcs     0.23  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n 66.3\u00b10.3\u03bcs       15.1\u00b10.7\u03bcs     0.23  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n 66.3\u00b10.1\u03bcs      15.1\u00b10.01\u03bcs     0.23  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n 66.4\u00b10.2\u03bcs      15.1\u00b10.02\u03bcs     0.23  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n66.0\u00b10.07\u03bcs      9.86\u00b10.02\u03bcs     0.15  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n66.2\u00b10.08\u03bcs      9.84\u00b10.01\u03bcs     0.15  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n72.3\u00b10.09\u03bcs      9.84\u00b10.03\u03bcs     0.14  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n72.5\u00b10.09\u03bcs      9.85\u00b10.02\u03bcs     0.14  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n 64.7\u00b10.3\u03bcs      6.82\u00b10.02\u03bcs     0.11  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n 65.0\u00b10.2\u03bcs      6.83\u00b10.01\u03bcs     0.11  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n 70.5\u00b10.2\u03bcs      6.83\u00b10.01\u03bcs     0.10  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n 70.9\u00b10.2\u03bcs      6.83\u00b10.01\u03bcs     0.10  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n```\r\n</details>\r\n\r\n----\r\n\r\n### AArch64\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:        aarch64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              8\r\nOn-line CPU(s) list: 0-7\r\nThread(s) per core:  1\r\nCore(s) per socket:  4\r\nSocket(s):           2\r\nVendor ID:           ARM\r\nModel:               4\r\nModel name:          Cortex-A53\r\nStepping:            r0p4\r\nCPU max MHz:         2314.0000\r\nCPU min MHz:         403.0000\r\nBogoMIPS:            52.00\r\nFlags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux localhost 4.14.113-seiko_fastboot #30 SMP PREEMPT Wed Dec 30 12:28:43 IST 2020 aarch64 aarch64 aarch64 GNU/Linux\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>NEON</summary>\r\n\r\n```Bash\r\npython runtests.py --bench-compare parent/main time_floor_divide_int\r\n```\r\n```Bash\r\n  before           after         ratio\r\n[3a61a14b]       [f505827a]\r\n<enh_simd_signed_division~10>       <enh_simd_signed_division>\r\n     109\u00b13\u03bcs         98.2\u00b12\u03bcs     0.90  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n     112\u00b13\u03bcs         97.7\u00b12\u03bcs     0.87  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n     117\u00b11\u03bcs       89.6\u00b10.7\u03bcs     0.77  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n     117\u00b12\u03bcs         89.2\u00b11\u03bcs     0.76  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n     107\u00b13\u03bcs       30.9\u00b10.6\u03bcs     0.29  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n     110\u00b13\u03bcs       31.2\u00b10.6\u03bcs     0.28  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n     111\u00b11\u03bcs       31.1\u00b10.5\u03bcs     0.28  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n   112\u00b10.2\u03bcs       31.1\u00b10.6\u03bcs     0.28  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n     104\u00b13\u03bcs       17.7\u00b10.3\u03bcs     0.17  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n     106\u00b12\u03bcs       17.9\u00b10.5\u03bcs     0.17  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n   107\u00b10.8\u03bcs       17.6\u00b10.4\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n     111\u00b13\u03bcs       17.9\u00b10.4\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n     104\u00b13\u03bcs       10.8\u00b10.2\u03bcs     0.10  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n     107\u00b11\u03bcs       10.8\u00b10.2\u03bcs     0.10  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n     112\u00b13\u03bcs       10.8\u00b10.3\u03bcs     0.10  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n     113\u00b11\u03bcs       10.8\u00b10.2\u03bcs     0.10  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n```\r\n</details>\r\n\r\n--------------------------------------\r\n\r\n##### Initialed benchmark(before the latest modifications runned on different X86/HW than the above one)\r\n<details>\r\n<summary>Basic Benchmarks:</summary>\r\n<pre> \r\nCPU dispatch  :                                                                                                                                                                                    \r\n  Requested   : 'max -xop -fma4'                                                                                                                                                                   \r\n  Enabled     : SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL                                                     \r\n  Generated   : \r\n\r\n=================================\r\n```\r\n       before           after         ratio\r\n     [635b3ad8]       [cf41b9c4]\r\n     <main>           <enh_simd_signed_division>\r\n-      33.9\u00b10.5\u03bcs      9.84\u00b10.03\u03bcs     0.29  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n-      33.9\u00b10.5\u03bcs      9.82\u00b10.03\u03bcs     0.29  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n-      40.3\u00b10.4\u03bcs       9.82\u00b10.1\u03bcs     0.24  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n-      40.2\u00b10.4\u03bcs      9.78\u00b10.07\u03bcs     0.24  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n-        28.0\u00b11\u03bcs      4.55\u00b10.03\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n-        28.2\u00b11\u03bcs      4.55\u00b10.02\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n-      42.3\u00b10.4\u03bcs      4.59\u00b10.01\u03bcs     0.11  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n-      42.3\u00b10.3\u03bcs      4.58\u00b10.02\u03bcs     0.11  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n-        30.7\u00b11\u03bcs      2.44\u00b10.01\u03bcs     0.08  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n-      32.7\u00b10.3\u03bcs      2.43\u00b10.02\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n-      34.6\u00b10.8\u03bcs      2.53\u00b10.01\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n-      34.9\u00b10.3\u03bcs      2.51\u00b10.02\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n-      45.6\u00b10.4\u03bcs      2.49\u00b10.07\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n-      45.7\u00b10.1\u03bcs      2.41\u00b10.01\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n-      51.3\u00b10.7\u03bcs      2.53\u00b10.03\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n-      51.6\u00b10.3\u03bcs      2.51\u00b10.01\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\n</pre>\r\n</details>\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -328,7 +328,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.floor_divide'),\n           'PyUFunc_DivisionTypeResolver',\n           TD(ints, cfunc_alias='divide',\n-              dispatch=[('loops_arithmetic', 'BHILQ')]),\n+              dispatch=[('loops_arithmetic', 'bBhHiIlLqQ')]),\n           TD(flts + cmplx),\n           [TypeDescription('m', FullTypeDescr, 'mq', 'm'),\n            TypeDescription('m', FullTypeDescr, 'md', 'm'),"
            },
            {
                "filename": "numpy/core/src/common/simd/intdiv.h",
                "patch": "@@ -368,18 +368,18 @@ NPY_FINLINE npyv_s32x3 npyv_divisor_s32(npy_int32 d)\n {\n     npy_int32 d1 = abs(d);\n     npy_int32 sh, m;\n-    if (d1 > 1) {\n+    // Handel abs overflow\n+    if ((npy_uint32)d == 0x80000000U) {\n+        m = 0x80000001;\n+        sh = 30;\n+    }\n+    else if (d1 > 1) {\n         sh = npyv__bitscan_revnz_u32(d1 - 1); // ceil(log2(abs(d))) - 1\n         m =  (1ULL << (32 + sh)) / d1 + 1;    // multiplier\n     }\n     else if (d1 == 1) {\n         sh = 0; m = 1;\n     }\n-    // fix abs overflow\n-    else if (d == (1 << 31)) {\n-        m = d + 1;\n-        sh = 30;\n-    }\n     else {\n         // raise arithmetic exception for d == 0\n         sh = m = 1 / ((npy_int32 volatile *)&d)[0]; // LCOV_EXCL_LINE\n@@ -445,18 +445,18 @@ NPY_FINLINE npyv_s64x3 npyv_divisor_s64(npy_int64 d)\n #else\n     npy_int64 d1 = llabs(d);\n     npy_int64 sh, m;\n-    if (d1 > 1) {\n+    // Handel abs overflow\n+    if ((npy_uint64)d == 0x8000000000000000ULL) {\n+        m = 0x8000000000000001LL;\n+        sh = 62;\n+    }\n+    else if (d1 > 1) {\n         sh = npyv__bitscan_revnz_u64(d1 - 1);       // ceil(log2(abs(d))) - 1\n         m  = npyv__divh128_u64(1ULL << sh, d1) + 1; // multiplier\n     }\n     else if (d1 == 1) {\n         sh = 0; m = 1;\n     }\n-    // fix abs overflow\n-    else if (d == (1LL << 63)) {\n-        m = d + 1;\n-        sh = 62;\n-    }\n     else {\n         // raise arithmetic exception for d == 0\n         sh = m = 1 / ((npy_int64 volatile *)&d)[0]; // LCOV_EXCL_LINE"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -843,92 +843,6 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 void\n     UNARY_LOOP_FAST(@type@, @type@, *out = in > 0 ? 1 : (in < 0 ? -1 : 0));\n }\n \n-/* Libdivide only supports 32 and 64 bit types\n- * We try to pick the best possible one */\n-#if NPY_BITSOF_@TYPE@ <= 32\n-#define libdivide_@type@_t libdivide_s32_t\n-#define libdivide_@type@_gen libdivide_s32_gen\n-#define libdivide_@type@_do libdivide_s32_do\n-#else\n-#define libdivide_@type@_t libdivide_s64_t\n-#define libdivide_@type@_gen libdivide_s64_gen\n-#define libdivide_@type@_do libdivide_s64_do\n-#endif\n-\n-NPY_NO_EXPORT void\n-@TYPE@_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    BINARY_DEFS\n-\n-    /* When the divisor is a constant, use libdivide for faster division */\n-    if (steps[1] == 0) {\n-        /* In case of empty array, just return */\n-        if (n == 0) {\n-            return;\n-        }\n-\n-        const @type@ in2 = *(@type@ *)ip2;\n-\n-        /* If divisor is 0, we need not compute anything */\n-        if (in2 == 0) {\n-            npy_set_floatstatus_divbyzero();\n-            BINARY_LOOP_SLIDING {\n-                *((@type@ *)op1) = 0;\n-            }\n-        }\n-        else {\n-            struct libdivide_@type@_t fast_d = libdivide_@type@_gen(in2);\n-            BINARY_LOOP_SLIDING {\n-                const @type@ in1 = *(@type@ *)ip1;\n-                /*\n-                 * FIXME: On x86 at least, dividing the smallest representable integer\n-                 * by -1 causes a SIFGPE (division overflow). We treat this case here\n-                 * (to avoid a SIGFPE crash at python level), but a good solution would\n-                 * be to treat integer division problems separately from FPU exceptions\n-                 * (i.e. a different approach than npy_set_floatstatus_divbyzero()).\n-                 */\n-                if (in1 == NPY_MIN_@TYPE@ && in2 == -1) {\n-                    npy_set_floatstatus_divbyzero();\n-                    *((@type@ *)op1) = 0;\n-                }\n-                else {\n-                    *((@type@ *)op1) = libdivide_@type@_do(in1, &fast_d);\n-\n-                    /* Negative quotients needs to be rounded down */\n-                    if (((in1 > 0) != (in2 > 0)) && (*((@type@ *)op1) * in2 != in1)) {\n-                        *((@type@ *)op1) = *((@type@ *)op1) - 1;\n-                    }\n-                }\n-            }\n-        }\n-    }\n-    else {\n-        BINARY_LOOP_SLIDING {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            const @type@ in2 = *(@type@ *)ip2;\n-            /*\n-             * FIXME: On x86 at least, dividing the smallest representable integer\n-             * by -1 causes a SIFGPE (division overflow). We treat this case here\n-             * (to avoid a SIGFPE crash at python level), but a good solution would\n-             * be to treat integer division problems separately from FPU exceptions\n-             * (i.e. a different approach than npy_set_floatstatus_divbyzero()).\n-             */\n-            if (in2 == 0 || (in1 == NPY_MIN_@TYPE@ && in2 == -1)) {\n-                npy_set_floatstatus_divbyzero();\n-                *((@type@ *)op1) = 0;\n-            }\n-            else {\n-                *((@type@ *)op1) = in1/in2;\n-\n-                /* Negative quotients needs to be rounded down */\n-                if (((in1 > 0) != (in2 > 0)) && (*((@type@ *)op1) * in2 != in1)) {\n-                    *((@type@ *)op1) = *((@type@ *)op1) - 1;\n-                }\n-            }\n-        }\n-    }\n-}\n-\n NPY_NO_EXPORT void\n @TYPE@_remainder(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -58,7 +58,8 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n #endif\n \n /**begin repeat\n- * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG#\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n+           BYTE,  SHORT,  INT,  LONG,  LONGLONG#\n  */\n  NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_divide,\n      (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n@@ -151,9 +152,6 @@ NPY_NO_EXPORT void\n NPY_NO_EXPORT void\n @S@@TYPE@_sign(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n-NPY_NO_EXPORT void\n-@TYPE@_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n-\n NPY_NO_EXPORT void\n @S@@TYPE@_remainder(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n "
            },
            {
                "filename": "numpy/core/src/umath/loops_arithmetic.dispatch.c.src",
                "patch": "@@ -20,10 +20,91 @@\n //###############################################################################\n /********************************************************************************\n  ** Defining the SIMD kernels\n+ *\n+ * Floor division of signed is based on T. Granlund and P. L. Montgomery\n+ * \u201cDivision by invariant integers using multiplication(see [Figure 6.1]\n+ * http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.2556)\"\n+ * For details on TRUNC division see simd/intdiv.h for more clarification\n+ ***********************************************************************************\n+ ** Figure 6.1: Signed division by run\u2013time invariant divisor, rounded towards -INF\n+ ***********************************************************************************\n+ * For q = FLOOR(a/d), all sword:\n+ *     sword \u2212dsign = SRL(d, N \u2212 1);\n+ *     uword \u2212nsign = (n < \u2212dsign);\n+ *     uword \u2212qsign = EOR(\u2212nsign, \u2212dsign);\n+ *     q = TRUNC((n \u2212 (\u2212dsign ) + (\u2212nsign))/d) \u2212 (\u2212qsign);\n  ********************************************************************************/\n+\n #if NPY_SIMD\n /**begin repeat\n- *  #sfx = u8, u16, u32, u64#\n+ * Signed types\n+ * #sfx    = s8, s16, s32, s64#\n+ * #len    = 8,  16,  32,  64#\n+ */\n+static NPY_INLINE void\n+simd_divide_by_scalar_contig_@sfx@(char **args, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ *src   = (npyv_lanetype_@sfx@ *) args[0];\n+    npyv_lanetype_@sfx@ scalar = *(npyv_lanetype_@sfx@ *) args[1];\n+    npyv_lanetype_@sfx@ *dst   = (npyv_lanetype_@sfx@ *) args[2];\n+    const int vstep            = npyv_nlanes_@sfx@;\n+    const npyv_@sfx@x3 divisor = npyv_divisor_@sfx@(scalar);\n+\n+    if (scalar == -1) {\n+        npyv_b@len@ noverflow = npyv_cvt_b@len@_@sfx@(npyv_setall_@sfx@(-1));\n+        npyv_@sfx@ vzero      = npyv_zero_@sfx@();\n+        for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+            npyv_@sfx@ a       = npyv_load_@sfx@(src);\n+            npyv_b@len@ gt_min = npyv_cmpgt_@sfx@(a, npyv_setall_@sfx@(NPY_MIN_INT@len@));\n+            noverflow          = npyv_and_b@len@(noverflow, gt_min);\n+            npyv_@sfx@ neg     = npyv_ifsub_@sfx@(gt_min, vzero, a, vzero);\n+            npyv_store_@sfx@(dst, neg);\n+        }\n+\n+        int raise_err = npyv_tobits_b@len@(npyv_not_b@len@(noverflow)) != 0;\n+        for (; len > 0; --len, ++src, ++dst) {\n+            npyv_lanetype_@sfx@ a = *src;\n+            if (a == NPY_MIN_INT@len@) {\n+                raise_err = 1;\n+                *dst  = 0;\n+            } else {\n+                *dst = -a;\n+            }\n+        }\n+        if (raise_err) {\n+            npy_set_floatstatus_divbyzero();\n+        }\n+    } else {\n+        for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+            npyv_@sfx@  nsign_d   = npyv_setall_@sfx@(scalar < 0);\n+            npyv_@sfx@  a         = npyv_load_@sfx@(src);\n+            npyv_@sfx@  nsign_a   = npyv_cvt_@sfx@_b@len@(npyv_cmplt_@sfx@(a, nsign_d));\n+            nsign_a               = npyv_and_@sfx@(nsign_a, npyv_setall_@sfx@(1));\n+            npyv_@sfx@  diff_sign = npyv_sub_@sfx@(nsign_a, nsign_d);\n+            npyv_@sfx@  to_ninf   = npyv_xor_@sfx@(nsign_a, nsign_d);\n+            npyv_@sfx@  trunc     = npyv_divc_@sfx@(npyv_add_@sfx@(a, diff_sign), divisor);\n+            npyv_@sfx@  floor     = npyv_sub_@sfx@(trunc, to_ninf);\n+            npyv_store_@sfx@(dst, floor);\n+        }\n+\n+        for (; len > 0; --len, ++src, ++dst) {\n+            const npyv_lanetype_@sfx@ a = *src;\n+            npyv_lanetype_@sfx@ r = a / scalar;\n+            // Negative quotients needs to be rounded down\n+            if (((a > 0) != (scalar > 0)) && ((r * scalar) != a)) {\n+                r--;\n+            }\n+            *dst = r;\n+        }\n+    }\n+    npyv_cleanup();\n+}\n+/**end repeat**/\n+\n+/**begin repeat\n+ * Unsigned types\n+ * #sfx    = u8, u16, u32, u64#\n+ * #len    = 8,  16,  32,  64#\n  */\n static NPY_INLINE void\n simd_divide_by_scalar_contig_@sfx@(char **args, npy_intp len)\n@@ -44,7 +125,6 @@ simd_divide_by_scalar_contig_@sfx@(char **args, npy_intp len)\n         const npyv_lanetype_@sfx@ a = *src;\n         *dst = a / scalar;\n     }\n-\n     npyv_cleanup();\n }\n /**end repeat**/\n@@ -54,6 +134,70 @@ simd_divide_by_scalar_contig_@sfx@(char **args, npy_intp len)\n  ** Defining ufunc inner functions\n  ********************************************************************************/\n \n+/**begin repeat\n+ * Signed types\n+ *  #type  = npy_byte, npy_short, npy_int, npy_long, npy_longlong#\n+ *  #TYPE  = BYTE,     SHORT,     INT,     LONG,     LONGLONG#\n+ */\n+#undef TO_SIMD_SFX\n+#if 0\n+/**begin repeat1\n+ * #len = 8, 16, 32, 64#\n+ */\n+#elif NPY_BITSOF_@TYPE@ == @len@\n+    #define TO_SIMD_SFX(X) X##_s@len@\n+/**end repeat1**/\n+#endif\n+\n+#if NPY_BITSOF_@TYPE@ == 64 && !defined(NPY_HAVE_VSX4) && (defined(NPY_HAVE_VSX) || defined(NPY_HAVE_NEON))\n+    #undef TO_SIMD_SFX\n+#endif\n+\n+NPY_FINLINE @type@ floor_div_@TYPE@(const @type@ n, const @type@ d)\n+{\n+    /*\n+     * FIXME: On x86 at least, dividing the smallest representable integer\n+     * by -1 causes a SIFGPE (division overflow). We treat this case here\n+     * (to avoid a SIGFPE crash at python level), but a good solution would\n+     * be to treat integer division problems separately from FPU exceptions\n+     * (i.e. a different approach than npy_set_floatstatus_divbyzero()).\n+     */\n+    if (NPY_UNLIKELY(d == 0 || (n == NPY_MIN_@TYPE@ && d == -1))) {\n+        npy_set_floatstatus_divbyzero();\n+        return 0;\n+    }\n+    @type@ r = n / d;\n+    // Negative quotients needs to be rounded down\n+    if (((n > 0) != (d > 0)) && ((r * d) != n)) {\n+        r--;\n+    }\n+    return r;\n+}\n+\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_divide)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    if (IS_BINARY_REDUCE) {\n+        BINARY_REDUCE_LOOP(@type@) {\n+            io1 = floor_div_@TYPE@(io1, *(@type@*)ip2);\n+        }\n+        *((@type@ *)iop1) = io1;\n+    }\n+#if NPY_SIMD && defined(TO_SIMD_SFX)\n+    // for contiguous block of memory, divisor is a scalar and not 0\n+    else if (IS_BLOCKABLE_BINARY_SCALAR2(sizeof(@type@), NPY_SIMD_WIDTH) &&\n+             (*(@type@ *)args[1]) != 0) {\n+        TO_SIMD_SFX(simd_divide_by_scalar_contig)(args, dimensions[0]);\n+    }\n+#endif\n+    else {\n+        BINARY_LOOP {\n+            *((@type@ *)op1) = floor_div_@TYPE@(*(@type@*)ip1, *(@type@*)ip2);\n+        }\n+    }\n+}\n+/**end repeat**/\n+\n /**begin repeat\n  * Unsigned types\n  *  #type  = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong#"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 18075,
        "body": "### Dispatch for unsigned floor division\r\n\r\nHi @seiko2plus / @Qiyu8 , I am attempting to add fast integer division using universal intrinsics. I wanted your opinion on my approach.\r\n\r\nI have marked much of the hardcoded parts with `// XXX`(everything is for `16bit`and only `signed`). Now I am able to understand the dispatch mechanism to an extent and hit the code paths through dispatch(not through below diff, I hardcoded few `run_binary_simd_*` for that). But there are few things that do not work with integer types like load and store.\r\n\r\n~We can add stuff in memory.h, but wanted your opinion on the load-store part. Also in general am I on the right path?~\r\n\r\ncc: @seberg ",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -215,5 +215,6 @@ numpy/core/src/_simd/_simd_inc.h\n # umath module\n numpy/core/src/umath/loops_unary_fp.dispatch.c\n numpy/core/src/umath/loops_arithm_fp.dispatch.c\n+numpy/core/src/umath/loops_arithmetic.dispatch.c\n numpy/core/src/umath/loops_trigonometric.dispatch.c\n numpy/core/src/umath/loops_exponent_log.dispatch.c"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -135,18 +135,19 @@ def time_less_than_scalar2(self, dtype):\n \n \n class CustomScalarFloorDivideInt(Benchmark):\n-    params = ([np.int8, np.int16, np.int32, np.int64], [8, -8, 43, -43, 0])\n+    params = (np.sctypes['int'] + np.sctypes['uint'], [8, -8, 43, -43])\n     param_names = ['dtype', 'divisors']\n-    max_value = 10**7\n-    min_value = -10**7\n \n     def setup(self, dtype, divisor):\n+        if dtype in np.sctypes['uint'] and divisor < 0:\n+            raise NotImplementedError(\n+                    \"Skipping test for negative divisor with unsigned type\")\n+\n         iinfo = np.iinfo(dtype)\n-        self.x = np.arange(\n-                max(iinfo.min, self.min_value),\n-                min(iinfo.max, self.max_value), dtype=dtype)\n+        self.x = np.random.randint(\n+                    iinfo.min, iinfo.max, size=10000, dtype=dtype)\n \n-    def time_floor_divide_int(self, dtpye, divisor):\n+    def time_floor_divide_int(self, dtype, divisor):\n         self.x // divisor\n \n "
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -245,6 +245,8 @@ def english_upper(s):\n O = 'O'\n P = 'P'\n ints = 'bBhHiIlLqQ'\n+sints = 'bhilq'\n+uints = 'BHILQ'\n times = 'Mm'\n timedeltaonly = 'm'\n intsO = ints + O\n@@ -325,7 +327,9 @@ def english_upper(s):\n     Ufunc(2, 1, None, # One is only a unit to the right, not the left\n           docstrings.get('numpy.core.umath.floor_divide'),\n           'PyUFunc_DivisionTypeResolver',\n-          TD(intfltcmplx),\n+          TD(uints, cfunc_alias='divide',\n+              dispatch=[('loops_arithmetic', 'BHILQ')]),\n+          TD(sints + flts + cmplx),\n           [TypeDescription('m', FullTypeDescr, 'mq', 'm'),\n            TypeDescription('m', FullTypeDescr, 'md', 'm'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'q'),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -931,6 +931,7 @@ def generate_umath_c(ext, build_dir):\n             join('src', 'umath', 'loops.c.src'),\n             join('src', 'umath', 'loops_unary_fp.dispatch.c.src'),\n             join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),\n+            join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),\n             join('src', 'umath', 'loops_trigonometric.dispatch.c.src'),\n             join('src', 'umath', 'loops_exponent_log.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -1014,22 +1014,6 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 void\n     UNARY_LOOP_FAST(@type@, @type@, *out = in > 0 ? 1 : 0);\n }\n \n-NPY_NO_EXPORT void\n-@TYPE@_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    BINARY_LOOP {\n-        const @type@ in1 = *(@type@ *)ip1;\n-        const @type@ in2 = *(@type@ *)ip2;\n-        if (in2 == 0) {\n-            npy_set_floatstatus_divbyzero();\n-            *((@type@ *)op1) = 0;\n-        }\n-        else {\n-            *((@type@ *)op1)= in1/in2;\n-        }\n-    }\n-}\n-\n NPY_NO_EXPORT void\n @TYPE@_remainder(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -53,6 +53,17 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n  *****************************************************************************\n  */\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_arithmetic.dispatch.h\"\n+#endif\n+\n+/**begin repeat\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG#\n+ */\n+ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_divide,\n+     (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat**/\n+\n /**begin repeat\n  * #TYPE = BYTE, SHORT, INT, LONG, LONGLONG#\n  */\n@@ -141,7 +152,7 @@ NPY_NO_EXPORT void\n @S@@TYPE@_sign(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n NPY_NO_EXPORT void\n-@S@@TYPE@_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+@TYPE@_divide(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n \n NPY_NO_EXPORT void\n @S@@TYPE@_remainder(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));"
            },
            {
                "filename": "numpy/core/src/umath/loops_arithmetic.dispatch.c.src",
                "patch": "@@ -0,0 +1,118 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** sse2 sse41 avx2 avx512f avx512_skx\n+ ** vsx2\n+ ** neon\n+ **/\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"lowlevel_strided_loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+//###############################################################################\n+//## Division\n+//###############################################################################\n+/********************************************************************************\n+ ** Defining the SIMD kernels\n+ ********************************************************************************/\n+#if NPY_SIMD\n+/**begin repeat\n+ *  #sfx = u8, u16, u32, u64#\n+ */\n+static NPY_INLINE void\n+simd_divide_by_scalar_contig_@sfx@(char **args, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ *src   = (npyv_lanetype_@sfx@ *) args[0];\n+    npyv_lanetype_@sfx@ scalar = *(npyv_lanetype_@sfx@ *) args[1];\n+    npyv_lanetype_@sfx@ *dst   = (npyv_lanetype_@sfx@ *) args[2];\n+    const int vstep            = npyv_nlanes_@sfx@;\n+    const npyv_@sfx@x3 divisor = npyv_divisor_@sfx@(scalar);\n+\n+    for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+        npyv_@sfx@ a = npyv_load_@sfx@(src);\n+        npyv_@sfx@ c = npyv_divc_@sfx@(a, divisor);\n+        npyv_store_@sfx@(dst, c);\n+    }\n+\n+    for (; len > 0; --len, ++src, ++dst) {\n+        const npyv_lanetype_@sfx@ a = *src;\n+        *dst = a / scalar;\n+    }\n+\n+    npyv_cleanup();\n+}\n+/**end repeat**/\n+#endif\n+\n+/********************************************************************************\n+ ** Defining ufunc inner functions\n+ ********************************************************************************/\n+\n+/**begin repeat\n+ * Unsigned types\n+ *  #type  = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong#\n+ *  #TYPE  = UBYTE,     USHORT,     UINT,     ULONG,     ULONGLONG#\n+ *  #STYPE = BYTE,      SHORT,      INT,      LONG,      LONGLONG#\n+ */\n+#undef TO_SIMD_SFX\n+#if 0\n+/**begin repeat1\n+ * #len = 8, 16, 32, 64#\n+ */\n+#elif NPY_BITSOF_@STYPE@ == @len@\n+    #define TO_SIMD_SFX(X) X##_u@len@\n+/**end repeat1**/\n+#endif\n+/*\n+ * For 64-bit division on Armv7, Aarch64, and IBM/Power, NPYV fall-backs to the scalar division\n+ * because emulating multiply-high on these architectures is going to be expensive comparing\n+ * to the native scalar dividers.\n+ * Therefore it's better to disable NPYV in this special case to avoid any unnecessary shuffles.\n+ * Power10(VSX4) is an exception here since it has native support for integer vector division,\n+ * note neither infrastructure nor NPYV has supported VSX4 yet.\n+ */\n+#if NPY_BITSOF_@STYPE@ == 64 && !defined(NPY_HAVE_VSX4) && (defined(NPY_HAVE_VSX) || defined(NPY_HAVE_NEON))\n+    #undef TO_SIMD_SFX\n+#endif\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_divide)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    if (IS_BINARY_REDUCE) {\n+        BINARY_REDUCE_LOOP(@type@) {\n+            const @type@ d = *(@type@ *)ip2;\n+            if (NPY_UNLIKELY(d == 0)) {\n+                npy_set_floatstatus_divbyzero();\n+                io1 = 0;\n+            } else {\n+                io1 /= d;\n+            }\n+        }\n+        *((@type@ *)iop1) = io1;\n+    }\n+#if NPY_SIMD && defined(TO_SIMD_SFX)\n+    // for contiguous block of memory, divisor is a scalar and not 0\n+    else if (IS_BLOCKABLE_BINARY_SCALAR2(sizeof(@type@), NPY_SIMD_WIDTH) &&\n+             (*(@type@ *)args[1]) != 0) {\n+        TO_SIMD_SFX(simd_divide_by_scalar_contig)(args, dimensions[0]);\n+    }\n+#endif\n+    else {\n+        BINARY_LOOP {\n+            const @type@ in1 = *(@type@ *)ip1;\n+            const @type@ in2 = *(@type@ *)ip2;\n+            if (NPY_UNLIKELY(in2 == 0)) {\n+                npy_set_floatstatus_divbyzero();\n+                *((@type@ *)op1) = 0;\n+            } else{\n+                *((@type@ *)op1) = in1 / in2;\n+            }\n+        }\n+    }\n+}\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -250,13 +250,22 @@ def test_division_int(self):\n         assert_equal(x % 100, [5, 10, 90, 0, 95, 90, 10, 0, 80])\n \n     @pytest.mark.parametrize(\"input_dtype\",\n-            [np.int8, np.int16, np.int32, np.int64])\n+            np.sctypes['int'] + np.sctypes['uint'])\n     def test_division_int_boundary(self, input_dtype):\n         iinfo = np.iinfo(input_dtype)\n \n+        # Unsigned:\n+        # Create list with 0, 25th, 50th, 75th percentile and max\n+        if iinfo.min == 0:\n+            lst = [0, iinfo.max//4, iinfo.max//2,\n+                    int(iinfo.max/1.33), iinfo.max]\n+            divisors = [iinfo.max//4, iinfo.max//2,\n+                    int(iinfo.max/1.33), iinfo.max]\n+        # Signed:\n         # Create list with min, 25th percentile, 0, 75th percentile, max\n-        lst = [iinfo.min, iinfo.min//2, 0, iinfo.max//2, iinfo.max]\n-        divisors = [iinfo.min, iinfo.min//2, iinfo.max//2, iinfo.max]\n+        else:\n+            lst = [iinfo.min, iinfo.min//2, 0, iinfo.max//2, iinfo.max]\n+            divisors = [iinfo.min, iinfo.min//2, iinfo.max//2, iinfo.max]\n         a = np.array(lst, dtype=input_dtype)\n \n         for divisor in divisors:\n@@ -926,7 +935,7 @@ def test_log_values(self):\n             assert_raises(FloatingPointError, np.log, np.float32(-np.inf))\n             assert_raises(FloatingPointError, np.log, np.float32(-1.0))\n \n-        # See https://github.com/numpy/numpy/issues/18005 \n+        # See https://github.com/numpy/numpy/issues/18005\n         with assert_no_warnings():\n             a = np.array(1e9, dtype='float32')\n             np.log(a)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 17737,
        "body": "#### print the supported CPU features during the run of performance tests\r\n\r\nreleated to #17736, #17716",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/__init__.py",
                "patch": "@@ -1 +1,53 @@\n from . import common\n+import sys\n+import os\n+\n+def show_cpu_features():\n+    from numpy.lib.utils import _opt_info\n+    info = _opt_info()\n+    info = \"NumPy CPU features: \" + (info if info else 'nothing enabled')\n+    # ASV wrapping stdout & stderr, so we assume having a tty here\n+    if 'SHELL' in os.environ and sys.platform != 'win32':\n+        # to avoid the red color that imposed by ASV\n+        print(f\"\\033[33m{info}\\033[0m\")\n+    else:\n+        print(info)\n+\n+def dirty_lock(lock_name, lock_on_count=1):\n+    # this lock occurred before each round to avoid duplicate printing\n+    if not hasattr(os, \"getppid\"):\n+        return False\n+    ppid = os.getppid()\n+    if not ppid or ppid == os.getpid():\n+        # not sure if this gonna happen, but ASV run each round in\n+        # a separate process so the lock should be based on the parent\n+        # process id only\n+        return False\n+    lock_path = os.path.abspath(os.path.join(\n+        os.path.dirname(__file__), \"..\", \"env\", lock_name)\n+    )\n+    # ASV load the 'benchmark_dir' to discovering the available benchmarks\n+    # the issue here is ASV doesn't capture any strings from stdout or stderr\n+    # during this stage so we escape it and lock on the second increment\n+    try:\n+        with open(lock_path, 'a+') as f:\n+            f.seek(0)\n+            count, _ppid = (f.read().split() + [0, 0])[:2]\n+            count, _ppid = int(count), int(_ppid)\n+            if _ppid == ppid:\n+                if count >= lock_on_count:\n+                    return True\n+                count += 1\n+            else:\n+                count = 0\n+            f.seek(0)\n+            f.truncate()\n+            f.write(f\"{str(count)} {str(ppid)}\")\n+    except IOError:\n+        pass\n+    return False\n+\n+\n+# FIXME: there's no official way to provide extra information to the test log\n+if not dirty_lock(\"print_cpu_features.lock\"):\n+    show_cpu_features()"
            },
            {
                "filename": "numpy/_pytesttester.py",
                "patch": "@@ -35,25 +35,13 @@\n \n \n def _show_numpy_info():\n-    from numpy.core._multiarray_umath import (\n-        __cpu_features__, __cpu_baseline__, __cpu_dispatch__\n-    )\n     import numpy as np\n \n     print(\"NumPy version %s\" % np.__version__)\n     relaxed_strides = np.ones((10, 1), order=\"C\").flags.f_contiguous\n     print(\"NumPy relaxed strides checking option:\", relaxed_strides)\n-\n-    if len(__cpu_baseline__) == 0 and len(__cpu_dispatch__) == 0:\n-        enabled_features = \"nothing enabled\"\n-    else:\n-        enabled_features = ' '.join(__cpu_baseline__)\n-        for feature in __cpu_dispatch__:\n-            if __cpu_features__[feature]:\n-                enabled_features += \" %s*\" % feature\n-            else:\n-                enabled_features += \" %s?\" % feature\n-    print(\"NumPy CPU features:\", enabled_features)\n+    info = np.lib.utils._opt_info()\n+    print(\"NumPy CPU features: \", (info if info else 'nothing enabled'))\n \n \n "
            },
            {
                "filename": "numpy/lib/utils.py",
                "patch": "@@ -197,28 +197,28 @@ def deprecate(*args, **kwargs):\n def deprecate_with_doc(msg):\n     \"\"\"\n     Deprecates a function and includes the deprecation in its docstring.\n-    \n-    This function is used as a decorator. It returns an object that can be \n-    used to issue a DeprecationWarning, by passing the to-be decorated \n-    function as argument, this adds warning to the to-be decorated function's \n+\n+    This function is used as a decorator. It returns an object that can be\n+    used to issue a DeprecationWarning, by passing the to-be decorated\n+    function as argument, this adds warning to the to-be decorated function's\n     docstring and returns the new function object.\n-    \n+\n     See Also\n     --------\n-    deprecate : Decorate a function such that it issues a `DeprecationWarning` \n-    \n+    deprecate : Decorate a function such that it issues a `DeprecationWarning`\n+\n     Parameters\n     ----------\n     msg : str\n-        Additional explanation of the deprecation. Displayed in the \n+        Additional explanation of the deprecation. Displayed in the\n         docstring after the warning.\n \n     Returns\n     -------\n     obj : object\n \n     \"\"\"\n-    return _Deprecate(message=msg)  \n+    return _Deprecate(message=msg)\n \n \n #--------------------------------------------\n@@ -1042,4 +1042,30 @@ def _median_nancheck(data, result, axis, out):\n         result[n] = np.nan\n     return result\n \n+def _opt_info():\n+    \"\"\"\n+    Returns a string contains the supported CPU features by the current build.\n+\n+    The string format can be explained as follows:\n+        - dispatched features that are supported by the running machine\n+          end with `*`.\n+        - dispatched features that are \"not\" supported by the running machine\n+          end with `?`.\n+        - remained features are representing the baseline.\n+    \"\"\"\n+    from numpy.core._multiarray_umath import (\n+        __cpu_features__, __cpu_baseline__, __cpu_dispatch__\n+    )\n+\n+    if len(__cpu_baseline__) == 0 and len(__cpu_dispatch__) == 0:\n+        return ''\n+\n+    enabled_features = ' '.join(__cpu_baseline__)\n+    for feature in __cpu_dispatch__:\n+        if __cpu_features__[feature]:\n+            enabled_features += f\" {feature}*\"\n+        else:\n+            enabled_features += f\" {feature}?\"\n+\n+    return enabled_features\n #-----------------------------------------------------------------------------"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 17736,
        "body": "#### Fix passing optimization build options to asv\r\ncloses #17716\r\n\r\nsee https://github.com/numpy/numpy/issues/17716#issuecomment-723712282",
        "changed_files": [
            {
                "filename": "benchmarks/asv_compare.conf.json.tpl",
                "patch": "@@ -78,7 +78,9 @@\n \n     \"build_command\" : [\n         \"python setup.py build {numpy_build_options}\",\n-        \"PIP_NO_BUILD_ISOLATION=false python -mpip wheel --no-deps --no-index -w {build_cache_dir} {build_dir}\"\n+        // pip ignores '--global-option' when pep517 is enabled, we also enabling pip verbose to\n+        // be reached from asv `--verbose` so we can verify the build options.\n+        \"PIP_NO_BUILD_ISOLATION=false python {build_dir}/benchmarks/asv_pip_nopep517.py -v {numpy_global_options} --no-deps --no-index -w {build_cache_dir} {build_dir}\"\n     ],\n     // The commits after which the regression search in `asv publish`\n     // should start looking for regressions. Dictionary whose keys are"
            },
            {
                "filename": "benchmarks/asv_pip_nopep517.py",
                "patch": "@@ -0,0 +1,15 @@\n+\"\"\"\n+This file is used by asv_compare.conf.json.tpl.\n+\"\"\"\n+import subprocess, sys\n+# pip ignores '--global-option' when pep517 is enabled therefore we disable it.\n+cmd = [sys.executable, '-mpip', 'wheel', '--no-use-pep517']\n+try:\n+    output = subprocess.check_output(cmd, stderr=subprocess.STDOUT, universal_newlines=True)\n+except Exception as e:\n+    output = str(e.output)\n+if \"no such option\" in output:\n+    print(\"old version of pip, escape '--no-use-pep517'\")\n+    cmd.pop()\n+\n+subprocess.run(cmd + sys.argv[1:])"
            },
            {
                "filename": "runtests.py",
                "patch": "@@ -524,6 +524,7 @@ def asv_compare_config(bench_path, args, h_commits):\n \n     is_cached = asv_substitute_config(conf_path, nconf_path,\n         numpy_build_options = ' '.join([f'\\\\\"{v}\\\\\"' for v in build]),\n+        numpy_global_options= ' '.join([f'--global-option=\\\\\"{v}\\\\\"' for v in [\"build\"] + build])\n     )\n     if not is_cached:\n         asv_clear_cache(bench_path, h_commits)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21501,
        "body": "Right now, example for [ascontiguousarray()](https://numpy.org/doc/stable/reference/generated/numpy.ascontiguousarray.html#numpy.ascontiguousarray) only demonstrates that the function is capable to change the `dtype`, but it doesn't really change the memory layout since `x` is already `C_CONTIGUOUS`. In this PR we make this example more relevant and be consistent with the example for [asfortranarray()](https://numpy.org/doc/stable/reference/generated/numpy.asfortranarray.html#numpy.asfortranarray).",
        "changed_files": [
            {
                "filename": "numpy/core/_add_newdocs.py",
                "patch": "@@ -1084,13 +1084,32 @@\n \n     Examples\n     --------\n-    >>> x = np.arange(6).reshape(2,3)\n-    >>> np.ascontiguousarray(x, dtype=np.float32)\n-    array([[0., 1., 2.],\n-           [3., 4., 5.]], dtype=float32)\n+    Starting with a Fortran-contiguous array:\n+\n+    >>> x = np.ones((2, 3), order='F')\n+    >>> x.flags['F_CONTIGUOUS']\n+    True\n+\n+    Calling ``ascontiguousarray`` makes a C-contiguous copy:\n+\n+    >>> y = np.ascontiguousarray(x)\n+    >>> y.flags['C_CONTIGUOUS']\n+    True\n+    >>> np.may_share_memory(x, y)\n+    False\n+\n+    Now, starting with a C-contiguous array:\n+\n+    >>> x = np.ones((2, 3), order='C')\n     >>> x.flags['C_CONTIGUOUS']\n     True\n \n+    Then, calling ``ascontiguousarray`` returns the same object:\n+\n+    >>> y = np.ascontiguousarray(x)\n+    >>> x is y\n+    True\n+\n     Note: This function returns an array with at least one-dimension (1-d)\n     so it will not preserve 0-d arrays.\n \n@@ -1130,12 +1149,31 @@\n \n     Examples\n     --------\n-    >>> x = np.arange(6).reshape(2,3)\n+    Starting with a C-contiguous array:\n+\n+    >>> x = np.ones((2, 3), order='C')\n+    >>> x.flags['C_CONTIGUOUS']\n+    True\n+\n+    Calling ``asfortranarray`` makes a Fortran-contiguous copy:\n+\n     >>> y = np.asfortranarray(x)\n-    >>> x.flags['F_CONTIGUOUS']\n-    False\n     >>> y.flags['F_CONTIGUOUS']\n     True\n+    >>> np.may_share_memory(x, y)\n+    False\n+\n+    Now, starting with a Fortran-contiguous array:\n+\n+    >>> x = np.ones((2, 3), order='F')\n+    >>> x.flags['F_CONTIGUOUS']\n+    True\n+\n+    Then, calling ``asfortranarray`` returns the same object:\n+\n+    >>> y = np.asfortranarray(x)\n+    >>> x is y\n+    True\n \n     Note: This function returns an array with at least one-dimension (1-d)\n     so it will not preserve 0-d arrays."
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 14497,
        "body": "This patch implemented AVX2/AVX512 version of small_correlate for small\r\nkernels range from 3~8 for DOUBLE type. Benchmarking with\r\nnumpy.convolve() shows performance improvement from 1.6x ~ 2.0x.\r\n\r\nMore detailed description:\r\nThis patch re-implemented one numpy internal function -- small_correlate(), which is the fundamental computing function for Numpy.convolve()/correlate() when kernel size is small (<11, openblas will be invoked when >=11). This patch uses X86 intrinsic implemented both AVX2 (+FMA) and AVX512 version. Benchmarked data shows that GCC's auto-generated AVX binary can bring up to 10% regression against non-avx binary, while this patch brings constant level of performance cross different kernel size, and obtain 1.6x ~ 2.0x performance boost against the non-avx binary (performance boost comparing with GCC's auto AVX binary is even bigger considering its regression) as measured by calling numpy.convolve() to process big arrays. The benchmark is attached as numpySmallConvBench.py.  \r\n\r\nBesides performance, as this patch re-implemented the small_correlate() function, I also created related test to verify its functionality, which is also merged in the attached numpySmallConvBench.py. Test results shows constant output of this patch comparing with the original numpy's output.\r\n\r\n<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/arraytypes.c.src",
                "patch": "@@ -27,7 +27,7 @@\n #include \"alloc.h\"\n #include \"typeinfo.h\"\n #ifdef NPY_HAVE_SSE2_INTRINSICS\n-#include <emmintrin.h>\n+#include <immintrin.h>\n #endif\n \n #include \"npy_longdouble.h\"\n@@ -3978,6 +3978,880 @@ static int\n  *****************************************************************************\n  */\n \n+#if defined(__AVX512F__)\n+static int\n+_sc_vectorized_k3(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m128d rt_10 = _mm_loadu_pd(&k[0]);                     // K1|K0\n+    __m128d rt_01 = _mm_permute_pd(rt_10, 0x1);              // K0|K1\n+    __m128d rt_02 = _mm_set_sd(k[2]);                        //  0|K2\n+\n+    __m512d rk0 = _mm512_broadcastsd_pd(rt_10);              // K0|K0|K0|K0|K0|K0|K0|K0\n+    __m512d rk1 = _mm512_broadcastsd_pd(rt_01);              // K1|K1|K1|K1|K1|K1|K1|K1\n+    __m512d rk2 = _mm512_broadcastsd_pd(rt_02);              // K2|K2|K2|K2|K2|K2|K2|K2\n+\n+    __m256d rk_special = _mm256_insertf128_pd(_mm256_castpd128_pd256(rt_01), rt_02, 1);  // 0|K2|K0|K1\n+    rk_special = _mm256_permute4x64_pd(rk_special, 0x87);    // K2|K1|K0|0\n+\n+    // 8-way processing, no preload\n+    for (i = 0; i < nd - (nd % 8); i += 8) {\n+        __m512d rr = _mm512_setzero_pd();\n+        __m512d rd0 = _mm512_loadu_pd(&d[i]);\n+        __m512d rd1 = _mm512_loadu_pd(&d[i + 1]);\n+        __m512d rd2 = _mm512_loadu_pd(&d[i + 2]);\n+        rr = _mm512_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk2, rd2, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+    }\n+\n+    // 4-way processing, no preload\n+    for (; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk0), rd0, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk1), rd1, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk2), rd2, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m128d rr;\n+        __m256d rd0 = _mm256_loadu_pd(&d[i-1]);              // PADD|PADD|D(i)|D(i-1)\n+        __m256d rr_210 = _mm256_mul_pd(rk_special, rd0);\n+        __m128d rr_210_lo  = _mm256_castpd256_pd128(rr_210);\n+        __m128d rr_210_hi = _mm256_extractf128_pd(rr_210, 1);\n+        __m128d rr_tmp =   _mm_add_pd(rr_210_lo, rr_210_hi);\n+        __m128d rr_tmp1 = _mm_shuffle_pd(rr_tmp, rr_tmp, 0x01);\n+        rr = _mm_add_pd(rr_tmp, rr_tmp1);\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k4(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i = 0;\n+\n+    __m128d rt128_10 = _mm_loadu_pd(&k[0]);                  // K1|K0\n+    __m128d rt128_32 = _mm_loadu_pd(&k[2]);                  // K3|K2\n+    __m128d rt128_01 = _mm_permute_pd(rt128_10, 0x1);        // K0|K1\n+    __m128d rt128_23 = _mm_permute_pd(rt128_32, 0x1);        // K2|K3\n+    // also load a full kernel, use load instead of combination of permute/blender operations to leverage cache hit\n+    __m256d rk = _mm256_loadu_pd(&k[0]);                     // K3|K2|K1|K0\n+\n+    __m512d rk512_0 = _mm512_broadcastsd_pd(rt128_10);       // K0|K0|K0|K0|K0|K0|K0|K0\n+    __m512d rk512_1 = _mm512_broadcastsd_pd(rt128_01);       // K1|K1|K1|K1|K1|K1|K1|K1\n+    __m512d rk512_2 = _mm512_broadcastsd_pd(rt128_32);       // K2|K2|K2|K2|K2|K2|K2|K2\n+    __m512d rk512_3 = _mm512_broadcastsd_pd(rt128_23);       // K3|K3|K3|K3|K3|K3|K3|K3\n+\n+    if (nd > 23) {         // 8-way processing if (data lines > 23), full preload\n+        __m512d rd0;\n+        __m512d rdt = _mm512_loadu_pd(&d[0]);\n+        __m512d rdn = _mm512_loadu_pd(&d[8]);\n+        for (i = 0; i < nd - (nd % 8) - 16; i += 8) {\n+            rd0 = rdt;                                                                                                        //  D7|D6|D5|D4|D3|D2|D1|D0\n+            rdt = rdn;\n+            rdn = _mm512_loadu_pd(&d[i + 16]);\n+            __m512d rd1 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2, 0, 1), rdt); //  D8|D7|D6|D5|D4|D3|D2|D1\n+            __m512d rd2 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2), rdt); //  D9|D8|D7|D6|D5|D4|D3|D2\n+            __m512d rd3 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0,10, 0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3), rdt); // D10|D9|D8|D7|D6|D5|D4|D3\n+\n+            __m512d rr = _mm512_setzero_pd();\n+            rr = _mm512_fmadd_pd(rk512_0, rd0, rr);\n+            rr = _mm512_fmadd_pd(rk512_1, rd1, rr);\n+            rr = _mm512_fmadd_pd(rk512_2, rd2, rr);\n+            rr = _mm512_fmadd_pd(rk512_3, rd3, rr);\n+            _mm512_storeu_pd(&o[i], rr);\n+        }\n+\n+        // 8-way processing for the remaining 16~23\n+        rd0 = rdt;\n+        rdt = rdn;\n+        __m512d rd1 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2, 0, 1), rdt);\n+        __m512d rd2 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2), rdt);\n+        __m512d rd3 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0,10, 0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3), rdt);\n+\n+        __m512d rr = _mm512_setzero_pd();\n+        rr = _mm512_fmadd_pd(rk512_0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk512_1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk512_2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk512_3, rd3, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+        i += 8;\n+\n+        // 8-way processing for the remaining 8~15\n+        rd0 = rdt;\n+        __m256d rd_remain_xx10 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i + 8]));                                 // X|X|D1|D0\n+        __m256d rd_remain_xx22 = _mm256_castpd128_pd256(_mm_loaddup_pd(&d[i + 10]));                              // X|X|D2|D2\n+        rdt = _mm512_castpd256_pd512(_mm256_permute2f128_pd(rd_remain_xx10, rd_remain_xx22, 0x20)); // X|X|X|X|D2|D2|D1|D0\n+        rd1 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2, 0, 1), rdt);\n+        rd2 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2), rdt);\n+        rd3 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0,10, 0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3), rdt);\n+\n+        rr = _mm512_setzero_pd();\n+        rr = _mm512_fmadd_pd(rk512_0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk512_1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk512_2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk512_3, rd3, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+        i += 8;\n+    } else if (nd > 15) {     // 8-way processing if (24 > data lines > 15), half preload\n+        __m512d rd0;\n+        __m512d rdt = _mm512_loadu_pd(&d[0]);\n+        for (i = 0; i < nd - (nd % 8) - 8; i += 8) {\n+            rd0 = rdt;\n+            rdt = _mm512_loadu_pd(&d[i + 8]);\n+            __m512d rd1 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2, 0, 1), rdt);\n+            __m512d rd2 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2), rdt);\n+            __m512d rd3 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0,10, 0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3), rdt);\n+\n+            __m512d rr = _mm512_setzero_pd();\n+            rr = _mm512_fmadd_pd(rk512_0, rd0, rr);\n+            rr = _mm512_fmadd_pd(rk512_1, rd1, rr);\n+            rr = _mm512_fmadd_pd(rk512_2, rd2, rr);\n+            rr = _mm512_fmadd_pd(rk512_3, rd3, rr);\n+            _mm512_storeu_pd(&o[i], rr);\n+        }\n+\n+        // 8-way processing for the remaining 8~15\n+        rd0 = rdt;\n+        __m256d rd_remain_xx10 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i + 8]));                                 // X|X|D1|D0\n+        __m256d rd_remain_xx22 = _mm256_castpd128_pd256(_mm_loaddup_pd(&d[i + 10]));                              // X|X|D2|D2\n+        rdt = _mm512_castpd256_pd512(_mm256_permute2f128_pd(rd_remain_xx10, rd_remain_xx22, 0x20)); // X|X|X|X|D2|D2|D1|D0\n+        __m512d rd1 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2, 0, 1), rdt);\n+        __m512d rd2 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2), rdt);\n+        __m512d rd3 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0,10, 0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3), rdt);\n+\n+        __m512d rr = _mm512_setzero_pd();\n+        rr = _mm512_fmadd_pd(rk512_0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk512_1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk512_2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk512_3, rd3, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+        i += 8;\n+    } else if (nd > 7) {        // 8-way processing if (16 > data lines > 7), no preload\n+        __m512d rd0 = _mm512_loadu_pd(&d[0]);\n+        __m256d rd_remain_xx10 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i + 8]));                                 // X|X|D1|D0\n+        __m256d rd_remain_xx22 = _mm256_castpd128_pd256(_mm_loaddup_pd(&d[i + 10]));                              // X|X|D2|D2\n+        __m512d rdt = _mm512_castpd256_pd512(_mm256_permute2f128_pd(rd_remain_xx10, rd_remain_xx22, 0x20)); // X|X|X|X|D2|D2|D1|D0\n+        __m512d rd1 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2, 0, 1), rdt);\n+        __m512d rd2 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3, 0, 2), rdt);\n+        __m512d rd3 = _mm512_permutex2var_pd(rd0, _mm512_set_epi32(0,10, 0, 9, 0, 8, 0, 7, 0, 6, 0, 5, 0, 4, 0, 3), rdt);\n+\n+        __m512d rr = _mm512_setzero_pd();\n+        rr = _mm512_fmadd_pd(rk512_0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk512_1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk512_2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk512_3, rd3, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+        i += 8;\n+    }\n+\n+    // 4-way processing if remaining lines > 3\n+    if (nd % 8 > 3) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rk0 = _mm512_castpd512_pd256(rk512_0);\n+        __m256d rk1 = _mm512_castpd512_pd256(rk512_1);\n+        __m256d rk2 = _mm512_castpd512_pd256(rk512_2);\n+        __m256d rk3 = _mm512_castpd512_pd256(rk512_3);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm256_fmadd_pd(rk3, rd3, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+        i += 4;\n+    }\n+\n+    // 2-way processing if remaining lines > 1\n+    if (nd % 4 > 1) {\n+        __m128d rr;\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d xy0 = _mm256_mul_pd(rk, rd0);\n+        __m256d xy1 = _mm256_mul_pd(rk, rd1);\n+        __m256d tmpAdd0 = _mm256_hadd_pd(xy0, xy1);\n+        __m128d hi_tmpAdd0 = _mm256_extractf128_pd(tmpAdd0, 1);\n+        __m128d lo_tmpAdd0  = _mm256_castpd256_pd128(tmpAdd0);\n+        rr = _mm_add_pd(hi_tmpAdd0, lo_tmpAdd0);\n+        _mm_storeu_pd(&o[i], rr);\n+    }\n+\n+    // Processing the remaining 1 data line\n+    if (nd % 2 == 1) {\n+        i = nd - 1;\n+        __m128d rr;\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d xy = _mm256_mul_pd(rk, rd0);\n+        __m128d lo_xy  = _mm256_castpd256_pd128(xy);\n+        __m128d hi_xy = _mm256_extractf128_pd(xy, 1);\n+        __m128d tmpAdd0 =   _mm_add_pd(lo_xy, hi_xy);\n+        __m128d lo_tmpAdd0 = _mm_shuffle_pd(tmpAdd0, tmpAdd0, 0x01);\n+        rr = _mm_add_pd(tmpAdd0, lo_tmpAdd0);\n+        _mm_store_sd(&o[i], rr);\n+    }\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k5(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rk_3210 = _mm256_loadu_pd(&k[0]);         // K3|K2|K1|K0\n+    __m128d rt_10 = _mm256_castpd256_pd128(rk_3210);  // K1|K0\n+    __m128d rt_32 = _mm_loadu_pd(&k[2]);              // K3|K2\n+    __m128d rt_01 = _mm_permute_pd(rt_10, 0x1);       // K0|K1\n+    __m128d rt_23 = _mm_permute_pd(rt_32, 0x1);       // K2|K3\n+    __m128d rk_4 = _mm_set_sd(k[4]);                  //  0|K4\n+\n+    __m512d rk0 = _mm512_broadcastsd_pd(rt_10);       // K0|K0|K0|K0|K0|K0|K0|K0\n+    __m512d rk1 = _mm512_broadcastsd_pd(rt_01);       // K1|K1|K1|K1|K1|K1|K1|K1\n+    __m512d rk2 = _mm512_broadcastsd_pd(rt_32);       // K2|K2|K2|K2|K2|K2|K2|K2\n+    __m512d rk3 = _mm512_broadcastsd_pd(rt_23);       // K3|K3|K3|K3|K3|K3|K3|K3\n+    __m512d rk4 = _mm512_broadcastsd_pd(rk_4);        // K4|K4|K4|K4|K4|K4|K4|K4\n+\n+\n+    // 8-way processing, no preload\n+    for (i = 0; i < nd - (nd % 8); i += 8) {\n+        __m512d rr = _mm512_setzero_pd();\n+        __m512d rd0 = _mm512_loadu_pd(&d[i]);\n+        __m512d rd1 = _mm512_loadu_pd(&d[i + 1]);\n+        __m512d rd2 = _mm512_loadu_pd(&d[i + 2]);\n+        __m512d rd3 = _mm512_loadu_pd(&d[i + 3]);\n+        __m512d rd4 = _mm512_loadu_pd(&d[i + 4]);\n+        rr = _mm512_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm512_fmadd_pd(rk4, rd4, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+    }\n+\n+    // 4-way processing, no preload\n+    for (; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk0), rd0, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk1), rd1, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk2), rd2, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk3), rd3, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk4), rd4, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m128d rr;\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                    //  D3|D2|D1|D0\n+        __m128d rd_4 = _mm_set_sd(d[i+4]);                           //   0|D4\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m128d rr_3210_lo  = _mm256_castpd256_pd128(rr_3210);\n+        __m128d rr_3210_hi = _mm256_extractf128_pd(rr_3210, 1);\n+        __m128d rr_tmp =   _mm_add_pd(rr_3210_lo, rr_3210_hi);\n+        __m128d rr_tmp1 = _mm_shuffle_pd(rr_tmp, rr_tmp, 0x01);\n+        rr_tmp1 = _mm_add_pd(rr_tmp, rr_tmp1);\n+        rr = _mm_mul_pd(rk_4, rd_4);\n+        rr = _mm_add_pd(rr_tmp1, rr);\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k6(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rk_3210 = _mm256_loadu_pd(&k[0]);         // K3|K2|K1|K0\n+    __m128d rt_10 = _mm256_castpd256_pd128(rk_3210);  // K1|K0\n+    __m128d rt_32 = _mm_loadu_pd(&k[2]);              // K3|K2\n+    __m128d rt_01 = _mm_permute_pd(rt_10, 0x1);       // K0|K1\n+    __m128d rt_23 = _mm_permute_pd(rt_32, 0x1);       // K2|K3\n+    __m128d rt_54 = _mm_loadu_pd(&k[4]);              // K5|K4\n+    __m128d rt_45 = _mm_permute_pd(rt_54, 0x1);       // K4|K5\n+\n+    __m512d rk0 = _mm512_broadcastsd_pd(rt_10);       // K0|K0|K0|K0|K0|K0|K0|K0\n+    __m512d rk1 = _mm512_broadcastsd_pd(rt_01);       // K1|K1|K1|K1|K1|K1|K1|K1\n+    __m512d rk2 = _mm512_broadcastsd_pd(rt_32);       // K2|K2|K2|K2|K2|K2|K2|K2\n+    __m512d rk3 = _mm512_broadcastsd_pd(rt_23);       // K3|K3|K3|K3|K3|K3|K3|K3\n+    __m512d rk4 = _mm512_broadcastsd_pd(rt_54);       // K4|K4|K4|K4|K4|K4|K4|K4\n+    __m512d rk5 = _mm512_broadcastsd_pd(rt_45);       // K5|K5|K5|K5|K5|K5|K5|K5\n+\n+    __m256d rk_0054 = _mm256_insertf128_pd(_mm256_castpd128_pd256(rt_54), _mm_setzero_pd(), 1);  //  0| 0|K5|K4\n+    // Below equivalent is better but will only be support in GCC10+\n+    // https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83250\n+    //__m256d rk_0054 = _mm256_zextpd128_pd256(rt_54); //  0| 0|K5|K4\n+\n+    // 8-way processing, no preload\n+    for (i = 0; i < nd - (nd % 8); i += 8) {\n+        __m512d rr = _mm512_setzero_pd();\n+        __m512d rd0 = _mm512_loadu_pd(&d[i]);\n+        __m512d rd1 = _mm512_loadu_pd(&d[i + 1]);\n+        __m512d rd2 = _mm512_loadu_pd(&d[i + 2]);\n+        __m512d rd3 = _mm512_loadu_pd(&d[i + 3]);\n+        __m512d rd4 = _mm512_loadu_pd(&d[i + 4]);\n+        __m512d rd5 = _mm512_loadu_pd(&d[i + 5]);\n+        rr = _mm512_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm512_fmadd_pd(rk4, rd4, rr);\n+        rr = _mm512_fmadd_pd(rk5, rd5, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+    }\n+\n+    // 4-way processing, no preload\n+    for (; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        __m256d rd5 = _mm256_loadu_pd(&d[i + 5]);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk0), rd0, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk1), rd1, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk2), rd2, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk3), rd3, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk4), rd4, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk5), rd5, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                        // D3|D2|D1|D0\n+        __m256d rd_xx54 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i+4])); // xx|xx|D5|D4\n+\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m256d rr_0054 = _mm256_mul_pd(rk_0054, rd_xx54);\n+        __m256d rr_hadd = _mm256_hadd_pd(rr_3210, rr_0054);\n+        __m128d rr_hi128 = _mm256_extractf128_pd(rr_hadd, 1);\n+        __m128d rr_lo128 = _mm256_castpd256_pd128(rr_hadd);\n+        __m128d rr_tmp1 = _mm_add_pd(rr_lo128, rr_hi128);\n+        __m128d rr_tmp2   = _mm_unpackhi_pd(rr_lo128, rr_lo128);\n+        __m128d rr = _mm_add_pd(rr_tmp1, rr_tmp2);\n+\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k7(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rk_3210 = _mm256_loadu_pd(&k[0]);                 // K3|K2|K1|K0\n+    __m128d rt_10 = _mm256_castpd256_pd128(rk_3210);          // K1|K0\n+    __m128d rt_32 = _mm_loadu_pd(&k[2]);                      // K3|K2\n+    __m128d rt_01 = _mm_permute_pd(rt_10, 0x1);               // K0|K1\n+    __m128d rt_23 = _mm_permute_pd(rt_32, 0x1);               // K2|K3\n+    __m128d rt_54 = _mm_loadu_pd(&k[4]);                      // K5|K4\n+    __m128d rt_45 = _mm_permute_pd(rt_54, 0x1);               // K4|K5\n+    __m128d rt_6 = _mm_set_sd(k[6]);                          //  0|K6\n+\n+    __m512d rk0 = _mm512_broadcastsd_pd(rt_10);               // K0|K0|K0|K0|K0|K0|K0|K0\n+    __m512d rk1 = _mm512_broadcastsd_pd(rt_01);               // K1|K1|K1|K1|K1|K1|K1|K1\n+    __m512d rk2 = _mm512_broadcastsd_pd(rt_32);               // K2|K2|K2|K2|K2|K2|K2|K2\n+    __m512d rk3 = _mm512_broadcastsd_pd(rt_23);               // K3|K3|K3|K3|K3|K3|K3|K3\n+    __m512d rk4 = _mm512_broadcastsd_pd(rt_54);               // K4|K4|K4|K4|K4|K4|K4|K4\n+    __m512d rk5 = _mm512_broadcastsd_pd(rt_45);               // K5|K5|K5|K5|K5|K5|K5|K5\n+    __m512d rk6 = _mm512_broadcastsd_pd(rt_6);                // K6|K6|K6|K6|K6|K6|K6|K6\n+\n+    __m256d rk_0654 = _mm256_insertf128_pd(_mm256_castpd128_pd256(rt_54), rt_6, 1);           //  0|K6|K5|K4\n+\n+    // 8-way processing, no preload\n+    for (i = 0; i < nd - (nd % 8); i += 8) {\n+        __m512d rr = _mm512_setzero_pd();\n+        __m512d rd0 = _mm512_loadu_pd(&d[i]);\n+        __m512d rd1 = _mm512_loadu_pd(&d[i + 1]);\n+        __m512d rd2 = _mm512_loadu_pd(&d[i + 2]);\n+        __m512d rd3 = _mm512_loadu_pd(&d[i + 3]);\n+        __m512d rd4 = _mm512_loadu_pd(&d[i + 4]);\n+        __m512d rd5 = _mm512_loadu_pd(&d[i + 5]);\n+        __m512d rd6 = _mm512_loadu_pd(&d[i + 6]);\n+        rr = _mm512_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm512_fmadd_pd(rk4, rd4, rr);\n+        rr = _mm512_fmadd_pd(rk5, rd5, rr);\n+        rr = _mm512_fmadd_pd(rk6, rd6, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+    }\n+\n+    // 4-way processing, no preload\n+    for (; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        __m256d rd5 = _mm256_loadu_pd(&d[i + 5]);\n+        __m256d rd6 = _mm256_loadu_pd(&d[i + 6]);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk0), rd0, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk1), rd1, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk2), rd2, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk3), rd3, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk4), rd4, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk5), rd5, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk6), rd6, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                                  // D3|D2|D1|D0\n+        __m256d rd_xx54 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i+4]));           // xx|xx|D5|D4\n+        __m256d rd_0654 = _mm256_insertf128_pd(rd_xx54, _mm_set_sd(d[i+6]), 1);    //  0|D6|D5|D4\n+\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m256d rr_0654 = _mm256_mul_pd(rk_0654, rd_0654);\n+        __m256d rr_hadd = _mm256_hadd_pd(rr_3210, rr_0654);\n+        __m128d rr_hi128 = _mm256_extractf128_pd(rr_hadd, 1);\n+        __m128d rr_lo128 = _mm256_castpd256_pd128(rr_hadd);\n+        __m128d rr_tmp1 = _mm_add_pd(rr_lo128, rr_hi128);\n+        __m128d rr_tmp2 = _mm_unpackhi_pd(rr_tmp1, rr_tmp1);\n+        __m128d rr = _mm_add_pd(rr_tmp1, rr_tmp2);\n+\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k8(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rk_3210 = _mm256_loadu_pd(&k[0]);                 // K3|K2|K1|K0\n+    __m128d rt_10 = _mm256_castpd256_pd128(rk_3210);          // K1|K0\n+    __m128d rt_32 = _mm_loadu_pd(&k[2]);                      // K3|K2\n+    __m128d rt_01 = _mm_permute_pd(rt_10, 0x1);               // K0|K1\n+    __m128d rt_23 = _mm_permute_pd(rt_32, 0x1);               // K2|K3\n+    __m256d rk_7654 = _mm256_loadu_pd(&k[4]);                 // K7|K6|K5|K4\n+    __m128d rt_54 = _mm256_castpd256_pd128(rk_7654);          // K5|K4\n+    __m128d rt_76 = _mm_loadu_pd(&k[6]);                      // K7|K6\n+    __m128d rt_45 = _mm_permute_pd(rt_54, 0x1);               // K4|K5\n+    __m128d rt_67 = _mm_permute_pd(rt_76, 0x1);               // K6|K7\n+\n+    __m512d rk0 = _mm512_broadcastsd_pd(rt_10);               // K0|K0|K0|K0|K0|K0|K0|K0\n+    __m512d rk1 = _mm512_broadcastsd_pd(rt_01);               // K1|K1|K1|K1|K1|K1|K1|K1\n+    __m512d rk2 = _mm512_broadcastsd_pd(rt_32);               // K2|K2|K2|K2|K2|K2|K2|K2\n+    __m512d rk3 = _mm512_broadcastsd_pd(rt_23);               // K3|K3|K3|K3|K3|K3|K3|K3\n+    __m512d rk4 = _mm512_broadcastsd_pd(rt_54);               // K4|K4|K4|K4|K4|K4|K4|K4\n+    __m512d rk5 = _mm512_broadcastsd_pd(rt_45);               // K5|K5|K5|K5|K5|K5|K5|K5\n+    __m512d rk6 = _mm512_broadcastsd_pd(rt_76);               // K6|K6|K6|K6|K6|K6|K6|K6\n+    __m512d rk7 = _mm512_broadcastsd_pd(rt_67);               // K7|K7|K7|K7|K7|K7|K7|K7\n+\n+    // 8-way processing, no preload\n+    for (i = 0; i < nd - (nd % 8); i += 8) {\n+        __m512d rr = _mm512_setzero_pd();\n+        __m512d rd0 = _mm512_loadu_pd(&d[i]);\n+        __m512d rd1 = _mm512_loadu_pd(&d[i + 1]);\n+        __m512d rd2 = _mm512_loadu_pd(&d[i + 2]);\n+        __m512d rd3 = _mm512_loadu_pd(&d[i + 3]);\n+        __m512d rd4 = _mm512_loadu_pd(&d[i + 4]);\n+        __m512d rd5 = _mm512_loadu_pd(&d[i + 5]);\n+        __m512d rd6 = _mm512_loadu_pd(&d[i + 6]);\n+        __m512d rd7 = _mm512_loadu_pd(&d[i + 7]);\n+        rr = _mm512_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm512_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm512_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm512_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm512_fmadd_pd(rk4, rd4, rr);\n+        rr = _mm512_fmadd_pd(rk5, rd5, rr);\n+        rr = _mm512_fmadd_pd(rk6, rd6, rr);\n+        rr = _mm512_fmadd_pd(rk7, rd7, rr);\n+        _mm512_storeu_pd(&o[i], rr);\n+    }\n+\n+    // 4-way processing, no preload\n+    for (; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        __m256d rd5 = _mm256_loadu_pd(&d[i + 5]);\n+        __m256d rd6 = _mm256_loadu_pd(&d[i + 6]);\n+        __m256d rd7 = _mm256_loadu_pd(&d[i + 7]);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk0), rd0, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk1), rd1, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk2), rd2, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk3), rd3, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk4), rd4, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk5), rd5, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk6), rd6, rr);\n+        rr = _mm256_fmadd_pd(_mm512_castpd512_pd256(rk7), rd7, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                       // D3|D2|D1|D0\n+        __m256d rd_7654 = _mm256_loadu_pd(&d[i+4]);                     // D7|D6|D5|D4\n+\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m256d rr_7654 = _mm256_mul_pd(rk_7654, rd_7654);\n+        __m256d rr_hadd = _mm256_hadd_pd(rr_3210, rr_7654);\n+        __m128d rr_hi128 = _mm256_extractf128_pd(rr_hadd, 1);\n+        __m128d rr_lo128 = _mm256_castpd256_pd128(rr_hadd);\n+        __m128d rr_tmp1 = _mm_add_pd(rr_lo128, rr_hi128);\n+        __m128d rr_tmp2 = _mm_unpackhi_pd(rr_tmp1, rr_tmp1);\n+        __m128d rr = _mm_add_pd(rr_tmp1, rr_tmp2);\n+\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+#elif defined(__AVX2__) && defined(__FMA__)\n+static int\n+_sc_vectorized_k3(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m128d rt_10 = _mm_loadu_pd(&k[0]);                     // K1|K0\n+    __m128d rt_01 = _mm_permute_pd(rt_10, 0x1);              // K0|K1\n+    __m128d rt_02 = _mm_set_sd(k[2]);                        //  0|K2\n+\n+    __m256d rk0 = _mm256_broadcastsd_pd(rt_10);              // K0|K0|K0|K0\n+    __m256d rk1 = _mm256_broadcastsd_pd(rt_01);              // K1|K1|K1|K1\n+    __m256d rk2 = _mm256_broadcastsd_pd(rt_02);              // K2|K2|K2|K2\n+\n+    __m256d rk_special = _mm256_insertf128_pd(_mm256_castpd128_pd256(rt_01), rt_02, 1);  // 0|K2|K0|K1\n+    rk_special = _mm256_permute4x64_pd(rk_special, 0x87);    // K2|K1|K0|0\n+\n+    // 4-way processing, no preload\n+    for (i = 0; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m128d rr;\n+        __m256d rd0 = _mm256_loadu_pd(&d[i-1]);              // rd0 is set to be PADD|PADD|D(i)|D(i-1)\n+        __m256d rr_210 = _mm256_mul_pd(rk_special, rd0);\n+        __m128d rr_210_lo = _mm256_castpd256_pd128(rr_210);\n+        __m128d rr_210_hi = _mm256_extractf128_pd(rr_210, 1);\n+        __m128d rr_tmp =   _mm_add_pd(rr_210_lo, rr_210_hi);\n+        __m128d rr_tmp1 = _mm_shuffle_pd(rr_tmp, rr_tmp, 0x01);\n+        rr = _mm_add_pd(rr_tmp, rr_tmp1);\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k4(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+\n+    __m256d rt0 = _mm256_broadcast_pd((__m128d*)&k[0]);      // K1|K0|K1|K0\n+    __m256d rt1 = _mm256_broadcast_pd((__m128d*)&k[2]);      // K3|K2|K3|K2\n+\n+    __m256d rk0 = _mm256_permute_pd(rt0, 0x0);               // K0|K0|K0|K0\n+    __m256d rk1 = _mm256_permute_pd(rt0, 0xf);               // K1|K1|K1|K1\n+    __m256d rk2 = _mm256_permute_pd(rt1, 0x0);               // K2|K2|K2|K2\n+    __m256d rk3 = _mm256_permute_pd(rt1, 0xf);               // K3|K3|K3|K3\n+\n+    __m256d rk = _mm256_permute2f128_pd(rt0, rt1, 0x30);     // K3|K2|K1|K0\n+\n+    // 4-way processing, no preload\n+    for (i = 0; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm256_fmadd_pd(rk3, rd3, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    // 2-way processing if remaining lines > 1\n+    if (nd % 4 > 1) {\n+        __m128d rr;\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d xy0 = _mm256_mul_pd(rk, rd0);\n+        __m256d xy1 = _mm256_mul_pd(rk, rd1);\n+        __m256d tmpAdd0 = _mm256_hadd_pd(xy0, xy1);\n+        __m128d hi_tmpAdd0 = _mm256_extractf128_pd(tmpAdd0, 1);\n+        __m128d lo_tmpAdd0  = _mm256_castpd256_pd128(tmpAdd0);\n+        rr = _mm_add_pd(hi_tmpAdd0, lo_tmpAdd0);\n+        _mm_storeu_pd(&o[i], rr);\n+    }\n+\n+    // Processing the remaining 1 data line\n+    if (nd % 2 == 1) {\n+        i = nd - 1;\n+        __m128d rr;\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d xy = _mm256_mul_pd(rk, rd0);\n+        __m128d lo_xy  = _mm256_castpd256_pd128(xy);\n+        __m128d hi_xy = _mm256_extractf128_pd(xy, 1);\n+        __m128d tmpAdd0 =   _mm_add_pd(lo_xy, hi_xy);\n+        __m128d lo_tmpAdd0 = _mm_shuffle_pd(tmpAdd0, tmpAdd0, 0x01);\n+        rr = _mm_add_pd(tmpAdd0, lo_tmpAdd0);\n+        _mm_store_sd(&o[i], rr);\n+    }\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k5(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rt0 = _mm256_broadcast_pd((__m128d*)&k[0]);                // K1|K0|K1|K0\n+    __m256d rt1 = _mm256_broadcast_pd((__m128d*)&k[2]);                // K3|K2|K3|K2\n+    __m128d rk_4 = _mm_set_sd(k[4]);                                   //  0|K4\n+\n+    __m256d rk0 = _mm256_permute_pd(rt0, 0x0);                         // K0|K0|K0|K0\n+    __m256d rk1 = _mm256_permute_pd(rt0, 0xf);                         // K1|K1|K1|K1\n+    __m256d rk2 = _mm256_permute_pd(rt1, 0x0);                         // K2|K2|K2|K2\n+    __m256d rk3 = _mm256_permute_pd(rt1, 0xf);                         // K3|K3|K3|K3\n+    __m256d rk4 = _mm256_broadcastsd_pd(rk_4);                         // K4|K4|K4|K4\n+\n+    __m256d rk_3210 = _mm256_permute2f128_pd(rt0, rt1, 0x30);          // K3|K2|K1|K0\n+\n+    // 4-way processing, no preload\n+    for (i = 0; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm256_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm256_fmadd_pd(rk4, rd4, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m128d rr;\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                      //  D3|D2|D1|D0\n+        __m128d rd_4 = _mm_set_sd(d[i+4]);                             //   0|D4\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m128d rr_3210_lo  = _mm256_castpd256_pd128(rr_3210);\n+        __m128d rr_3210_hi = _mm256_extractf128_pd(rr_3210, 1);\n+        __m128d rr_tmp =   _mm_add_pd(rr_3210_lo, rr_3210_hi);\n+        __m128d rr_tmp1 = _mm_shuffle_pd(rr_tmp, rr_tmp, 0x01);\n+        rr_tmp1 = _mm_add_pd(rr_tmp, rr_tmp1);\n+        rr = _mm_mul_pd(rk_4, rd_4);\n+        rr = _mm_add_pd(rr_tmp1, rr);\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k6(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rt_1010 = _mm256_broadcast_pd((__m128d*)&k[0]);            // K1|K0|K1|K0\n+    __m256d rt_3232 = _mm256_broadcast_pd((__m128d*)&k[2]);            // K3|K2|K3|K2\n+    __m256d rt_5454 = _mm256_broadcast_pd((__m128d*)&k[4]);            // K5|K4|K5|K4\n+\n+    __m256d rk0 = _mm256_permute_pd(rt_1010, 0x0);                     // K0|K0|K0|K0\n+    __m256d rk1 = _mm256_permute_pd(rt_1010, 0xf);                     // K1|K1|K1|K1\n+    __m256d rk2 = _mm256_permute_pd(rt_3232, 0x0);                     // K2|K2|K2|K2\n+    __m256d rk3 = _mm256_permute_pd(rt_3232, 0xf);                     // K3|K3|K3|K3\n+    __m256d rk4 = _mm256_permute_pd(rt_5454, 0x0);                     // K4|K4|K4|K4\n+    __m256d rk5 = _mm256_permute_pd(rt_5454, 0xf);                     // K5|K5|K5|K5\n+\n+    __m256d rk_3210 = _mm256_permute2f128_pd(rt_1010, rt_3232, 0x30);  // K3|K2|K1|K0\n+\n+    __m256d rk_0054 = _mm256_insertf128_pd(_mm256_castpd128_pd256(_mm_loadu_pd(&k[4])), _mm_setzero_pd(), 1);  //  0| 0|K5|K4\n+    // Below equivalent is better but will only be support in GCC10+\n+    // https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83250\n+    //__m256d rk_0054 = _mm256_zextpd128_pd256(_mm_loadu_pd(&k[4]));   //  0| 0|K5|K4\n+\n+    // 4-way processing, no preload\n+    for (i = 0; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        __m256d rd5 = _mm256_loadu_pd(&d[i + 5]);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm256_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm256_fmadd_pd(rk4, rd4, rr);\n+        rr = _mm256_fmadd_pd(rk5, rd5, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                        // D3|D2|D1|D0\n+        __m256d rd_xx54 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i+4])); // xx|xx|D5|D4\n+\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m256d rr_0054 = _mm256_mul_pd(rk_0054, rd_xx54);\n+        __m256d rr_hadd = _mm256_hadd_pd(rr_3210, rr_0054);\n+        __m128d rr_hi128 = _mm256_extractf128_pd(rr_hadd, 1);\n+        __m128d rr_lo128 = _mm256_castpd256_pd128(rr_hadd);\n+        __m128d rr_tmp1 = _mm_add_pd(rr_lo128, rr_hi128);\n+        __m128d rr_tmp2   = _mm_unpackhi_pd(rr_lo128, rr_lo128);\n+        __m128d rr = _mm_add_pd(rr_tmp1, rr_tmp2);\n+\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k7(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rt_1010 = _mm256_broadcast_pd((__m128d*)&k[0]);             // K1|K0|K1|K0\n+    __m256d rt_3232 = _mm256_broadcast_pd((__m128d*)&k[2]);             // K3|K2|K3|K2\n+    __m256d rt_5454 = _mm256_broadcast_pd((__m128d*)&k[4]);             // K5|K4|K5|K4\n+    __m128d rt_6 = _mm_set_sd(k[6]);                                    //  0|K6\n+\n+    __m256d rk0 = _mm256_permute_pd(rt_1010, 0x0);                      // K0|K0|K0|K0\n+    __m256d rk1 = _mm256_permute_pd(rt_1010, 0xf);                      // K1|K1|K1|K1\n+    __m256d rk2 = _mm256_permute_pd(rt_3232, 0x0);                      // K2|K2|K2|K2\n+    __m256d rk3 = _mm256_permute_pd(rt_3232, 0xf);                      // K3|K3|K3|K3\n+    __m256d rk4 = _mm256_permute_pd(rt_5454, 0x0);                      // K4|K4|K4|K4\n+    __m256d rk5 = _mm256_permute_pd(rt_5454, 0xf);                      // K5|K5|K5|K5\n+    __m256d rk6 = _mm256_broadcastsd_pd(rt_6);                          // K6|K6|K6|K6\n+\n+    __m256d rk_3210 = _mm256_permute2f128_pd(rt_1010, rt_3232, 0x30);   // K3|K2|K1|K0\n+    __m256d rk_0654 = _mm256_insertf128_pd(rt_5454, rt_6, 1);           //  0|K6|K5|K4\n+\n+    // 4-way processing, no preload\n+    for (i = 0; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        __m256d rd5 = _mm256_loadu_pd(&d[i + 5]);\n+        __m256d rd6 = _mm256_loadu_pd(&d[i + 6]);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm256_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm256_fmadd_pd(rk4, rd4, rr);\n+        rr = _mm256_fmadd_pd(rk5, rd5, rr);\n+        rr = _mm256_fmadd_pd(rk6, rd6, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                                  // D3|D2|D1|D0\n+        __m256d rd_xx54 = _mm256_castpd128_pd256(_mm_loadu_pd(&d[i+4]));           // xx|xx|D5|D4\n+        __m256d rd_0654 = _mm256_insertf128_pd(rd_xx54, _mm_set_sd(d[i+6]), 1);    //  0|D6|D5|D4\n+\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m256d rr_0654 = _mm256_mul_pd(rk_0654, rd_0654);\n+        __m256d rr_hadd = _mm256_hadd_pd(rr_3210, rr_0654);\n+        __m128d rr_hi128 = _mm256_extractf128_pd(rr_hadd, 1);\n+        __m128d rr_lo128 = _mm256_castpd256_pd128(rr_hadd);\n+        __m128d rr_tmp1 = _mm_add_pd(rr_lo128, rr_hi128);\n+        __m128d rr_tmp2 = _mm_unpackhi_pd(rr_tmp1, rr_tmp1);\n+        __m128d rr = _mm_add_pd(rr_tmp1, rr_tmp2);\n+\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+\n+static int\n+_sc_vectorized_k8(double * d, double *k, double *o, npy_uintp nd)\n+{\n+    npy_intp i;\n+    __m256d rt_1010 = _mm256_broadcast_pd((__m128d*)&k[0]);             // K1|K0|K1|K0\n+    __m256d rt_3232 = _mm256_broadcast_pd((__m128d*)&k[2]);             // K3|K2|K3|K2\n+    __m256d rt_5454 = _mm256_broadcast_pd((__m128d*)&k[4]);             // K5|K4|K5|K4\n+    __m256d rt_7676 = _mm256_broadcast_pd((__m128d*)&k[6]);             // K7|K6|K7|K6\n+\n+    __m256d rk0 = _mm256_permute_pd(rt_1010, 0x0);                      // K0|K0|K0|K0\n+    __m256d rk1 = _mm256_permute_pd(rt_1010, 0xf);                      // K1|K1|K1|K1\n+    __m256d rk2 = _mm256_permute_pd(rt_3232, 0x0);                      // K2|K2|K2|K2\n+    __m256d rk3 = _mm256_permute_pd(rt_3232, 0xf);                      // K3|K3|K3|K3\n+    __m256d rk4 = _mm256_permute_pd(rt_5454, 0x0);                      // K4|K4|K4|K4\n+    __m256d rk5 = _mm256_permute_pd(rt_5454, 0xf);                      // K5|K5|K5|K5\n+    __m256d rk6 = _mm256_permute_pd(rt_7676, 0x0);                      // K6|K6|K6|K6\n+    __m256d rk7 = _mm256_permute_pd(rt_7676, 0xf);                      // K7|K7|K7|K7\n+\n+    __m256d rk_3210 = _mm256_permute2f128_pd(rt_1010, rt_3232, 0x30);   // K3|K2|K1|K0\n+    __m256d rk_7654 = _mm256_permute2f128_pd(rt_5454, rt_7676, 0x30);   // K7|K6|K5|K4\n+\n+    // 4-way processing, no preload\n+    for (i = 0; i < nd - (nd % 4); i += 4) {\n+        __m256d rr = _mm256_setzero_pd();\n+        __m256d rd0 = _mm256_loadu_pd(&d[i]);\n+        __m256d rd1 = _mm256_loadu_pd(&d[i + 1]);\n+        __m256d rd2 = _mm256_loadu_pd(&d[i + 2]);\n+        __m256d rd3 = _mm256_loadu_pd(&d[i + 3]);\n+        __m256d rd4 = _mm256_loadu_pd(&d[i + 4]);\n+        __m256d rd5 = _mm256_loadu_pd(&d[i + 5]);\n+        __m256d rd6 = _mm256_loadu_pd(&d[i + 6]);\n+        __m256d rd7 = _mm256_loadu_pd(&d[i + 7]);\n+        rr = _mm256_fmadd_pd(rk0, rd0, rr);\n+        rr = _mm256_fmadd_pd(rk1, rd1, rr);\n+        rr = _mm256_fmadd_pd(rk2, rd2, rr);\n+        rr = _mm256_fmadd_pd(rk3, rd3, rr);\n+        rr = _mm256_fmadd_pd(rk4, rd4, rr);\n+        rr = _mm256_fmadd_pd(rk5, rd5, rr);\n+        rr = _mm256_fmadd_pd(rk6, rd6, rr);\n+        rr = _mm256_fmadd_pd(rk7, rd7, rr);\n+        _mm256_storeu_pd(&o[i], rr);\n+    }\n+\n+    for (; i < nd; i++) {\n+        __m256d rd_3210 = _mm256_loadu_pd(&d[i]);                       // D3|D2|D1|D0\n+        __m256d rd_7654 = _mm256_loadu_pd(&d[i+4]);                     // D7|D6|D5|D4\n+\n+        __m256d rr_3210 = _mm256_mul_pd(rk_3210, rd_3210);\n+        __m256d rr_7654 = _mm256_mul_pd(rk_7654, rd_7654);\n+        __m256d rr_hadd = _mm256_hadd_pd(rr_3210, rr_7654);\n+        __m128d rr_hi128 = _mm256_extractf128_pd(rr_hadd, 1);\n+        __m128d rr_lo128 = _mm256_castpd256_pd128(rr_hadd);\n+        __m128d rr_tmp1 = _mm_add_pd(rr_lo128, rr_hi128);\n+        __m128d rr_tmp2 = _mm_unpackhi_pd(rr_tmp1, rr_tmp1);\n+        __m128d rr = _mm_add_pd(rr_tmp1, rr_tmp2);\n+\n+        _mm_store_sd(&o[i], rr);\n+    }\n+\n+    return 1;\n+}\n+#endif\n+\n /*\n  * Compute correlation of data with with small kernels\n  * Calling a BLAS dot product for the inner loop of the correlation is overkill\n@@ -4006,6 +4880,24 @@ small_correlate(const char * d_, npy_intp dstride,\n         return 0;\n     }\n \n+#if defined(__AVX512F__) || (defined(__AVX2__) && defined(__FMA__))\n+    if (nd >=2 && dtype==NPY_DOUBLE && dstride==sizeof(double) && kstride==sizeof(double) && ostride==sizeof(double)) {\n+        if (nk == 3) {\n+            return _sc_vectorized_k3((double*)d_, (double*)k_, (double*)out_, (npy_uintp)nd);\n+        } else if (nk == 4) {\n+            return _sc_vectorized_k4((double*)d_, (double*)k_, (double*)out_, (npy_uintp)nd);\n+        } else if (nk == 5) {\n+            return _sc_vectorized_k5((double*)d_, (double*)k_, (double*)out_, (npy_uintp)nd);\n+\t} else if (nk == 6) {\n+            return _sc_vectorized_k6((double*)d_, (double*)k_, (double*)out_, (npy_uintp)nd);\n+\t} else if (nk == 7) {\n+            return _sc_vectorized_k7((double*)d_, (double*)k_, (double*)out_, (npy_uintp)nd);\n+\t} else if (nk == 8) {\n+            return _sc_vectorized_k8((double*)d_, (double*)k_, (double*)out_, (npy_uintp)nd);\n+        }\n+    }\n+#endif\n+\n     switch (dtype) {\n /**begin repeat\n  * Float types"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 8777,
        "body": "I have changed the NCACHE_DIM from 15 to 7 in order to make the behavior consistent with the older version. This needs to be discussed, I will suggest to change NBUCKETS_DIM from 16 to 8 and NCACHE_DIM from 7 to  15. \r\n\r\nMoved from #8755\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/alloc.c",
                "patch": "@@ -9,101 +9,63 @@\n #include <numpy/npy_common.h>\n #include \"npy_config.h\"\n \n-#include <assert.h>\n-\n-#define NBUCKETS 1024 /* number of buckets for data*/\n+#define NBUCKETS_DATA 1024 /* number of buckets for data*/\n #define NBUCKETS_DIM 16 /* number of buckets for dimensions/strides */\n-#define NCACHE 7 /* number of cache entries per bucket */\n+#define NCACHE_DATA 7 /* number of cache entries per data bucket */\n+#define NCACHE_DIM 7 /* number of cache entries per dim bucket */\n+\n /* this structure fits neatly into a cacheline */\n typedef struct {\n     npy_uintp available; /* number of cached pointers */\n-    void * ptrs[NCACHE];\n-} cache_bucket;\n-static cache_bucket datacache[NBUCKETS];\n-static cache_bucket dimcache[NBUCKETS_DIM];\n+    void * ptrs[NCACHE_DATA];\n+} data_cache_bucket;\n \n-/*\n- * very simplistic small memory block cache to avoid more expensive libc\n- * allocations\n- * base function for data cache with 1 byte buckets and dimension cache with\n- * sizeof(npy_intp) byte buckets\n- */\n-static NPY_INLINE void *\n-_npy_alloc_cache(npy_uintp nelem, npy_uintp esz, npy_uint msz,\n-                 cache_bucket * cache, void * (*alloc)(size_t))\n-{\n-    assert((esz == 1 && cache == datacache) ||\n-           (esz == sizeof(npy_intp) && cache == dimcache));\n-    if (nelem < msz) {\n-        if (cache[nelem].available > 0) {\n-            return cache[nelem].ptrs[--(cache[nelem].available)];\n-        }\n-    }\n-#ifdef _PyPyGC_AddMemoryPressure\n-    {\n-        size_t size = nelem * esz;\n-        void * ret = alloc(size);\n-        if (ret != NULL)\n-        {\n-            _PyPyPyGC_AddMemoryPressure(size);\n-        }\n-        return ret;\n-    }\n-#else\n-     return alloc(nelem * esz);\n-#endif\n-}\n-\n-/*\n- * return pointer p to cache, nelem is number of elements of the cache bucket\n- * size (1 or sizeof(npy_intp)) of the block pointed too\n- */\n-static NPY_INLINE void\n-_npy_free_cache(void * p, npy_uintp nelem, npy_uint msz,\n-                cache_bucket * cache, void (*dealloc)(void *))\n-{\n-    if (p != NULL && nelem < msz) {\n-        if (cache[nelem].available < NCACHE) {\n-            cache[nelem].ptrs[cache[nelem].available++] = p;\n-            return;\n-        }\n-    }\n-    dealloc(p);\n-}\n+typedef struct {\n+    npy_uintp available; /* number of cached pointers */\n+    void * ptrs[NCACHE_DIM];\n+} dim_cache_bucket;\n \n+static data_cache_bucket datacache[NBUCKETS_DATA];\n+static dim_cache_bucket dimcache[NBUCKETS_DIM];\n \n /*\n  * array data cache, sz is number of bytes to allocate\n  */\n NPY_NO_EXPORT void *\n npy_alloc_cache(npy_uintp sz)\n {\n-    return _npy_alloc_cache(sz, 1, NBUCKETS, datacache, &PyDataMem_NEW);\n+    if (sz > 0 && sz-1 < NBUCKETS_DATA && datacache[sz-1].available > 0) {\n+        return datacache[sz-1].ptrs[--(datacache[sz-1].available)];\n+    }  \n+    return PyDataMem_NEW(sz);\n }\n \n-/* zero initialized data, sz is number of bytes to allocate */\n+/*\n+ * zero initialized data, sz is number of bytes to allocate\n+ */\n NPY_NO_EXPORT void *\n npy_alloc_cache_zero(npy_uintp sz)\n {\n-    void * p;\n-    NPY_BEGIN_THREADS_DEF;\n-    if (sz < NBUCKETS) {\n-        p = _npy_alloc_cache(sz, 1, NBUCKETS, datacache, &PyDataMem_NEW);\n-        if (p) {\n-            memset(p, 0, sz);\n-        }\n+    void * p;    \n+    if (sz > 0 && sz-1 < NBUCKETS_DATA && datacache[sz-1].available > 0) {\n+        p = datacache[sz-1].ptrs[--(datacache[sz-1].available)];\n+        memset(p, 0, sz);\n         return p;\n     }\n-    NPY_BEGIN_THREADS;\n+    \n     p = PyDataMem_NEW_ZEROED(sz, 1);\n-    NPY_END_THREADS;\n     return p;\n }\n \n NPY_NO_EXPORT void\n npy_free_cache(void * p, npy_uintp sz)\n {\n-    _npy_free_cache(p, sz, NBUCKETS, datacache, &PyDataMem_FREE);\n+    if (p != NULL && sz > 0 && sz-1 < NBUCKETS_DATA &&\n+            datacache[sz-1].available < NCACHE_DATA) {\n+        datacache[sz-1].ptrs[datacache[sz-1].available++] = p;\n+        return ;\n+    }\n+    PyDataMem_FREE(p);\n }\n \n /*\n@@ -113,12 +75,18 @@ npy_free_cache(void * p, npy_uintp sz)\n NPY_NO_EXPORT void *\n npy_alloc_cache_dim(npy_uintp sz)\n {\n+    void * p;\n     /* dims + strides */\n     if (NPY_UNLIKELY(sz < 2)) {\n         sz = 2;\n     }\n-    return _npy_alloc_cache(sz, sizeof(npy_intp), NBUCKETS_DIM, dimcache,\n-                            &PyArray_malloc);\n+\n+    if (sz-2 < NBUCKETS_DIM && dimcache[sz-2].available > 0) {\n+        return dimcache[sz-2].ptrs[--(dimcache[sz-2].available)];\n+    }\n+    /* type of dimension elements is npy_intp */\n+    p = PyArray_malloc(sz * sizeof(npy_intp));\n+    return p;\n }\n \n NPY_NO_EXPORT void\n@@ -128,8 +96,13 @@ npy_free_cache_dim(void * p, npy_uintp sz)\n     if (NPY_UNLIKELY(sz < 2)) {\n         sz = 2;\n     }\n-    _npy_free_cache(p, sz, NBUCKETS_DIM, dimcache,\n-                    &PyArray_free);\n+    \n+    if (p != NULL && sz-2 < NBUCKETS_DIM &&\n+            dimcache[sz-2].available < NCACHE_DIM) {\n+        dimcache[sz-2].ptrs[dimcache[sz-2].available++] = p;\n+        return ;\n+    }\n+    PyArray_free(p);\n }\n \n \n@@ -181,7 +154,7 @@ NPY_NO_EXPORT void *\n PyDataMem_NEW(size_t size)\n {\n     void *result;\n-\n+  \n     result = malloc(size);\n     if (_PyDataMem_eventhook != NULL) {\n         NPY_ALLOW_C_API_DEF\n@@ -192,6 +165,13 @@ PyDataMem_NEW(size_t size)\n         }\n         NPY_DISABLE_C_API\n     }\n+\n+#ifdef _PyPyGC_AddMemoryPressure\n+    if (result != NULL){\n+        _PyPyPyGC_AddMemoryPressure(size);\n+    }\n+#endif\n+  \n     return result;\n }\n \n@@ -213,6 +193,13 @@ PyDataMem_NEW_ZEROED(size_t size, size_t elsize)\n         }\n         NPY_DISABLE_C_API\n     }\n+  \n+#ifdef _PyPyGC_AddMemoryPressure\n+    if (result != NULL){\n+        _PyPyPyGC_AddMemoryPressure(size);\n+    }\n+#endif\n+  \n     return result;\n }\n \n@@ -252,5 +239,11 @@ PyDataMem_RENEW(void *ptr, size_t size)\n         }\n         NPY_DISABLE_C_API\n     }\n+  \n+#ifdef _PyPyGC_AddMemoryPressure\n+    if (result != NULL){\n+        _PyPyPyGC_AddMemoryPressure(size);\n+    }\n+#endif\n     return result;\n }"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 12065,
        "body": "This commit adds some simple functionality to np.isin and np.in1d that greatly increases performance when both arrays are integral. It works by creating a boolean array with elements set to 1 where the parent array (ar2) has elements and 0 otherwise. This array is then indexed by the child array (ar1) to create the output.",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_lib.py",
                "patch": "@@ -137,3 +137,22 @@ def setup(self, array_size, percent_nans):\n \n     def time_unique(self, array_size, percent_nans):\n         np.unique(self.arr)\n+\n+\n+class Isin(Benchmark):\n+    \"\"\"Benchmarks for `numpy.isin`.\"\"\"\n+\n+    param_names = [\"size\", \"highest_element\"]\n+    params = [\n+        [10, 100000, 3000000],\n+        [10, 10000, int(1e8)]\n+    ]\n+\n+    def setup(self, size, highest_element):\n+        self.array = np.random.randint(\n+                low=0, high=highest_element, size=size)\n+        self.in_array = np.random.randint(\n+                low=0, high=highest_element, size=size)\n+\n+    def time_isin(self, size, highest_element):\n+        np.isin(self.array, self.in_array)"
            },
            {
                "filename": "doc/release/upcoming_changes/12065.performance.rst",
                "patch": "@@ -0,0 +1,6 @@\n+Faster version of ``np.isin`` and ``np.in1d`` for integer arrays\n+----------------------------------------------------------------\n+``np.in1d`` (used by ``np.isin``) can now switch to a faster algorithm\n+(up to >10x faster) when it is passed two integer arrays.\n+This is often automatically used, but you can use ``kind=\"sort\"`` or \n+``kind=\"table\"`` to force the old or new method, respectively.\n\\ No newline at end of file"
            },
            {
                "filename": "numpy/lib/arraysetops.py",
                "patch": "@@ -516,12 +516,13 @@ def setxor1d(ar1, ar2, assume_unique=False):\n     return aux[flag[1:] & flag[:-1]]\n \n \n-def _in1d_dispatcher(ar1, ar2, assume_unique=None, invert=None):\n+def _in1d_dispatcher(ar1, ar2, assume_unique=None, invert=None, *,\n+                     kind=None):\n     return (ar1, ar2)\n \n \n @array_function_dispatch(_in1d_dispatcher)\n-def in1d(ar1, ar2, assume_unique=False, invert=False):\n+def in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n     \"\"\"\n     Test whether each element of a 1-D array is also present in a second array.\n \n@@ -544,6 +545,26 @@ def in1d(ar1, ar2, assume_unique=False, invert=False):\n         False where an element of `ar1` is in `ar2` and True otherwise).\n         Default is False. ``np.in1d(a, b, invert=True)`` is equivalent\n         to (but is faster than) ``np.invert(in1d(a, b))``.\n+    kind : {None, 'sort', 'table'}, optional\n+        The algorithm to use. This will not affect the final result,\n+        but will affect the speed and memory use. The default, None,\n+        will select automatically based on memory considerations.\n+\n+        * If 'sort', will use a mergesort-based approach. This will have\n+          a memory usage of roughly 6 times the sum of the sizes of\n+          `ar1` and `ar2`, not accounting for size of dtypes.\n+        * If 'table', will use a lookup table approach similar\n+          to a counting sort. This is only available for boolean and\n+          integer arrays. This will have a memory usage of the\n+          size of `ar1` plus the max-min value of `ar2`. `assume_unique`\n+          has no effect when the 'table' option is used.\n+        * If None, will automatically choose 'table' if\n+          the required memory allocation is less than or equal to\n+          6 times the sum of the sizes of `ar1` and `ar2`,\n+          otherwise will use 'sort'. This is done to not use\n+          a large amount of memory by default, even though\n+          'table' may be faster in most cases. If 'table' is chosen,\n+          `assume_unique` will have no effect.\n \n         .. versionadded:: 1.8.0\n \n@@ -569,6 +590,13 @@ def in1d(ar1, ar2, assume_unique=False, invert=False):\n     ``asarray(ar2)`` is an object array rather than the expected array of\n     contained values.\n \n+    Using ``kind='table'`` tends to be faster than `kind='sort'` if the\n+    following relationship is true:\n+    ``log10(len(ar2)) > (log10(max(ar2)-min(ar2)) - 2.27) / 0.927``,\n+    but may use greater memory. The default value for `kind` will\n+    be automatically selected based only on memory usage, so one may\n+    manually set ``kind='table'`` if memory constraints can be relaxed.\n+\n     .. versionadded:: 1.4.0\n \n     Examples\n@@ -593,6 +621,76 @@ def in1d(ar1, ar2, assume_unique=False, invert=False):\n     # Ensure that iteration through object arrays yields size-1 arrays\n     if ar2.dtype == object:\n         ar2 = ar2.reshape(-1, 1)\n+    # Convert booleans to uint8 so we can use the fast integer algorithm\n+    if ar1.dtype == bool:\n+        ar1 = ar1 + np.uint8(0)\n+    if ar2.dtype == bool:\n+        ar2 = ar2 + np.uint8(0)\n+\n+    # Check if we can use a fast integer algorithm:\n+    integer_arrays = (np.issubdtype(ar1.dtype, np.integer) and\n+                      np.issubdtype(ar2.dtype, np.integer))\n+\n+    if kind not in {None, 'sort', 'table'}:\n+        raise ValueError(\n+            f\"Invalid kind: '{kind}'. Please use None, 'sort' or 'table'.\")\n+\n+    if integer_arrays and kind in {None, 'table'}:\n+        ar2_min = np.min(ar2)\n+        ar2_max = np.max(ar2)\n+\n+        ar2_range = int(ar2_max) - int(ar2_min)\n+\n+        # Constraints on whether we can actually use the table method:\n+        range_safe_from_overflow = ar2_range < np.iinfo(ar2.dtype).max\n+        below_memory_constraint = ar2_range <= 6 * (ar1.size + ar2.size)\n+\n+        # Optimal performance is for approximately\n+        # log10(size) > (log10(range) - 2.27) / 0.927.\n+        # However, here we set the requirement that by default\n+        # the intermediate array can only be 6x\n+        # the combined memory allocation of the original\n+        # arrays. See discussion on \n+        # https://github.com/numpy/numpy/pull/12065.\n+\n+        if (\n+            range_safe_from_overflow and \n+            (below_memory_constraint or kind == 'table')\n+        ):\n+\n+            if invert:\n+                outgoing_array = np.ones_like(ar1, dtype=bool)\n+            else:\n+                outgoing_array = np.zeros_like(ar1, dtype=bool)\n+\n+            # Make elements 1 where the integer exists in ar2\n+            if invert:\n+                isin_helper_ar = np.ones(ar2_range + 1, dtype=bool)\n+                isin_helper_ar[ar2 - ar2_min] = 0\n+            else:\n+                isin_helper_ar = np.zeros(ar2_range + 1, dtype=bool)\n+                isin_helper_ar[ar2 - ar2_min] = 1\n+\n+            # Mask out elements we know won't work\n+            basic_mask = (ar1 <= ar2_max) & (ar1 >= ar2_min)\n+            outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -\n+                                                        ar2_min]\n+\n+            return outgoing_array\n+        elif kind == 'table':  # not range_safe_from_overflow\n+            raise RuntimeError(\n+                \"You have specified kind='table', \"\n+                \"but the range of values in `ar2` exceeds the \"\n+                \"maximum integer of the datatype. \"\n+                \"Please set `kind` to None or 'sort'.\"\n+            )\n+    elif kind == 'table':\n+        raise ValueError(\n+            \"The 'table' method is only \"\n+            \"supported for boolean or integer arrays. \"\n+            \"Please select 'sort' or None for kind.\"\n+        )\n+\n \n     # Check if one of the arrays may contain arbitrary objects\n     contains_object = ar1.dtype.hasobject or ar2.dtype.hasobject\n@@ -637,12 +735,14 @@ def in1d(ar1, ar2, assume_unique=False, invert=False):\n         return ret[rev_idx]\n \n \n-def _isin_dispatcher(element, test_elements, assume_unique=None, invert=None):\n+def _isin_dispatcher(element, test_elements, assume_unique=None, invert=None,\n+                     *, kind=None):\n     return (element, test_elements)\n \n \n @array_function_dispatch(_isin_dispatcher)\n-def isin(element, test_elements, assume_unique=False, invert=False):\n+def isin(element, test_elements, assume_unique=False, invert=False, *,\n+         kind=None):\n     \"\"\"\n     Calculates ``element in test_elements``, broadcasting over `element` only.\n     Returns a boolean array of the same shape as `element` that is True\n@@ -664,6 +764,27 @@ def isin(element, test_elements, assume_unique=False, invert=False):\n         calculating `element not in test_elements`. Default is False.\n         ``np.isin(a, b, invert=True)`` is equivalent to (but faster\n         than) ``np.invert(np.isin(a, b))``.\n+    kind : {None, 'sort', 'table'}, optional\n+        The algorithm to use. This will not affect the final result,\n+        but will affect the speed and memory use. The default, None,\n+        will select automatically based on memory considerations.\n+\n+        * If 'sort', will use a mergesort-based approach. This will have\n+          a memory usage of roughly 6 times the sum of the sizes of\n+          `ar1` and `ar2`, not accounting for size of dtypes.\n+        * If 'table', will use a lookup table approach similar\n+          to a counting sort. This is only available for boolean and\n+          integer arrays. This will have a memory usage of the\n+          size of `ar1` plus the max-min value of `ar2`. `assume_unique`\n+          has no effect when the 'table' option is used.\n+        * If None, will automatically choose 'table' if\n+          the required memory allocation is less than or equal to\n+          6 times the sum of the sizes of `ar1` and `ar2`,\n+          otherwise will use 'sort'. This is done to not use\n+          a large amount of memory by default, even though\n+          'table' may be faster in most cases. If 'table' is chosen,\n+          `assume_unique` will have no effect.\n+\n \n     Returns\n     -------\n@@ -691,6 +812,13 @@ def isin(element, test_elements, assume_unique=False, invert=False):\n     of the `array` constructor's way of handling non-sequence collections.\n     Converting the set to a list usually gives the desired behavior.\n \n+    Using ``kind='table'`` tends to be faster than `kind='sort'` if the\n+    following relationship is true:\n+    ``log10(len(ar2)) > (log10(max(ar2)-min(ar2)) - 2.27) / 0.927``,\n+    but may use greater memory. The default value for `kind` will\n+    be automatically selected based only on memory usage, so one may\n+    manually set ``kind='table'`` if memory constraints can be relaxed.\n+\n     .. versionadded:: 1.13.0\n \n     Examples\n@@ -737,7 +865,7 @@ def isin(element, test_elements, assume_unique=False, invert=False):\n     \"\"\"\n     element = np.asarray(element)\n     return in1d(element, test_elements, assume_unique=assume_unique,\n-                invert=invert).reshape(element.shape)\n+                invert=invert, kind=kind).reshape(element.shape)\n \n \n def _union1d_dispatcher(ar1, ar2):"
            },
            {
                "filename": "numpy/lib/tests/test_arraysetops.py",
                "patch": "@@ -195,7 +195,8 @@ def test_ediff1d_scalar_handling(self,\n         assert_equal(actual, expected)\n         assert actual.dtype == expected.dtype\n \n-    def test_isin(self):\n+    @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n+    def test_isin(self, kind):\n         # the tests for in1d cover most of isin's behavior\n         # if in1d is removed, would need to change those tests to test\n         # isin instead.\n@@ -205,7 +206,7 @@ def _isin_slow(a, b):\n         isin_slow = np.vectorize(_isin_slow, otypes=[bool], excluded={1})\n \n         def assert_isin_equal(a, b):\n-            x = isin(a, b)\n+            x = isin(a, b, kind=kind)\n             y = isin_slow(a, b)\n             assert_array_equal(x, y)\n \n@@ -231,70 +232,73 @@ def assert_isin_equal(a, b):\n         assert_isin_equal(5, 6)\n \n         # empty array-like:\n-        x = []\n-        assert_isin_equal(x, b)\n-        assert_isin_equal(a, x)\n-        assert_isin_equal(x, x)\n-\n-    def test_in1d(self):\n+        if kind in {None, \"sort\"}:\n+            x = []\n+            assert_isin_equal(x, b)\n+            assert_isin_equal(a, x)\n+            assert_isin_equal(x, x)\n+\n+    @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n+    def test_in1d(self, kind):\n         # we use two different sizes for the b array here to test the\n         # two different paths in in1d().\n         for mult in (1, 10):\n             # One check without np.array to make sure lists are handled correct\n             a = [5, 7, 1, 2]\n             b = [2, 4, 3, 1, 5] * mult\n             ec = np.array([True, False, True, True])\n-            c = in1d(a, b, assume_unique=True)\n+            c = in1d(a, b, assume_unique=True, kind=kind)\n             assert_array_equal(c, ec)\n \n             a[0] = 8\n             ec = np.array([False, False, True, True])\n-            c = in1d(a, b, assume_unique=True)\n+            c = in1d(a, b, assume_unique=True, kind=kind)\n             assert_array_equal(c, ec)\n \n             a[0], a[3] = 4, 8\n             ec = np.array([True, False, True, False])\n-            c = in1d(a, b, assume_unique=True)\n+            c = in1d(a, b, assume_unique=True, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 4, 5, 3, 4, 4, 3, 4, 3, 5, 2, 1, 5, 5])\n             b = [2, 3, 4] * mult\n             ec = [False, True, False, True, True, True, True, True, True,\n                   False, True, False, False, False]\n-            c = in1d(a, b)\n+            c = in1d(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             b = b + [5, 5, 4] * mult\n             ec = [True, True, True, True, True, True, True, True, True, True,\n                   True, False, True, True]\n-            c = in1d(a, b)\n+            c = in1d(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 7, 1, 2])\n             b = np.array([2, 4, 3, 1, 5] * mult)\n             ec = np.array([True, False, True, True])\n-            c = in1d(a, b)\n+            c = in1d(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 7, 1, 1, 2])\n             b = np.array([2, 4, 3, 3, 1, 5] * mult)\n             ec = np.array([True, False, True, True, True])\n-            c = in1d(a, b)\n+            c = in1d(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 5])\n             b = np.array([2, 2] * mult)\n             ec = np.array([False, False])\n-            c = in1d(a, b)\n+            c = in1d(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n         a = np.array([5])\n         b = np.array([2])\n         ec = np.array([False])\n-        c = in1d(a, b)\n+        c = in1d(a, b, kind=kind)\n         assert_array_equal(c, ec)\n \n-        assert_array_equal(in1d([], []), [])\n+        if kind in {None, \"sort\"}:\n+            assert_array_equal(in1d([], [], kind=kind), [])\n \n     def test_in1d_char_array(self):\n         a = np.array(['a', 'b', 'c', 'd', 'e', 'c', 'e', 'b'])\n@@ -305,27 +309,74 @@ def test_in1d_char_array(self):\n \n         assert_array_equal(c, ec)\n \n-    def test_in1d_invert(self):\n+    @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n+    def test_in1d_invert(self, kind):\n         \"Test in1d's invert parameter\"\n         # We use two different sizes for the b array here to test the\n         # two different paths in in1d().\n         for mult in (1, 10):\n             a = np.array([5, 4, 5, 3, 4, 4, 3, 4, 3, 5, 2, 1, 5, 5])\n             b = [2, 3, 4] * mult\n-            assert_array_equal(np.invert(in1d(a, b)), in1d(a, b, invert=True))\n-\n-    def test_in1d_ravel(self):\n+            assert_array_equal(np.invert(in1d(a, b, kind=kind)),\n+                               in1d(a, b, invert=True, kind=kind))\n+\n+        # float:\n+        if kind in {None, \"sort\"}:\n+            for mult in (1, 10):\n+                a = np.array([5, 4, 5, 3, 4, 4, 3, 4, 3, 5, 2, 1, 5, 5],\n+                            dtype=np.float32)\n+                b = [2, 3, 4] * mult\n+                b = np.array(b, dtype=np.float32)\n+                assert_array_equal(np.invert(in1d(a, b, kind=kind)),\n+                                   in1d(a, b, invert=True, kind=kind))\n+\n+    @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n+    def test_in1d_ravel(self, kind):\n         # Test that in1d ravels its input arrays. This is not documented\n         # behavior however. The test is to ensure consistentency.\n         a = np.arange(6).reshape(2, 3)\n         b = np.arange(3, 9).reshape(3, 2)\n         long_b = np.arange(3, 63).reshape(30, 2)\n         ec = np.array([False, False, False, True, True, True])\n \n-        assert_array_equal(in1d(a, b, assume_unique=True), ec)\n-        assert_array_equal(in1d(a, b, assume_unique=False), ec)\n-        assert_array_equal(in1d(a, long_b, assume_unique=True), ec)\n-        assert_array_equal(in1d(a, long_b, assume_unique=False), ec)\n+        assert_array_equal(in1d(a, b, assume_unique=True, kind=kind),\n+                           ec)\n+        assert_array_equal(in1d(a, b, assume_unique=False,\n+                                kind=kind),\n+                           ec)\n+        assert_array_equal(in1d(a, long_b, assume_unique=True,\n+                                kind=kind),\n+                           ec)\n+        assert_array_equal(in1d(a, long_b, assume_unique=False,\n+                                kind=kind),\n+                           ec)\n+\n+    def test_in1d_hit_alternate_algorithm(self):\n+        \"\"\"Hit the standard isin code with integers\"\"\"\n+        # Need extreme range to hit standard code\n+        # This hits it without the use of kind='table'\n+        a = np.array([5, 4, 5, 3, 4, 4, 1e9], dtype=np.int64)\n+        b = np.array([2, 3, 4, 1e9], dtype=np.int64)\n+        expected = np.array([0, 1, 0, 1, 1, 1, 1], dtype=bool)\n+        assert_array_equal(expected, in1d(a, b))\n+        assert_array_equal(np.invert(expected), in1d(a, b, invert=True))\n+\n+        a = np.array([5, 7, 1, 2], dtype=np.int64)\n+        b = np.array([2, 4, 3, 1, 5, 1e9], dtype=np.int64)\n+        ec = np.array([True, False, True, True])\n+        c = in1d(a, b, assume_unique=True)\n+        assert_array_equal(c, ec)\n+\n+    @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n+    def test_in1d_boolean(self, kind):\n+        \"\"\"Test that in1d works for boolean input\"\"\"\n+        a = np.array([True, False])\n+        b = np.array([False, False, False])\n+        expected = np.array([False, True])\n+        assert_array_equal(expected,\n+                           in1d(a, b, kind=kind))\n+        assert_array_equal(np.invert(expected),\n+                           in1d(a, b, invert=True, kind=kind))\n \n     def test_in1d_first_array_is_object(self):\n         ar1 = [None]\n@@ -391,6 +442,40 @@ def test_in1d_with_arrays_containing_tuples(self):\n         result = np.in1d(ar1, ar2, invert=True)\n         assert_array_equal(result, np.invert(expected))\n \n+    def test_in1d_errors(self):\n+        \"\"\"Test that in1d raises expected errors.\"\"\"\n+\n+        # Error 1: `kind` is not one of 'sort' 'table' or None.\n+        ar1 = np.array([1, 2, 3, 4, 5])\n+        ar2 = np.array([2, 4, 6, 8, 10])\n+        assert_raises(ValueError, in1d, ar1, ar2, kind='quicksort')\n+\n+        # Error 2: `kind=\"table\"` does not work for non-integral arrays.\n+        obj_ar1 = np.array([1, 'a', 3, 'b', 5], dtype=object)\n+        obj_ar2 = np.array([1, 'a', 3, 'b', 5], dtype=object)\n+        assert_raises(ValueError, in1d, obj_ar1, obj_ar2, kind='table')\n+\n+        for dtype in [np.int32, np.int64]:\n+            ar1 = np.array([-1, 2, 3, 4, 5], dtype=dtype)\n+            # The range of this array will overflow:\n+            overflow_ar2 = np.array([-1, np.iinfo(dtype).max], dtype=dtype)\n+\n+            # Error 3: `kind=\"table\"` will trigger a runtime error\n+            #  if there is an integer overflow expected when computing the\n+            #  range of ar2\n+            assert_raises(\n+                RuntimeError,\n+                in1d, ar1, overflow_ar2, kind='table'\n+            )\n+\n+            # Non-error: `kind=None` will *not* trigger a runtime error\n+            #  if there is an integer overflow, it will switch to\n+            #  the `sort` algorithm.\n+            result = np.in1d(ar1, overflow_ar2, kind=None)\n+            assert_array_equal(result, [True] + [False] * 4)\n+            result = np.in1d(ar1, overflow_ar2, kind='sort')\n+            assert_array_equal(result, [True] + [False] * 4)\n+\n     def test_union1d(self):\n         a = np.array([5, 4, 7, 1, 2])\n         b = np.array([2, 4, 3, 3, 2, 1, 5])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21705,
        "body": "relates #21038 \r\n\r\nThis pull-request changes:\r\n\r\n- Fix detecting armhf by strictly detects armhf via compiler definitions,\r\nto avoid building neon objects on arch armel even when\r\nthe compiler supports armv7 features on it.\r\n\r\n- Hardened the Neon/ASIMD compile-time tests to avoid\r\n compiler optimizing out the generated instructions,\r\nso we make sure the required instructions have been tested against the linker/assembler.",
        "changed_files": [
            {
                "filename": "numpy/distutils/ccompiler_opt.py",
                "patch": "@@ -955,51 +955,57 @@ class _CCompiler:\n     def __init__(self):\n         if hasattr(self, \"cc_is_cached\"):\n             return\n-        #      attr                regex\n+        #      attr            regex        compiler-expression\n         detect_arch = (\n-            (\"cc_on_x64\",      \".*(x|x86_|amd)64.*\"),\n-            (\"cc_on_x86\",      \".*(win32|x86|i386|i686).*\"),\n-            (\"cc_on_ppc64le\",  \".*(powerpc|ppc)64(el|le).*\"),\n-            (\"cc_on_ppc64\",    \".*(powerpc|ppc)64.*\"),\n-            (\"cc_on_aarch64\",  \".*(aarch64|arm64).*\"),\n-            (\"cc_on_armhf\",    \".*arm.*\"),\n-            (\"cc_on_s390x\",    \".*s390x.*\"),\n+            (\"cc_on_x64\",      \".*(x|x86_|amd)64.*\", \"\"),\n+            (\"cc_on_x86\",      \".*(win32|x86|i386|i686).*\", \"\"),\n+            (\"cc_on_ppc64le\",  \".*(powerpc|ppc)64(el|le).*\", \"\"),\n+            (\"cc_on_ppc64\",    \".*(powerpc|ppc)64.*\", \"\"),\n+            (\"cc_on_aarch64\",  \".*(aarch64|arm64).*\", \"\"),\n+            (\"cc_on_armhf\",    \".*arm.*\", \"defined(__ARM_ARCH_7__) || \"\n+                                          \"defined(__ARM_ARCH_7A__)\"),\n+            (\"cc_on_s390x\",    \".*s390x.*\", \"\"),\n             # undefined platform\n-            (\"cc_on_noarch\",    \"\"),\n+            (\"cc_on_noarch\",   \"\", \"\"),\n         )\n         detect_compiler = (\n-            (\"cc_is_gcc\",     r\".*(gcc|gnu\\-g).*\"),\n-            (\"cc_is_clang\",    \".*clang.*\"),\n-            (\"cc_is_iccw\",     \".*(intelw|intelemw|iccw).*\"), # intel msvc like\n-            (\"cc_is_icc\",      \".*(intel|icc).*\"), # intel unix like\n-            (\"cc_is_msvc\",     \".*msvc.*\"),\n+            (\"cc_is_gcc\",     r\".*(gcc|gnu\\-g).*\", \"\"),\n+            (\"cc_is_clang\",    \".*clang.*\", \"\"),\n+            # intel msvc like\n+            (\"cc_is_iccw\",     \".*(intelw|intelemw|iccw).*\", \"\"),\n+            (\"cc_is_icc\",      \".*(intel|icc).*\", \"\"),  # intel unix like\n+            (\"cc_is_msvc\",     \".*msvc.*\", \"\"),\n             # undefined compiler will be treat it as gcc\n-            (\"cc_is_nocc\",     \"\"),\n+            (\"cc_is_nocc\",     \"\", \"\"),\n         )\n         detect_args = (\n-           (\"cc_has_debug\",  \".*(O0|Od|ggdb|coverage|debug:full).*\"),\n-           (\"cc_has_native\", \".*(-march=native|-xHost|/QxHost).*\"),\n+           (\"cc_has_debug\",  \".*(O0|Od|ggdb|coverage|debug:full).*\", \"\"),\n+           (\"cc_has_native\", \".*(-march=native|-xHost|/QxHost).*\", \"\"),\n            # in case if the class run with -DNPY_DISABLE_OPTIMIZATION\n-           (\"cc_noopt\", \".*DISABLE_OPT.*\"),\n+           (\"cc_noopt\", \".*DISABLE_OPT.*\", \"\"),\n         )\n \n         dist_info = self.dist_info()\n         platform, compiler_info, extra_args = dist_info\n         # set False to all attrs\n         for section in (detect_arch, detect_compiler, detect_args):\n-            for attr, rgex in section:\n+            for attr, rgex, cexpr in section:\n                 setattr(self, attr, False)\n \n         for detect, searchin in ((detect_arch, platform), (detect_compiler, compiler_info)):\n-            for attr, rgex in detect:\n+            for attr, rgex, cexpr in detect:\n                 if rgex and not re.match(rgex, searchin, re.IGNORECASE):\n                     continue\n+                if cexpr and not self.cc_test_cexpr(cexpr):\n+                    continue\n                 setattr(self, attr, True)\n                 break\n \n-        for attr, rgex in detect_args:\n+        for attr, rgex, cexpr in detect_args:\n             if rgex and not re.match(rgex, extra_args, re.IGNORECASE):\n                 continue\n+            if cexpr and not self.cc_test_cexpr(cexpr):\n+                continue\n             setattr(self, attr, True)\n \n         if self.cc_on_noarch:\n@@ -1071,6 +1077,25 @@ def cc_test_flags(self, flags):\n             self.dist_log(\"testing failed\", stderr=True)\n         return test\n \n+    @_Cache.me\n+    def cc_test_cexpr(self, cexpr, flags=[]):\n+        \"\"\"\n+        Same as the above but supports compile-time expressions.\n+        \"\"\"\n+        self.dist_log(\"testing compiler expression\", cexpr)\n+        test_path = os.path.join(self.conf_tmp_path, \"npy_dist_test_cexpr.c\")\n+        with open(test_path, \"w\") as fd:\n+            fd.write(textwrap.dedent(f\"\"\"\\\n+               #if !({cexpr})\n+                   #error \"unsupported expression\"\n+               #endif\n+               int dummy;\n+            \"\"\"))\n+        test = self.dist_test(test_path, flags)\n+        if not test:\n+            self.dist_log(\"testing failed\", stderr=True)\n+        return test\n+\n     def cc_normalize_flags(self, flags):\n         \"\"\"\n         Remove the conflicts that caused due gathering implied features flags."
            },
            {
                "filename": "numpy/distutils/checks/cpu_asimd.c",
                "patch": "@@ -3,17 +3,19 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    float32x4_t v1 = vdupq_n_f32(1.0f), v2 = vdupq_n_f32(2.0f);\n+    float *src = (float*)argv[argc-1];\n+    float32x4_t v1 = vdupq_n_f32(src[0]), v2 = vdupq_n_f32(src[1]);\n     /* MAXMIN */\n     int ret  = (int)vgetq_lane_f32(vmaxnmq_f32(v1, v2), 0);\n         ret += (int)vgetq_lane_f32(vminnmq_f32(v1, v2), 0);\n     /* ROUNDING */\n     ret += (int)vgetq_lane_f32(vrndq_f32(v1), 0);\n #ifdef __aarch64__\n     {\n-        float64x2_t vd1 = vdupq_n_f64(1.0), vd2 = vdupq_n_f64(2.0);\n+        double *src2 = (float*)argv[argc-1];\n+        float64x2_t vd1 = vdupq_n_f64(src2[0]), vd2 = vdupq_n_f64(src2[1]);\n         /* MAXMIN */\n         ret += (int)vgetq_lane_f64(vmaxnmq_f64(vd1, vd2), 0);\n         ret += (int)vgetq_lane_f64(vminnmq_f64(vd1, vd2), 0);"
            },
            {
                "filename": "numpy/distutils/checks/cpu_asimddp.c",
                "patch": "@@ -3,9 +3,10 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    uint8x16_t v1 = vdupq_n_u8((unsigned char)1), v2 = vdupq_n_u8((unsigned char)2);\n+    unsigned char *src = (unsigned char*)argv[argc-1];\n+    uint8x16_t v1 = vdupq_n_u8(src[0]), v2 = vdupq_n_u8(src[1]);\n     uint32x4_t va = vdupq_n_u32(3);\n     int ret = (int)vgetq_lane_u32(vdotq_u32(va, v1, v2), 0);\n #ifdef __aarch64__"
            },
            {
                "filename": "numpy/distutils/checks/cpu_asimdfhm.c",
                "patch": "@@ -3,12 +3,14 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    float16x8_t vhp  = vdupq_n_f16((float16_t)1);\n-    float16x4_t vlhp = vdup_n_f16((float16_t)1);\n-    float32x4_t vf   = vdupq_n_f32(1.0f);\n-    float32x2_t vlf  = vdup_n_f32(1.0f);\n+    float16_t *src = (float16_t*)argv[argc-1];\n+    float *src2 = (float*)argv[argc-2];\n+    float16x8_t vhp  = vdupq_n_f16(src[0]);\n+    float16x4_t vlhp = vdup_n_f16(src[1]);\n+    float32x4_t vf   = vdupq_n_f32(src2[0]);\n+    float32x2_t vlf  = vdup_n_f32(src2[1]);\n \n     int ret  = (int)vget_lane_f32(vfmlal_low_f16(vlf, vlhp, vlhp), 0);\n         ret += (int)vgetq_lane_f32(vfmlslq_high_f16(vf, vhp, vhp), 0);"
            },
            {
                "filename": "numpy/distutils/checks/cpu_asimdhp.c",
                "patch": "@@ -3,10 +3,11 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    float16x8_t vhp  = vdupq_n_f16((float16_t)-1);\n-    float16x4_t vlhp = vdup_n_f16((float16_t)-1);\n+    float16_t *src = (float16_t*)argv[argc-1];\n+    float16x8_t vhp  = vdupq_n_f16(src[0]);\n+    float16x4_t vlhp = vdup_n_f16(src[1]);\n \n     int ret  =  (int)vgetq_lane_f16(vabdq_f16(vhp, vhp), 0);\n         ret  += (int)vget_lane_f16(vabd_f16(vlhp, vlhp), 0);"
            },
            {
                "filename": "numpy/distutils/checks/cpu_neon.c",
                "patch": "@@ -3,12 +3,16 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    float32x4_t v1 = vdupq_n_f32(1.0f), v2 = vdupq_n_f32(2.0f);\n+    // passing from untraced pointers to avoid optimizing out any constants\n+    // so we can test against the linker.\n+    float *src = (float*)argv[argc-1];\n+    float32x4_t v1 = vdupq_n_f32(src[0]), v2 = vdupq_n_f32(src[1]);\n     int ret = (int)vgetq_lane_f32(vmulq_f32(v1, v2), 0);\n #ifdef __aarch64__\n-    float64x2_t vd1 = vdupq_n_f64(1.0), vd2 = vdupq_n_f64(2.0);\n+    double *src2 = (double*)argv[argc-2];\n+    float64x2_t vd1 = vdupq_n_f64(src2[0]), vd2 = vdupq_n_f64(src2[1]);\n     ret += (int)vgetq_lane_f64(vmulq_f64(vd1, vd2), 0);\n #endif\n     return ret;"
            },
            {
                "filename": "numpy/distutils/checks/cpu_neon_fp16.c",
                "patch": "@@ -3,9 +3,9 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    short z4[] = {0, 0, 0, 0, 0, 0, 0, 0};\n-    float32x4_t v_z4 = vcvt_f32_f16((float16x4_t)vld1_s16((const short*)z4));\n+    short *src = (short*)argv[argc-1];\n+    float32x4_t v_z4 = vcvt_f32_f16((float16x4_t)vld1_s16(src));\n     return (int)vgetq_lane_f32(v_z4, 0);\n }"
            },
            {
                "filename": "numpy/distutils/checks/cpu_neon_vfpv4.c",
                "patch": "@@ -3,16 +3,18 @@\n #endif\n #include <arm_neon.h>\n \n-int main(void)\n+int main(int argc, char **argv)\n {\n-    float32x4_t v1 = vdupq_n_f32(1.0f);\n-    float32x4_t v2 = vdupq_n_f32(2.0f);\n-    float32x4_t v3 = vdupq_n_f32(3.0f);\n+    float *src = (float*)argv[argc-1];\n+    float32x4_t v1 = vdupq_n_f32(src[0]);\n+    float32x4_t v2 = vdupq_n_f32(src[1]);\n+    float32x4_t v3 = vdupq_n_f32(src[2]);\n     int ret = (int)vgetq_lane_f32(vfmaq_f32(v1, v2, v3), 0);\n #ifdef __aarch64__\n-    float64x2_t vd1 = vdupq_n_f64(1.0);\n-    float64x2_t vd2 = vdupq_n_f64(2.0);\n-    float64x2_t vd3 = vdupq_n_f64(3.0);\n+    double *src2 = (double*)argv[argc-2];\n+    float64x2_t vd1 = vdupq_n_f64(src2[0]);\n+    float64x2_t vd2 = vdupq_n_f64(src2[1]);\n+    float64x2_t vd3 = vdupq_n_f64(src2[2]);\n     ret += (int)vgetq_lane_f64(vfmaq_f64(vd1, vd2, vd3), 0);\n #endif\n     return ret;"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21752,
        "body": "* The input arguments `a1` and `a2` are cast to arrays, so no need to do it again after comparison\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/numeric.py",
                "patch": "@@ -2460,14 +2460,14 @@ def array_equal(a1, a2, equal_nan=False):\n     if a1.shape != a2.shape:\n         return False\n     if not equal_nan:\n-        return bool(asarray(a1 == a2).all())\n+        return bool((a1 == a2).all())\n     # Handling NaN values if equal_nan is True\n     a1nan, a2nan = isnan(a1), isnan(a2)\n     # NaN's occur at different locations\n     if not (a1nan == a2nan).all():\n         return False\n     # Shapes of a1, a2 and masks are guaranteed to be consistent by this point\n-    return bool(asarray(a1[~a1nan] == a2[~a1nan]).all())\n+    return bool((a1[~a1nan] == a2[~a1nan]).all())\n \n \n def _array_equiv_dispatcher(a1, a2):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21041,
        "body": "This makes all comparison operators and ufuncs work on strings\r\nusing the ufunc machinery.\r\nIt requires a half-manual \"ufunc\" to keep supporting void comparisons\r\nand especially `np.compare_chararrays` (that one may have a bit more\r\noverhead now).\r\n\r\nIn general the new code should be much faster, and has a lot of easier\r\noptimization potential.  It is also much simpler since it can outsource\r\nsome complexities to the ufunc/iterator machinery.\r\n\r\nThis further fixes a couple of bugs with byte-swapped strings.\r\n\r\nThe backward compatibility related change is that using the normal\r\nufunc machinery means that string comparisons between string and\r\nunicode now give a `FutureWarning` (instead of just False).\r\n\r\n---\r\n\r\nThis also shows two things:\r\n1. How to use new-style loops (very simple one)\r\n2. Writes them in C++ templating.\r\n\r\nI do admit, that I think we will have to refactor some of this when we get more ufuncs created in this way.  Note that for now `np.equal.types` will *not* list `SS->?` and `UU->?`.\r\n\r\nIn general, this is finished, however, it should get a bit of new tests for direct `np.equal`, etc. ufunc use and for byte-swapped or unaligned unicode inputs.",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_strings.py",
                "patch": "@@ -0,0 +1,45 @@\n+from __future__ import absolute_import, division, print_function\n+\n+from .common import Benchmark\n+\n+import numpy as np\n+import operator\n+\n+\n+_OPERATORS = {\n+    '==': operator.eq,\n+    '!=': operator.ne,\n+    '<': operator.lt,\n+    '<=': operator.le,\n+    '>': operator.gt,\n+    '>=': operator.ge,\n+}\n+\n+\n+class StringComparisons(Benchmark):\n+    # Basic string comparison speed tests\n+    params = [\n+        [100, 10000, (1000, 20)],\n+        ['U', 'S'],\n+        [True, False],\n+        ['==', '!=', '<', '<=', '>', '>=']]\n+    param_names = ['shape', 'dtype', 'contig', 'operator']\n+    int64 = np.dtype(np.int64)\n+\n+    def setup(self, shape, dtype, contig, operator):\n+        self.arr = np.arange(np.prod(shape)).astype(dtype).reshape(shape)\n+        self.arr_identical = self.arr.copy()\n+        self.arr_different = self.arr[::-1].copy()\n+\n+        if not contig:\n+            self.arr = self.arr[..., ::2]\n+            self.arr_identical = self.arr_identical[..., ::2]\n+            self.arr_different = self.arr_different[..., ::2]\n+\n+        self.operator = _OPERATORS[operator]\n+\n+    def time_compare_identical(self, shape, dtype, contig, operator):\n+        self.operator(self.arr, self.arr_identical)\n+\n+    def time_compare_different(self, shape, dtype, contig, operator):\n+        self.operator(self.arr, self.arr_different)"
            },
            {
                "filename": "numpy/core/include/numpy/experimental_dtype_api.h",
                "patch": "@@ -214,7 +214,7 @@ typedef struct {\n } PyArrayMethod_Spec;\n \n \n-typedef PyObject *_ufunc_addloop_fromspec_func(\n+typedef int _ufunc_addloop_fromspec_func(\n         PyObject *ufunc, PyArrayMethod_Spec *spec);\n /*\n  * The main ufunc registration function.  This adds a new implementation/loop"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1082,6 +1082,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'scalarmath.c.src'),\n             join('src', 'umath', 'ufunc_type_resolution.c'),\n             join('src', 'umath', 'override.c'),\n+            join('src', 'umath', 'string_ufuncs.cpp'),\n             # For testing. Eventually, should use public API and be separate:\n             join('src', 'umath', '_scaled_float_dtype.c'),\n             ]"
            },
            {
                "filename": "numpy/core/src/common/numpyos.h",
                "patch": "@@ -1,6 +1,10 @@\n #ifndef NUMPY_CORE_SRC_COMMON_NPY_NUMPYOS_H_\n #define NUMPY_CORE_SRC_COMMON_NPY_NUMPYOS_H_\n \n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n NPY_NO_EXPORT char*\n NumPyOS_ascii_formatd(char *buffer, size_t buf_size,\n                       const char *format,\n@@ -39,4 +43,8 @@ NumPyOS_strtoll(const char *str, char **endptr, int base);\n NPY_NO_EXPORT npy_ulonglong\n NumPyOS_strtoull(const char *str, char **endptr, int base);\n \n+#ifdef __cplusplus\n+}\n+#endif\n+\n #endif  /* NUMPY_CORE_SRC_COMMON_NPY_NUMPYOS_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/array_method.h",
                "patch": "@@ -7,6 +7,9 @@\n #include <Python.h>\n #include <numpy/ndarraytypes.h>\n \n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n \n typedef enum {\n     /* Flag for whether the GIL is required */\n@@ -249,6 +252,10 @@ PyArrayMethod_FromSpec(PyArrayMethod_Spec *spec);\n  *       need better tests when a public version is exposed.\n  */\n NPY_NO_EXPORT PyBoundArrayMethodObject *\n-PyArrayMethod_FromSpec_int(PyArrayMethod_Spec *spec, int private);\n+PyArrayMethod_FromSpec_int(PyArrayMethod_Spec *spec, int priv);\n+\n+#ifdef __cplusplus\n+}\n+#endif\n \n #endif  /* NUMPY_CORE_SRC_MULTIARRAY_ARRAY_METHOD_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/arrayobject.c",
                "patch": "@@ -645,375 +645,11 @@ PyArray_FailUnlessWriteable(PyArrayObject *obj, const char *name)\n     return 0;\n }\n \n-/* This also handles possibly mis-aligned data */\n-/* Compare s1 and s2 which are not necessarily NULL-terminated.\n-   s1 is of length len1\n-   s2 is of length len2\n-   If they are NULL terminated, then stop comparison.\n-*/\n-static int\n-_myunincmp(npy_ucs4 const *s1, npy_ucs4 const *s2, int len1, int len2)\n-{\n-    npy_ucs4 const *sptr;\n-    npy_ucs4 *s1t = NULL;\n-    npy_ucs4 *s2t = NULL;\n-    int val;\n-    npy_intp size;\n-    int diff;\n-\n-    /* Replace `s1` and `s2` with aligned copies if needed */\n-    if ((npy_intp)s1 % sizeof(npy_ucs4) != 0) {\n-        size = len1*sizeof(npy_ucs4);\n-        s1t = malloc(size);\n-        memcpy(s1t, s1, size);\n-        s1 = s1t;\n-    }\n-    if ((npy_intp)s2 % sizeof(npy_ucs4) != 0) {\n-        size = len2*sizeof(npy_ucs4);\n-        s2t = malloc(size);\n-        memcpy(s2t, s2, size);\n-        s2 = s1t;\n-    }\n-\n-    val = PyArray_CompareUCS4(s1, s2, PyArray_MIN(len1,len2));\n-    if ((val != 0) || (len1 == len2)) {\n-        goto finish;\n-    }\n-    if (len2 > len1) {\n-        sptr = s2+len1;\n-        val = -1;\n-        diff = len2-len1;\n-    }\n-    else {\n-        sptr = s1+len2;\n-        val = 1;\n-        diff=len1-len2;\n-    }\n-    while (diff--) {\n-        if (*sptr != 0) {\n-            goto finish;\n-        }\n-        sptr++;\n-    }\n-    val = 0;\n-\n- finish:\n-    /* Cleanup the aligned copies */\n-    if (s1t) {\n-        free(s1t);\n-    }\n-    if (s2t) {\n-        free(s2t);\n-    }\n-    return val;\n-}\n-\n-\n-\n-\n-/*\n- * Compare s1 and s2 which are not necessarily NULL-terminated.\n- * s1 is of length len1\n- * s2 is of length len2\n- * If they are NULL terminated, then stop comparison.\n- */\n-static int\n-_mystrncmp(char const *s1, char const *s2, int len1, int len2)\n-{\n-    char const *sptr;\n-    int val;\n-    int diff;\n-\n-    val = memcmp(s1, s2, PyArray_MIN(len1, len2));\n-    if ((val != 0) || (len1 == len2)) {\n-        return val;\n-    }\n-    if (len2 > len1) {\n-        sptr = s2 + len1;\n-        val = -1;\n-        diff = len2 - len1;\n-    }\n-    else {\n-        sptr = s1 + len2;\n-        val = 1;\n-        diff = len1 - len2;\n-    }\n-    while (diff--) {\n-        if (*sptr != 0) {\n-            return val;\n-        }\n-        sptr++;\n-    }\n-    return 0; /* Only happens if NULLs are everywhere */\n-}\n-\n-/* Borrowed from Numarray */\n-\n-#define SMALL_STRING 2048\n-\n-static void _rstripw(char *s, int n)\n-{\n-    int i;\n-    for (i = n - 1; i >= 1; i--) { /* Never strip to length 0. */\n-        int c = s[i];\n-\n-        if (!c || NumPyOS_ascii_isspace((int)c)) {\n-            s[i] = 0;\n-        }\n-        else {\n-            break;\n-        }\n-    }\n-}\n-\n-static void _unistripw(npy_ucs4 *s, int n)\n-{\n-    int i;\n-    for (i = n - 1; i >= 1; i--) { /* Never strip to length 0. */\n-        npy_ucs4 c = s[i];\n-        if (!c || NumPyOS_ascii_isspace((int)c)) {\n-            s[i] = 0;\n-        }\n-        else {\n-            break;\n-        }\n-    }\n-}\n-\n-\n-static char *\n-_char_copy_n_strip(char const *original, char *temp, int nc)\n-{\n-    if (nc > SMALL_STRING) {\n-        temp = malloc(nc);\n-        if (!temp) {\n-            PyErr_NoMemory();\n-            return NULL;\n-        }\n-    }\n-    memcpy(temp, original, nc);\n-    _rstripw(temp, nc);\n-    return temp;\n-}\n-\n-static void\n-_char_release(char *ptr, int nc)\n-{\n-    if (nc > SMALL_STRING) {\n-        free(ptr);\n-    }\n-}\n-\n-static char *\n-_uni_copy_n_strip(char const *original, char *temp, int nc)\n-{\n-    if (nc*sizeof(npy_ucs4) > SMALL_STRING) {\n-        temp = malloc(nc*sizeof(npy_ucs4));\n-        if (!temp) {\n-            PyErr_NoMemory();\n-            return NULL;\n-        }\n-    }\n-    memcpy(temp, original, nc*sizeof(npy_ucs4));\n-    _unistripw((npy_ucs4 *)temp, nc);\n-    return temp;\n-}\n-\n-static void\n-_uni_release(char *ptr, int nc)\n-{\n-    if (nc*sizeof(npy_ucs4) > SMALL_STRING) {\n-        free(ptr);\n-    }\n-}\n-\n-\n-/* End borrowed from numarray */\n-\n-#define _rstrip_loop(CMP) {                                     \\\n-        void *aptr, *bptr;                                      \\\n-        char atemp[SMALL_STRING], btemp[SMALL_STRING];          \\\n-        while(size--) {                                         \\\n-            aptr = stripfunc(iself->dataptr, atemp, N1);        \\\n-            if (!aptr) return -1;                               \\\n-            bptr = stripfunc(iother->dataptr, btemp, N2);       \\\n-            if (!bptr) {                                        \\\n-                relfunc(aptr, N1);                              \\\n-                return -1;                                      \\\n-            }                                                   \\\n-            val = compfunc(aptr, bptr, N1, N2);                 \\\n-            *dptr = (val CMP 0);                                \\\n-            PyArray_ITER_NEXT(iself);                           \\\n-            PyArray_ITER_NEXT(iother);                          \\\n-            dptr += 1;                                          \\\n-            relfunc(aptr, N1);                                  \\\n-            relfunc(bptr, N2);                                  \\\n-        }                                                       \\\n-    }\n-\n-#define _reg_loop(CMP) {                                \\\n-        while(size--) {                                 \\\n-            val = compfunc((void *)iself->dataptr,      \\\n-                          (void *)iother->dataptr,      \\\n-                          N1, N2);                      \\\n-            *dptr = (val CMP 0);                        \\\n-            PyArray_ITER_NEXT(iself);                   \\\n-            PyArray_ITER_NEXT(iother);                  \\\n-            dptr += 1;                                  \\\n-        }                                               \\\n-    }\n-\n-static int\n-_compare_strings(PyArrayObject *result, PyArrayMultiIterObject *multi,\n-                 int cmp_op, void *func, int rstrip)\n-{\n-    PyArrayIterObject *iself, *iother;\n-    npy_bool *dptr;\n-    npy_intp size;\n-    int val;\n-    int N1, N2;\n-    int (*compfunc)(void *, void *, int, int);\n-    void (*relfunc)(char *, int);\n-    char* (*stripfunc)(char const *, char *, int);\n-\n-    compfunc = func;\n-    dptr = (npy_bool *)PyArray_DATA(result);\n-    iself = multi->iters[0];\n-    iother = multi->iters[1];\n-    size = multi->size;\n-    N1 = PyArray_DESCR(iself->ao)->elsize;\n-    N2 = PyArray_DESCR(iother->ao)->elsize;\n-    if ((void *)compfunc == (void *)_myunincmp) {\n-        N1 >>= 2;\n-        N2 >>= 2;\n-        stripfunc = _uni_copy_n_strip;\n-        relfunc = _uni_release;\n-    }\n-    else {\n-        stripfunc = _char_copy_n_strip;\n-        relfunc = _char_release;\n-    }\n-    switch (cmp_op) {\n-    case Py_EQ:\n-        if (rstrip) {\n-            _rstrip_loop(==);\n-        } else {\n-            _reg_loop(==);\n-        }\n-        break;\n-    case Py_NE:\n-        if (rstrip) {\n-            _rstrip_loop(!=);\n-        } else {\n-            _reg_loop(!=);\n-        }\n-        break;\n-    case Py_LT:\n-        if (rstrip) {\n-            _rstrip_loop(<);\n-        } else {\n-            _reg_loop(<);\n-        }\n-        break;\n-    case Py_LE:\n-        if (rstrip) {\n-            _rstrip_loop(<=);\n-        } else {\n-            _reg_loop(<=);\n-        }\n-        break;\n-    case Py_GT:\n-        if (rstrip) {\n-            _rstrip_loop(>);\n-        } else {\n-            _reg_loop(>);\n-        }\n-        break;\n-    case Py_GE:\n-        if (rstrip) {\n-            _rstrip_loop(>=);\n-        } else {\n-            _reg_loop(>=);\n-        }\n-        break;\n-    default:\n-        PyErr_SetString(PyExc_RuntimeError, \"bad comparison operator\");\n-        return -1;\n-    }\n-    return 0;\n-}\n-\n-#undef _reg_loop\n-#undef _rstrip_loop\n-#undef SMALL_STRING\n \n+/* From umath/string_ufuncs.cpp/h */\n NPY_NO_EXPORT PyObject *\n-_strings_richcompare(PyArrayObject *self, PyArrayObject *other, int cmp_op,\n-                     int rstrip)\n-{\n-    PyArrayObject *result;\n-    PyArrayMultiIterObject *mit;\n-    int val;\n-\n-    if (PyArray_TYPE(self) != PyArray_TYPE(other)) {\n-        /*\n-         * Comparison between Bytes and Unicode is not defined in Py3K;\n-         * we follow.\n-         */\n-        Py_INCREF(Py_NotImplemented);\n-        return Py_NotImplemented;\n-    }\n-    if (PyArray_ISNOTSWAPPED(self) != PyArray_ISNOTSWAPPED(other)) {\n-        /* Cast `other` to the same byte order as `self` (both unicode here) */\n-        PyArray_Descr* unicode = PyArray_DescrNew(PyArray_DESCR(self));\n-        if (unicode == NULL) {\n-            return NULL;\n-        }\n-        unicode->elsize = PyArray_DESCR(other)->elsize;\n-        PyObject *new = PyArray_FromAny((PyObject *)other,\n-                unicode, 0, 0, 0, NULL);\n-        if (new == NULL) {\n-            return NULL;\n-        }\n-        other = (PyArrayObject *)new;\n-    }\n-    else {\n-        Py_INCREF(other);\n-    }\n-\n-    /* Broad-cast the arrays to a common shape */\n-    mit = (PyArrayMultiIterObject *)PyArray_MultiIterNew(2, self, other);\n-    Py_DECREF(other);\n-    if (mit == NULL) {\n-        return NULL;\n-    }\n-\n-    result = (PyArrayObject *)PyArray_NewFromDescr(&PyArray_Type,\n-                                  PyArray_DescrFromType(NPY_BOOL),\n-                                  mit->nd,\n-                                  mit->dimensions,\n-                                  NULL, NULL, 0,\n-                                  NULL);\n-    if (result == NULL) {\n-        goto finish;\n-    }\n-\n-    if (PyArray_TYPE(self) == NPY_UNICODE) {\n-        val = _compare_strings(result, mit, cmp_op, _myunincmp, rstrip);\n-    }\n-    else {\n-        val = _compare_strings(result, mit, cmp_op, _mystrncmp, rstrip);\n-    }\n-\n-    if (val < 0) {\n-        Py_DECREF(result);\n-        result = NULL;\n-    }\n-\n- finish:\n-    Py_DECREF(mit);\n-    return (PyObject *)result;\n-}\n+_umath_strings_richcompare(\n+        PyArrayObject *self, PyArrayObject *other, int cmp_op, int rstrip);\n \n /*\n  * VOID-type arrays can only be compared equal and not-equal\n@@ -1144,7 +780,7 @@ _void_compare(PyArrayObject *self, PyArrayObject *other, int cmp_op)\n     }\n     else {\n         /* compare as a string. Assumes self and other have same descr->type */\n-        return _strings_richcompare(self, other, cmp_op, 0);\n+        return _umath_strings_richcompare(self, other, cmp_op, 0);\n     }\n }\n \n@@ -1278,36 +914,6 @@ array_richcompare(PyArrayObject *self, PyObject *other, int cmp_op)\n     PyObject *obj_self = (PyObject *)self;\n     PyObject *result = NULL;\n \n-    /* Special case for string arrays (which don't and currently can't have\n-     * ufunc loops defined, so there's no point in trying).\n-     */\n-    if (PyArray_ISSTRING(self)) {\n-        array_other = (PyArrayObject *)PyArray_FromObject(other,\n-                                                          NPY_NOTYPE, 0, 0);\n-        if (array_other == NULL) {\n-            PyErr_Clear();\n-            /* Never mind, carry on, see what happens */\n-        }\n-        else if (!PyArray_ISSTRING(array_other)) {\n-            Py_DECREF(array_other);\n-            /* Never mind, carry on, see what happens */\n-        }\n-        else {\n-            result = _strings_richcompare(self, array_other, cmp_op, 0);\n-            Py_DECREF(array_other);\n-            return result;\n-        }\n-        /* If we reach this point, it means that we are not comparing\n-         * string-to-string. It's possible that this will still work out,\n-         * e.g. if the other array is an object array, then both will be cast\n-         * to object or something? I don't know how that works actually, but\n-         * it does, b/c this works:\n-         *   l = [\"a\", \"b\"]\n-         *   assert np.array(l, dtype=\"S1\") == np.array(l, dtype=\"O\")\n-         * So we fall through and see what happens.\n-         */\n-    }\n-\n     switch (cmp_op) {\n     case Py_LT:\n         RICHCMP_GIVE_UP_IF_NEEDED(obj_self, other);"
            },
            {
                "filename": "numpy/core/src/multiarray/common_dtype.h",
                "patch": "@@ -7,11 +7,19 @@\n #include <numpy/ndarraytypes.h>\n #include \"dtypemeta.h\"\n \n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n NPY_NO_EXPORT PyArray_DTypeMeta *\n PyArray_CommonDType(PyArray_DTypeMeta *dtype1, PyArray_DTypeMeta *dtype2);\n \n NPY_NO_EXPORT PyArray_DTypeMeta *\n PyArray_PromoteDTypeSequence(\n         npy_intp length, PyArray_DTypeMeta **dtypes_in);\n \n+#ifdef __cplusplus\n+}\n+#endif\n+\n #endif  /* NUMPY_CORE_SRC_MULTIARRAY_COMMON_DTYPE_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/convert_datatype.h",
                "patch": "@@ -3,6 +3,10 @@\n \n #include \"array_method.h\"\n \n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n extern NPY_NO_EXPORT npy_intp REQUIRED_STR_LEN[];\n \n NPY_NO_EXPORT PyObject *\n@@ -34,7 +38,7 @@ dtype_kind_to_ordering(char kind);\n /* Used by PyArray_CanCastArrayTo and in the legacy ufunc type resolution */\n NPY_NO_EXPORT npy_bool\n can_cast_scalar_to(PyArray_Descr *scal_type, char *scal_data,\n-                    PyArray_Descr *to, NPY_CASTING casting);\n+                   PyArray_Descr *to, NPY_CASTING casting);\n \n NPY_NO_EXPORT int\n should_use_min_scalar(npy_intp narrs, PyArrayObject **arr,\n@@ -59,7 +63,7 @@ NPY_NO_EXPORT int\n PyArray_AddCastingImplementation(PyBoundArrayMethodObject *meth);\n \n NPY_NO_EXPORT int\n-PyArray_AddCastingImplementation_FromSpec(PyArrayMethod_Spec *spec, int private);\n+PyArray_AddCastingImplementation_FromSpec(PyArrayMethod_Spec *spec, int private_);\n \n NPY_NO_EXPORT NPY_CASTING\n PyArray_MinCastSafety(NPY_CASTING casting1, NPY_CASTING casting2);\n@@ -99,4 +103,8 @@ simple_cast_resolve_descriptors(\n NPY_NO_EXPORT int\n PyArray_InitializeCasts(void);\n \n+#ifdef __cplusplus\n+}\n+#endif\n+\n #endif  /* NUMPY_CORE_SRC_MULTIARRAY_CONVERT_DATATYPE_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/dtypemeta.h",
                "patch": "@@ -1,6 +1,9 @@\n #ifndef NUMPY_CORE_SRC_MULTIARRAY_DTYPEMETA_H_\n #define NUMPY_CORE_SRC_MULTIARRAY_DTYPEMETA_H_\n \n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n \n /* DType flags, currently private, since we may just expose functions */\n #define NPY_DT_LEGACY 1 << 0\n@@ -126,4 +129,8 @@ python_builtins_are_known_scalar_types(\n NPY_NO_EXPORT int\n dtypemeta_wrap_legacy_descriptor(PyArray_Descr *dtypem);\n \n+#ifdef __cplusplus\n+}\n+#endif\n+\n #endif  /* NUMPY_CORE_SRC_MULTIARRAY_DTYPEMETA_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/experimental_public_dtype_api.c",
                "patch": "@@ -300,37 +300,13 @@ PyArrayInitDTypeMeta_FromSpec(\n }\n \n \n-/* Function is defined in umath/dispatching.c (same/one compilation unit) */\n+/* Functions defined in umath/dispatching.c (same/one compilation unit) */\n NPY_NO_EXPORT int\n PyUFunc_AddLoop(PyUFuncObject *ufunc, PyObject *info, int ignore_duplicate);\n \n-static int\n-PyUFunc_AddLoopFromSpec(PyObject *ufunc, PyArrayMethod_Spec *spec)\n-{\n-    if (!PyObject_TypeCheck(ufunc, &PyUFunc_Type)) {\n-        PyErr_SetString(PyExc_TypeError,\n-                \"ufunc object passed is not a ufunc!\");\n-        return -1;\n-    }\n-    PyBoundArrayMethodObject *bmeth =\n-            (PyBoundArrayMethodObject *)PyArrayMethod_FromSpec(spec);\n-    if (bmeth == NULL) {\n-        return -1;\n-    }\n-    int nargs = bmeth->method->nin + bmeth->method->nout;\n-    PyObject *dtypes = PyArray_TupleFromItems(\n-            nargs, (PyObject **)bmeth->dtypes, 1);\n-    if (dtypes == NULL) {\n-        return -1;\n-    }\n-    PyObject *info = PyTuple_Pack(2, dtypes, bmeth->method);\n-    Py_DECREF(bmeth);\n-    Py_DECREF(dtypes);\n-    if (info == NULL) {\n-        return -1;\n-    }\n-    return PyUFunc_AddLoop((PyUFuncObject *)ufunc, info, 0);\n-}\n+NPY_NO_EXPORT int\n+PyUFunc_AddLoopFromSpec(PyUFuncObject *ufunc, PyObject *info, int ignore_duplicate);\n+\n \n /*\n  * Function is defined in umath/wrapping_array_method.c"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -85,6 +85,10 @@ NPY_NO_EXPORT int NPY_NUMUSERTYPES = 0;\n \n NPY_NO_EXPORT int initscalarmath(PyObject *);\n NPY_NO_EXPORT int set_matmul_flags(PyObject *d); /* in ufunc_object.c */\n+/* From umath/string_ufuncs.cpp/h */\n+NPY_NO_EXPORT PyObject *\n+_umath_strings_richcompare(\n+        PyArrayObject *self, PyArrayObject *other, int cmp_op, int rstrip);\n \n /*\n  * global variable to determine if legacy printing is enabled, accessible from\n@@ -3726,6 +3730,12 @@ format_longfloat(PyObject *NPY_UNUSED(dummy), PyObject *args, PyObject *kwds)\n                               TrimMode_LeaveOneZero, -1, -1);\n }\n \n+\n+/*\n+ * The only purpose of this function is that it allows the \"rstrip\".\n+ * From my (@seberg's) perspective, this function should be deprecated\n+ * and I do not think it matters if it is not particularly fast.\n+ */\n static PyObject *\n compare_chararrays(PyObject *NPY_UNUSED(dummy), PyObject *args, PyObject *kwds)\n {\n@@ -3791,7 +3801,7 @@ compare_chararrays(PyObject *NPY_UNUSED(dummy), PyObject *args, PyObject *kwds)\n         return NULL;\n     }\n     if (PyArray_ISSTRING(newarr) && PyArray_ISSTRING(newoth)) {\n-        res = _strings_richcompare(newarr, newoth, cmp_op, rstrip != 0);\n+        res = _umath_strings_richcompare(newarr, newoth, cmp_op, rstrip != 0);\n     }\n     else {\n         PyErr_SetString(PyExc_TypeError,"
            },
            {
                "filename": "numpy/core/src/umath/dispatching.c",
                "patch": "@@ -145,6 +145,38 @@ PyUFunc_AddLoop(PyUFuncObject *ufunc, PyObject *info, int ignore_duplicate)\n }\n \n \n+/*\n+ * Add loop directly to a ufunc from a given ArrayMethod spec.\n+ */\n+NPY_NO_EXPORT int\n+PyUFunc_AddLoopFromSpec(PyObject *ufunc, PyArrayMethod_Spec *spec)\n+{\n+    if (!PyObject_TypeCheck(ufunc, &PyUFunc_Type)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"ufunc object passed is not a ufunc!\");\n+        return -1;\n+    }\n+    PyBoundArrayMethodObject *bmeth =\n+            (PyBoundArrayMethodObject *)PyArrayMethod_FromSpec(spec);\n+    if (bmeth == NULL) {\n+        return -1;\n+    }\n+    int nargs = bmeth->method->nin + bmeth->method->nout;\n+    PyObject *dtypes = PyArray_TupleFromItems(\n+            nargs, (PyObject **)bmeth->dtypes, 1);\n+    if (dtypes == NULL) {\n+        return -1;\n+    }\n+    PyObject *info = PyTuple_Pack(2, dtypes, bmeth->method);\n+    Py_DECREF(bmeth);\n+    Py_DECREF(dtypes);\n+    if (info == NULL) {\n+        return -1;\n+    }\n+    return PyUFunc_AddLoop((PyUFuncObject *)ufunc, info, 0);\n+}\n+\n+\n /**\n  * Resolves the implementation to use, this uses typical multiple dispatching\n  * methods of finding the best matching implementation or resolver."
            },
            {
                "filename": "numpy/core/src/umath/dispatching.h",
                "patch": "@@ -6,6 +6,9 @@\n #include <numpy/ufuncobject.h>\n #include \"array_method.h\"\n \n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n \n typedef int promoter_function(PyUFuncObject *ufunc,\n         PyArray_DTypeMeta *op_dtypes[], PyArray_DTypeMeta *signature[],\n@@ -14,6 +17,9 @@ typedef int promoter_function(PyUFuncObject *ufunc,\n NPY_NO_EXPORT int\n PyUFunc_AddLoop(PyUFuncObject *ufunc, PyObject *info, int ignore_duplicate);\n \n+NPY_NO_EXPORT int\n+PyUFunc_AddLoopFromSpec(PyObject *ufunc, PyArrayMethod_Spec *spec);\n+\n NPY_NO_EXPORT PyArrayMethodObject *\n promote_and_get_ufuncimpl(PyUFuncObject *ufunc,\n         PyArrayObject *const ops[],\n@@ -41,5 +47,8 @@ object_only_ufunc_promoter(PyUFuncObject *ufunc,\n NPY_NO_EXPORT int\n install_logical_ufunc_promoter(PyObject *ufunc);\n \n+#ifdef __cplusplus\n+}\n+#endif\n \n #endif  /*_NPY_DISPATCHING_H */"
            },
            {
                "filename": "numpy/core/src/umath/string_ufuncs.cpp",
                "patch": "@@ -0,0 +1,449 @@\n+#include <Python.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#define _UMATHMODULE\n+\n+#include \"numpy/ndarraytypes.h\"\n+\n+#include \"numpyos.h\"\n+#include \"dispatching.h\"\n+#include \"dtypemeta.h\"\n+#include \"common_dtype.h\"\n+#include \"convert_datatype.h\"\n+\n+#include \"string_ufuncs.h\"\n+\n+\n+template <typename character>\n+static NPY_INLINE int\n+character_cmp(character a, character b)\n+{\n+    if (a == b) {\n+        return 0;\n+    }\n+    else if (a < b) {\n+        return -1;\n+    }\n+    else {\n+        return 1;\n+    }\n+}\n+\n+\n+/*\n+ * Compare two strings of different length.  Note that either string may be\n+ * zero padded (trailing zeros are ignored in other words, the shorter word\n+ * is always padded with zeros).\n+ */\n+template <bool rstrip, typename character>\n+static NPY_INLINE int\n+string_cmp(int len1, const character *str1, int len2, const character *str2)\n+{\n+    if (rstrip) {\n+        /*\n+         * Ignore/\"trim\" trailing whitespace (and 0s).  Note that this function\n+         * does not support unicode whitespace (and never has).\n+         */\n+        while (len1 > 0) {\n+            character c = str1[len1-1];\n+            if (c != (character)0 && !NumPyOS_ascii_isspace(c)) {\n+                break;\n+            }\n+            len1--;\n+        }\n+        while (len2 > 0) {\n+            character c = str2[len2-1];\n+            if (c != (character)0 && !NumPyOS_ascii_isspace(c)) {\n+                break;\n+            }\n+            len2--;\n+        }\n+    }\n+\n+    int n = PyArray_MIN(len1, len2);\n+\n+    if (sizeof(character) == 1) {\n+        /*\n+         * TODO: `memcmp` makes things 2x faster for longer words that match\n+         *       exactly, but at least 2x slower for short or mismatching ones.\n+         */\n+        int cmp = memcmp(str1, str2, n);\n+        if (cmp != 0) {\n+            return cmp;\n+        }\n+        str1 += n;\n+        str2 += n;\n+    }\n+    else {\n+        for (int i = 0; i < n; i++) {\n+            int cmp = character_cmp(*str1, *str2);\n+            if (cmp != 0) {\n+                return cmp;\n+            }\n+            str1++;\n+            str2++;\n+        }\n+    }\n+    if (len1 > len2) {\n+        for (int i = n; i < len1; i++) {\n+            int cmp = character_cmp(*str1, (character)0);\n+            if (cmp != 0) {\n+                return cmp;\n+            }\n+            str1++;\n+        }\n+    }\n+    else if (len2 > len1) {\n+        for (int i = n; i < len2; i++) {\n+            int cmp = character_cmp((character)0, *str2);\n+            if (cmp != 0) {\n+                return cmp;\n+            }\n+            str2++;\n+        }\n+    }\n+    return 0;\n+}\n+\n+\n+/*\n+ * Helper for templating, avoids warnings about uncovered switch paths.\n+ */\n+enum class COMP {\n+    EQ, NE, LT, LE, GT, GE,\n+};\n+\n+static char const *\n+comp_name(COMP comp) {\n+    switch(comp) {\n+        case COMP::EQ: return \"equal\";\n+        case COMP::NE: return \"not_equal\";\n+        case COMP::LT: return \"less\";\n+        case COMP::LE: return \"less_equal\";\n+        case COMP::GT: return \"greater\";\n+        case COMP::GE: return \"greater_equal\";\n+        default:\n+            assert(0);\n+            return nullptr;\n+    }\n+}\n+\n+\n+template <bool rstrip, COMP comp, typename character>\n+static int\n+string_comparison_loop(PyArrayMethod_Context *context,\n+        char *const data[], npy_intp const dimensions[],\n+        npy_intp const strides[], NpyAuxData *NPY_UNUSED(auxdata))\n+{\n+    /*\n+     * Note, fetching `elsize` from the descriptor is OK even without the GIL,\n+     * however it may be that this should be moved into `auxdata` eventually,\n+     * which may also be slightly faster/cleaner (but more involved).\n+     */\n+    int len1 = context->descriptors[0]->elsize / sizeof(character);\n+    int len2 = context->descriptors[1]->elsize / sizeof(character);\n+\n+    char *in1 = data[0];\n+    char *in2 = data[1];\n+    char *out = data[2];\n+\n+    npy_intp N = dimensions[0];\n+\n+    while (N--) {\n+        int cmp = string_cmp<rstrip>(\n+                len1, (character *)in1, len2, (character *)in2);\n+        npy_bool res;\n+        switch (comp) {\n+            case COMP::EQ:\n+                res = cmp == 0;\n+                break;\n+            case COMP::NE:\n+                res = cmp != 0;\n+                break;\n+            case COMP::LT:\n+                res = cmp < 0;\n+                break;\n+            case COMP::LE:\n+                res = cmp <= 0;\n+                break;\n+            case COMP::GT:\n+                res = cmp > 0;\n+                break;\n+            case COMP::GE:\n+                res = cmp >= 0;\n+                break;\n+        }\n+        *(npy_bool *)out = res;\n+\n+        in1 += strides[0];\n+        in2 += strides[1];\n+        out += strides[2];\n+    }\n+    return 0;\n+}\n+\n+\n+/*\n+ * Machinery to add the string loops to the existing ufuncs.\n+ */\n+\n+/*\n+ * This function replaces the strided loop with the passed in one,\n+ * and registers it with the given ufunc.\n+ */\n+static int\n+add_loop(PyObject *umath, const char *ufunc_name,\n+         PyArrayMethod_Spec *spec, PyArrayMethod_StridedLoop *loop)\n+{\n+    PyObject *name = PyUnicode_FromString(ufunc_name);\n+    if (name == nullptr) {\n+        return -1;\n+    }\n+    PyObject *ufunc = PyObject_GetItem(umath, name);\n+    Py_DECREF(name);\n+    if (ufunc == nullptr) {\n+        return -1;\n+    }\n+    spec->slots[0].pfunc = (void *)loop;\n+\n+    int res = PyUFunc_AddLoopFromSpec(ufunc, spec);\n+    Py_DECREF(ufunc);\n+    return res;\n+}\n+\n+\n+template<bool rstrip, typename character, COMP...>\n+struct add_loops;\n+\n+template<bool rstrip, typename character>\n+struct add_loops<rstrip, character> {\n+    int operator()(PyObject*, PyArrayMethod_Spec*) {\n+        return 0;\n+    }\n+};\n+\n+template<bool rstrip, typename character, COMP comp, COMP... comps>\n+struct add_loops<rstrip, character, comp, comps...> {\n+    int operator()(PyObject* umath, PyArrayMethod_Spec* spec) {\n+        PyArrayMethod_StridedLoop* loop = string_comparison_loop<rstrip, comp, character>;\n+\n+        if (add_loop(umath, comp_name(comp), spec, loop) < 0) {\n+            return -1;\n+        }\n+        else {\n+            return add_loops<rstrip, character, comps...>()(umath, spec);\n+        }\n+    }\n+};\n+\n+\n+NPY_NO_EXPORT int\n+init_string_ufuncs(PyObject *umath)\n+{\n+    int res = -1;\n+    /* NOTE: This should recieve global symbols? */\n+    PyArray_DTypeMeta *String = PyArray_DTypeFromTypeNum(NPY_STRING);\n+    PyArray_DTypeMeta *Unicode = PyArray_DTypeFromTypeNum(NPY_UNICODE);\n+    PyArray_DTypeMeta *Bool = PyArray_DTypeFromTypeNum(NPY_BOOL);\n+\n+    /* We start with the string loops: */\n+    PyArray_DTypeMeta *dtypes[] = {String, String, Bool};\n+    /*\n+     * We only have one loop right now, the strided one.  The default type\n+     * resolver ensures native byte order/canonical representation.\n+     */\n+    PyType_Slot slots[] = {\n+        {NPY_METH_strided_loop, nullptr},\n+        {0, nullptr}\n+    };\n+\n+    PyArrayMethod_Spec spec = {};\n+    spec.name = \"templated_string_comparison\";\n+    spec.nin = 2;\n+    spec.nout = 1;\n+    spec.dtypes = dtypes;\n+    spec.slots = slots;\n+    spec.flags = NPY_METH_NO_FLOATINGPOINT_ERRORS;\n+\n+    /* All String loops */\n+    using string_looper = add_loops<false, npy_byte, COMP::EQ, COMP::NE, COMP::LT, COMP::LE, COMP::GT, COMP::GE>;\n+    if (string_looper()(umath, &spec) < 0) {\n+        goto finish;\n+    }\n+\n+    /* All Unicode loops */\n+    using ucs_looper = add_loops<false, npy_ucs4, COMP::EQ, COMP::NE, COMP::LT, COMP::LE, COMP::GT, COMP::GE>;\n+    dtypes[0] = Unicode;\n+    dtypes[1] = Unicode;\n+    if (ucs_looper()(umath, &spec) < 0) {\n+        goto finish;\n+    }\n+\n+    res = 0;\n+  finish:\n+    Py_DECREF(String);\n+    Py_DECREF(Unicode);\n+    Py_DECREF(Bool);\n+    return res;\n+}\n+\n+\n+template <bool rstrip, typename character>\n+static PyArrayMethod_StridedLoop *\n+get_strided_loop(int comp)\n+{\n+    switch (comp) {\n+        case Py_EQ:\n+            return string_comparison_loop<rstrip, COMP::EQ, character>;\n+        case Py_NE:\n+            return string_comparison_loop<rstrip, COMP::NE, character>;\n+        case Py_LT:\n+            return string_comparison_loop<rstrip, COMP::LT, character>;\n+        case Py_LE:\n+            return string_comparison_loop<rstrip, COMP::LE, character>;\n+        case Py_GT:\n+            return string_comparison_loop<rstrip, COMP::GT, character>;\n+        case Py_GE:\n+            return string_comparison_loop<rstrip, COMP::GE, character>;\n+        default:\n+            assert(false);  /* caller ensures this */\n+    }\n+    return nullptr;\n+}\n+\n+\n+/*\n+ * This function is used for `compare_chararrays` and currently also void\n+ * comparisons (unstructured voids).  The first could probably be deprecated\n+ * and removed but is used by `np.char.chararray` the latter should also be\n+ * moved to the ufunc probably (removing the need for manual looping).\n+ *\n+ * The `rstrip` mechanism is presumably for some fortran compat, but the\n+ * question is whether it would not be better to have/use `rstrip` on such\n+ * an array first...\n+ *\n+ * NOTE: This function is also used for unstructured voids, this works because\n+ *       `npy_byte` is correct.\n+ */\n+NPY_NO_EXPORT PyObject *\n+_umath_strings_richcompare(\n+        PyArrayObject *self, PyArrayObject *other, int cmp_op, int rstrip)\n+{\n+    NpyIter *iter = nullptr;\n+    PyObject *result = nullptr;\n+\n+    char **dataptr = nullptr;\n+    npy_intp *strides = nullptr;\n+    npy_intp *countptr = nullptr;\n+    npy_intp size = 0;\n+\n+    PyArrayMethod_Context context = {};\n+    NpyIter_IterNextFunc *iternext = nullptr;\n+\n+    npy_uint32 it_flags = (\n+            NPY_ITER_EXTERNAL_LOOP | NPY_ITER_ZEROSIZE_OK |\n+            NPY_ITER_BUFFERED | NPY_ITER_GROWINNER);\n+    npy_uint32 op_flags[3] = {\n+            NPY_ITER_READONLY | NPY_ITER_ALIGNED,\n+            NPY_ITER_READONLY | NPY_ITER_ALIGNED,\n+            NPY_ITER_WRITEONLY | NPY_ITER_ALLOCATE | NPY_ITER_ALIGNED};\n+\n+    PyArrayMethod_StridedLoop *strided_loop = nullptr;\n+    NPY_BEGIN_THREADS_DEF;\n+\n+    if (PyArray_TYPE(self) != PyArray_TYPE(other)) {\n+        /*\n+         * Comparison between Bytes and Unicode is not defined in Py3K;\n+         * we follow.\n+         * TODO: This makes no sense at all for `compare_chararrays`, kept\n+         *       only under the assumption that we are more likely to deprecate\n+         *       than fix it to begin with.\n+         */\n+        Py_INCREF(Py_NotImplemented);\n+        return Py_NotImplemented;\n+    }\n+\n+    PyArrayObject *ops[3] = {self, other, nullptr};\n+    PyArray_Descr *descrs[3] = {nullptr, nullptr, PyArray_DescrFromType(NPY_BOOL)};\n+    /* TODO: ensuring native byte order is not really necessary for == and != */\n+    descrs[0] = NPY_DT_CALL_ensure_canonical(PyArray_DESCR(self));\n+    if (descrs[0] == nullptr) {\n+        goto finish;\n+    }\n+    descrs[1] = NPY_DT_CALL_ensure_canonical(PyArray_DESCR(other));\n+    if (descrs[1] == nullptr) {\n+        goto finish;\n+    }\n+\n+    /*\n+     * Create the iterator:\n+     */\n+    iter = NpyIter_AdvancedNew(\n+            3, ops, it_flags, NPY_KEEPORDER, NPY_SAFE_CASTING, op_flags, descrs,\n+            -1, nullptr, nullptr, 0);\n+    if (iter == nullptr) {\n+        goto finish;\n+    }\n+\n+    size = NpyIter_GetIterSize(iter);\n+    if (size == 0) {\n+        result = (PyObject *)NpyIter_GetOperandArray(iter)[2];\n+        Py_INCREF(result);\n+        goto finish;\n+    }\n+\n+    iternext = NpyIter_GetIterNext(iter, nullptr);\n+    if (iternext == nullptr) {\n+        goto finish;\n+    }\n+\n+    /*\n+     * Prepare the inner-loop and execute it (we only need descriptors to be\n+     * passed in).\n+     */\n+    context.descriptors = descrs;\n+\n+    dataptr = NpyIter_GetDataPtrArray(iter);\n+    strides = NpyIter_GetInnerStrideArray(iter);\n+    countptr = NpyIter_GetInnerLoopSizePtr(iter);\n+\n+    if (rstrip == 0) {\n+        /* NOTE: Also used for VOID, so can be STRING, UNICODE, or VOID: */\n+        if (descrs[0]->type_num != NPY_UNICODE) {\n+            strided_loop = get_strided_loop<false, npy_byte>(cmp_op);\n+        }\n+        else {\n+            strided_loop = get_strided_loop<false, npy_ucs4>(cmp_op);\n+        }\n+    }\n+    else {\n+        if (descrs[0]->type_num != NPY_UNICODE) {\n+            strided_loop = get_strided_loop<true, npy_byte>(cmp_op);\n+        }\n+        else {\n+            strided_loop = get_strided_loop<true, npy_ucs4>(cmp_op);\n+        }\n+    }\n+\n+    NPY_BEGIN_THREADS_THRESHOLDED(size);\n+\n+    do {\n+         /* We know the loop cannot fail */\n+         strided_loop(&context, dataptr, countptr, strides, nullptr);\n+    } while (iternext(iter) != 0);\n+\n+    NPY_END_THREADS;\n+\n+    result = (PyObject *)NpyIter_GetOperandArray(iter)[2];\n+    Py_INCREF(result);\n+\n+ finish:\n+    if (NpyIter_Deallocate(iter) < 0) {\n+        Py_CLEAR(result);\n+    }\n+    Py_XDECREF(descrs[0]);\n+    Py_XDECREF(descrs[1]);\n+    Py_XDECREF(descrs[2]);\n+    return result;\n+}"
            },
            {
                "filename": "numpy/core/src/umath/string_ufuncs.h",
                "patch": "@@ -0,0 +1,19 @@\n+#ifndef _NPY_CORE_SRC_UMATH_STRING_UFUNCS_H_\n+#define _NPY_CORE_SRC_UMATH_STRING_UFUNCS_H_\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+NPY_NO_EXPORT int\n+init_string_ufuncs(PyObject *umath);\n+\n+NPY_NO_EXPORT PyObject *\n+_umath_strings_richcompare(\n+        PyArrayObject *self, PyArrayObject *other, int cmp_op, int rstrip);\n+\n+#ifdef __cplusplus\n+}\n+#endif\n+\n+#endif  /* _NPY_CORE_SRC_UMATH_STRING_UFUNCS_H_ */\n\\ No newline at end of file"
            },
            {
                "filename": "numpy/core/src/umath/umathmodule.c",
                "patch": "@@ -23,11 +23,13 @@\n #include \"numpy/npy_math.h\"\n #include \"number.h\"\n #include \"dispatching.h\"\n+#include \"string_ufuncs.h\"\n \n /* Automatically generated code to define all ufuncs: */\n #include \"funcs.inc\"\n #include \"__umath_generated.c\"\n \n+\n static PyUFuncGenericFunction pyfunc_functions[] = {PyUFunc_On_Om};\n \n static int\n@@ -342,5 +344,10 @@ int initumath(PyObject *m)\n     if (install_logical_ufunc_promoter(s) < 0) {\n         return -1;\n     }\n+\n+    if (init_string_ufuncs(d) < 0) {\n+        return -1;\n+    }\n+\n     return 0;\n }"
            },
            {
                "filename": "numpy/core/tests/test_deprecations.py",
                "patch": "@@ -166,7 +166,7 @@ def test_string(self):\n         # For two string arrays, strings always raised the broadcasting error:\n         a = np.array(['a', 'b'])\n         b = np.array(['a', 'b', 'c'])\n-        assert_raises(ValueError, lambda x, y: x == y, a, b)\n+        assert_warns(FutureWarning, lambda x, y: x == y, a, b)\n \n         # The empty list is not cast to string, and this used to pass due\n         # to dtype mismatch; now (2018-06-21) it correctly leads to a"
            },
            {
                "filename": "numpy/core/tests/test_strings.py",
                "patch": "@@ -0,0 +1,85 @@\n+import pytest\n+\n+import operator\n+import numpy as np\n+\n+from numpy.testing import assert_array_equal\n+\n+\n+COMPARISONS = [\n+    (operator.eq, np.equal, \"==\"),\n+    (operator.ne, np.not_equal, \"!=\"),\n+    (operator.lt, np.less, \"<\"),\n+    (operator.le, np.less_equal, \"<=\"),\n+    (operator.gt, np.greater, \">\"),\n+    (operator.ge, np.greater_equal, \">=\"),\n+]\n+\n+\n+@pytest.mark.parametrize([\"op\", \"ufunc\", \"sym\"], COMPARISONS)\n+def test_mixed_string_comparison_ufuncs_fail(op, ufunc, sym):\n+    arr_string = np.array([\"a\", \"b\"], dtype=\"S\")\n+    arr_unicode = np.array([\"a\", \"c\"], dtype=\"U\")\n+\n+    with pytest.raises(TypeError, match=\"did not contain a loop\"):\n+        ufunc(arr_string, arr_unicode)\n+\n+    with pytest.raises(TypeError, match=\"did not contain a loop\"):\n+        ufunc(arr_unicode, arr_string)\n+\n+@pytest.mark.parametrize([\"op\", \"ufunc\", \"sym\"], COMPARISONS)\n+def test_mixed_string_comparisons_ufuncs_with_cast(op, ufunc, sym):\n+    arr_string = np.array([\"a\", \"b\"], dtype=\"S\")\n+    arr_unicode = np.array([\"a\", \"c\"], dtype=\"U\")\n+\n+    # While there is no loop, manual casting is acceptable:\n+    res1 = ufunc(arr_string, arr_unicode, signature=\"UU->?\", casting=\"unsafe\")\n+    res2 = ufunc(arr_string, arr_unicode, signature=\"SS->?\", casting=\"unsafe\")\n+\n+    expected = op(arr_string.astype('U'), arr_unicode)\n+    assert_array_equal(res1, expected)\n+    assert_array_equal(res2, expected)\n+\n+\n+@pytest.mark.parametrize([\"op\", \"ufunc\", \"sym\"], COMPARISONS)\n+@pytest.mark.parametrize(\"dtypes\", [\n+        (\"S2\", \"S2\"), (\"S2\", \"S10\"),\n+        (\"<U1\", \"<U1\"), (\"<U1\", \">U1\"), (\">U1\", \">U1\"),\n+        (\"<U1\", \"<U10\"), (\"<U1\", \">U10\")])\n+@pytest.mark.parametrize(\"aligned\", [True, False])\n+def test_string_comparisons(op, ufunc, sym, dtypes, aligned):\n+    # ensure native byte-order for the first view to stay within unicode range\n+    native_dt = np.dtype(dtypes[0]).newbyteorder(\"=\")\n+    arr = np.arange(2**15).view(native_dt).astype(dtypes[0])\n+    if not aligned:\n+        # Make `arr` unaligned:\n+        new = np.zeros(arr.nbytes + 1, dtype=np.uint8)[1:].view(dtypes[0])\n+        new[...] = arr\n+        arr = new\n+\n+    arr2 = arr.astype(dtypes[1], copy=True)\n+    np.random.shuffle(arr2)\n+    arr[0] = arr2[0]  # make sure one matches\n+\n+    expected = [op(d1, d2) for d1, d2 in zip(arr.tolist(), arr2.tolist())]\n+    assert_array_equal(op(arr, arr2), expected)\n+    assert_array_equal(ufunc(arr, arr2), expected)\n+    assert_array_equal(np.compare_chararrays(arr, arr2, sym, False), expected)\n+\n+    expected = [op(d2, d1) for d1, d2 in zip(arr.tolist(), arr2.tolist())]\n+    assert_array_equal(op(arr2, arr), expected)\n+    assert_array_equal(ufunc(arr2, arr), expected)\n+    assert_array_equal(np.compare_chararrays(arr2, arr, sym, False), expected)\n+\n+\n+@pytest.mark.parametrize([\"op\", \"ufunc\", \"sym\"], COMPARISONS)\n+@pytest.mark.parametrize(\"dtypes\", [\n+        (\"S2\", \"S2\"), (\"S2\", \"S10\"), (\"<U1\", \"<U1\"), (\"<U1\", \">U10\")])\n+def test_string_comparisons_empty(op, ufunc, sym, dtypes):\n+    arr = np.empty((1, 0, 1, 5), dtype=dtypes[0])\n+    arr2 = np.empty((100, 1, 0, 1), dtype=dtypes[1])\n+\n+    expected = np.empty(np.broadcast_shapes(arr.shape, arr2.shape), dtype=bool)\n+    assert_array_equal(op(arr, arr2), expected)\n+    assert_array_equal(ufunc(arr, arr2), expected)\n+    assert_array_equal(np.compare_chararrays(arr, arr2, sym, False), expected)"
            },
            {
                "filename": "numpy/core/tests/test_unicode.py",
                "patch": "@@ -1,3 +1,5 @@\n+import pytest\n+\n import numpy as np\n from numpy.testing import assert_, assert_equal, assert_array_equal\n \n@@ -33,8 +35,11 @@ def test_string_cast():\n     uni_arr1 = str_arr.astype('>U')\n     uni_arr2 = str_arr.astype('<U')\n \n-    assert_(str_arr != uni_arr1)\n-    assert_(str_arr != uni_arr2)\n+    with pytest.warns(FutureWarning):\n+        assert str_arr != uni_arr1\n+    with pytest.warns(FutureWarning):\n+        assert str_arr != uni_arr2\n+\n     assert_array_equal(uni_arr1, uni_arr2)\n \n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20913,
        "body": "#### Extend universal intrinsics to support IBMZ\r\n\r\nIt covers SIMD operations for all datatypes starting\r\nfrom z/Arch11 a.k.a IBM Z13, except for single-precision\r\nwhich requires minimum z/Arch12 a.k.a IBMZ 14 to be dispatched.\r\n\r\nThis patch rename the branch /simd/vsx to /simd/vec, the new\r\nthe path holds the definitions of universal intrinsics for\r\nboth Power and Z architectures.\r\n\r\nThis patch also adds new preprocessor identifiers:\r\n\r\n  - NPY_SIMD_BIGENDIAN: 1 if the enabled SIMD extension\r\n  is running on big-endian mode otherwise 0.\r\n\r\n  - NPY_SIMD_F32: 1 if the enabled SIMD extension\r\n  supports single-precision otherwise 0.\r\n\r\n\r\nTODO:\r\n- [x] release note\r\n- [x] benchmark \r\n\r\n### Benchmark\r\nThe following benchmark is inferential and does not accurately reflect the true change, since it was accomplished using an unstable VM. \r\n\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:                    s390x\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Big Endian\r\nCPU(s):                          2\r\nOn-line CPU(s) list:             0,1\r\nThread(s) per core:              1\r\nCore(s) per socket:              1\r\nSocket(s) per book:              1\r\nBook(s) per drawer:              1\r\nDrawer(s):                       2\r\nNUMA node(s):                    1\r\nVendor ID:                       IBM/S390\r\nMachine type:                    8561\r\nCPU dynamic MHz:                 5200\r\nCPU static MHz:                  5200\r\nBogoMIPS:                        3241.00\r\nHypervisor:                      z/VM 7.1.0\r\nHypervisor vendor:               IBM\r\nVirtualization type:             full\r\nDispatching mode:                horizontal\r\nL1d cache:                       256 KiB\r\nL1i cache:                       256 KiB\r\nL2d cache:                       8 MiB\r\nL2i cache:                       8 MiB\r\nL3 cache:                        256 MiB\r\nL4 cache:                        960 MiB\r\nNUMA node0 CPU(s):               0,1\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Not affected\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; etokens\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           esan3 zarch stfle msa ldisp eimm dfp edat etf3eh highgprs te\r\n                                 vx vxd vxe gs vxe2 vxp sort dflt sie\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux numpy 5.4.0-104-generic #118-Ubuntu SMP Wed Mar 2 19:02:13 UTC 2022 s390x s390x s390x GNU/Linux\r\nPython 3.8.10\r\ngcc (Ubuntu 11.1.0-1ubuntu1~20.04) 11.1.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n<details>\r\n <summary>VXE2</summary>\r\n\r\n```Bash\r\nunset NPY_DISABLE_CPU_FEATURES\r\npython runtests.py --bench-compare parent/main\r\n```\r\n```Bash\r\n    before           after         ratio\r\n     [982fcd38]       [47d54c6d]\r\n     <zsystem_sup~5>       <zsystem_sup>\r\n+     8.41\u00b10.03\u03bcs       10.2\u00b10.3\u03bcs     1.21  bench_function_base.Sort.time_sort('merge', 'int64', ('uniform',))\r\n+      82.8\u00b10.4\u03bcs       99.0\u00b10.3\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'h')\r\n+      83.0\u00b10.3\u03bcs       99.2\u00b10.3\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'h')\r\n+      83.5\u00b10.7\u03bcs       99.7\u00b10.7\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'h')\r\n+      84.3\u00b10.8\u03bcs        101\u00b10.5\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'h')\r\n+      83.1\u00b10.2\u03bcs       99.2\u00b10.2\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'h')\r\n+      85.0\u00b10.7\u03bcs        101\u00b10.4\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'h')\r\n+      83.1\u00b10.4\u03bcs       99.0\u00b10.3\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'h')\r\n+     10.3\u00b10.03\u03bcs      12.2\u00b10.06\u03bcs     1.19  bench_function_base.Sort.time_sort('merge', 'float32', ('ordered',))\r\n+      83.7\u00b10.5\u03bcs       9.2\u00b10.4\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'h')\r\n+       127\u00b10.4\u03bcs          149\u00b13\u03bcs     1.17  bench_function_base.Sort.time_argsort('quick', 'int32', ('reversed',))\r\n+         322\u00b11\u03bcs          364\u00b12\u03bcs     1.13  bench_function_base.Sort.time_argsort('quick', 'int64', ('sorted_block', 1000))\r\n+        87.6\u00b13\u03bcs       99.0\u00b10.8\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'b')\r\n+         249\u00b11\u03bcs          281\u00b12\u03bcs     1.13  bench_core.CountNonzero.time_count_nonzero(3, 10000, <class 'str'>)\r\n+      87.4\u00b10.8\u03bcs       98.6\u00b10.2\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'b')\r\n+         131\u00b15\u03bcs          148\u00b12\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'q')\r\n+     11.9\u00b10.08\u03bcs       13.4\u00b10.3\u03bcs     1.13  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 1, 'D')\r\n+      87.6\u00b10.5\u03bcs       98.5\u00b10.2\u03bcs     1.12  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'b')\r\n+      13.3\u00b10.2\u03bcs       14.9\u00b10.2\u03bcs     1.12  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 2, 'D')\r\n+     1.47\u00b10.02ms      1.64\u00b10.05ms     1.12  bench_lib.Pad.time_pad((256, 128, 1), 8, 'edge')\r\n+       167\u00b10.9\u03bcs          186\u00b14\u03bcs     1.11  bench_core.CountNonzero.time_count_nonzero(2, 10000, <class 'str'>)\r\n+         113\u00b13\u03bcs          126\u00b12\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'Q')\r\n+      5.71\u00b10.2ms       6.32\u00b10.2ms     1.11  bench_lib.Pad.time_pad((4194304,), 1, 'mean')\r\n+         283\u00b17\u03bcs         312\u00b110\u03bcs     1.10  bench_core.PackBits.time_packbits_axis0(<class 'bool'>)\r\n+         329\u00b13\u03bcs          363\u00b12\u03bcs     1.10  bench_function_base.Sort.time_argsort('quick', 'int32', ('sorted_block', 1000))\r\n+      70.6\u00b10.5\u03bcs       77.9\u00b10.7\u03bcs     1.10  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 10000)\r\n+         114\u00b11\u03bcs          126\u00b13\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'L')\r\n+      87.7\u00b10.9\u03bcs       96.6\u00b10.2\u03bcs     1.10  bench_function_base.Sort.time_sort('merge', 'float64', ('sorted_block', 100))\r\n+       299\u00b10.8\u03bcs          329\u00b11\u03bcs     1.10  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 1000))\r\n+      79.2\u00b10.2\u03bcs       87.1\u00b10.1\u03bcs     1.10  bench_function_base.Where.time_interleaved_zeros_x8\r\n+      61.0\u00b10.5\u03bcs       67.0\u00b10.5\u03bcs     1.10  bench_function_base.Select.time_select\r\n+         117\u00b15\u03bcs          129\u00b13\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'q')\r\n+      82.3\u00b10.4\u03bcs         89.9\u00b12\u03bcs     1.09  bench_core.CountNonzero.time_count_nonzero(1, 10000, <class 'str'>)\r\n+      52.9\u00b10.6\u03bcs       57.5\u00b10.3\u03bcs     1.09  bench_function_base.Sort.time_argsort('merge', 'int16', ('sorted_block', 10))\r\n+         195\u00b11\u03bcs        212\u00b10.6\u03bcs     1.09  bench_function_base.Sort.time_sort('merge', 'int32', ('sorted_block', 10))\r\n+     3.33\u00b10.04ms       3.60\u00b10.3ms     1.08  bench_lib.Pad.time_pad((1, 1, 1, 1, 1), 8, 'edge')\r\n+      7.01\u00b10.1ms       7.57\u00b10.2ms     1.08  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 1000000)\r\n+      5.26\u00b10.1ms       5.68\u00b10.2ms     1.08  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('str', 10000)\r\n+         138\u00b14\u03bcs          148\u00b13\u03bcs     1.08  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'L')\r\n+      20.5\u00b10.4ms       22.0\u00b10.3ms     1.07  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('object', 100000)\r\n+         408\u00b12\u03bcs        438\u00b10.8\u03bcs     1.07  bench_function_base.Sort.time_argsort('quick', 'int64', ('sorted_block', 100))\r\n+         184\u00b12\u03bcs          197\u00b18\u03bcs     1.07  bench_lib.Pad.time_pad((4, 4, 4, 4), 8, 'constant')\r\n+        52.7\u00b12ms         56.4\u00b12ms     1.07  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('str', 100000)\r\n+         121\u00b12\u03bcs          129\u00b13\u03bcs     1.07  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'L')\r\n+       104\u00b10.7\u03bcs        111\u00b10.3\u03bcs     1.07  bench_function_base.Sort.time_sort('quick', 'float32', ('uniform',))\r\n+       368\u00b10.7\u03bcs          393\u00b11\u03bcs     1.07  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 100))\r\n+         510\u00b15\u03bcs          544\u00b13\u03bcs     1.06  bench_function_base.Sort.time_argsort('heap', 'float64', ('ordered',))\r\n+     1.80\u00b10.02ms      1.91\u00b10.03ms     1.06  bench_core.CountNonzero.time_count_nonzero(3, 1000000, <class 'numpy.int64'>)\r\n+     3.44\u00b10.03ms      3.65\u00b10.07ms     1.06  bench_core.CountNonzero.time_count_nonzero_axis(2, 1000000, <class 'numpy.int64'>)\r\n+         534\u00b12\u03bcs         566\u00b110\u03bcs     1.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'exp2'>, 4, 4, 'd')\r\n+      49.3\u00b10.4\u03bcs       52.3\u00b10.4\u03bcs     1.06  bench_function_base.Sort.time_sort('merge', 'float32', ('sorted_block', 1000))\r\n+         141\u00b14\u03bcs          149\u00b13\u03bcs     1.06  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'l')\r\n+         121\u00b13\u03bcs          129\u00b14\u03bcs     1.06  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'q')\r\n+      93.3\u00b10.7\u03bcs         98.8\u00b11\u03bcs     1.06  bench_function_base.Sort.time_argsort('merge', 'int64', ('sorted_block', 100))\r\n+        942\u00b130\u03bcs         997\u00b120\u03bcs     1.06  bench_reduce.AddReduceSeparate.time_reduce(1, 'int16')\r\n+         410\u00b16\u03bcs        434\u00b10.9\u03bcs     1.06  bench_function_base.Sort.time_argsort('quick', 'int32', ('sorted_block', 100))\r\n+         531\u00b12\u03bcs          561\u00b12\u03bcs     1.06  bench_function_base.Sort.time_argsort('heap', 'float32', ('ordered',))\r\n+         526\u00b11\u03bcs         556\u00b110\u03bcs     1.06  bench_function_base.Sort.time_argsort('merge', 'int64', ('random',))\r\n+        75.5\u00b11\u03bcs         79.8\u00b12\u03bcs     1.06  bench_function_base.Sort.time_argsort('quick', 'uint32', ('ordered',))\r\n+      57.0\u00b10.7\u03bcs       60.2\u00b10.4\u03bcs     1.06  bench_function_base.Sort.time_argsort('merge', 'int16', ('sorted_block', 100))\r\n+         534\u00b17\u03bcs          562\u00b14\u03bcs     1.05  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'exp2'>, 4, 2, 'd')\r\n+       727\u00b10.9\u03bcs         765\u00b130\u03bcs     1.05  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'log'>, 2, 2, 'd')\r\n+        683\u00b110\u03bcs          719\u00b14\u03bcs     1.05  bench_lib.Pad.time_pad((256, 128, 1), 8, 'constant')\r\n+        46.0\u00b12\u03bcs       48.4\u00b10.2\u03bcs     1.05  bench_function_base.Sort.time_argsort('heap', 'int64', ('uniform',))\r\n+         121\u00b11\u03bcs          128\u00b14\u03bcs     1.05  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'Q')\r\n+     2.10\u00b10.02ms      2.21\u00b10.03ms     1.05  bench_lib.Pad.time_pad((256, 128, 1), (0, 32), 'mean')\r\n+         237\u00b11ms          249\u00b12ms     1.05  bench_function_base.Histogram2D.time_fine_binning\r\n+     1.20\u00b10.01ms      1.26\u00b10.01ms     1.05  bench_core.CountNonzero.time_count_nonzero(2, 1000000, <class 'numpy.int64'>)\r\n-      5.47\u00b10.1\u03bcs      5.21\u00b10.05\u03bcs     0.95  bench_ma.Indexing.time_1d(False, 1, 10)\r\n-     1.50\u00b10.02ms      1.43\u00b10.01ms     0.95  bench_lib.Nan.time_nanargmin(200000, 50.0)\r\n-      30.5\u00b10.1\u03bcs       29.1\u00b10.2\u03bcs     0.95  bench_function_base.Median.time_odd_inplace\r\n-      88.8\u00b10.7\u03bcs       84.6\u00b10.4\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'H')\r\n-      13.7\u00b10.1\u03bcs      13.1\u00b10.08\u03bcs     0.95  bench_ma.UFunc.time_2d(False, False, 10)\r\n-         696\u00b14\u03bcs        662\u00b10.5\u03bcs     0.95  bench_function_base.Sort.time_sort('heap', 'float32', ('sorted_block', 10))\r\n-      36.9\u00b10.1\u03bcs       35.1\u00b10.1\u03bcs     0.95  bench_core.CountNonzero.time_count_nonzero_axis(2, 10000, <class 'numpy.int8'>)\r\n-      72.6\u00b10.6\u03bcs         69.0\u00b11\u03bcs     0.95  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 10000)\r\n-      90.9\u00b10.6\u03bcs       86.3\u00b10.8\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'H')\r\n-      88.3\u00b10.3\u03bcs       83.9\u00b10.4\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'H')\r\n-       473\u00b10.7\u03bcs          449\u00b11\u03bcs     0.95  bench_function_base.Sort.time_argsort('quick', 'int16', ('sorted_block', 10))\r\n-      88.3\u00b10.3\u03bcs       83.8\u00b10.5\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'H')\r\n-      38.8\u00b10.9\u03bcs      36.8\u00b10.07\u03bcs     0.95  bench_linalg.Linalg.time_op('norm', 'complex64')\r\n-      88.9\u00b10.7\u03bcs         84.2\u00b11\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'H')\r\n-      92.0\u00b10.1\u03bcs         87.1\u00b11\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'H')\r\n-      5.79\u00b10.1\u03bcs      5.48\u00b10.01\u03bcs     0.95  bench_ma.Indexing.time_1d(True, 1, 1000)\r\n-      87.2\u00b10.3\u03bcs       82.5\u00b10.4\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'H')\r\n-     7.23\u00b10.05ms      6.85\u00b10.03ms     0.95  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 1000000)\r\n-     2.34\u00b10.02ms      2.21\u00b10.02ms     0.95  bench_lib.Nan.time_nanstd(200000, 90.0)\r\n-      5.46\u00b10.1\u03bcs      5.17\u00b10.05\u03bcs     0.95  bench_ma.Indexing.time_1d(False, 2, 10)\r\n-      5.46\u00b10.1\u03bcs       5.16\u00b10.1\u03bcs     0.95  bench_ma.Indexing.time_1d(False, 2, 1000)\r\n-     5.86\u00b10.05\u03bcs      5.54\u00b10.02\u03bcs     0.95  bench_indexing.ScalarIndexing.time_assign(0)\r\n-      5.45\u00b10.1\u03bcs       5.15\u00b10.1\u03bcs     0.95  bench_ma.Indexing.time_0d(False, 2, 10)\r\n-      5.80\u00b10.1\u03bcs      5.48\u00b10.04\u03bcs     0.94  bench_ma.Indexing.time_0d(True, 1, 10)\r\n-      88.5\u00b10.6\u03bcs       83.6\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'H')\r\n-      87.7\u00b10.4\u03bcs       82.8\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'H')\r\n-      5.83\u00b10.1\u03bcs      5.50\u00b10.04\u03bcs     0.94  bench_ma.Indexing.time_0d(True, 1, 100)\r\n-      90.6\u00b10.3\u03bcs       85.5\u00b10.8\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'H')\r\n-       151\u00b10.8\u03bcs        143\u00b10.5\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'int16', ('reversed',))\r\n-      89.5\u00b10.5\u03bcs       84.4\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'H')\r\n-     2.33\u00b10.02ms      2.20\u00b10.01ms     0.94  bench_lib.Nan.time_nanvar(200000, 90.0)\r\n-      87.8\u00b10.3\u03bcs       82.8\u00b10.4\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'H')\r\n-      18.6\u00b10.2\u03bcs       17.5\u00b10.2\u03bcs     0.94  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 4, 'D')\r\n-      92.8\u00b10.9\u03bcs         87.4\u00b11\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'H')\r\n-      89.9\u00b10.5\u03bcs       84.7\u00b10.6\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'H')\r\n-      90.1\u00b10.4\u03bcs       84.8\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'H')\r\n-      89.9\u00b10.4\u03bcs       84.6\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'H')\r\n-      5.73\u00b10.1\u03bcs      5.40\u00b10.03\u03bcs     0.94  bench_ma.Indexing.time_1d(True, 1, 100)\r\n-        89.3\u00b11\u03bcs         83.9\u00b11\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'H')\r\n-      91.4\u00b10.5\u03bcs       85.9\u00b10.8\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'H')\r\n-      5.87\u00b10.1\u03bcs      5.52\u00b10.05\u03bcs     0.94  bench_ma.Indexing.time_0d(True, 2, 10)\r\n-      78.0\u00b10.8\u03bcs       73.2\u00b10.2\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'int32', ('ordered',))\r\n-        90.1\u00b11\u03bcs       84.7\u00b10.7\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'H')\r\n-      88.1\u00b10.4\u03bcs       82.7\u00b10.2\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'H')\r\n-         817\u00b14\u03bcs          767\u00b12\u03bcs     0.94  bench_ufunc.UFunc.time_ufunc_types('reciprocal')\r\n-     1.42\u00b10.01ms      1.33\u00b10.01ms     0.94  bench_lib.Nan.time_nanvar(200000, 0.1)\r\n-      90.2\u00b10.3\u03bcs       84.6\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'H')\r\n-         491\u00b13\u03bcs          461\u00b12\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'int16', ('sorted_block', 100))\r\n-        91.9\u00b11\u03bcs         86.1\u00b11\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'H')\r\n-      5.89\u00b10.2\u03bcs      5.52\u00b10.06\u03bcs     0.94  bench_ma.Indexing.time_1d(True, 2, 100)\r\n-      5.49\u00b10.2\u03bcs      5.15\u00b10.05\u03bcs     0.94  bench_ma.Indexing.time_1d(False, 1, 100)\r\n-        91.4\u00b11\u03bcs       85.6\u00b10.2\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'H')\r\n-         355\u00b16\u03bcs          332\u00b12\u03bcs     0.94  bench_ufunc.UFunc.time_ufunc_types('square')\r\n-      87.8\u00b10.5\u03bcs       82.2\u00b10.2\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'H')\r\n-      90.2\u00b10.8\u03bcs       84.5\u00b10.4\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'H')\r\n-     5.46\u00b10.05\u03bcs       5.11\u00b10.1\u03bcs     0.94  bench_ma.Indexing.time_0d(False, 1, 100)\r\n-     1.50\u00b10.01ms      1.41\u00b10.01ms     0.94  bench_lib.Nan.time_nanstd(200000, 2.0)\r\n-     5.82\u00b10.04\u03bcs      5.44\u00b10.02\u03bcs     0.94  bench_ma.Indexing.time_0d(True, 1, 1000)\r\n-      87.6\u00b10.3\u03bcs       81.9\u00b10.2\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'H')\r\n-      89.7\u00b10.2\u03bcs       83.8\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'H')\r\n-     87.6\u00b10.09\u03bcs       81.9\u00b10.1\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'H')\r\n-      91.2\u00b10.9\u03bcs       85.2\u00b10.9\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'H')\r\n-      90.5\u00b10.9\u03bcs       84.5\u00b10.8\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'H')\r\n-      91.2\u00b10.7\u03bcs         85.2\u00b11\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'H')\r\n-     5.46\u00b10.07\u03bcs      5.10\u00b10.09\u03bcs     0.93  bench_ma.Indexing.time_0d(False, 2, 1000)\r\n-     1.42\u00b10.01ms      1.33\u00b10.02ms     0.93  bench_lib.Nan.time_nanvar(200000, 0)\r\n-      92.3\u00b10.5\u03bcs       86.1\u00b10.7\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'H')\r\n-     5.50\u00b10.07\u03bcs      5.13\u00b10.08\u03bcs     0.93  bench_ma.Indexing.time_0d(False, 2, 100)\r\n-      91.3\u00b10.3\u03bcs       85.1\u00b10.6\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'H')\r\n-        89.8\u00b11\u03bcs       83.7\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'H')\r\n-         466\u00b14\u03bcs          434\u00b11\u03bcs     0.93  bench_function_base.Sort.time_sort('merge', 'uint32', ('random',))\r\n-         277\u00b13\u03bcs          258\u00b11\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 1, 'd')\r\n-      90.9\u00b10.5\u03bcs       84.6\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'H')\r\n-      94.1\u00b10.8\u03bcs       87.6\u00b10.6\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 4, 'H')\r\n-      89.1\u00b10.6\u03bcs       82.9\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'H')\r\n-      17.8\u00b10.2\u03bcs      16.5\u00b10.09\u03bcs     0.93  bench_core.VarComplex.time_var(1000)\r\n-         275\u00b13\u03bcs          256\u00b13\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 1, 'd')\r\n-       273\u00b10.5\u03bcs          254\u00b12\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 2, 'd')\r\n-      90.0\u00b10.2\u03bcs       83.6\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'H')\r\n-      89.4\u00b10.6\u03bcs       83.1\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'H')\r\n-      93.0\u00b10.7\u03bcs       86.4\u00b10.6\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'H')\r\n-      89.9\u00b10.4\u03bcs       83.4\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'H')\r\n-         272\u00b12\u03bcs          253\u00b15\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 2, 'd')\r\n-     1.42\u00b10.01ms      1.32\u00b10.01ms     0.93  bench_lib.Nan.time_nanstd(200000, 0.1)\r\n-      88.4\u00b10.4\u03bcs       82.0\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'H')\r\n-      5.95\u00b10.1\u03bcs      5.52\u00b10.08\u03bcs     0.93  bench_ma.Indexing.time_1d(True, 2, 10)\r\n-      91.4\u00b10.5\u03bcs       84.8\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'H')\r\n-         274\u00b12\u03bcs          254\u00b12\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 1, 'd')\r\n-     5.48\u00b10.04\u03bcs      5.07\u00b10.04\u03bcs     0.93  bench_ma.Indexing.time_0d(False, 1, 10)\r\n-     5.94\u00b10.08\u03bcs      5.49\u00b10.02\u03bcs     0.93  bench_ma.Indexing.time_0d(True, 2, 1000)\r\n-      90.0\u00b10.6\u03bcs       83.1\u00b10.6\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'H')\r\n-      93.6\u00b10.6\u03bcs       86.4\u00b10.7\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 4, 'H')\r\n-      92.6\u00b10.5\u03bcs       85.5\u00b10.9\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'H')\r\n-         276\u00b13\u03bcs        253\u00b10.8\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 4, 'd')\r\n-     1.51\u00b10.01ms      1.39\u00b10.01ms     0.92  bench_lib.Nan.time_nanvar(200000, 2.0)\r\n-     1.86\u00b10.01\u03bcs         1.71\u00b10\u03bcs     0.92  bench_itemselection.Take.time_contiguous((1000, 1), 'clip', 'int64')\r\n-      5.59\u00b10.2\u03bcs      5.14\u00b10.04\u03bcs     0.92  bench_ma.Indexing.time_1d(False, 1, 1000)\r\n-     1.87\u00b10.01\u03bcs      1.72\u00b10.03\u03bcs     0.92  bench_itemselection.Take.time_contiguous((1000, 1), 'clip', 'float32')\r\n-      5.91\u00b10.1\u03bcs      5.42\u00b10.05\u03bcs     0.92  bench_ma.Indexing.time_1d(True, 1, 10)\r\n-      5.97\u00b10.1\u03bcs      5.47\u00b10.04\u03bcs     0.92  bench_ma.Indexing.time_0d(True, 2, 100)\r\n-      76.1\u00b10.3\u03bcs       69.7\u00b10.6\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'I')\r\n-         248\u00b12ms          227\u00b11ms     0.92  bench_app.LaplaceInplace.time_it('normal')\r\n-     6.08\u00b10.02\u03bcs      5.56\u00b10.04\u03bcs     0.92  bench_itemselection.PutMask.time_dense(False, 'complex256')\r\n-     1.87\u00b10.01\u03bcs      1.70\u00b10.01\u03bcs     0.91  bench_itemselection.Take.time_contiguous((1000, 1), 'clip', 'float64')\r\n-     1.89\u00b10.01\u03bcs      1.72\u00b10.02\u03bcs     0.91  bench_itemselection.Take.time_contiguous((1000, 1), 'clip', 'int32')\r\n-     4.47\u00b10.08\u03bcs      4.08\u00b10.02\u03bcs     0.91  bench_ma.MA.time_masked_array\r\n-        93.7\u00b12\u03bcs       85.4\u00b10.5\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'H')\r\n-         357\u00b12\u03bcs          325\u00b11\u03bcs     0.91  bench_function_base.Sort.time_argsort('quick', 'uint32', ('sorted_block', 1000))\r\n-      75.7\u00b10.4\u03bcs       69.0\u00b10.8\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'i')\r\n-      77.6\u00b10.8\u03bcs       70.6\u00b10.4\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'I')\r\n-      75.4\u00b10.3\u03bcs       68.6\u00b10.2\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'I')\r\n-      82.5\u00b10.7\u03bcs         74.9\u00b13\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'Q')\r\n-     81.9\u00b10.09\u03bcs         74.3\u00b11\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'Q')\r\n-      75.4\u00b10.8\u03bcs       68.4\u00b10.5\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'I')\r\n-      75.6\u00b10.3\u03bcs       68.6\u00b10.6\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'i')\r\n-     1.91\u00b10.02\u03bcs      1.73\u00b10.02\u03bcs     0.91  bench_itemselection.Take.time_contiguous((1000, 2), 'clip', 'int16')\r\n-      75.4\u00b10.5\u03bcs       68.2\u00b10.4\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'I')\r\n-     1.16\u00b10.03\u03bcs      1.05\u00b10.01\u03bcs     0.90  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 100)\r\n-        944\u00b120\u03bcs          853\u00b15\u03bcs     0.90  bench_lib.Nan.time_nanargmax(200000, 90.0)\r\n-     3.28\u00b10.03\u03bcs         2.96\u00b10\u03bcs     0.90  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'clip', 'float64')\r\n-        87.9\u00b11\u03bcs         79.3\u00b11\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'l')\r\n-      75.6\u00b10.3\u03bcs       68.2\u00b10.2\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'i')\r\n-     4.02\u00b10.04ms      3.63\u00b10.04ms     0.90  bench_core.VarComplex.time_var(1000000)\r\n-     1.88\u00b10.01\u03bcs      1.70\u00b10.02\u03bcs     0.90  bench_itemselection.Take.time_contiguous((1000, 2), 'clip', 'float16')\r\n-      76.8\u00b10.5\u03bcs       69.1\u00b10.2\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'I')\r\n-      76.7\u00b10.3\u03bcs       69.0\u00b10.4\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'I')\r\n-      75.8\u00b10.5\u03bcs       68.1\u00b10.5\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'i')\r\n-      76.4\u00b10.3\u03bcs       68.7\u00b10.2\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'I')\r\n-         563\u00b14\u03bcs        506\u00b10.7\u03bcs     0.90  bench_function_base.Sort.time_sort('heap', 'float32', ('reversed',))\r\n-     1.89\u00b10.02\u03bcs      1.69\u00b10.01\u03bcs     0.90  bench_itemselection.Take.time_contiguous((1000, 2), 'clip', 'float32')\r\n-         107\u00b14\u03bcs         96.4\u00b12\u03bcs     0.90  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 4, 'd')\r\n-        89.7\u00b12\u03bcs         80.6\u00b12\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'L')\r\n-     1.90\u00b10.02\u03bcs         1.71\u00b10\u03bcs     0.90  bench_itemselection.Take.time_contiguous((1000, 1), 'clip', 'complex64')\r\n-        76.7\u00b11\u03bcs       68.8\u00b10.2\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'I')\r\n-         936\u00b15\u03bcs         839\u00b110\u03bcs     0.90  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-     1.45\u00b10.06ms      1.30\u00b10.04ms     0.90  bench_lib.Pad.time_pad((1024, 1024), (0, 32), 'mean')\r\n-      79.2\u00b10.6\u03bcs       71.0\u00b10.8\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'i')\r\n-         480\u00b12\u03bcs          430\u00b12\u03bcs     0.90  bench_function_base.Sort.time_argsort('quick', 'int16', ('sorted_block', 1000))\r\n-        74.1\u00b12\u03bcs         66.3\u00b13\u03bcs     0.90  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 4, 'd')\r\n-      77.4\u00b10.4\u03bcs       69.2\u00b10.3\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'i')\r\n-        78.6\u00b12\u03bcs       70.3\u00b10.6\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'I')\r\n-      4.78\u00b10.2ms      4.28\u00b10.08ms     0.89  bench_lib.Pad.time_pad((4, 4, 4, 4), (0, 32), 'linear_ramp')\r\n-        82.8\u00b11\u03bcs         74.0\u00b12\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'l')\r\n-        61.6\u00b11\u03bcs       55.1\u00b10.9\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 1, 2, 'd')\r\n-        78.5\u00b11\u03bcs       70.2\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'i')\r\n-      79.9\u00b10.6\u03bcs       71.5\u00b10.7\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'I')\r\n-      84.6\u00b10.8\u03bcs         75.6\u00b12\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'I')\r\n-      88.1\u00b10.9\u03bcs         78.7\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'i')\r\n-        88.4\u00b11\u03bcs         79.0\u00b12\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'q')\r\n-     1.90\u00b10.02\u03bcs      1.70\u00b10.01\u03bcs     0.89  bench_itemselection.Take.time_contiguous((1000, 2), 'clip', 'int32')\r\n-      79.0\u00b10.8\u03bcs       70.5\u00b10.8\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'I')\r\n-      76.5\u00b10.6\u03bcs       68.3\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'i')\r\n-         269\u00b12\u03bcs          240\u00b11\u03bcs     0.89  bench_ufunc.UFunc.time_ufunc_types('maximum')\r\n-      76.9\u00b10.2\u03bcs       68.6\u00b10.6\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'i')\r\n-      60.6\u00b10.6\u03bcs       54.1\u00b10.3\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 2, 1, 'd')\r\n-      77.5\u00b10.5\u03bcs       69.1\u00b10.3\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'i')\r\n-      80.3\u00b10.6\u03bcs       71.7\u00b10.8\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'i')\r\n-        81.3\u00b11\u03bcs         72.5\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'I')\r\n-        77.4\u00b11\u03bcs       69.0\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'I')\r\n-      89.7\u00b10.9\u03bcs       79.9\u00b10.9\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'i')\r\n-      82.9\u00b10.5\u03bcs         73.8\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'q')\r\n-         430\u00b18\u03bcs          383\u00b13\u03bcs     0.89  bench_ufunc.UFunc.time_ufunc_types('multiply')\r\n-      79.6\u00b10.4\u03bcs       70.9\u00b10.4\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'i')\r\n-      80.9\u00b10.2\u03bcs       72.0\u00b10.6\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'i')\r\n-      81.5\u00b10.2\u03bcs         72.6\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'i')\r\n-      88.6\u00b10.9\u03bcs       78.8\u00b10.9\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'q')\r\n-        89.3\u00b12\u03bcs       79.5\u00b10.7\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'l')\r\n-     1.48\u00b10.01\u03bcs      1.31\u00b10.02\u03bcs     0.89  bench_itemselection.Take.time_contiguous((1000, 1), 'raise', 'int16')\r\n-     3.25\u00b10.02\u03bcs      2.89\u00b10.02\u03bcs     0.89  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'clip', 'int32')\r\n-      83.1\u00b10.7\u03bcs       73.8\u00b10.7\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'l')\r\n-        79.3\u00b12\u03bcs       70.4\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'I')\r\n-      78.1\u00b10.7\u03bcs       69.3\u00b10.3\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'i')\r\n-         263\u00b13\u03bcs        234\u00b10.6\u03bcs     0.89  bench_ufunc.UFunc.time_ufunc_types('minimum')\r\n-      79.5\u00b10.3\u03bcs       70.6\u00b10.6\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'I')\r\n-        78.8\u00b12\u03bcs       69.9\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'i')\r\n-        81.3\u00b11\u03bcs       72.1\u00b10.7\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'I')\r\n-        89.2\u00b11\u03bcs         79.1\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'I')\r\n-        88.4\u00b11\u03bcs       78.4\u00b10.8\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'q')\r\n-      89.2\u00b10.5\u03bcs       79.1\u00b10.6\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'Q')\r\n-      81.2\u00b10.8\u03bcs       71.9\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'i')\r\n-        84.0\u00b11\u03bcs       74.4\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'i')\r\n-      78.0\u00b10.7\u03bcs       69.1\u00b10.4\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'I')\r\n-      80.5\u00b10.6\u03bcs       71.3\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'I')\r\n-     3.28\u00b10.01\u03bcs      2.90\u00b10.02\u03bcs     0.89  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'clip', 'int64')\r\n-      81.4\u00b10.5\u03bcs       72.1\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'I')\r\n-        83.1\u00b11\u03bcs       73.6\u00b10.7\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'L')\r\n-        82.8\u00b11\u03bcs         73.3\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'i')\r\n-        80.9\u00b12\u03bcs       71.6\u00b10.6\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'I')\r\n-      84.8\u00b10.8\u03bcs         75.0\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'I')\r\n-      78.1\u00b10.4\u03bcs       69.1\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'i')\r\n-      82.2\u00b10.5\u03bcs       72.7\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'q')\r\n-        87.8\u00b11\u03bcs       77.7\u00b10.8\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'I')\r\n-      88.7\u00b10.9\u03bcs         78.4\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'L')\r\n-     3.29\u00b10.02\u03bcs      2.91\u00b10.04\u03bcs     0.88  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'clip', 'complex64')\r\n-      80.6\u00b10.5\u03bcs       71.3\u00b10.2\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'i')\r\n-      57.8\u00b10.6\u03bcs       51.1\u00b10.8\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 2, 'd')\r\n-      89.0\u00b10.8\u03bcs       78.6\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'I')\r\n-      82.6\u00b10.5\u03bcs       73.0\u00b10.7\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'i')\r\n-      81.1\u00b10.5\u03bcs       71.7\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'I')\r\n-      88.7\u00b10.6\u03bcs       78.3\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'I')\r\n-      81.4\u00b10.8\u03bcs         71.8\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'I')\r\n-      80.2\u00b10.6\u03bcs       70.8\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'i')\r\n-      81.0\u00b10.9\u03bcs       71.5\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'I')\r\n-         325\u00b19\u03bcs          287\u00b15\u03bcs     0.88  bench_ufunc.UFunc.time_ufunc_types('add')\r\n-     3.26\u00b10.03\u03bcs      2.87\u00b10.02\u03bcs     0.88  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'clip', 'float32')\r\n-      57.7\u00b10.9\u03bcs       50.9\u00b10.8\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 2, 'd')\r\n-        82.3\u00b11\u03bcs         72.5\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'I')\r\n-      83.5\u00b10.7\u03bcs         73.6\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'Q')\r\n-      89.0\u00b10.6\u03bcs         78.5\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'i')\r\n-        89.7\u00b11\u03bcs         79.0\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'i')\r\n-      89.0\u00b10.8\u03bcs         78.5\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'Q')\r\n-      82.4\u00b10.7\u03bcs       72.6\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'q')\r\n-      81.7\u00b10.3\u03bcs       72.0\u00b10.7\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'i')\r\n-      81.6\u00b10.6\u03bcs       71.8\u00b10.3\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'I')\r\n-      85.5\u00b10.6\u03bcs       75.2\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'i')\r\n-      82.3\u00b10.6\u03bcs       72.4\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'i')\r\n-      81.6\u00b10.7\u03bcs       71.7\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'I')\r\n-        81.1\u00b11\u03bcs       71.3\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'I')\r\n-      85.3\u00b10.5\u03bcs       75.0\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'I')\r\n-      81.0\u00b10.3\u03bcs       71.1\u00b10.8\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'i')\r\n-        89.3\u00b12\u03bcs       78.4\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'Q')\r\n-      82.6\u00b10.3\u03bcs       72.5\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'I')\r\n-        81.8\u00b11\u03bcs         71.8\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'I')\r\n-      89.2\u00b10.5\u03bcs         78.3\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'I')\r\n-        81.5\u00b11\u03bcs       71.5\u00b10.7\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'i')\r\n-        83.5\u00b11\u03bcs         73.3\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'Q')\r\n-      86.1\u00b10.4\u03bcs       75.5\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'i')\r\n-        81.2\u00b11\u03bcs       71.2\u00b10.8\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'i')\r\n-        82.5\u00b11\u03bcs       72.3\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'l')\r\n-        86.2\u00b11\u03bcs       75.5\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'I')\r\n-        80.0\u00b12\u03bcs       70.1\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'i')\r\n-      83.1\u00b10.8\u03bcs       72.9\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'I')\r\n-      83.7\u00b10.3\u03bcs       73.4\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'L')\r\n-      83.2\u00b10.5\u03bcs       72.9\u00b10.8\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'Q')\r\n-      81.3\u00b10.4\u03bcs       71.2\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'I')\r\n-        84.3\u00b11\u03bcs         73.8\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'l')\r\n-        83.0\u00b11\u03bcs       72.7\u00b10.7\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'i')\r\n-        81.9\u00b11\u03bcs         71.7\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'i')\r\n-      87.8\u00b10.7\u03bcs       76.9\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'l')\r\n-         109\u00b11\u03bcs         95.8\u00b13\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 2, 'd')\r\n-        89.4\u00b11\u03bcs       78.2\u00b10.8\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'L')\r\n-        83.9\u00b11\u03bcs       73.3\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'L')\r\n-        90.1\u00b11\u03bcs         78.7\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'l')\r\n-         110\u00b13\u03bcs         96.5\u00b13\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 4, 'd')\r\n-        83.2\u00b11\u03bcs       72.7\u00b10.6\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'I')\r\n-      85.9\u00b10.8\u03bcs         75.0\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'I')\r\n-      86.1\u00b10.7\u03bcs         75.2\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'i')\r\n-      82.5\u00b10.6\u03bcs       72.1\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'i')\r\n-        81.4\u00b11\u03bcs       71.1\u00b10.7\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'i')\r\n-        85.2\u00b11\u03bcs       74.3\u00b10.2\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'i')\r\n-      83.1\u00b10.8\u03bcs       72.5\u00b10.9\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'L')\r\n-      89.5\u00b10.7\u03bcs       78.1\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'i')\r\n-     1.41\u00b10.05ms      1.23\u00b10.03ms     0.87  bench_lib.Pad.time_pad((1024, 1024), 8, 'mean')\r\n-     1.48\u00b10.01\u03bcs      1.29\u00b10.01\u03bcs     0.87  bench_itemselection.Take.time_contiguous((1000, 1), 'raise', 'float16')\r\n-        83.0\u00b11\u03bcs       72.4\u00b10.6\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'L')\r\n-      80.8\u00b10.9\u03bcs       70.4\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'I')\r\n-         487\u00b12\u03bcs          425\u00b12\u03bcs     0.87  bench_function_base.Sort.time_sort('heap', 'float64', ('ordered',))\r\n-        90.1\u00b13\u03bcs         78.5\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'Q')\r\n-      86.6\u00b10.5\u03bcs       75.4\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'i')\r\n-        89.5\u00b11\u03bcs         77.9\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'q')\r\n-      83.2\u00b10.7\u03bcs       72.4\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'Q')\r\n-        89.0\u00b11\u03bcs         77.4\u00b12\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'Q')\r\n-      85.1\u00b10.6\u03bcs       74.0\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'I')\r\n-         150\u00b13\u03bcs          131\u00b14\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 4, 'd')\r\n-        83.0\u00b12\u03bcs       72.2\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'I')\r\n-        83.6\u00b11\u03bcs       72.7\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'i')\r\n-      83.2\u00b10.4\u03bcs       72.3\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'q')\r\n-     11.7\u00b10.05\u03bcs       10.2\u00b10.1\u03bcs     0.87  bench_reduce.MinMax.time_max(<class 'numpy.float64'>)\r\n-      83.4\u00b10.4\u03bcs       72.4\u00b10.3\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'l')\r\n-      88.3\u00b10.6\u03bcs       76.7\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'L')\r\n-      84.3\u00b10.5\u03bcs       73.2\u00b10.7\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'I')\r\n-        77.3\u00b13\u03bcs         67.0\u00b11\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 4, 'd')\r\n-        90.4\u00b12\u03bcs       78.5\u00b10.9\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'i')\r\n-      89.3\u00b10.3\u03bcs       77.4\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'Q')\r\n-      81.7\u00b10.7\u03bcs       70.7\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'i')\r\n-        91.4\u00b12\u03bcs         79.1\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'L')\r\n-        81.8\u00b11\u03bcs       70.8\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'i')\r\n-        83.9\u00b12\u03bcs       72.6\u00b10.2\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'i')\r\n-        82.2\u00b11\u03bcs       71.1\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'i')\r\n-     1.24\u00b10.03ms      1.07\u00b10.06ms     0.86  bench_core.Temporaries.time_large2\r\n-        321\u00b110\u03bcs          277\u00b12\u03bcs     0.86  bench_ufunc.UFunc.time_ufunc_types('subtract')\r\n-        90.8\u00b12\u03bcs         78.4\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'q')\r\n-        90.9\u00b11\u03bcs       78.5\u00b10.9\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'L')\r\n-         415\u00b15\u03bcs          358\u00b14\u03bcs     0.86  bench_ufunc.UFunc.time_ufunc_types('rint')\r\n-         475\u00b13\u03bcs         410\u00b110\u03bcs     0.86  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-       111\u00b10.7\u03bcs         96.0\u00b13\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 4, 'd')\r\n-        89.1\u00b11\u03bcs       76.7\u00b10.8\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'q')\r\n-         522\u00b14\u03bcs          449\u00b19\u03bcs     0.86  bench_lib.Nan.time_nanargmax(200000, 2.0)\r\n-        84.7\u00b11\u03bcs         72.8\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'q')\r\n-         474\u00b15\u03bcs          407\u00b16\u03bcs     0.86  bench_lib.Nan.time_nanargmax(200000, 0.1)\r\n-        91.3\u00b12\u03bcs         78.4\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'l')\r\n-         108\u00b13\u03bcs         92.8\u00b12\u03bcs     0.86  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 2, 'd')\r\n-      84.5\u00b10.6\u03bcs         72.4\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'l')\r\n-        86.2\u00b12\u03bcs         73.8\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'i')\r\n-        84.4\u00b12\u03bcs       72.2\u00b10.8\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'L')\r\n-        91.2\u00b11\u03bcs       78.0\u00b10.7\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'l')\r\n-         385\u00b12\u03bcs          329\u00b15\u03bcs     0.86  bench_ufunc.UFunc.time_ufunc_types('fmin')\r\n-      94.0\u00b10.5\u03bcs       80.4\u00b10.8\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'B')\r\n-        90.4\u00b12\u03bcs       77.3\u00b10.8\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'I')\r\n-         480\u00b12\u03bcs         410\u00b110\u03bcs     0.85  bench_lib.Nan.time_nanargmax(200000, 0)\r\n-         112\u00b12\u03bcs         95.8\u00b13\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 2, 'd')\r\n-      58.5\u00b10.6\u03bcs       49.9\u00b10.4\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 2, 'd')\r\n-        84.4\u00b11\u03bcs       72.0\u00b10.5\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'I')\r\n-     2.51\u00b10.02\u03bcs      2.14\u00b10.04\u03bcs     0.85  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'raise', 'float16')\r\n-         113\u00b13\u03bcs         96.5\u00b12\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 4, 'd')\r\n-         150\u00b12\u03bcs          128\u00b13\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 4, 'd')\r\n-      60.5\u00b10.3\u03bcs       51.5\u00b10.7\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 1, 1, 'd')\r\n-     11.8\u00b10.03\u03bcs      10.1\u00b10.08\u03bcs     0.85  bench_reduce.MinMax.time_min(<class 'numpy.float64'>)\r\n-      1.51\u00b10.08s       1.29\u00b10.02s     0.85  bench_io.Savez.time_vb_savez_squares\r\n-      93.7\u00b10.4\u03bcs       79.6\u00b10.9\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'B')\r\n-        94.5\u00b11\u03bcs       80.3\u00b10.9\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'B')\r\n-      94.3\u00b10.3\u03bcs         80.0\u00b11\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'B')\r\n-         479\u00b15\u03bcs          405\u00b14\u03bcs     0.85  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-         522\u00b12\u03bcs          442\u00b13\u03bcs     0.85  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-         254\u00b12\u03bcs          214\u00b12\u03bcs     0.84  bench_ufunc.UFunc.time_ufunc_types('floor')\r\n-      93.5\u00b10.4\u03bcs       79.0\u00b10.2\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'B')\r\n-      93.5\u00b10.2\u03bcs         79.0\u00b11\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'B')\r\n-      94.5\u00b10.8\u03bcs       79.8\u00b10.9\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'B')\r\n-     2.50\u00b10.02\u03bcs      2.11\u00b10.01\u03bcs     0.84  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'raise', 'int16')\r\n-      94.0\u00b10.6\u03bcs       79.0\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'B')\r\n-      94.1\u00b10.2\u03bcs       79.0\u00b10.3\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'B')\r\n-      95.2\u00b10.4\u03bcs         79.9\u00b11\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'B')\r\n-      94.8\u00b10.7\u03bcs       79.6\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 4, 'B')\r\n-         110\u00b13\u03bcs         91.9\u00b13\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 2, 'd')\r\n-      93.8\u00b10.4\u03bcs       78.5\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'B')\r\n-      49.8\u00b10.9\u03bcs         41.6\u00b11\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 2, 'd')\r\n-         388\u00b14\u03bcs          324\u00b12\u03bcs     0.84  bench_ufunc.UFunc.time_ufunc_types('fmax')\r\n-      94.6\u00b10.9\u03bcs         79.1\u00b11\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'B')\r\n-      93.4\u00b10.2\u03bcs       78.1\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'B')\r\n-      93.3\u00b10.5\u03bcs       78.0\u00b10.7\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'B')\r\n-      92.5\u00b10.8\u03bcs         77.2\u00b13\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'B')\r\n-         258\u00b12\u03bcs          215\u00b11\u03bcs     0.84  bench_ufunc.UFunc.time_ufunc_types('ceil')\r\n-         151\u00b13\u03bcs          126\u00b12\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 4, 'd')\r\n-         527\u00b15\u03bcs        439\u00b10.7\u03bcs     0.83  bench_function_base.Sort.time_sort('heap', 'float32', ('ordered',))\r\n-      93.4\u00b10.4\u03bcs       77.8\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'B')\r\n-         258\u00b18\u03bcs          215\u00b14\u03bcs     0.83  bench_ufunc.UFunc.time_ufunc_types('trunc')\r\n-        95.3\u00b11\u03bcs         79.3\u00b11\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'B')\r\n-      58.7\u00b10.9\u03bcs       48.8\u00b10.7\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 2, 'd')\r\n-      93.8\u00b10.2\u03bcs       78.0\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'B')\r\n-      93.7\u00b10.4\u03bcs       78.0\u00b10.7\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'B')\r\n-      94.2\u00b10.9\u03bcs       78.3\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'B')\r\n-      93.9\u00b10.9\u03bcs       78.1\u00b10.7\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'B')\r\n-      94.3\u00b10.3\u03bcs       78.4\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'B')\r\n-      95.0\u00b10.7\u03bcs       79.0\u00b10.8\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 4, 'B')\r\n-      94.6\u00b10.8\u03bcs       78.6\u00b10.8\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'B')\r\n-      93.6\u00b10.6\u03bcs         77.8\u00b11\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'B')\r\n-     5.28\u00b10.04\u03bcs      4.38\u00b10.04\u03bcs     0.83  bench_lib.Nan.time_nanmin(200, 0.1)\r\n-         153\u00b12\u03bcs        127\u00b10.6\u03bcs     0.83  bench_function_base.Sort.time_argsort('quick', 'uint32', ('reversed',))\r\n-      93.8\u00b10.2\u03bcs       77.9\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'B')\r\n-      94.9\u00b10.5\u03bcs       78.7\u00b10.5\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'B')\r\n-      92.8\u00b10.9\u03bcs       76.9\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'B')\r\n-      93.0\u00b10.3\u03bcs       77.1\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'B')\r\n-      93.4\u00b10.8\u03bcs       77.3\u00b10.5\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'B')\r\n-      93.2\u00b10.3\u03bcs       77.0\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'B')\r\n-      93.9\u00b10.4\u03bcs       77.3\u00b10.6\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'B')\r\n-      94.9\u00b10.3\u03bcs       78.0\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'B')\r\n-     5.24\u00b10.03\u03bcs      4.31\u00b10.04\u03bcs     0.82  bench_lib.Nan.time_nanmin(200, 2.0)\r\n-      94.5\u00b10.3\u03bcs       77.7\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'B')\r\n-        93.2\u00b12\u03bcs       76.6\u00b10.7\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'B')\r\n-        92.8\u00b11\u03bcs       76.2\u00b10.8\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'B')\r\n-      95.3\u00b10.4\u03bcs       78.3\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'B')\r\n-      92.9\u00b10.5\u03bcs       76.2\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'B')\r\n-      93.5\u00b10.4\u03bcs       76.7\u00b10.9\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'B')\r\n-         150\u00b12\u03bcs          123\u00b12\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 4, 'd')\r\n-     1.24\u00b10.02ms      1.02\u00b10.04ms     0.82  bench_core.Temporaries.time_large\r\n-      92.9\u00b10.5\u03bcs       76.1\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'B')\r\n-      94.0\u00b10.4\u03bcs       76.9\u00b10.3\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'B')\r\n-      94.0\u00b10.1\u03bcs       76.9\u00b10.9\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'B')\r\n-      92.6\u00b10.2\u03bcs       75.6\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'B')\r\n-     5.31\u00b10.02\u03bcs      4.33\u00b10.01\u03bcs     0.82  bench_lib.Nan.time_nanmin(200, 0)\r\n-      93.4\u00b10.3\u03bcs       76.2\u00b10.3\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'B')\r\n-      50.9\u00b10.8\u03bcs       41.5\u00b10.7\u03bcs     0.81  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 2, 'd')\r\n-      94.5\u00b10.4\u03bcs       76.9\u00b10.4\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'B')\r\n-        93.9\u00b11\u03bcs       76.2\u00b10.5\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'B')\r\n-      94.6\u00b10.9\u03bcs       76.6\u00b10.4\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'B')\r\n-      94.5\u00b10.6\u03bcs         76.3\u00b16\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'B')\r\n-        94.5\u00b12\u03bcs       76.2\u00b10.5\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'B')\r\n-      93.9\u00b10.8\u03bcs       75.7\u00b10.8\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'B')\r\n-      5.42\u00b10.1\u03bcs      4.36\u00b10.03\u03bcs     0.80  bench_lib.Nan.time_nanmax(200, 0.1)\r\n-      51.8\u00b10.3\u03bcs       41.7\u00b10.2\u03bcs     0.80  bench_core.VarComplex.time_var(10000)\r\n-       122\u00b10.3ms         97.9\u00b11ms     0.80  bench_app.LaplaceInplace.time_it('inplace')\r\n-        94.3\u00b12\u03bcs       75.6\u00b10.7\u03bcs     0.80  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'B')\r\n-        51.6\u00b11\u03bcs       41.3\u00b10.9\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 2, 'd')\r\n-      17.3\u00b10.1\u03bcs      13.9\u00b10.08\u03bcs     0.80  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 4, 'D')\r\n-     5.40\u00b10.04\u03bcs      4.31\u00b10.06\u03bcs     0.80  bench_lib.Nan.time_nanmin(200, 90.0)\r\n-     5.40\u00b10.09\u03bcs      4.32\u00b10.04\u03bcs     0.80  bench_lib.Nan.time_nanmax(200, 2.0)\r\n-      5.47\u00b10.1\u03bcs      4.32\u00b10.04\u03bcs     0.79  bench_lib.Nan.time_nanmax(200, 0)\r\n-        52.1\u00b11\u03bcs       41.1\u00b10.8\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 2, 'd')\r\n-      5.54\u00b10.1\u03bcs      4.33\u00b10.02\u03bcs     0.78  bench_lib.Nan.time_nanmax(200, 90.0)\r\n-         164\u00b14\u03bcs          127\u00b12\u03bcs     0.78  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 4, 'd')\r\n-         119\u00b11\u03bcs         91.4\u00b13\u03bcs     0.77  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 2, 'd')\r\n-         372\u00b12\u03bcs          282\u00b13\u03bcs     0.76  bench_core.VarComplex.time_var(100000)\r\n-        21.6\u00b15\u03bcs      16.3\u00b10.05\u03bcs     0.75  bench_scalar.ScalarMath.time_power_of_two('complex64')\r\n-        66.9\u00b11\u03bcs       50.1\u00b10.4\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 2, 'd')\r\n-      43.4\u00b10.5\u03bcs       32.4\u00b10.3\u03bcs     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 1, 'd')\r\n-         129\u00b12\u03bcs         95.8\u00b12\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 2, 'd')\r\n-      43.3\u00b10.8\u03bcs       32.2\u00b10.4\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 1, 'd')\r\n-        43.6\u00b11\u03bcs       32.4\u00b10.2\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 1, 'd')\r\n-        90.6\u00b12\u03bcs         67.0\u00b14\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 4, 'd')\r\n-      43.0\u00b10.3\u03bcs       31.8\u00b10.3\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 1, 'd')\r\n-         136\u00b13\u03bcs         99.0\u00b13\u03bcs     0.73  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 4, 'd')\r\n-      5.98\u00b10.1\u03bcs      4.35\u00b10.02\u03bcs     0.73  bench_lib.Nan.time_nanmax(200, 50.0)\r\n-      19.0\u00b10.1\u03bcs       13.7\u00b10.2\u03bcs     0.72  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 4, 'D')\r\n-     6.00\u00b10.08\u03bcs      4.31\u00b10.03\u03bcs     0.72  bench_lib.Nan.time_nanmin(200, 50.0)\r\n-      14.5\u00b10.1\u03bcs       10.4\u00b10.2\u03bcs     0.72  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 2, 'D')\r\n-     11.3\u00b10.05\u03bcs      8.02\u00b10.04\u03bcs     0.71  bench_ufunc.CustomScalar.time_add_scalar2(<class 'numpy.float64'>)\r\n-         180\u00b12\u03bcs          127\u00b12\u03bcs     0.70  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 4, 'd')\r\n-        542\u00b110\u03bcs          380\u00b12\u03bcs     0.70  bench_reduce.AddReduceSeparate.time_reduce(0, 'float64')\r\n-     1.24\u00b10.01ms         848\u00b120\u03bcs     0.69  bench_reduce.AddReduceSeparate.time_reduce(0, 'complex128')\r\n-      59.0\u00b10.4\u03bcs       40.2\u00b10.6\u03bcs     0.68  bench_core.Temporaries.time_mid2\r\n-      15.0\u00b10.3\u03bcs       10.2\u00b10.2\u03bcs     0.68  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 2, 'D')\r\n-      59.2\u00b10.3\u03bcs       39.9\u00b10.4\u03bcs     0.67  bench_core.Temporaries.time_mid\r\n-      64.7\u00b10.7\u03bcs       43.4\u00b10.4\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 1, 'd')\r\n-      63.8\u00b10.8\u03bcs       42.8\u00b10.4\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 1, 'd')\r\n-      64.9\u00b10.6\u03bcs       43.4\u00b10.3\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 1, 'd')\r\n-        82.4\u00b11\u03bcs       54.8\u00b10.4\u03bcs     0.66  bench_ufunc.CustomInplace.time_double_add_temp\r\n-     13.0\u00b10.08\u03bcs      8.57\u00b10.07\u03bcs     0.66  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 1, 'D')\r\n-      80.1\u00b10.9\u03bcs       52.2\u00b10.3\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 2, 'd')\r\n-      66.1\u00b10.5\u03bcs       43.0\u00b10.3\u03bcs     0.65  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 1, 'd')\r\n-      63.0\u00b10.7\u03bcs         40.5\u00b11\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 2, 'd')\r\n-      41.0\u00b10.6\u03bcs       26.3\u00b10.2\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 1, 'd')\r\n-      40.8\u00b10.5\u03bcs       26.0\u00b10.2\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 1, 'd')\r\n-      40.9\u00b10.5\u03bcs       26.0\u00b10.2\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 1, 'd')\r\n-      68.1\u00b10.7\u03bcs       43.3\u00b10.2\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 1, 'd')\r\n-      40.8\u00b10.5\u03bcs      25.7\u00b10.05\u03bcs     0.63  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 1, 'd')\r\n-      75.2\u00b10.9\u03bcs       47.0\u00b10.3\u03bcs     0.63  bench_ufunc.CustomInplace.time_double_add\r\n-      13.2\u00b10.2\u03bcs      7.70\u00b10.06\u03bcs     0.58  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 1, 'D')\r\n-     11.6\u00b10.04\u03bcs      6.69\u00b10.06\u03bcs     0.58  bench_reduce.MinMax.time_min(<class 'numpy.float32'>)\r\n-     11.8\u00b10.08\u03bcs      6.63\u00b10.05\u03bcs     0.56  bench_reduce.MinMax.time_max(<class 'numpy.float32'>)\r\n-      80.6\u00b10.7\u03bcs         43.6\u00b11\u03bcs     0.54  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 2, 'd')\r\n-     2.00\u00b10.03ms         1.08\u00b10ms     0.54  bench_reduce.AddReduceSeparate.time_reduce(0, 'complex64')\r\n-         160\u00b11\u03bcs       82.4\u00b10.4\u03bcs     0.52  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-         159\u00b11\u03bcs       81.4\u00b10.5\u03bcs     0.51  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-      66.8\u00b10.4\u03bcs       34.0\u00b10.2\u03bcs     0.51  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n-      66.7\u00b10.3\u03bcs       33.9\u00b10.2\u03bcs     0.51  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n-      85.7\u00b10.7\u03bcs       42.6\u00b10.1\u03bcs     0.50  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 1, 'd')\r\n-     20.7\u00b10.07\u03bcs       10.2\u00b10.1\u03bcs     0.49  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 4, 'F')\r\n-      64.5\u00b10.8\u03bcs       31.6\u00b10.2\u03bcs     0.49  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 1, 'd')\r\n-      71.8\u00b10.4\u03bcs      34.4\u00b10.04\u03bcs     0.48  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n-      72.1\u00b10.4\u03bcs       34.4\u00b10.1\u03bcs     0.48  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n-         430\u00b13\u03bcs          199\u00b12\u03bcs     0.46  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 4, 'd')\r\n-      78.7\u00b10.8\u03bcs       36.0\u00b10.6\u03bcs     0.46  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'Q')\r\n-        78.7\u00b11\u03bcs       35.9\u00b10.7\u03bcs     0.46  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'l')\r\n-      78.3\u00b10.4\u03bcs       35.6\u00b10.4\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'l')\r\n-      78.2\u00b10.6\u03bcs       35.3\u00b10.5\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'L')\r\n-      78.9\u00b10.5\u03bcs       35.6\u00b10.6\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'q')\r\n-        78.6\u00b11\u03bcs       35.5\u00b10.3\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'Q')\r\n-      79.1\u00b10.3\u03bcs       35.6\u00b10.5\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'L')\r\n-      79.1\u00b10.4\u03bcs       35.5\u00b10.5\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'q')\r\n-      12.7\u00b10.2\u03bcs      5.65\u00b10.04\u03bcs     0.45  bench_reduce.MinMax.time_min(<class 'numpy.int64'> (0))\r\n-      12.7\u00b10.2\u03bcs      5.59\u00b10.02\u03bcs     0.44  bench_reduce.MinMax.time_min(<class 'numpy.int64'> (1))\r\n-         438\u00b14\u03bcs          192\u00b13\u03bcs     0.44  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 4, 'd')\r\n-      20.7\u00b10.1\u03bcs      9.06\u00b10.04\u03bcs     0.44  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 2, 'F')\r\n-     20.8\u00b10.09\u03bcs      9.02\u00b10.03\u03bcs     0.43  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 1, 'F')\r\n-      12.9\u00b10.1\u03bcs      5.58\u00b10.02\u03bcs     0.43  bench_reduce.MinMax.time_min(<class 'numpy.uint64'>)\r\n-      12.8\u00b10.2\u03bcs      5.53\u00b10.03\u03bcs     0.43  bench_reduce.MinMax.time_max(<class 'numpy.int64'> (0))\r\n-      12.8\u00b10.2\u03bcs      5.51\u00b10.04\u03bcs     0.43  bench_reduce.MinMax.time_max(<class 'numpy.int64'> (1))\r\n-      21.0\u00b10.2\u03bcs       9.00\u00b10.2\u03bcs     0.43  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 4, 'F')\r\n-      60.9\u00b10.5\u03bcs       25.8\u00b10.1\u03bcs     0.42  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 1, 'd')\r\n-     13.1\u00b10.04\u03bcs      5.55\u00b10.02\u03bcs     0.42  bench_reduce.MinMax.time_max(<class 'numpy.uint64'>)\r\n-      36.2\u00b10.3\u03bcs       14.4\u00b10.3\u03bcs     0.40  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 4, 'F')\r\n-         426\u00b13\u03bcs          166\u00b18\u03bcs     0.39  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 2, 'd')\r\n-      80.0\u00b10.9\u03bcs       31.0\u00b10.2\u03bcs     0.39  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 1, 'd')\r\n-       424\u00b10.8\u03bcs          164\u00b12\u03bcs     0.39  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 4, 'd')\r\n-         426\u00b13\u03bcs         164\u00b110\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 4, 'd')\r\n-         436\u00b17\u03bcs         167\u00b110\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 4, 'd')\r\n-      2.75\u00b10.1ms      1.05\u00b10.03ms     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'd')\r\n-         424\u00b12\u03bcs          161\u00b14\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 4, 'd')\r\n-     2.70\u00b10.01ms      1.02\u00b10.02ms     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'd')\r\n-         435\u00b12\u03bcs          164\u00b16\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 4, 'd')\r\n-      12.2\u00b10.4\u03bcs      4.62\u00b10.06\u03bcs     0.38  bench_reduce.MinMax.time_min(<class 'numpy.uint32'>)\r\n-      12.3\u00b10.2\u03bcs      4.65\u00b10.03\u03bcs     0.38  bench_reduce.MinMax.time_max(<class 'numpy.int32'>)\r\n-         435\u00b14\u03bcs          164\u00b16\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 2, 'd')\r\n-        2.72\u00b10ms         1.01\u00b10ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'd')\r\n-      12.3\u00b10.2\u03bcs      4.58\u00b10.01\u03bcs     0.37  bench_reduce.MinMax.time_max(<class 'numpy.uint32'>)\r\n-         435\u00b14\u03bcs          162\u00b16\u03bcs     0.37  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 4, 'd')\r\n-     2.75\u00b10.03ms         1.02\u00b10ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'd')\r\n-      12.4\u00b10.4\u03bcs      4.57\u00b10.03\u03bcs     0.37  bench_reduce.MinMax.time_min(<class 'numpy.int32'>)\r\n-     2.77\u00b10.03ms      1.02\u00b10.01ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'd')\r\n-     2.76\u00b10.03ms         1.02\u00b10ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'd')\r\n-     2.73\u00b10.01ms      1.00\u00b10.01ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'd')\r\n-      36.6\u00b10.2\u03bcs      13.4\u00b10.03\u03bcs     0.37  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 2, 'F')\r\n-     2.86\u00b10.06ms      1.05\u00b10.03ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'd')\r\n-        523\u00b110\u03bcs          192\u00b14\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 1, 4, 'f')\r\n-         505\u00b11\u03bcs          185\u00b12\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 1, 2, 'f')\r\n-         425\u00b11\u03bcs          155\u00b17\u03bcs     0.37  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 1, 'd')\r\n-         507\u00b17\u03bcs          185\u00b13\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 4, 4, 'f')\r\n-         504\u00b11\u03bcs        184\u00b10.9\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 4, 1, 'f')\r\n-         506\u00b11\u03bcs        184\u00b10.9\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 1, 1, 'f')\r\n-     2.74\u00b10.04ms         997\u00b110\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'd')\r\n-         507\u00b13\u03bcs          183\u00b11\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 2, 1, 'f')\r\n-         514\u00b14\u03bcs          185\u00b11\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 2, 2, 'f')\r\n-         426\u00b17\u03bcs          153\u00b13\u03bcs     0.36  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 4, 'd')\r\n-         513\u00b15\u03bcs        184\u00b10.8\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 4, 2, 'f')\r\n-         510\u00b15\u03bcs          182\u00b11\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 2, 4, 'f')\r\n-        440\u00b110\u03bcs          156\u00b16\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 4, 'd')\r\n-        449\u00b110\u03bcs          159\u00b17\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 4, 'd')\r\n-         436\u00b18\u03bcs          152\u00b16\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 1, 'd')\r\n-         434\u00b17\u03bcs          150\u00b14\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 4, 'd')\r\n-      36.2\u00b10.3\u03bcs       12.5\u00b10.2\u03bcs     0.34  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 1, 'F')\r\n-     21.0\u00b10.08\u03bcs      7.10\u00b10.02\u03bcs     0.34  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 2, 'F')\r\n-     8.90\u00b10.04\u03bcs      3.00\u00b10.03\u03bcs     0.34  bench_reduce.MinMax.time_min(<class 'numpy.uint8'>)\r\n-         434\u00b11\u03bcs          146\u00b13\u03bcs     0.34  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 2, 'd')\r\n-     8.92\u00b10.06\u03bcs      2.96\u00b10.03\u03bcs     0.33  bench_reduce.MinMax.time_max(<class 'numpy.uint8'>)\r\n-         432\u00b12\u03bcs          143\u00b13\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 2, 'd')\r\n-      74.3\u00b10.6\u03bcs       24.6\u00b10.7\u03bcs     0.33  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'I')\r\n-         421\u00b12\u03bcs          139\u00b14\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 2, 'd')\r\n-       157\u00b10.5\u03bcs       51.7\u00b10.4\u03bcs     0.33  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-      74.9\u00b10.7\u03bcs       24.6\u00b10.5\u03bcs     0.33  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'i')\r\n-         423\u00b13\u03bcs          138\u00b12\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 2, 'd')\r\n-      74.7\u00b10.6\u03bcs       24.4\u00b10.5\u03bcs     0.33  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'i')\r\n-        440\u00b110\u03bcs          143\u00b12\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 4, 'd')\r\n-      74.4\u00b10.5\u03bcs       24.2\u00b10.5\u03bcs     0.32  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'I')\r\n-         423\u00b13\u03bcs          137\u00b16\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 2, 'd')\r\n-       158\u00b10.6\u03bcs       51.4\u00b10.4\u03bcs     0.32  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-         423\u00b13\u03bcs          137\u00b14\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 2, 'd')\r\n-       159\u00b10.6\u03bcs       51.3\u00b10.4\u03bcs     0.32  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-         159\u00b12\u03bcs       51.3\u00b10.2\u03bcs     0.32  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-         425\u00b12\u03bcs          136\u00b17\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 1, 'd')\r\n-         422\u00b11\u03bcs          135\u00b19\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 1, 'd')\r\n-        451\u00b110\u03bcs          143\u00b14\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 4, 'd')\r\n-      79.7\u00b10.3\u03bcs       25.1\u00b10.2\u03bcs     0.31  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 1, 'd')\r\n-         430\u00b13\u03bcs          135\u00b14\u03bcs     0.31  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 2, 'd')\r\n-         437\u00b13\u03bcs          137\u00b17\u03bcs     0.31  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 1, 'd')\r\n-        439\u00b120\u03bcs          137\u00b16\u03bcs     0.31  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 4, 'd')\r\n-        448\u00b110\u03bcs          138\u00b13\u03bcs     0.31  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 4, 'd')\r\n-      20.7\u00b10.2\u03bcs      6.39\u00b10.07\u03bcs     0.31  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 1, 'F')\r\n-       102\u00b10.3\u03bcs       31.2\u00b10.2\u03bcs     0.31  bench_ufunc.CustomScalar.time_divide_scalar2_inplace(<class 'numpy.float32'>)\r\n-       102\u00b10.4\u03bcs      31.1\u00b10.09\u03bcs     0.30  bench_ufunc.CustomScalar.time_divide_scalar2(<class 'numpy.float32'>)\r\n-         431\u00b12\u03bcs          129\u00b15\u03bcs     0.30  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 2, 'd')\r\n-        525\u00b110\u03bcs          155\u00b14\u03bcs     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 4, 'f')\r\n-         507\u00b13\u03bcs          150\u00b11\u03bcs     0.30  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 2, 'f')\r\n-         506\u00b14\u03bcs        149\u00b10.7\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 2, 'f')\r\n-         432\u00b14\u03bcs          126\u00b15\u03bcs     0.29  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 1, 'd')\r\n-       158\u00b10.8\u03bcs       46.1\u00b10.4\u03bcs     0.29  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-         159\u00b11\u03bcs       46.2\u00b10.6\u03bcs     0.29  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-         506\u00b12\u03bcs          147\u00b11\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 1, 'f')\r\n-         511\u00b16\u03bcs          149\u00b11\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 1, 'f')\r\n-         505\u00b13\u03bcs        147\u00b10.7\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 2, 'f')\r\n-         510\u00b14\u03bcs        147\u00b10.6\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 1, 'f')\r\n-       159\u00b10.8\u03bcs       45.8\u00b10.7\u03bcs     0.29  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-         508\u00b12\u03bcs        147\u00b10.4\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 4, 'f')\r\n-         518\u00b19\u03bcs          149\u00b13\u03bcs     0.29  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 4, 'f')\r\n-         433\u00b11\u03bcs         124\u00b110\u03bcs     0.29  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 1, 'd')\r\n-         433\u00b11\u03bcs          123\u00b13\u03bcs     0.29  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 1, 'd')\r\n-         161\u00b12\u03bcs       45.7\u00b10.8\u03bcs     0.28  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-         428\u00b15\u03bcs          119\u00b18\u03bcs     0.28  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 1, 'd')\r\n-         419\u00b12\u03bcs          115\u00b13\u03bcs     0.27  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 1, 'd')\r\n-         187\u00b12\u03bcs       51.0\u00b10.5\u03bcs     0.27  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-         191\u00b12\u03bcs       50.8\u00b10.2\u03bcs     0.27  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-     1.23\u00b10.01ms          321\u00b12\u03bcs     0.26  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 4, 'f')\r\n-     1.23\u00b10.01ms          318\u00b12\u03bcs     0.26  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 2, 'f')\r\n-     13.7\u00b10.04\u03bcs      3.54\u00b10.06\u03bcs     0.26  bench_reduce.MinMax.time_max(<class 'numpy.uint16'>)\r\n-        1.23\u00b10ms          317\u00b13\u03bcs     0.26  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 4, 'f')\r\n-      13.8\u00b10.1\u03bcs      3.50\u00b10.02\u03bcs     0.25  bench_reduce.MinMax.time_min(<class 'numpy.uint16'>)\r\n-        437\u00b110\u03bcs          111\u00b12\u03bcs     0.25  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 4, 'd')\r\n-         432\u00b13\u03bcs          110\u00b11\u03bcs     0.25  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 2, 'd')\r\n-         428\u00b14\u03bcs          109\u00b12\u03bcs     0.25  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 2, 'd')\r\n-     1.24\u00b10.01ms          315\u00b12\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 2, 'f')\r\n-     1.25\u00b10.01ms          316\u00b12\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 4, 'f')\r\n-     1.25\u00b10.01ms          313\u00b14\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 4, 'f')\r\n-     1.25\u00b10.01ms          311\u00b15\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 2, 'f')\r\n-     1.23\u00b10.02ms        306\u00b10.8\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 1, 'f')\r\n-     1.23\u00b10.01ms        306\u00b10.8\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 1, 'f')\r\n-        450\u00b120\u03bcs          110\u00b12\u03bcs     0.24  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 4, 'd')\r\n-     1.28\u00b10.02ms          311\u00b15\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 2, 'f')\r\n-     1.24\u00b10.01ms        302\u00b10.8\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 1, 'f')\r\n-        1.25\u00b10ms        302\u00b10.9\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 1, 'f')\r\n-         197\u00b13\u03bcs         47.3\u00b11\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 4, 'f')\r\n-         195\u00b12\u03bcs         46.7\u00b11\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 4, 'f')\r\n-     1.25\u00b10.02ms          300\u00b13\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 4, 'f')\r\n-     12.7\u00b10.04\u03bcs      3.03\u00b10.01\u03bcs     0.24  bench_reduce.MinMax.time_min(<class 'numpy.int8'>)\r\n-         197\u00b12\u03bcs       46.9\u00b10.7\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 4, 'f')\r\n-     1.23\u00b10.01ms          293\u00b11\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 2, 'f')\r\n-         199\u00b12\u03bcs       47.2\u00b10.6\u03bcs     0.24  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 4, 'f')\r\n-     15.1\u00b10.08\u03bcs      3.55\u00b10.03\u03bcs     0.24  bench_reduce.MinMax.time_max(<class 'numpy.int16'>)\r\n-     15.1\u00b10.04\u03bcs      3.55\u00b10.07\u03bcs     0.24  bench_reduce.MinMax.time_min(<class 'numpy.int16'>)\r\n-     12.7\u00b10.07\u03bcs      2.95\u00b10.02\u03bcs     0.23  bench_reduce.MinMax.time_max(<class 'numpy.int8'>)\r\n-         198\u00b12\u03bcs       45.8\u00b10.4\u03bcs     0.23  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 4, 'f')\r\n-     1.27\u00b10.01ms          293\u00b15\u03bcs     0.23  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 4, 'f')\r\n-         476\u00b16\u03bcs          109\u00b15\u03bcs     0.23  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 4, 'f')\r\n-         475\u00b14\u03bcs          109\u00b18\u03bcs     0.23  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 4, 'f')\r\n-     1.24\u00b10.01ms          283\u00b11\u03bcs     0.23  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 1, 'f')\r\n-     1.26\u00b10.01ms          288\u00b13\u03bcs     0.23  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 2, 'f')\r\n-     1.26\u00b10.01ms          279\u00b12\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 1, 'f')\r\n-         197\u00b13\u03bcs         42.5\u00b12\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 4, 'f')\r\n-         195\u00b11\u03bcs       41.9\u00b10.4\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 4, 'f')\r\n-         882\u00b13\u03bcs          187\u00b13\u03bcs     0.21  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 4, 'd')\r\n-         216\u00b13\u03bcs       45.7\u00b10.4\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 4, 'f')\r\n-         896\u00b13\u03bcs          190\u00b15\u03bcs     0.21  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 4, 'd')\r\n-         195\u00b12\u03bcs         41.2\u00b11\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 4, 'f')\r\n-         196\u00b13\u03bcs         41.0\u00b11\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 4, 'f')\r\n-         196\u00b11\u03bcs       40.8\u00b10.9\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 4, 'f')\r\n-         197\u00b11\u03bcs       39.3\u00b10.2\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 2, 'f')\r\n-         195\u00b11\u03bcs       38.7\u00b10.1\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 2, 'f')\r\n-         196\u00b11\u03bcs       38.9\u00b10.4\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 2, 'f')\r\n-       196\u00b10.9\u03bcs       38.8\u00b10.5\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 2, 'f')\r\n-         196\u00b11\u03bcs         38.2\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 2, 'f')\r\n-         196\u00b11\u03bcs       38.2\u00b10.4\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 2, 'f')\r\n-         196\u00b12\u03bcs         37.9\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 2, 'f')\r\n-       214\u00b10.4\u03bcs         41.2\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 4, 'f')\r\n-         200\u00b13\u03bcs         38.3\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 4, 'f')\r\n-       195\u00b10.9\u03bcs         37.1\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 2, 'f')\r\n-         196\u00b11\u03bcs         37.2\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 2, 'f')\r\n-         197\u00b12\u03bcs         37.3\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 2, 'f')\r\n-         201\u00b13\u03bcs       37.5\u00b10.4\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 4, 'f')\r\n-         200\u00b13\u03bcs         37.4\u00b11\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 4, 'f')\r\n-         201\u00b13\u03bcs       37.2\u00b10.5\u03bcs     0.19  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 4, 'f')\r\n-         201\u00b13\u03bcs         36.9\u00b11\u03bcs     0.18  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 4, 'f')\r\n-         882\u00b14\u03bcs        157\u00b10.6\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 4, 'd')\r\n-         880\u00b14\u03bcs          156\u00b12\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 4, 'd')\r\n-         215\u00b11\u03bcs         38.1\u00b12\u03bcs     0.18  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 2, 'f')\r\n-         898\u00b13\u03bcs          158\u00b13\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 4, 'd')\r\n-         896\u00b15\u03bcs          156\u00b14\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 4, 'd')\r\n-         216\u00b11\u03bcs       37.5\u00b10.4\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 2, 'f')\r\n-         423\u00b15\u03bcs       72.5\u00b10.5\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 2, 'd')\r\n-         646\u00b12\u03bcs          111\u00b16\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 4, 'f')\r\n-        896\u00b110\u03bcs          153\u00b14\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 2, 'd')\r\n-         429\u00b11\u03bcs       73.3\u00b10.4\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 2, 'd')\r\n-         901\u00b14\u03bcs          154\u00b15\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 2, 'd')\r\n-         640\u00b18\u03bcs          109\u00b17\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 4, 'f')\r\n-         421\u00b16\u03bcs       70.7\u00b10.5\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 1, 'd')\r\n-         879\u00b11\u03bcs          147\u00b15\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 1, 'd')\r\n-       196\u00b10.7\u03bcs       32.7\u00b10.4\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 2, 'f')\r\n-         220\u00b12\u03bcs         36.6\u00b11\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 4, 'f')\r\n-       195\u00b10.6\u03bcs       32.5\u00b10.3\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 2, 'f')\r\n-         195\u00b11\u03bcs       32.4\u00b10.3\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 2, 'f')\r\n-         430\u00b11\u03bcs       71.4\u00b10.3\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 1, 'd')\r\n-         196\u00b11\u03bcs       32.4\u00b10.3\u03bcs     0.17  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 2, 'f')\r\n-     24.4\u00b10.07\u03bcs      4.03\u00b10.01\u03bcs     0.17  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint32'>, 43)\r\n-        884\u00b110\u03bcs          146\u00b17\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 4, 'd')\r\n-         892\u00b12\u03bcs          147\u00b18\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 4, 'd')\r\n-         197\u00b13\u03bcs       32.3\u00b10.6\u03bcs     0.16  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 2, 'f')\r\n-         428\u00b15\u03bcs         69.6\u00b12\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 1, 'd')\r\n-         883\u00b14\u03bcs          144\u00b15\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 4, 'd')\r\n-         892\u00b14\u03bcs          145\u00b15\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 1, 'd')\r\n-      25.0\u00b10.2\u03bcs      4.04\u00b10.02\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint32'>, 8)\r\n-         422\u00b14\u03bcs       68.2\u00b10.4\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 2, 'd')\r\n-        913\u00b120\u03bcs          147\u00b14\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 4, 'd')\r\n-         433\u00b15\u03bcs         69.5\u00b11\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 1, 'd')\r\n-        82.3\u00b11\u03bcs       13.1\u00b10.2\u03bcs     0.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'h')\r\n-         874\u00b13\u03bcs          139\u00b14\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 2, 'd')\r\n-         878\u00b15\u03bcs          140\u00b15\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 2, 'd')\r\n-         433\u00b11\u03bcs       68.5\u00b10.3\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 2, 'd')\r\n-        897\u00b110\u03bcs          140\u00b13\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 4, 'd')\r\n-         894\u00b12\u03bcs          138\u00b16\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 2, 'd')\r\n-      83.3\u00b10.3\u03bcs      12.8\u00b10.09\u03bcs     0.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'h')\r\n-        933\u00b120\u03bcs          142\u00b13\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 4, 'd')\r\n-         895\u00b13\u03bcs          135\u00b15\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 2, 'd')\r\n-         423\u00b14\u03bcs         63.8\u00b11\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 1, 'd')\r\n-       428\u00b10.8\u03bcs       63.9\u00b10.5\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 1, 'd')\r\n-      86.4\u00b10.3\u03bcs      12.9\u00b10.05\u03bcs     0.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'H')\r\n-      87.7\u00b10.5\u03bcs      13.0\u00b10.05\u03bcs     0.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'H')\r\n-         873\u00b12\u03bcs          129\u00b13\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 1, 'd')\r\n-         215\u00b11\u03bcs       31.8\u00b10.5\u03bcs     0.15  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 2, 'f')\r\n-         475\u00b18\u03bcs         68.9\u00b11\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 2, 'f')\r\n-         473\u00b11\u03bcs       68.6\u00b10.5\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 4, 'f')\r\n-         898\u00b13\u03bcs          130\u00b13\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 1, 'd')\r\n-         883\u00b17\u03bcs        128\u00b10.7\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 2, 'd')\r\n-         478\u00b15\u03bcs         69.0\u00b11\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 4, 'f')\r\n-         470\u00b11\u03bcs         67.7\u00b11\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 4, 'f')\r\n-         895\u00b16\u03bcs          129\u00b13\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 1, 'd')\r\n-         472\u00b12\u03bcs       67.7\u00b10.9\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 2, 'f')\r\n-         477\u00b13\u03bcs         68.0\u00b11\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 4, 'f')\r\n-        913\u00b130\u03bcs          128\u00b12\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 4, 'd')\r\n-         898\u00b18\u03bcs          123\u00b15\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 2, 'd')\r\n-         884\u00b16\u03bcs          121\u00b12\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 2, 'd')\r\n-         879\u00b14\u03bcs          119\u00b16\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 1, 'd')\r\n-         422\u00b11\u03bcs         57.4\u00b12\u03bcs     0.14  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 2, 'd')\r\n-        930\u00b120\u03bcs          125\u00b14\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 4, 'd')\r\n-        921\u00b130\u03bcs          122\u00b14\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 4, 'd')\r\n-        934\u00b140\u03bcs          124\u00b15\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 4, 'd')\r\n-         433\u00b15\u03bcs       57.1\u00b10.8\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 2, 'd')\r\n-         893\u00b12\u03bcs          117\u00b14\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 2, 'd')\r\n-         471\u00b12\u03bcs         61.1\u00b11\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 2, 'f')\r\n-         472\u00b14\u03bcs       61.1\u00b10.4\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 2, 'f')\r\n-         470\u00b13\u03bcs       60.5\u00b10.5\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 4, 'f')\r\n-        476\u00b110\u03bcs       61.4\u00b10.6\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 4, 'f')\r\n-         473\u00b14\u03bcs       60.7\u00b10.1\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 2, 'f')\r\n-         473\u00b12\u03bcs       60.5\u00b10.4\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 2, 'f')\r\n-       197\u00b10.4\u03bcs       24.6\u00b10.5\u03bcs     0.13  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 1, 'f')\r\n-         197\u00b11\u03bcs       24.5\u00b10.4\u03bcs     0.12  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 1, 'f')\r\n-       196\u00b10.4\u03bcs       24.3\u00b10.5\u03bcs     0.12  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 1, 'f')\r\n-         467\u00b12\u03bcs       57.8\u00b10.7\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 4, 'f')\r\n-       197\u00b10.5\u03bcs       24.3\u00b10.8\u03bcs     0.12  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 1, 'f')\r\n-         196\u00b11\u03bcs       24.1\u00b10.5\u03bcs     0.12  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 1, 'f')\r\n-      40.2\u00b10.3\u03bcs      4.90\u00b10.08\u03bcs     0.12  bench_ufunc.CustomScalar.time_add_scalar2(<class 'numpy.float32'>)\r\n-         469\u00b12\u03bcs       57.1\u00b10.6\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 2, 'f')\r\n-         476\u00b14\u03bcs       57.9\u00b10.7\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 4, 'f')\r\n-         893\u00b16\u03bcs          108\u00b14\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 2, 'd')\r\n-         476\u00b16\u03bcs       56.8\u00b10.4\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 2, 'f')\r\n-         478\u00b13\u03bcs         56.7\u00b11\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 4, 'f')\r\n-         881\u00b19\u03bcs          104\u00b14\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 2, 'd')\r\n-         474\u00b11\u03bcs         56.2\u00b11\u03bcs     0.12  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 4, 'f')\r\n-       214\u00b10.2\u03bcs       24.9\u00b10.6\u03bcs     0.12  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 1, 'f')\r\n-         885\u00b12\u03bcs          100\u00b12\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 1, 'd')\r\n-         891\u00b11\u03bcs          100\u00b13\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 1, 'd')\r\n-         890\u00b11\u03bcs         99.5\u00b16\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 1, 'd')\r\n-       196\u00b10.6\u03bcs       21.7\u00b10.3\u03bcs     0.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 1, 'f')\r\n-         639\u00b12\u03bcs         70.4\u00b11\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 4, 'f')\r\n-        911\u00b130\u03bcs          100\u00b12\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 4, 'd')\r\n-         472\u00b13\u03bcs       51.8\u00b10.5\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 1, 'f')\r\n-         642\u00b13\u03bcs         70.0\u00b11\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 4, 'f')\r\n-         196\u00b12\u03bcs       21.4\u00b10.3\u03bcs     0.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 1, 'f')\r\n-         641\u00b14\u03bcs         69.9\u00b11\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 4, 'f')\r\n-         884\u00b12\u03bcs         96.3\u00b13\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 1, 'd')\r\n-        931\u00b130\u03bcs          101\u00b12\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 4, 'd')\r\n-         196\u00b11\u03bcs       21.3\u00b10.4\u03bcs     0.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 1, 'f')\r\n-       195\u00b10.8\u03bcs       21.2\u00b10.3\u03bcs     0.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 1, 'f')\r\n-       196\u00b10.7\u03bcs       21.2\u00b10.3\u03bcs     0.11  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 1, 'f')\r\n-         642\u00b13\u03bcs         69.2\u00b11\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 4, 'f')\r\n-         640\u00b15\u03bcs         68.8\u00b11\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 2, 'f')\r\n-         474\u00b15\u03bcs       50.9\u00b10.7\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 4, 'f')\r\n-         474\u00b15\u03bcs       50.9\u00b10.5\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 1, 'f')\r\n-         478\u00b19\u03bcs       51.0\u00b10.8\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 4, 'f')\r\n-         642\u00b14\u03bcs         68.4\u00b11\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 2, 'f')\r\n-         470\u00b11\u03bcs       50.0\u00b10.4\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 4, 'f')\r\n-        472\u00b110\u03bcs       50.1\u00b10.7\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 4, 'f')\r\n-      68.6\u00b10.6\u03bcs       7.16\u00b10.2\u03bcs     0.10  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'b')\r\n-        68.5\u00b13\u03bcs       7.05\u00b10.1\u03bcs     0.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'b')\r\n-         472\u00b12\u03bcs       48.0\u00b10.3\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 2, 'f')\r\n-         469\u00b13\u03bcs       47.7\u00b10.2\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 1, 'f')\r\n-         471\u00b11\u03bcs       47.8\u00b10.4\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 2, 'f')\r\n-         472\u00b13\u03bcs       47.9\u00b10.2\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 2, 'f')\r\n-     1.97\u00b10.01ms          197\u00b17\u03bcs     0.10  bench_reduce.AddReduceSeparate.time_reduce(0, 'float32')\r\n-         473\u00b12\u03bcs       47.4\u00b10.7\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 1, 'f')\r\n-         475\u00b17\u03bcs       47.6\u00b10.4\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 2, 'f')\r\n-         158\u00b13\u03bcs      15.8\u00b10.09\u03bcs     0.10  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       157\u00b10.7\u03bcs      15.7\u00b10.08\u03bcs     0.10  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-         471\u00b14\u03bcs       47.1\u00b10.2\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 1, 'f')\r\n-         474\u00b15\u03bcs       47.2\u00b10.7\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 1, 'f')\r\n-         419\u00b14\u03bcs       41.7\u00b10.3\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 1, 'd')\r\n-         426\u00b12\u03bcs       41.8\u00b10.5\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 1, 'd')\r\n-         480\u00b15\u03bcs         46.9\u00b11\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 4, 'f')\r\n-         641\u00b12\u03bcs         62.3\u00b11\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 4, 'f')\r\n-         470\u00b11\u03bcs       45.4\u00b10.4\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 1, 'f')\r\n-       215\u00b10.2\u03bcs       20.7\u00b10.1\u03bcs     0.10  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 1, 'f')\r\n-         476\u00b15\u03bcs         45.9\u00b11\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 4, 'f')\r\n-         641\u00b18\u03bcs       61.5\u00b10.7\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 2, 'f')\r\n-         643\u00b13\u03bcs         61.6\u00b11\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 4, 'f')\r\n-         641\u00b14\u03bcs       61.3\u00b10.8\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 2, 'f')\r\n-         644\u00b15\u03bcs       61.5\u00b10.8\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 2, 'f')\r\n-      81.0\u00b10.6\u03bcs      7.72\u00b10.02\u03bcs     0.10  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-         473\u00b15\u03bcs       45.1\u00b10.3\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 1, 'f')\r\n-         640\u00b13\u03bcs       60.9\u00b10.7\u03bcs     0.10  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 2, 'f')\r\n-         598\u00b17\u03bcs       56.7\u00b10.8\u03bcs     0.09  bench_ufunc.CustomInplace.time_float_add_temp\r\n-      70.9\u00b10.2\u03bcs      6.45\u00b10.05\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n-         468\u00b12\u03bcs       42.6\u00b10.1\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 2, 'f')\r\n-      71.0\u00b10.2\u03bcs      6.44\u00b10.06\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n-         466\u00b15\u03bcs       42.0\u00b10.2\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 2, 'f')\r\n-         642\u00b15\u03bcs       57.8\u00b10.5\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 2, 'f')\r\n-         640\u00b13\u03bcs       57.4\u00b10.8\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 4, 'f')\r\n-         644\u00b13\u03bcs       57.8\u00b10.4\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 4, 'f')\r\n-         643\u00b14\u03bcs       57.2\u00b10.8\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 2, 'f')\r\n-         472\u00b14\u03bcs       41.9\u00b10.4\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 2, 'f')\r\n-         473\u00b17\u03bcs       41.8\u00b10.2\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 2, 'f')\r\n-         470\u00b16\u03bcs       41.5\u00b10.6\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 1, 'f')\r\n-         468\u00b13\u03bcs       41.0\u00b10.2\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 1, 'f')\r\n-         641\u00b14\u03bcs       55.6\u00b10.8\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 4, 'f')\r\n-         641\u00b12\u03bcs       55.5\u00b10.4\u03bcs     0.09  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 4, 'f')\r\n-      76.1\u00b10.2\u03bcs      6.53\u00b10.05\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n-         475\u00b14\u03bcs       40.2\u00b10.1\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 1, 'f')\r\n-      76.2\u00b10.6\u03bcs      6.42\u00b10.05\u03bcs     0.08  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n-         473\u00b15\u03bcs       39.7\u00b10.3\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 1, 'f')\r\n-         189\u00b12\u03bcs       15.8\u00b10.1\u03bcs     0.08  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-         190\u00b12\u03bcs      15.8\u00b10.07\u03bcs     0.08  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-      89.2\u00b10.6\u03bcs       7.28\u00b10.2\u03bcs     0.08  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'B')\r\n-         469\u00b12\u03bcs       38.0\u00b10.3\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 2, 'f')\r\n-         465\u00b12\u03bcs       37.6\u00b10.2\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 2, 'f')\r\n-         655\u00b19\u03bcs       52.8\u00b10.8\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 4, 'f')\r\n-         590\u00b16\u03bcs       47.1\u00b10.2\u03bcs     0.08  bench_ufunc.CustomInplace.time_float_add\r\n-         654\u00b18\u03bcs         52.1\u00b11\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 4, 'f')\r\n-         196\u00b12\u03bcs       15.6\u00b10.2\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 1, 'f')\r\n-       195\u00b10.5\u03bcs      15.6\u00b10.08\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 1, 'f')\r\n-       196\u00b10.4\u03bcs       15.5\u00b10.2\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 1, 'f')\r\n-         196\u00b11\u03bcs       15.5\u00b10.2\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 1, 'f')\r\n-      89.0\u00b10.5\u03bcs       7.04\u00b10.1\u03bcs     0.08  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'B')\r\n-         637\u00b12\u03bcs       50.3\u00b10.5\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 4, 'f')\r\n-         639\u00b13\u03bcs       50.4\u00b10.4\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 4, 'f')\r\n-         642\u00b16\u03bcs       50.6\u00b10.4\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 1, 'f')\r\n-         198\u00b12\u03bcs       15.5\u00b10.2\u03bcs     0.08  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 1, 'f')\r\n-         642\u00b11\u03bcs       50.2\u00b10.7\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 1, 'f')\r\n-         641\u00b11\u03bcs       49.8\u00b10.5\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 4, 'f')\r\n-       641\u00b10.9\u03bcs       49.2\u00b10.6\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 4, 'f')\r\n-         638\u00b12\u03bcs       48.9\u00b10.6\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 2, 'f')\r\n-         640\u00b11\u03bcs         48.4\u00b12\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 2, 'f')\r\n-         466\u00b12\u03bcs       35.2\u00b10.5\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 1, 'f')\r\n-         643\u00b16\u03bcs       48.1\u00b10.3\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 2, 'f')\r\n-         642\u00b12\u03bcs       47.8\u00b10.4\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 2, 'f')\r\n-         475\u00b16\u03bcs       34.7\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 1, 'f')\r\n-         640\u00b11\u03bcs       46.0\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 1, 'f')\r\n-         467\u00b13\u03bcs       33.5\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 1, 'f')\r\n-         475\u00b15\u03bcs       33.9\u00b10.3\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 1, 'f')\r\n-         639\u00b14\u03bcs         45.5\u00b12\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 2, 'f')\r\n-         639\u00b12\u03bcs       45.2\u00b10.5\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 1, 'f')\r\n-       638\u00b10.8\u03bcs       45.1\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 1, 'f')\r\n-      41.7\u00b10.4\u03bcs      2.94\u00b10.02\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint16'>, 43)\r\n-       215\u00b10.9\u03bcs       15.2\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 1, 'f')\r\n-         638\u00b12\u03bcs       45.0\u00b10.3\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 1, 'f')\r\n-         886\u00b17\u03bcs       61.7\u00b10.7\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 2, 'd')\r\n-         637\u00b11\u03bcs       44.3\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 2, 'f')\r\n-         876\u00b14\u03bcs       60.8\u00b10.8\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 2, 'd')\r\n-         639\u00b11\u03bcs       44.4\u00b10.3\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 2, 'f')\r\n-         890\u00b11\u03bcs         61.6\u00b12\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 1, 'd')\r\n-     42.3\u00b10.08\u03bcs      2.92\u00b10.01\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint16'>, 8)\r\n-     4.91\u00b10.03ms          335\u00b15\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'f')\r\n-         896\u00b15\u03bcs       60.9\u00b10.5\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 2, 'd')\r\n-         890\u00b13\u03bcs       60.3\u00b10.7\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 2, 'd')\r\n-         888\u00b16\u03bcs         60.0\u00b11\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 1, 'd')\r\n-     4.88\u00b10.02ms          329\u00b11\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'f')\r\n-      45.8\u00b10.2\u03bcs      3.07\u00b10.02\u03bcs     0.07  bench_reduce.FMinMax.time_max(<class 'numpy.float32'>)\r\n-     4.93\u00b10.04ms          330\u00b14\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'f')\r\n-     4.93\u00b10.01ms          328\u00b12\u03bcs     0.07  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'f')\r\n-         638\u00b12\u03bcs       42.0\u00b10.7\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 2, 'f')\r\n-         639\u00b11\u03bcs       42.0\u00b10.3\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 2, 'f')\r\n-      46.0\u00b10.4\u03bcs      3.01\u00b10.01\u03bcs     0.07  bench_reduce.FMinMax.time_min(<class 'numpy.float32'>)\r\n-         639\u00b13\u03bcs       41.2\u00b10.5\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 1, 'f')\r\n-         640\u00b11\u03bcs       41.3\u00b10.3\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 2, 'f')\r\n-         639\u00b11\u03bcs       41.2\u00b10.4\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 1, 'f')\r\n-         639\u00b12\u03bcs       40.9\u00b10.3\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 1, 'f')\r\n-     4.90\u00b10.02ms        310\u00b10.9\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-         643\u00b19\u03bcs       40.6\u00b10.2\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 1, 'f')\r\n-     4.92\u00b10.02ms          309\u00b11\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-      65.7\u00b10.5\u03bcs      4.12\u00b10.02\u03bcs     0.06  bench_reduce.FMinMax.time_min(<class 'numpy.float64'>)\r\n-      65.8\u00b10.4\u03bcs      4.08\u00b10.03\u03bcs     0.06  bench_reduce.FMinMax.time_max(<class 'numpy.float64'>)\r\n-         642\u00b13\u03bcs       39.0\u00b10.2\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 1, 'f')\r\n-         638\u00b11\u03bcs       38.5\u00b10.2\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 1, 'f')\r\n-         158\u00b12\u03bcs      9.24\u00b10.07\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-         157\u00b11\u03bcs      9.19\u00b10.07\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n-     4.95\u00b10.05ms          288\u00b13\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'f')\r\n-       158\u00b10.8\u03bcs      9.18\u00b10.02\u03bcs     0.06  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-         158\u00b12\u03bcs      9.18\u00b10.05\u03bcs     0.06  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-         890\u00b16\u03bcs         51.3\u00b11\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 2, 'd')\r\n-         877\u00b16\u03bcs         50.1\u00b11\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 2, 'd')\r\n-     4.92\u00b10.03ms          281\u00b12\u03bcs     0.06  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'f')\r\n-         465\u00b14\u03bcs       26.6\u00b10.5\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 1, 'f')\r\n-         471\u00b14\u03bcs       26.6\u00b10.4\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 1, 'f')\r\n-         874\u00b13\u03bcs       49.2\u00b10.3\u03bcs     0.06  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 1, 'd')\r\n-         889\u00b14\u03bcs       48.5\u00b10.6\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 1, 'd')\r\n-         877\u00b15\u03bcs       47.8\u00b10.4\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 1, 'd')\r\n-     4.90\u00b10.02ms          263\u00b11\u03bcs     0.05  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n-      70.8\u00b10.5\u03bcs      3.72\u00b10.04\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n-      41.6\u00b10.3\u03bcs         2.19\u00b10\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint8'>, 8)\r\n-         899\u00b17\u03bcs       47.0\u00b10.3\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 1, 'd')\r\n-      41.7\u00b10.1\u03bcs      2.18\u00b10.01\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint8'>, 43)\r\n-      71.1\u00b10.4\u03bcs      3.71\u00b10.02\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n-         640\u00b12\u03bcs       33.2\u00b10.4\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 1, 'f')\r\n-         640\u00b12\u03bcs       32.5\u00b10.3\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 1, 'f')\r\n-         643\u00b15\u03bcs       32.5\u00b10.4\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 1, 'f')\r\n-         637\u00b12\u03bcs       31.9\u00b10.4\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 1, 'f')\r\n-      74.5\u00b10.3\u03bcs      3.73\u00b10.03\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n-        74.3\u00b11\u03bcs      3.70\u00b10.01\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n-         674\u00b13\u03bcs       30.0\u00b10.2\u03bcs     0.04  bench_lib.Nan.time_nanmax(200000, 0.1)\r\n-         669\u00b19\u03bcs       29.7\u00b10.2\u03bcs     0.04  bench_lib.Nan.time_nanmin(200000, 0.1)\r\n-         674\u00b14\u03bcs       29.9\u00b10.5\u03bcs     0.04  bench_lib.Nan.time_nanmax(200000, 2.0)\r\n-         675\u00b16\u03bcs       29.7\u00b10.5\u03bcs     0.04  bench_lib.Nan.time_nanmin(200000, 0)\r\n-         674\u00b18\u03bcs       29.6\u00b10.2\u03bcs     0.04  bench_lib.Nan.time_nanmin(200000, 2.0)\r\n-         681\u00b18\u03bcs       29.7\u00b10.4\u03bcs     0.04  bench_lib.Nan.time_nanmax(200000, 0)\r\n-         639\u00b12\u03bcs       24.5\u00b10.7\u03bcs     0.04  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 1, 'f')\r\n-         640\u00b12\u03bcs       24.3\u00b10.7\u03bcs     0.04  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 1, 'f')\r\n-         888\u00b13\u03bcs       31.9\u00b10.3\u03bcs     0.04  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 1, 'd')\r\n-         878\u00b15\u03bcs       31.5\u00b10.6\u03bcs     0.04  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 1, 'd')\r\n-      71.1\u00b10.3\u03bcs      2.37\u00b10.01\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n-      72.1\u00b10.2\u03bcs      2.35\u00b10.01\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n-      75.0\u00b10.5\u03bcs      2.34\u00b10.01\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n-      75.6\u00b10.6\u03bcs      2.36\u00b10.02\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n-     1.10\u00b10.01ms       30.1\u00b10.2\u03bcs     0.03  bench_lib.Nan.time_nanmax(200000, 90.0)\r\n-     1.10\u00b10.01ms       29.7\u00b10.2\u03bcs     0.03  bench_lib.Nan.time_nanmin(200000, 90.0)\r\n-     1.53\u00b10.01ms       29.8\u00b10.4\u03bcs     0.02  bench_lib.Nan.time_nanmax(200000, 50.0)\r\n-     1.55\u00b10.01ms       29.6\u00b10.2\u03bcs     0.02  bench_lib.Nan.time_nanmin(200000, 50.0)\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n <summary>VXE</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"VXE2\"\r\npython runtests.py --bench-compare parent/main\r\n```\r\n```Bash\r\n    before           after         ratio\r\n     [982fcd38]       [47d54c6d]\r\n     <zsystem_sup~5>       <zsystem_sup>\r\n+      7.37\u00b10.1ms      10.1\u00b10.05ms     1.37  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 1000000)\r\n+      74.3\u00b10.2\u03bcs          101\u00b11\u03bcs     1.36  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 10000)\r\n+     3.28\u00b10.05ms       4.43\u00b10.1ms     1.35  bench_reduce.AddReduceSeparate.time_reduce(1, 'float16')\r\n+      83.4\u00b10.9\u03bcs          102\u00b11\u03bcs     1.22  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'h')\r\n+      9.46\u00b10.2ms       11.4\u00b10.2ms     1.21  bench_reduce.AddReduce.time_axis_1\r\n+       127\u00b10.6\u03bcs          153\u00b13\u03bcs     1.20  bench_function_base.Sort.time_argsort('quick', 'int32', ('reversed',))\r\n+        85.1\u00b11\u03bcs        102\u00b10.4\u03bcs     1.20  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'h')\r\n+      83.6\u00b10.9\u03bcs       99.9\u00b10.9\u03bcs     1.20  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'h')\r\n+      84.4\u00b10.3\u03bcs        101\u00b10.3\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'h')\r\n+        84.1\u00b11\u03bcs       99.9\u00b10.4\u03bcs     1.19  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'h')\r\n+      84.5\u00b10.6\u03bcs      100.0\u00b10.6\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'h')\r\n+        84.1\u00b11\u03bcs       99.4\u00b10.3\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'h')\r\n+        84.7\u00b13\u03bcs        100\u00b10.4\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'b')\r\n+        84.6\u00b11\u03bcs        100.0\u00b11\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'h')\r\n+        560\u00b120ms         661\u00b120ms     1.18  bench_lib.Pad.time_pad((1, 1, 1, 1, 1), (0, 32), 'wrap')\r\n+        83.5\u00b11\u03bcs      98.6\u00b10.09\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'h')\r\n+        84.9\u00b11\u03bcs        100\u00b10.4\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'h')\r\n+        85.2\u00b11\u03bcs        100\u00b10.3\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'h')\r\n+        85.5\u00b12\u03bcs        101\u00b10.5\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'h')\r\n+        85.6\u00b11\u03bcs        101\u00b10.6\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'h')\r\n+      84.4\u00b10.2\u03bcs       99.3\u00b10.3\u03bcs     1.18  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'h')\r\n+      85.0\u00b10.3\u03bcs       99.6\u00b10.6\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'h')\r\n+      84.9\u00b10.7\u03bcs       99.6\u00b10.5\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'b')\r\n+      85.9\u00b10.7\u03bcs        101\u00b10.5\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'h')\r\n+        85.6\u00b11\u03bcs          100\u00b12\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'h')\r\n+        85.3\u00b11\u03bcs       99.6\u00b10.2\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'h')\r\n+      85.3\u00b10.9\u03bcs       99.5\u00b10.7\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'b')\r\n+      85.6\u00b10.1\u03bcs       99.9\u00b10.6\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'h')\r\n+        84.7\u00b11\u03bcs       98.8\u00b10.7\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'b')\r\n+        85.5\u00b11\u03bcs       99.6\u00b10.4\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'h')\r\n+        86.5\u00b12\u03bcs        101\u00b10.4\u03bcs     1.17  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'h')\r\n+      85.0\u00b10.9\u03bcs       99.0\u00b1.1\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'h')\r\n+        85.8\u00b12\u03bcs       99.8\u00b10.7\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'h')\r\n+        86.1\u00b11\u03bcs        100\u00b10.6\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'h')\r\n+      85.3\u00b10.8\u03bcs       99.2\u00b10.9\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'b')\r\n+        86.3\u00b11\u03bcs        100\u00b10.4\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'h')\r\n+      84.6\u00b10.6\u03bcs       98.3\u00b10.3\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'b')\r\n+      85.0\u00b10.6\u03bcs       98.6\u00b10.3\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'b')\r\n+        86.5\u00b12\u03bcs          100\u00b11\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'b')\r\n+        87.0\u00b11\u03bcs        101\u00b10.6\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'h')\r\n+      84.9\u00b10.3\u03bcs       98.5\u00b10.3\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'b')\r\n+      86.4\u00b10.4\u03bcs        100\u00b10.3\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'h')\r\n+        87.0\u00b12\u03bcs        101\u00b10.5\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'h')\r\n+      86.1\u00b10.8\u03bcs       99.7\u00b10.5\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'h')\r\n+        86.6\u00b12\u03bcs        100\u00b10.6\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'h')\r\n+      85.7\u00b10.8\u03bcs         99.1\u00b11\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'b')\r\n+      87.3\u00b10.9\u03bcs        101\u00b10.5\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'h')\r\n+      85.3\u00b10.7\u03bcs       98.7\u00b10.6\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'b')\r\n+        85.0\u00b11\u03bcs       98.3\u00b10.3\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'b')\r\n+      87.5\u00b10.6\u03bcs        101\u00b10.7\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'h')\r\n+      86.3\u00b10.8\u03bcs       99.8\u00b10.3\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'b')\r\n+      86.6\u00b10.9\u03bcs        100\u00b10.6\u03bcs     1.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'h')\r\n+        85.7\u00b12\u03bcs       99.0\u00b10.2\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'h')\r\n+        87.4\u00b11\u03bcs        101\u00b10.7\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'h')\r\n+        85.2\u00b11\u03bcs       98.3\u00b10.3\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'b')\r\n+        87.6\u00b11\u03bcs        101\u00b10.4\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'h')\r\n+         301\u00b14\u03bcs        347\u00b10.8\u03bcs     1.15  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 1000))\r\n+        87.5\u00b11\u03bcs        101\u00b10.5\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'h')\r\n+        86.5\u00b12\u03bcs       99.7\u00b10.8\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'h')\r\n+        87.8\u00b12\u03bcs        101\u00b10.9\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 4, 'b')\r\n+        85.7\u00b11\u03bcs       98.6\u00b10.3\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'b')\r\n+      85.7\u00b10.9\u03bcs       98.6\u00b10.5\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'b')\r\n+      87.5\u00b10.7\u03bcs        101\u00b10.5\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'h')\r\n+      86.1\u00b10.6\u03bcs       99.0\u00b10.7\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'b')\r\n+      85.6\u00b10.6\u03bcs       98.4\u00b10.3\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'b')\r\n+      86.3\u00b10.3\u03bcs       99.2\u00b10.2\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'b')\r\n+        87.8\u00b12\u03bcs        101\u00b10.9\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'h')\r\n+      87.5\u00b10.6\u03bcs        100\u00b10.3\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'h')\r\n+        88.6\u00b11\u03bcs        102\u00b10.4\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 4, 'h')\r\n+        86.1\u00b12\u03bcs       98.8\u00b10.2\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'b')\r\n+      12.3\u00b10.2\u03bcs       14.1\u00b10.2\u03bcs     1.15  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 1, 'D')\r\n+      85.8\u00b10.8\u03bcs       98.3\u00b10.3\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'b')\r\n+     1.15\u00b10.03\u03bcs      1.32\u00b10.02\u03bcs     1.15  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 100)\r\n+        86.9\u00b12\u03bcs       99.4\u00b10.3\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'h')\r\n+        88.2\u00b11\u03bcs        101\u00b10.5\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'h')\r\n+        88.9\u00b11\u03bcs        102\u00b10.5\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 4, 'h')\r\n+      86.2\u00b10.4\u03bcs       98.6\u00b10.1\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'b')\r\n+      87.2\u00b10.3\u03bcs       99.6\u00b10.5\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'b')\r\n+      87.7\u00b10.2\u03bcs        100\u00b10.6\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'h')\r\n+      86.2\u00b10.4\u03bcs       98.3\u00b10.3\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'b')\r\n+        87.0\u00b12\u03bcs       99.0\u00b10.5\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'b')\r\n+        86.5\u00b11\u03bcs       98.4\u00b10.5\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'b')\r\n+        89.1\u00b12\u03bcs          101\u00b11\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'h')\r\n+        86.8\u00b11\u03bcs         98.6\u00b11\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'b')\r\n+        86.9\u00b11\u03bcs       98.8\u00b10.3\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'b')\r\n+      87.7\u00b10.4\u03bcs       99.7\u00b10.2\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'h')\r\n+        87.2\u00b11\u03bcs       99.0\u00b10.5\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'b')\r\n+      87.2\u00b10.4\u03bcs       99.0\u00b10.2\u03bcs     1.14  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'h')\r\n+        86.6\u00b11\u03bcs       98.3\u00b10.2\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'b')\r\n+      87.5\u00b10.4\u03bcs         99.2\u00b11\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'b')\r\n+      87.2\u00b10.4\u03bcs       98.8\u00b10.3\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'b')\r\n+      87.7\u00b10.3\u03bcs       99.4\u00b10.3\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'b')\r\n+        88.3\u00b11\u03bcs       99.9\u00b10.5\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'h')\r\n+      87.0\u00b10.3\u03bcs       98.4\u00b10.3\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'b')\r\n+        89.0\u00b12\u03bcs          101\u00b11\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 4, 'b')\r\n+      86.9\u00b10.8\u03bcs       98.2\u00b10.3\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'b')\r\n+      88.0\u00b10.6\u03bcs         99.4\u00b11\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'b')\r\n+        87.4\u00b12\u03bcs       98.6\u00b10.2\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'b')\r\n+      88.0\u00b10.9\u03bcs       99.3\u00b10.6\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'b')\r\n+         326\u00b11\u03bcs          368\u00b12\u03bcs     1.13  bench_function_base.Sort.time_argsort('quick', 'int64', ('sorted_block', 1000))\r\n+        87.3\u00b12\u03bcs       98.5\u00b10.1\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'b')\r\n+      87.1\u00b10.4\u03bcs       98.2\u00b10.2\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'b')\r\n+      87.2\u00b10.6\u03bcs       98.2\u00b10.4\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'b')\r\n+        87.7\u00b12\u03bcs       98.7\u00b10.1\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'b')\r\n+        87.4\u00b12\u03bcs       98.4\u00b10.2\u03bcs     1.13  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'b')\r\n+      88.8\u00b10.4\u03bcs       99.8\u00b10.7\u03bcs     1.12  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'h')\r\n+        88.3\u00b12\u03bcs       99.0\u00b10.4\u03bcs     1.12  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'b')\r\n+        88.4\u00b12\u03bcs       98.9\u00b10.5\u03bcs     1.12  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'b')\r\n+        89.2\u00b11\u03bcs       99.2\u00b10.4\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'h')\r\n+         120\u00b14\u03bcs          133\u00b13\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'L')\r\n+      89.7\u00b10.6\u03bcs       99.5\u00b10.5\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'b')\r\n+        88.7\u00b12\u03bcs       98.3\u00b10.1\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'b')\r\n+        89.4\u00b12\u03bcs       98.8\u00b10.3\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'b')\r\n+      79.2\u00b10.1\u03bcs       87.4\u00b10.2\u03bcs     1.10  bench_function_base.Where.time_interleaved_zeros_x8\r\n+         133\u00b12\u03bcs          147\u00b13\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'Q')\r\n+      13.4\u00b10.2\u03bcs       14.7\u00b10.3\u03bcs     1.10  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 2, 'D')\r\n+         505\u00b14\u03bcs        554\u00b10.6\u03bcs     1.10  bench_function_base.Sort.time_argsort('heap', 'float64', ('ordered',))\r\n+     6.90\u00b10.05ms      7.56\u00b10.02ms     1.10  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 1000000)\r\n+        89.9\u00b12\u03bcs       98.5\u00b10.1\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'b')\r\n+        89.9\u00b13\u03bcs       98.4\u00b10.6\u03bcs     1.09  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'b')\r\n+         330\u00b13\u03bcs          361\u00b13\u03bcs     1.09  bench_function_base.Sort.time_argsort('quick', 'int32', ('sorted_block', 1000))\r\n+         134\u00b13\u03bcs          146\u00b14\u03bcs     1.09  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'q')\r\n+      70.1\u00b10.3\u03bcs       76.6\u00b10.6\u03bcs     1.09  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 10000)\r\n+         119\u00b12\u03bcs          130\u00b12\u03bcs     1.09  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'Q')\r\n+         409\u00b12\u03bcs        443\u00b10.5\u03bcs     1.08  bench_function_base.Sort.time_argsort('quick', 'int64', ('sorted_block', 100))\r\n+       371\u00b10.4\u03bcs          401\u00b13\u03bcs     1.08  bench_function_base.Sort.time_sort('quick', 'int16', ('sorted_block', 100))\r\n+     2.68\u00b10.03\u03bcs      2.89\u00b10.08\u03bcs     1.08  bench_core.Core.time_hstack_l\r\n+     17.2\u00b10.04\u03bcs       18.6\u00b10.7\u03bcs     1.08  bench_core.CountNonzero.time_count_nonzero(2, 10000, <class 'numpy.int32'>)\r\n+      86.8\u00b10.3\u03bcs       93.1\u00b10.5\u03bcs     1.07  bench_function_base.Sort.time_sort('merge', 'float64', ('sorted_block', 100))\r\n+         123\u00b12\u03bcs          131\u00b12\u03bcs     1.07  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'L')\r\n+      13.5\u00b10.1\u03bcs       14.4\u00b10.3\u03bcs     1.07  bench_ma.MA.time_masked_array_l100_t100\r\n+       139\u00b10.6\u03bcs          148\u00b12\u03bcs     1.07  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'Q')\r\n+     17.8\u00b10.07\u03bcs       19.0\u00b10.4\u03bcs     1.07  bench_lib.Nan.time_nanmean(200, 0)\r\n+     5.04\u00b10.06ms       5.37\u00b10.1ms     1.07  bench_lib.Pad.time_pad((256, 128, 1), (0, 32), 'wrap')\r\n+     10.3\u00b10.06\u03bcs       11.0\u00b10.3\u03bcs     1.07  bench_ma.MA.time_masked_array_l100\r\n+     19.3\u00b10.06\u03bcs       20.6\u00b10.6\u03bcs     1.07  bench_linalg.Einsum.time_einsum_noncon_mul(<class 'numpy.float32'>)\r\n+     1.15\u00b10.02\u03bcs      1.23\u00b10.03\u03bcs     1.07  bench_itemselection.Take.time_contiguous((1000, 1), 'wrap', 'int64')\r\n+     1.20\u00b10.01ms      1.28\u00b10.05ms     1.06  bench_core.CountNonzero.time_count_nonzero(2, 1000000, <class 'numpy.int16'>)\r\n+        55.1\u00b11ms         58.6\u00b12ms     1.06  bench_ma.Concatenate.time_it('masked', 2000)\r\n+         128\u00b12ms          136\u00b12ms     1.06  bench_lib.Pad.time_pad((1, 1, 1, 1, 1), (0, 32), 'constant')\r\n+      21.0\u00b10.1\u03bcs       22.3\u00b10.4\u03bcs     1.06  bench_linalg.Einsum.time_einsum_noncon_contig_contig(<class 'numpy.float32'>)\r\n+        638\u00b110\u03bcs         677\u00b120\u03bcs     1.06  bench_core.CountNonzero.time_count_nonzero(1, 1000000, <class 'numpy.int16'>)\r\n+     1.32\u00b10.01\u03bcs      1.40\u00b10.05\u03bcs     1.06  bench_core.Core.time_ones_100\r\n+     18.3\u00b10.08\u03bcs       19.4\u00b10.4\u03bcs     1.06  bench_ma.UFunc.time_2d(True, True, 10)\r\n+        64.8\u00b11ms         68.7\u00b11ms     1.06  bench_ma.Concatenate.time_it('unmasked+masked', 2000)\r\n+         108\u00b12\u03bcs        115\u00b10.4\u03bcs     1.06  bench_function_base.Sort.time_argsort('merge', 'int32', ('sorted_block', 100))\r\n+         335\u00b13\u03bcs          355\u00b15\u03bcs     1.06  bench_lib.Pad.time_pad((1, 1, 1, 1, 1), 1, 'linear_ramp')\r\n+     27.9\u00b10.09\u03bcs       29.5\u00b10.5\u03bcs     1.06  bench_lib.Pad.time_pad((1, 1, 1, 1, 1), 1, 'constant')\r\n+         492\u00b11ns          520\u00b18ns     1.06  bench_array_coercion.ArrayCoercionSmall.time_array_all_kwargs([1])\r\n+     9.82\u00b10.04\u03bcs       10.4\u00b10.3\u03bcs     1.06  bench_lib.Unique.time_unique(200, 90.0)\r\n+         565\u00b12\u03bcs          597\u00b13\u03bcs     1.06  bench_function_base.Sort.time_argsort('heap', 'float64', ('reversed',))\r\n+         110\u00b11\u03bcs          116\u00b14\u03bcs     1.06  bench_lib.Pad.time_pad((256, 128, 1), 1, 'reflect')\r\n+      60.7\u00b10.6ms         64.1\u00b12ms     1.06  bench_ma.Concatenate.time_it('ndarray+masked', 2000)\r\n+     4.27\u00b10.01\u03bcs       4.50\u00b10.1\u03bcs     1.06  bench_core.CountNonzero.time_count_nonzero_multi_axis(3, 100, <class 'numpy.int64'>)\r\n+      20.1\u00b10.1\u03bcs       21.2\u00b10.6\u03bcs     1.06  bench_linalg.Einsum.time_einsum_noncon_contig_contig(<class 'numpy.float64'>)\r\n+      14.6\u00b10.2\u03bcs      15.4\u00b10.04\u03bcs     1.06  bench_ma.UFunc.time_scalar_1d(False, False, 100)\r\n+      14.8\u00b10.2\u03bcs       15.6\u00b10.2\u03bcs     1.06  bench_ma.UFunc.time_scalar_1d(False, False, 1000)\r\n+     10.8\u00b10.02\u03bcs      11.4\u00b10.07\u03bcs     1.06  bench_lib.Unique.time_unique(200, 0)\r\n+        837\u00b120ns         883\u00b120ns     1.05  bench_io.Copy.time_memcpy('int8')\r\n+       110\u00b10.4\u03bcs          116\u00b11\u03bcs     1.05  bench_function_base.Sort.time_argsort('merge', 'float32', ('sorted_block', 100))\r\n+      21.2\u00b10.2\u03bcs       22.4\u00b10.8\u03bcs     1.05  bench_ma.UFunc.time_scalar_1d(False, True, 100)\r\n+      14.5\u00b10.2\u03bcs       15.3\u00b10.1\u03bcs     1.05  bench_ma.UFunc.time_scalar_1d(False, False, 10)\r\n+     2.18\u00b10.01ms      2.30\u00b10.04ms     1.05  bench_indexing.IndexingSeparate.time_mmap_fancy_indexing\r\n+        673\u00b110ns          709\u00b17ns     1.05  bench_core.CountNonzero.time_count_nonzero(1, 100, <class 'numpy.int8'>)\r\n+         532\u00b12\u03bcs          559\u00b12\u03bcs     1.05  bench_function_base.Sort.time_argsort('heap', 'float32', ('ordered',))\r\n+     2.85\u00b10.01\u03bcs      3.00\u00b10.02\u03bcs     1.05  bench_core.CountNonzero.time_count_nonzero(3, 100, <class 'str'>)\r\n+        1.52\u00b10\u03bcs      1.60\u00b10.05\u03bcs     1.05  bench_reduce.AnyAll.time_all_fast\r\n+     1.21\u00b10.02ms      1.27\u00b10.05ms     1.05  bench_core.CountNonzero.time_count_nonzero(2, 1000000, <class 'numpy.int64'>)\r\n+      56.2\u00b10.2\u03bcs       59.1\u00b10.9\u03bcs     1.05  bench_function_base.Sort.time_argsort('merge', 'float64', ('sorted_block', 1000))\r\n-        90.6\u00b11\u03bcs       86.2\u00b10.3\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'H')\r\n-         344\u00b15\u03bcs          327\u00b11\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 2, 1, 'd')\r\n-       342\u00b10.9\u03bcs          325\u00b12\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 2, 2, 'd')\r\n-     1.41\u00b10.01ms      1.34\u00b10.02ms     0.95  bench_lib.Nan.time_nanvar(200000, 0)\r\n-         215\u00b12\u03bcs          204\u00b12\u03bcs     0.95  bench_function_base.Sort.time_sort('quick', 'float32', ('reversed',))\r\n-     1.07\u00b10.02ms      1.02\u00b10.01ms     0.95  bench_reduce.AddReduceSeparate.time_reduce(0, 'longfloat')\r\n-         718\u00b17\u03bcs          682\u00b15\u03bcs     0.95  bench_indexing.Indexing.time_op('indexes_rand_', 'np.ix_(I, I)', '=1')\r\n-      13.2\u00b10.4ms      12.5\u00b10.07ms     0.95  bench_linalg.Linalg.time_op('svd', 'complex128')\r\n-        71.4\u00b11\u03bcs       67.7\u00b10.9\u03bcs     0.95  bench_function_base.Sort.time_sort('quick', 'int32', ('ordered',))\r\n-      88.2\u00b10.5\u03bcs       83.6\u00b10.8\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'H')\r\n-         273\u00b16\u03bcs          259\u00b16\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 2, 'd')\r\n-      62.2\u00b10.9\u03bcs       59.0\u00b10.3\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 2, 2, 'd')\r\n-      74.8\u00b10.4\u03bcs       70.8\u00b10.1\u03bcs     0.95  bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 10000)\r\n-         346\u00b14\u03bcs          328\u00b11\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 4, 1, 'd')\r\n-      92.7\u00b10.7\u03bcs         87.8\u00b11\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'H')\r\n-         347\u00b15\u03bcs          328\u00b13\u03bcs     0.95  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sqrt'>, 4, 2, 'd')\r\n-        88.3\u00b11\u03bcs       83.6\u00b10.4\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'H')\r\n-     1.41\u00b10.01ms      1.34\u00b10.01ms     0.95  bench_lib.Nan.time_nanvar(200000, 0.1)\r\n-       151\u00b10.8\u03bcs        143\u00b10.8\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'int16', ('reversed',))\r\n-         246\u00b12ms          232\u00b14ms     0.94  bench_app.LaplaceInplace.time_it('normal')\r\n-        89.0\u00b11\u03bcs       83.8\u00b10.6\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'H')\r\n-         347\u00b13\u03bcs          327\u00b11\u03bcs     0.94  bench_ufunc.UFunc.time_ufunc_types('square')\r\n-     1.54\u00b10.01ms      1.45\u00b10.01ms     0.94  bench_lib.Nan.time_nanargmin(200000, 50.0)\r\n-      91.1\u00b10.9\u03bcs       85.5\u00b10.8\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'H')\r\n-        90.3\u00b11\u03bcs       84.7\u00b10.9\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'H')\r\n-         430\u00b11\u03bcs          404\u00b12\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'uint32', ('sorted_block', 100))\r\n-       526\u00b10.5\u03bcs          493\u00b14\u03bcs     0.94  bench_function_base.Sort.time_sort('heap', 'float64', ('reversed',))\r\n-         478\u00b17\u03bcs          448\u00b13\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'int16', ('sorted_block', 10))\r\n-        92.8\u00b12\u03bcs         87.0\u00b11\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'H')\r\n-        63.4\u00b12\u03bcs       59.5\u00b10.1\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc '_ones_like'>, 2, 1, 'f')\r\n-     2.31\u00b10.01ms      2.16\u00b10.02ms     0.94  bench_lib.Nan.time_nanvar(200000, 90.0)\r\n-        79.5\u00b11\u03bcs         74.4\u00b11\u03bcs     0.94  bench_function_base.Sort.time_argsort('quick', 'int32', ('ordered',))\r\n-       272\u00b10.7\u03bcs          255\u00b13\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 1, 'd')\r\n-     2.33\u00b10.01ms      2.18\u00b10.02ms     0.94  bench_lib.Nan.time_nanstd(200000, 90.0)\r\n-      87.9\u00b10.6\u03bcs       82.3\u00b10.5\u03bcs     0.94  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'H')\r\n-         273\u00b12\u03bcs          255\u00b12\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 1, 'd')\r\n-     16.3\u00b10.08\u03bcs         15.3\u00b11\u03bcs     0.93  bench_core.CountNonzero.time_count_nonzero(2, 10000, <class 'numpy.int64'>)\r\n-     1.43\u00b10.02ms      1.33\u00b10.03ms     0.93  bench_lib.Pad.time_pad((1024, 1024), 1, 'mean')\r\n-        89.1\u00b11\u03bcs       83.1\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'H')\r\n-         275\u00b11\u03bcs          256\u00b13\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 1, 'd')\r\n-         725\u00b13\u03bcs          675\u00b13\u03bcs     0.93  bench_indexing.Indexing.time_op('indexes_', 'np.ix_(I, I)', '=1')\r\n-        91.8\u00b11\u03bcs       85.4\u00b10.6\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'H')\r\n-        91.0\u00b12\u03bcs       84.6\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'H')\r\n-        92.0\u00b12\u03bcs       85.6\u00b10.4\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'H')\r\n-      88.0\u00b10.8\u03bcs       81.8\u00b10.3\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'H')\r\n-        93.4\u00b12\u03bcs         86.7\u00b11\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'H')\r\n-      91.9\u00b10.6\u03bcs         85.3\u00b11\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'H')\r\n-        89.9\u00b11\u03bcs       83.4\u00b10.6\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'H')\r\n-        92.0\u00b11\u03bcs       85.3\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'H')\r\n-      91.1\u00b10.5\u03bcs       84.5\u00b10.6\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'H')\r\n-         276\u00b14\u03bcs          256\u00b14\u03bcs     0.93  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 2, 'd')\r\n-      88.6\u00b10.7\u03bcs       82.1\u00b10.5\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'H')\r\n-      92.5\u00b10.8\u03bcs       85.6\u00b10.9\u03bcs     0.93  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'H')\r\n-        93.4\u00b11\u03bcs       86.3\u00b10.5\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'H')\r\n-        90.8\u00b12\u03bcs       83.9\u00b10.6\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'H')\r\n-      95.0\u00b10.3\u03bcs       87.8\u00b10.4\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 4, 'H')\r\n-        89.6\u00b11\u03bcs       82.7\u00b10.2\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'H')\r\n-      91.5\u00b10.5\u03bcs       84.5\u00b10.2\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'H')\r\n-        89.4\u00b11\u03bcs       82.4\u00b10.3\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'H')\r\n-      89.9\u00b10.8\u03bcs       82.9\u00b10.4\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'H')\r\n-      92.4\u00b10.9\u03bcs       85.1\u00b10.4\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'H')\r\n-         420\u00b13\u03bcs          386\u00b12\u03bcs     0.92  bench_ufunc.UFunc.time_ufunc_types('multiply')\r\n-        91.9\u00b12\u03bcs         84.6\u00b11\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'H')\r\n-        93.8\u00b11\u03bcs       86.2\u00b10.7\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'H')\r\n-     1.38\u00b10.04ms      1.27\u00b10.03ms     0.92  bench_lib.Pad.time_pad((1024, 1024), 8, 'mean')\r\n-      91.8\u00b10.8\u03bcs       84.3\u00b10.3\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'H')\r\n-        91.5\u00b11\u03bcs       83.9\u00b10.7\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'H')\r\n-      95.3\u00b10.9\u03bcs       87.4\u00b10.6\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 4, 'H')\r\n-         265\u00b13\u03bcs          243\u00b14\u03bcs     0.92  bench_ufunc.UFunc.time_ufunc_types('minimum')\r\n-        94.0\u00b12\u03bcs       86.1\u00b10.7\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'H')\r\n-        93.6\u00b11\u03bcs       85.8\u00b10.5\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'H')\r\n-      90.8\u00b10.8\u03bcs       83.1\u00b10.4\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'H')\r\n-        91.5\u00b12\u03bcs       83.6\u00b10.3\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'H')\r\n-         557\u00b13\u03bcs          509\u00b11\u03bcs     0.91  bench_function_base.Sort.time_sort('heap', 'float32', ('reversed',))\r\n-      91.2\u00b10.7\u03bcs       83.3\u00b10.6\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'H')\r\n-        91.4\u00b11\u03bcs       83.4\u00b10.3\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'H')\r\n-         277\u00b11\u03bcs        253\u00b10.5\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 4, 'd')\r\n-     6.03\u00b10.07\u03bcs      5.49\u00b10.02\u03bcs     0.91  bench_itemselection.PutMask.time_dense(False, 'complex256')\r\n-        91.0\u00b11\u03bcs       83.0\u00b10.3\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'H')\r\n-         277\u00b11\u03bcs          253\u00b12\u03bcs     0.91  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 2, 'd')\r\n-        92.3\u00b11\u03bcs       84.1\u00b10.3\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'H')\r\n-      94.2\u00b10.8\u03bcs       85.6\u00b10.8\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'H')\r\n-      93.0\u00b10.2\u03bcs       84.5\u00b10.4\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'H')\r\n-      87.4\u00b10.9\u03bcs       79.4\u00b10.5\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'q')\r\n-      91.4\u00b10.9\u03bcs       82.9\u00b10.3\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'H')\r\n-      89.9\u00b10.5\u03bcs       81.4\u00b10.2\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'H')\r\n-         503\u00b13\u03bcs          455\u00b12\u03bcs     0.91  bench_function_base.Sort.time_argsort('quick', 'int16', ('sorted_block', 100))\r\n-        93.1\u00b11\u03bcs       84.1\u00b10.4\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'H')\r\n-        92.9\u00b12\u03bcs       84.0\u00b10.5\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'H')\r\n-         269\u00b14\u03bcs          243\u00b12\u03bcs     0.90  bench_ufunc.UFunc.time_ufunc_types('maximum')\r\n-         315\u00b13\u03bcs          284\u00b13\u03bcs     0.90  bench_ufunc.UFunc.time_ufunc_types('subtract')\r\n-        77.9\u00b11\u03bcs       70.0\u00b10.3\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'I')\r\n-         362\u00b14\u03bcs          324\u00b12\u03bcs     0.90  bench_function_base.Sort.time_argsort('quick', 'uint32', ('sorted_block', 1000))\r\n-        85.5\u00b12\u03bcs       76.5\u00b10.7\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'i')\r\n-        78.6\u00b12\u03bcs       70.2\u00b10.3\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'i')\r\n-        76.6\u00b11\u03bcs       68.4\u00b10.4\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'I')\r\n-         107\u00b16\u03bcs         95.4\u00b11\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 2, 'd')\r\n-        78.9\u00b11\u03bcs       70.4\u00b10.6\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'I')\r\n-      76.6\u00b10.9\u03bcs       68.3\u00b10.1\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'i')\r\n-         108\u00b14\u03bcs         96.5\u00b13\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 4, 'd')\r\n-        83.4\u00b11\u03bcs         74.3\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'Q')\r\n-        78.2\u00b11\u03bcs       69.6\u00b10.3\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'i')\r\n-        79.6\u00b12\u03bcs       70.8\u00b10.9\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'i')\r\n-        82.5\u00b11\u03bcs         73.4\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'i')\r\n-        82.0\u00b11\u03bcs       72.9\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'i')\r\n-         938\u00b16\u03bcs         833\u00b110\u03bcs     0.89  bench_lib.Nan.time_nanargmax(200000, 90.0)\r\n-         413\u00b11\u03bcs          367\u00b13\u03bcs     0.89  bench_ufunc.UFunc.time_ufunc_types('rint')\r\n-        79.3\u00b11\u03bcs       70.4\u00b10.9\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'I')\r\n-         110\u00b13\u03bcs         97.9\u00b12\u03bcs     0.89  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 4, 'd')\r\n-         391\u00b15\u03bcs          348\u00b12\u03bcs     0.89  bench_ufunc.UFunc.time_ufunc_types('fmin')\r\n-      77.3\u00b10.9\u03bcs       68.7\u00b10.2\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'i')\r\n-        78.5\u00b12\u03bcs       69.7\u00b10.3\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'i')\r\n-      89.7\u00b10.9\u03bcs       79.6\u00b10.4\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'L')\r\n-        88.9\u00b12\u03bcs       78.9\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'Q')\r\n-      77.7\u00b10.6\u03bcs       68.9\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'I')\r\n-        78.3\u00b12\u03bcs       69.3\u00b10.4\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'I')\r\n-      89.4\u00b10.4\u03bcs         79.2\u00b12\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'q')\r\n-      77.7\u00b10.6\u03bcs       68.6\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'i')\r\n-        88.8\u00b12\u03bcs         78.5\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'I')\r\n-      82.8\u00b10.6\u03bcs       73.2\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'i')\r\n-        81.7\u00b13\u03bcs       72.1\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'i')\r\n-         6.63\u00b10s          5.85\u00b10s     0.88  bench_ufunc_strides.LogisticRegression.time_train(<class 'numpy.float32'>)\r\n-      4.01\u00b10.1ms      3.53\u00b10.07ms     0.88  bench_core.VarComplex.time_var(1000000)\r\n-        69.6\u00b11ms       61.4\u00b10.9ms     0.88  bench_core.CountNonzero.time_count_nonzero_axis(2, 1000000, <class 'str'>)\r\n-        78.7\u00b11\u03bcs       69.3\u00b10.5\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'i')\r\n-        89.1\u00b11\u03bcs         78.5\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'Q')\r\n-        77.1\u00b12\u03bcs       67.9\u00b10.2\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'I')\r\n-        83.2\u00b11\u03bcs       73.2\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'Q')\r\n-        78.5\u00b11\u03bcs       69.0\u00b10.2\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'i')\r\n-        88.4\u00b11\u03bcs       77.7\u00b10.4\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'l')\r\n-        88.8\u00b11\u03bcs       78.0\u00b10.7\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'l')\r\n-      81.8\u00b10.9\u03bcs       71.8\u00b10.7\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'I')\r\n-        82.0\u00b11\u03bcs       72.0\u00b10.3\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'i')\r\n-        81.5\u00b12\u03bcs       71.6\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'I')\r\n-        90.4\u00b12\u03bcs         79.3\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'I')\r\n-        82.3\u00b11\u03bcs         72.2\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'I')\r\n-        79.0\u00b11\u03bcs       69.3\u00b10.3\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'I')\r\n-        78.0\u00b11\u03bcs       68.4\u00b10.2\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'I')\r\n-        77.6\u00b11\u03bcs       68.1\u00b10.2\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'i')\r\n-        89.2\u00b12\u03bcs       78.2\u00b10.6\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'l')\r\n-        82.1\u00b11\u03bcs         71.9\u00b11\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'i')\r\n-        79.7\u00b11\u03bcs       69.9\u00b10.3\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'I')\r\n-        77.7\u00b11\u03bcs       68.2\u00b10.3\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'i')\r\n-      81.5\u00b10.2\u03bcs       71.4\u00b10.9\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'i')\r\n-        90.3\u00b11\u03bcs         79.0\u00b12\u03bcs     0.88  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'L')\r\n-         473\u00b13\u03bcs          414\u00b12\u03bcs     0.88  bench_lib.Nan.time_nanargmax(200000, 0.1)\r\n-      61.4\u00b10.9\u03bcs       53.8\u00b10.6\u03bcs     0.88  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 2, 1, 'd')\r\n-      78.1\u00b10.2\u03bcs       68.3\u00b10.2\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'i')\r\n-         150\u00b11\u03bcs          131\u00b13\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 4, 'd')\r\n-        82.9\u00b12\u03bcs       72.5\u00b10.9\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'i')\r\n-      81.5\u00b10.6\u03bcs       71.3\u00b10.9\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'I')\r\n-         639\u00b12\u03bcs          559\u00b17\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 2, 'f')\r\n-        79.7\u00b12\u03bcs       69.7\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'i')\r\n-        84.4\u00b12\u03bcs       73.8\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'i')\r\n-         473\u00b13\u03bcs          413\u00b12\u03bcs     0.87  bench_lib.Nan.time_nanargmax(200000, 0)\r\n-         637\u00b13\u03bcs          556\u00b12\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 1, 'f')\r\n-        82.9\u00b11\u03bcs       72.4\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'i')\r\n-         523\u00b11\u03bcs          457\u00b13\u03bcs     0.87  bench_lib.Nan.time_nanargmax(200000, 2.0)\r\n-      89.5\u00b10.8\u03bcs       78.1\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'Q')\r\n-      89.1\u00b10.8\u03bcs         77.8\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'q')\r\n-      80.8\u00b10.7\u03bcs       70.5\u00b10.6\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'i')\r\n-      62.7\u00b10.9\u03bcs       54.7\u00b10.7\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 1, 2, 'd')\r\n-      82.0\u00b10.6\u03bcs       71.5\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'I')\r\n-         636\u00b11\u03bcs          555\u00b14\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 2, 'f')\r\n-        82.5\u00b11\u03bcs       71.9\u00b10.3\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'I')\r\n-        86.1\u00b12\u03bcs       75.1\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'I')\r\n-         638\u00b11\u03bcs          556\u00b15\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 1, 'f')\r\n-        78.0\u00b12\u03bcs       67.9\u00b10.3\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'I')\r\n-         639\u00b12\u03bcs          557\u00b13\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 4, 'f')\r\n-         108\u00b11ms         93.9\u00b13ms     0.87  bench_core.CountNonzero.time_count_nonzero_axis(3, 1000000, <class 'str'>)\r\n-     1.49\u00b10.01\u03bcs      1.29\u00b10.01\u03bcs     0.87  bench_itemselection.Take.time_contiguous((1000, 1), 'raise', 'int16')\r\n-         641\u00b12\u03bcs          558\u00b15\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 1, 'f')\r\n-        84.9\u00b12\u03bcs       73.9\u00b10.3\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'q')\r\n-        90.8\u00b12\u03bcs         79.0\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'i')\r\n-         527\u00b15\u03bcs          458\u00b19\u03bcs     0.87  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-        85.5\u00b11\u03bcs         74.4\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'I')\r\n-        90.0\u00b11\u03bcs         78.3\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'i')\r\n-        90.3\u00b12\u03bcs         78.6\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'I')\r\n-      83.1\u00b10.9\u03bcs       72.3\u00b10.2\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'q')\r\n-         641\u00b11\u03bcs          557\u00b17\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 1, 'f')\r\n-      82.6\u00b10.9\u03bcs       71.8\u00b10.6\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'i')\r\n-         642\u00b13\u03bcs          557\u00b17\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 1, 'f')\r\n-         110\u00b13\u03bcs         95.6\u00b11\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 2, 'd')\r\n-      78.8\u00b10.6\u03bcs       68.4\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'i')\r\n-       639\u00b10.6\u03bcs          554\u00b13\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 2, 'f')\r\n-     1.49\u00b10.01\u03bcs         1.30\u00b10\u03bcs     0.87  bench_itemselection.Take.time_contiguous((1000, 1), 'raise', 'float16')\r\n-        90.3\u00b12\u03bcs       78.4\u00b10.4\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'i')\r\n-        82.9\u00b12\u03bcs       71.9\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'I')\r\n-         112\u00b12\u03bcs         97.2\u00b12\u03bcs     0.87  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 4, 'd')\r\n-         640\u00b13\u03bcs          555\u00b13\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 4, 'f')\r\n-         636\u00b12\u03bcs          552\u00b11\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 4, 'f')\r\n-         642\u00b13\u03bcs          557\u00b13\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 2, 'f')\r\n-        85.6\u00b11\u03bcs       74.3\u00b10.8\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'i')\r\n-         641\u00b16\u03bcs          555\u00b11\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 1, 'f')\r\n-         640\u00b13\u03bcs          555\u00b11\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 1, 'f')\r\n-         642\u00b12\u03bcs          557\u00b15\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 2, 'f')\r\n-         641\u00b11\u03bcs          556\u00b12\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 1, 'f')\r\n-         490\u00b12\u03bcs          425\u00b11\u03bcs     0.87  bench_function_base.Sort.time_sort('heap', 'float64', ('ordered',))\r\n-        89.9\u00b11\u03bcs         77.9\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'L')\r\n-        84.1\u00b11\u03bcs       72.8\u00b10.9\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'I')\r\n-      82.2\u00b10.9\u03bcs         71.2\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'i')\r\n-         639\u00b13\u03bcs          553\u00b12\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 2, 'f')\r\n-         638\u00b13\u03bcs          552\u00b12\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 1, 'f')\r\n-        82.6\u00b12\u03bcs       71.5\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'I')\r\n-        86.7\u00b11\u03bcs       75.0\u00b10.5\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'I')\r\n-      81.1\u00b10.4\u03bcs       70.2\u00b10.7\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'I')\r\n-         638\u00b11\u03bcs        552\u00b10.6\u03bcs     0.87  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 2, 'f')\r\n-        90.0\u00b12\u03bcs         77.9\u00b11\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'i')\r\n-         642\u00b12\u03bcs          555\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 4, 'f')\r\n-         638\u00b11\u03bcs        551\u00b10.8\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 4, 'f')\r\n-        88.7\u00b11\u03bcs         76.7\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'I')\r\n-         643\u00b15\u03bcs          556\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 4, 'f')\r\n-        83.8\u00b11\u03bcs         72.5\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'I')\r\n-      90.5\u00b10.9\u03bcs       78.2\u00b10.5\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'L')\r\n-         644\u00b12\u03bcs          557\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 4, 'f')\r\n-      88.1\u00b10.5\u03bcs       76.1\u00b10.9\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'i')\r\n-         642\u00b12\u03bcs          555\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 2, 'f')\r\n-         659\u00b17\u03bcs          570\u00b17\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 4, 'f')\r\n-         641\u00b12\u03bcs          554\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 4, 'f')\r\n-      81.5\u00b10.9\u03bcs       70.4\u00b10.5\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'I')\r\n-      84.3\u00b10.9\u03bcs       72.8\u00b10.6\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'i')\r\n-        89.4\u00b11\u03bcs       77.2\u00b10.9\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'q')\r\n-         652\u00b18\u03bcs          563\u00b15\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 4, 'f')\r\n-         643\u00b14\u03bcs          556\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 4, 'f')\r\n-        82.8\u00b12\u03bcs         71.5\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'i')\r\n-         639\u00b12\u03bcs        551\u00b10.6\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 2, 'f')\r\n-        80.8\u00b11\u03bcs       69.8\u00b10.6\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'i')\r\n-         642\u00b13\u03bcs          554\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 2, 'f')\r\n-      83.6\u00b10.7\u03bcs       72.2\u00b10.9\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'L')\r\n-        81.5\u00b12\u03bcs       70.3\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'I')\r\n-         639\u00b13\u03bcs          552\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 4, 'f')\r\n-        81.0\u00b12\u03bcs       69.8\u00b10.2\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'i')\r\n-        79.2\u00b12\u03bcs       68.3\u00b10.3\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'I')\r\n-         643\u00b12\u03bcs        554\u00b10.9\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 2, 'f')\r\n-        83.3\u00b11\u03bcs       71.8\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'i')\r\n-         641\u00b12\u03bcs          553\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 2, 'f')\r\n-         640\u00b12\u03bcs          552\u00b12\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 2, 'f')\r\n-        83.4\u00b12\u03bcs       71.9\u00b10.7\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'I')\r\n-         639\u00b12\u03bcs        551\u00b10.5\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 1, 'f')\r\n-       640\u00b10.7\u03bcs          552\u00b12\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 1, 'f')\r\n-        83.8\u00b11\u03bcs       72.3\u00b10.7\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'Q')\r\n-        84.9\u00b12\u03bcs       73.2\u00b10.5\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'L')\r\n-         642\u00b12\u03bcs          553\u00b11\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 1, 'f')\r\n-         641\u00b13\u03bcs          552\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 1, 'f')\r\n-         643\u00b12\u03bcs          554\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 2, 'f')\r\n-         641\u00b13\u03bcs          552\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 2, 'f')\r\n-        84.3\u00b12\u03bcs       72.7\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'l')\r\n-         645\u00b14\u03bcs          555\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 1, 'f')\r\n-      86.9\u00b10.7\u03bcs       74.8\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'I')\r\n-        89.7\u00b11\u03bcs         77.3\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'l')\r\n-     2.47\u00b10.02\u03bcs      2.13\u00b10.02\u03bcs     0.86  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'raise', 'float16')\r\n-         642\u00b11\u03bcs          553\u00b12\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 1, 'f')\r\n-         641\u00b15\u03bcs          552\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 1, 'f')\r\n-         642\u00b11\u03bcs          553\u00b13\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 2, 'f')\r\n-        90.6\u00b11\u03bcs         77.9\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'Q')\r\n-        82.6\u00b11\u03bcs       71.0\u00b10.5\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'i')\r\n-         645\u00b12\u03bcs          555\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 4, 'f')\r\n-      83.5\u00b10.8\u03bcs       71.8\u00b10.6\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'l')\r\n-         391\u00b12\u03bcs          336\u00b14\u03bcs     0.86  bench_ufunc.UFunc.time_ufunc_types('fmax')\r\n-        83.4\u00b11\u03bcs       71.7\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'L')\r\n-        89.3\u00b12\u03bcs       76.8\u00b10.8\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'Q')\r\n-      86.9\u00b10.8\u03bcs       74.7\u00b10.8\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'i')\r\n-        85.3\u00b11\u03bcs       73.3\u00b10.8\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'l')\r\n-      87.0\u00b10.9\u03bcs       74.7\u00b10.2\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'I')\r\n-         108\u00b12ms         93.0\u00b12ms     0.86  bench_core.CountNonzero.time_count_nonzero_multi_axis(3, 1000000, <class 'str'>)\r\n-        85.5\u00b11\u03bcs       73.4\u00b10.5\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'i')\r\n-         645\u00b16\u03bcs          553\u00b11\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 2, 'f')\r\n-        83.7\u00b11\u03bcs         71.9\u00b11\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'I')\r\n-         644\u00b13\u03bcs          553\u00b12\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 1, 'f')\r\n-         643\u00b12\u03bcs          551\u00b12\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 4, 'f')\r\n-        91.5\u00b12\u03bcs       78.4\u00b10.7\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'i')\r\n-        87.6\u00b11\u03bcs       75.1\u00b10.7\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'i')\r\n-        84.7\u00b11\u03bcs       72.6\u00b10.5\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'q')\r\n-         644\u00b13\u03bcs          552\u00b12\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 4, 'f')\r\n-        69.4\u00b12ms         59.5\u00b11ms     0.86  bench_core.CountNonzero.time_count_nonzero_multi_axis(2, 1000000, <class 'str'>)\r\n-         482\u00b16\u03bcs          413\u00b16\u03bcs     0.86  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-      79.2\u00b10.5\u03bcs       67.8\u00b10.4\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'I')\r\n-         645\u00b15\u03bcs          552\u00b11\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 4, 'f')\r\n-        90.0\u00b11\u03bcs       77.0\u00b10.8\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'L')\r\n-         649\u00b14\u03bcs          556\u00b14\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 2, 'f')\r\n-         647\u00b12\u03bcs          554\u00b15\u03bcs     0.86  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 1, 'f')\r\n-         521\u00b13\u03bcs          446\u00b11\u03bcs     0.86  bench_function_base.Sort.time_sort('heap', 'float32', ('ordered',))\r\n-      79.7\u00b10.4\u03bcs       68.2\u00b10.2\u03bcs     0.86  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'I')\r\n-         648\u00b17\u03bcs          554\u00b12\u03bcs     0.85  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 4, 'f')\r\n-        85.0\u00b12\u03bcs       72.7\u00b10.8\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'l')\r\n-      91.5\u00b10.7\u03bcs       78.2\u00b10.6\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'q')\r\n-        86.0\u00b11\u03bcs         73.4\u00b11\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'Q')\r\n-         149\u00b14\u03bcs          127\u00b13\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 4, 'd')\r\n-        84.9\u00b11\u03bcs       72.4\u00b10.6\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'L')\r\n-      83.5\u00b10.8\u03bcs       71.3\u00b10.7\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'i')\r\n-      86.5\u00b10.8\u03bcs         73.8\u00b11\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'Q')\r\n-        85.3\u00b12\u03bcs       72.8\u00b10.3\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'L')\r\n-        84.7\u00b11\u03bcs       72.3\u00b10.4\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'I')\r\n-      83.0\u00b10.9\u03bcs       70.7\u00b10.4\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'I')\r\n-     2.47\u00b10.02\u03bcs      2.10\u00b10.02\u03bcs     0.85  bench_itemselection.Take.time_contiguous((2, 1000, 1), 'raise', 'int16')\r\n-      58.6\u00b10.6\u03bcs       50.0\u00b10.6\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 2, 'd')\r\n-        87.3\u00b11\u03bcs       74.4\u00b10.9\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'i')\r\n-         109\u00b13\u03bcs         92.6\u00b14\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 2, 'd')\r\n-        91.4\u00b11\u03bcs         77.8\u00b12\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'I')\r\n-         653\u00b15\u03bcs          556\u00b13\u03bcs     0.85  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 4, 'f')\r\n-      83.9\u00b10.8\u03bcs       71.4\u00b10.9\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'I')\r\n-      57.8\u00b10.6\u03bcs       49.2\u00b10.5\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 2, 'd')\r\n-      84.7\u00b10.8\u03bcs       72.0\u00b10.6\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'i')\r\n-        82.4\u00b12\u03bcs       70.1\u00b10.2\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'I')\r\n-      82.7\u00b10.8\u03bcs       70.3\u00b10.5\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'i')\r\n-         260\u00b11\u03bcs          221\u00b12\u03bcs     0.85  bench_ufunc.UFunc.time_ufunc_types('floor')\r\n-        86.2\u00b12\u03bcs       73.3\u00b10.9\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'q')\r\n-        84.4\u00b12\u03bcs       71.7\u00b10.4\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'I')\r\n-        82.7\u00b12\u03bcs       70.2\u00b10.4\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'i')\r\n-        87.8\u00b11\u03bcs       74.5\u00b10.8\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'I')\r\n-        85.6\u00b11\u03bcs       72.7\u00b10.2\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'I')\r\n-         151\u00b12\u03bcs          128\u00b13\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 4, 'd')\r\n-     5.24\u00b10.01\u03bcs      4.44\u00b10.04\u03bcs     0.85  bench_lib.Nan.time_nanmin(200, 2.0)\r\n-        91.8\u00b11\u03bcs       77.8\u00b10.8\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'L')\r\n-        92.1\u00b12\u03bcs       78.0\u00b10.8\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'I')\r\n-      84.1\u00b10.8\u03bcs       71.1\u00b10.3\u03bcs     0.85  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'I')\r\n-         258\u00b13\u03bcs          218\u00b15\u03bcs     0.85  bench_ufunc.UFunc.time_ufunc_types('trunc')\r\n-         327\u00b14\u03bcs         277\u00b110\u03bcs     0.85  bench_ufunc.UFunc.time_ufunc_types('add')\r\n-        78.1\u00b12\u03bcs         66.0\u00b13\u03bcs     0.85  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 4, 'd')\r\n-        90.4\u00b11\u03bcs       76.3\u00b10.7\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'Q')\r\n-      91.4\u00b10.8\u03bcs         77.1\u00b11\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'l')\r\n-        83.3\u00b11\u03bcs       70.2\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'I')\r\n-      85.3\u00b10.3\u03bcs       71.9\u00b10.8\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'q')\r\n-        91.5\u00b13\u03bcs         77.1\u00b11\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'l')\r\n-        483\u00b110\u03bcs          407\u00b15\u03bcs     0.84  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-        94.9\u00b11\u03bcs       79.9\u00b10.3\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 4, 'B')\r\n-      94.3\u00b10.6\u03bcs       79.3\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 4, 'B')\r\n-      85.0\u00b10.9\u03bcs       71.5\u00b10.8\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'q')\r\n-      84.3\u00b10.7\u03bcs       70.9\u00b10.2\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'I')\r\n-     5.28\u00b10.08\u03bcs      4.43\u00b10.07\u03bcs     0.84  bench_lib.Nan.time_nanmin(200, 0.1)\r\n-      85.8\u00b10.5\u03bcs       72.1\u00b10.4\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'L')\r\n-      85.3\u00b10.3\u03bcs       71.6\u00b10.3\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'Q')\r\n-      94.5\u00b10.9\u03bcs       79.3\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 1, 'B')\r\n-        993\u00b160\u03bcs          833\u00b19\u03bcs     0.84  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-        60.4\u00b11\u03bcs       50.7\u00b10.5\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 2, 'd')\r\n-        91.5\u00b12\u03bcs       76.8\u00b10.7\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'q')\r\n-      18.9\u00b10.3ms       15.9\u00b10.9ms     0.84  bench_core.CountNonzero.time_count_nonzero_axis(1, 1000000, <class 'str'>)\r\n-      95.3\u00b10.6\u03bcs       79.9\u00b10.9\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 2, 'B')\r\n-     1.24\u00b10.02ms      1.04\u00b10.06ms     0.84  bench_core.Temporaries.time_large2\r\n-      94.5\u00b10.5\u03bcs       79.1\u00b10.4\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'B')\r\n-      93.4\u00b10.6\u03bcs       78.3\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'B')\r\n-        59.3\u00b11\u03bcs       49.7\u00b10.5\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 2, 'd')\r\n-      95.9\u00b10.4\u03bcs       80.1\u00b10.7\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 4, 'B')\r\n-      95.3\u00b10.9\u03bcs       79.7\u00b10.8\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 4, 'B')\r\n-        95.1\u00b11\u03bcs       79.5\u00b10.5\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 4, 'B')\r\n-         259\u00b12\u03bcs          217\u00b13\u03bcs     0.84  bench_ufunc.UFunc.time_ufunc_types('ceil')\r\n-      93.6\u00b10.5\u03bcs       78.2\u00b10.8\u03bcs     0.84  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'B')\r\n-         113\u00b12\u03bcs         94.8\u00b13\u03bcs     0.84  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 2, 'd')\r\n-        85.7\u00b11\u03bcs       71.6\u00b10.5\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'l')\r\n-        93.6\u00b11\u03bcs       78.0\u00b10.9\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'B')\r\n-         113\u00b13\u03bcs         94.5\u00b12\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 4, 'd')\r\n-        94.0\u00b11\u03bcs       78.3\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 1, 'B')\r\n-      93.0\u00b10.1\u03bcs       77.4\u00b10.6\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'B')\r\n-     5.32\u00b10.01\u03bcs      4.43\u00b10.04\u03bcs     0.83  bench_lib.Nan.time_nanmax(200, 0)\r\n-      94.4\u00b10.3\u03bcs       78.6\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 1, 'B')\r\n-     5.31\u00b10.07\u03bcs      4.42\u00b10.04\u03bcs     0.83  bench_lib.Nan.time_nanmin(200, 0)\r\n-      93.2\u00b10.5\u03bcs         77.5\u00b12\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'B')\r\n-        76.7\u00b12\u03bcs         63.7\u00b12\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 4, 'd')\r\n-         153\u00b13\u03bcs          127\u00b13\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 4, 'd')\r\n-      92.7\u00b10.2\u03bcs       77.0\u00b10.6\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 2, 'B')\r\n-      95.1\u00b10.8\u03bcs       79.0\u00b10.5\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'B')\r\n-      94.3\u00b10.9\u03bcs       78.3\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 2, 'B')\r\n-     94.4\u00b10.09\u03bcs       78.4\u00b10.6\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 4, 'B')\r\n-      95.6\u00b10.9\u03bcs         79.4\u00b11\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 2, 'B')\r\n-        94.6\u00b11\u03bcs       78.5\u00b10.2\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 4, 'B')\r\n-        75.5\u00b12\u03bcs         62.7\u00b12\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 4, 'd')\r\n-        94.6\u00b11\u03bcs       78.4\u00b10.5\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 4, 'B')\r\n-      61.4\u00b10.7\u03bcs       50.9\u00b10.4\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sign'>, 1, 1, 'd')\r\n-      86.9\u00b10.9\u03bcs       72.0\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'l')\r\n-     5.34\u00b10.04\u03bcs      4.42\u00b10.08\u03bcs     0.83  bench_lib.Nan.time_nanmax(200, 0.1)\r\n-        95.1\u00b11\u03bcs       78.8\u00b10.5\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'B')\r\n-        50.1\u00b11\u03bcs       41.5\u00b10.8\u03bcs     0.83  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 2, 'd')\r\n-      94.3\u00b10.8\u03bcs       78.0\u00b10.2\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'B')\r\n-      94.7\u00b10.8\u03bcs       78.2\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 4, 1, 'B')\r\n-      94.3\u00b10.7\u03bcs       77.8\u00b10.3\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'B')\r\n-        94.6\u00b11\u03bcs       78.1\u00b10.4\u03bcs     0.83  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'B')\r\n-      94.8\u00b10.2\u03bcs       78.1\u00b10.1\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 4, 2, 'B')\r\n-      94.6\u00b10.4\u03bcs       77.8\u00b10.6\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'B')\r\n-        95.6\u00b11\u03bcs       78.6\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'B')\r\n-      92.7\u00b10.3\u03bcs       76.2\u00b10.8\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 2, 'B')\r\n-      93.3\u00b10.4\u03bcs       76.7\u00b10.8\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 1, 'B')\r\n-      92.7\u00b10.4\u03bcs       76.2\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 2, 'B')\r\n-      93.1\u00b10.3\u03bcs       76.5\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 2, 'B')\r\n-      93.5\u00b10.5\u03bcs       76.8\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 4, 'B')\r\n-         154\u00b14\u03bcs          126\u00b11\u03bcs     0.82  bench_function_base.Sort.time_argsort('quick', 'uint32', ('reversed',))\r\n-      17.5\u00b10.5\u03bcs       14.3\u00b10.3\u03bcs     0.82  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 4, 'D')\r\n-     1.23\u00b10.03ms      1.01\u00b10.04ms     0.82  bench_core.Temporaries.time_large\r\n-      95.5\u00b10.6\u03bcs       78.1\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 2, 4, 'B')\r\n-      51.8\u00b10.4\u03bcs         42.3\u00b11\u03bcs     0.82  bench_core.VarComplex.time_var(10000)\r\n-        79.6\u00b12\u03bcs         65.0\u00b13\u03bcs     0.82  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 4, 'd')\r\n-      94.0\u00b10.8\u03bcs       76.7\u00b10.6\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 1, 'B')\r\n-        94.6\u00b11\u03bcs       77.2\u00b10.4\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 4, 'B')\r\n-         192\u00b12\u03bcs        157\u00b10.3\u03bcs     0.82  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-        94.1\u00b11\u03bcs       76.7\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'B')\r\n-     5.40\u00b10.03\u03bcs      4.41\u00b10.01\u03bcs     0.82  bench_lib.Nan.time_nanmin(200, 90.0)\r\n-      92.9\u00b10.2\u03bcs       75.7\u00b10.5\u03bcs     0.82  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 2, 'B')\r\n-     5.43\u00b10.03\u03bcs      4.43\u00b10.07\u03bcs     0.81  bench_lib.Nan.time_nanmax(200, 2.0)\r\n-      94.1\u00b10.2\u03bcs       76.6\u00b10.2\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'B')\r\n-      92.8\u00b10.3\u03bcs       75.5\u00b10.4\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 1, 'B')\r\n-      93.7\u00b10.6\u03bcs         76.0\u00b13\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'B')\r\n-         196\u00b13\u03bcs        159\u00b10.8\u03bcs     0.81  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-       121\u00b10.4ms         98.0\u00b12ms     0.81  bench_app.LaplaceInplace.time_it('inplace')\r\n-      95.3\u00b10.9\u03bcs       77.1\u00b10.3\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 4, 'B')\r\n-      94.2\u00b10.8\u03bcs       76.2\u00b10.3\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'B')\r\n-      93.8\u00b10.4\u03bcs       75.8\u00b10.6\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'B')\r\n-        94.8\u00b11\u03bcs       76.5\u00b10.4\u03bcs     0.81  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 2, 'B')\r\n-     5.43\u00b10.02\u03bcs      4.37\u00b10.02\u03bcs     0.81  bench_lib.Nan.time_nanmax(200, 90.0)\r\n-        76.2\u00b12\u03bcs         61.3\u00b12\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 4, 'd')\r\n-         119\u00b14\u03bcs         95.4\u00b12\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 4, 'd')\r\n-      94.0\u00b10.8\u03bcs       75.6\u00b10.5\u03bcs     0.80  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'B')\r\n-         120\u00b12\u03bcs         96.3\u00b13\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 2, 'd')\r\n-      95.9\u00b10.7\u03bcs       77.0\u00b10.4\u03bcs     0.80  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 2, 'B')\r\n-        95.7\u00b12\u03bcs       76.6\u00b10.2\u03bcs     0.80  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 4, 'B')\r\n-        94.5\u00b11\u03bcs       75.7\u00b10.4\u03bcs     0.80  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 1, 1, 'B')\r\n-      95.3\u00b10.6\u03bcs       76.1\u00b10.3\u03bcs     0.80  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 4, 'B')\r\n-      51.2\u00b10.3\u03bcs       40.8\u00b10.4\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 2, 'd')\r\n-        50.9\u00b12\u03bcs       40.6\u00b10.4\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 2, 'd')\r\n-        52.0\u00b11\u03bcs       41.3\u00b10.6\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 2, 'd')\r\n-         488\u00b15\u03bcs          387\u00b13\u03bcs     0.79  bench_function_base.Sort.time_argsort('quick', 'int16', ('sorted_block', 1000))\r\n-         371\u00b16\u03bcs          291\u00b15\u03bcs     0.79  bench_core.VarComplex.time_var(100000)\r\n-      8.18\u00b10.4ms       6.39\u00b10.3ms     0.78  bench_ufunc.Broadcast.time_broadcast\r\n-      42.6\u00b10.1\u03bcs       32.5\u00b10.3\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 1, 'd')\r\n-      42.7\u00b10.5\u03bcs       32.4\u00b10.2\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 1, 'd')\r\n-         168\u00b13\u03bcs          127\u00b14\u03bcs     0.76  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 4, 'd')\r\n-     4.95\u00b10.03ms      3.71\u00b10.06ms     0.75  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'f')\r\n-     4.88\u00b10.02ms      3.63\u00b10.01ms     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'f')\r\n-     5.90\u00b10.04\u03bcs      4.38\u00b10.03\u03bcs     0.74  bench_lib.Nan.time_nanmin(200, 50.0)\r\n-     4.87\u00b10.01ms         3.61\u00b10ms     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-     4.89\u00b10.03ms      3.63\u00b10.01ms     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n-        67.3\u00b12\u03bcs       49.7\u00b10.9\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 2, 'd')\r\n-      43.4\u00b10.5\u03bcs       32.0\u00b10.3\u03bcs     0.74  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 1, 'd')\r\n-     6.05\u00b10.05\u03bcs      4.45\u00b10.06\u03bcs     0.74  bench_lib.Nan.time_nanmax(200, 50.0)\r\n-        43.5\u00b11\u03bcs       32.0\u00b10.2\u03bcs     0.73  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 1, 'd')\r\n-     4.95\u00b10.06ms      3.63\u00b10.02ms     0.73  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-     4.94\u00b10.06ms      3.62\u00b10.01ms     0.73  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'f')\r\n-     4.95\u00b10.02ms      3.62\u00b10.02ms     0.73  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'f')\r\n-     4.94\u00b10.04ms      3.61\u00b10.01ms     0.73  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'f')\r\n-      18.7\u00b10.6\u03bcs      13.5\u00b10.06\u03bcs     0.72  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 4, 'D')\r\n-     5.04\u00b10.03ms      3.63\u00b10.05ms     0.72  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'f')\r\n-     11.3\u00b10.07\u03bcs      8.02\u00b10.05\u03bcs     0.71  bench_ufunc.CustomScalar.time_add_scalar2(<class 'numpy.float64'>)\r\n-         133\u00b12\u03bcs         94.0\u00b11\u03bcs     0.71  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 2, 'd')\r\n-      14.8\u00b10.3\u03bcs       10.4\u00b10.3\u03bcs     0.70  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 2, 'D')\r\n-      58.7\u00b10.2\u03bcs       40.7\u00b10.7\u03bcs     0.69  bench_core.Temporaries.time_mid2\r\n-      59.0\u00b10.5\u03bcs       40.7\u00b10.9\u03bcs     0.69  bench_core.Temporaries.time_mid\r\n-      14.8\u00b10.3\u03bcs      10.2\u00b10.09\u03bcs     0.69  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 2, 'D')\r\n-         540\u00b12\u03bcs          371\u00b11\u03bcs     0.69  bench_reduce.AddReduceSeparate.time_reduce(0, 'float64')\r\n-         140\u00b14\u03bcs         96.0\u00b13\u03bcs     0.69  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 4, 'd')\r\n-        92.0\u00b13\u03bcs         63.1\u00b13\u03bcs     0.69  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 4, 'd')\r\n-         187\u00b14\u03bcs          128\u00b14\u03bcs     0.68  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 4, 'd')\r\n-      21.0\u00b10.3\u03bcs       14.4\u00b10.2\u03bcs     0.68  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 4, 'F')\r\n-      21.3\u00b10.5\u03bcs       14.4\u00b10.3\u03bcs     0.68  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 4, 'F')\r\n-      12.9\u00b10.2\u03bcs       8.68\u00b10.2\u03bcs     0.67  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 1, 'D')\r\n-      64.6\u00b10.5\u03bcs         43.2\u00b11\u03bcs     0.67  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 1, 'd')\r\n-      81.7\u00b10.5\u03bcs       54.6\u00b10.6\u03bcs     0.67  bench_ufunc.CustomInplace.time_double_add_temp\r\n-        65.4\u00b11\u03bcs       43.1\u00b10.9\u03bcs     0.66  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 1, 'd')\r\n-      65.7\u00b10.6\u03bcs       43.3\u00b10.8\u03bcs     0.66  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 1, 'd')\r\n-     1.23\u00b10.05ms         803\u00b110\u03bcs     0.65  bench_reduce.AddReduceSeparate.time_reduce(0, 'complex128')\r\n-      73.7\u00b10.4\u03bcs       47.6\u00b10.2\u03bcs     0.65  bench_ufunc.CustomInplace.time_double_add\r\n-      40.9\u00b10.4\u03bcs       26.2\u00b10.3\u03bcs     0.64  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 1, 'd')\r\n-        64.4\u00b11\u03bcs       40.9\u00b10.5\u03bcs     0.63  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 2, 'd')\r\n-      20.9\u00b10.3\u03bcs      13.2\u00b10.08\u03bcs     0.63  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 2, 'F')\r\n-      41.2\u00b10.9\u03bcs       26.0\u00b10.1\u03bcs     0.63  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 1, 'd')\r\n-        68.9\u00b12\u03bcs       43.2\u00b10.7\u03bcs     0.63  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 1, 'd')\r\n-      41.3\u00b10.6\u03bcs       25.8\u00b10.4\u03bcs     0.63  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 1, 'd')\r\n-        69.3\u00b12\u03bcs       43.3\u00b10.8\u03bcs     0.62  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 1, 'd')\r\n-      41.2\u00b10.7\u03bcs       25.7\u00b10.2\u03bcs     0.62  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 1, 'd')\r\n-      20.9\u00b10.4\u03bcs       12.8\u00b10.8\u03bcs     0.61  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 1, 'F')\r\n-      21.1\u00b10.3\u03bcs       12.9\u00b10.2\u03bcs     0.61  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('add', 2, 'F')\r\n-      21.0\u00b10.4\u03bcs       12.6\u00b10.4\u03bcs     0.60  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 1, 'F')\r\n-      81.6\u00b10.6\u03bcs       49.0\u00b10.5\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 2, 'd')\r\n-      13.6\u00b10.4\u03bcs       7.80\u00b10.3\u03bcs     0.57  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('subtract', 1, 'D')\r\n-       158\u00b10.8\u03bcs         88.8\u00b11\u03bcs     0.56  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-         157\u00b11\u03bcs       88.1\u00b10.3\u03bcs     0.56  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-     1.96\u00b10.01ms         1.08\u00b10ms     0.55  bench_reduce.AddReduceSeparate.time_reduce(0, 'complex64')\r\n-      66.5\u00b10.3\u03bcs      33.8\u00b10.05\u03bcs     0.51  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 43)\r\n-     66.7\u00b10.09\u03bcs       33.9\u00b10.1\u03bcs     0.51  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -43)\r\n-        82.3\u00b12\u03bcs       41.7\u00b10.6\u03bcs     0.51  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 2, 'd')\r\n-      71.7\u00b10.4\u03bcs       35.0\u00b10.1\u03bcs     0.49  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, 8)\r\n-      71.6\u00b10.3\u03bcs      34.4\u00b10.06\u03bcs     0.48  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int64'>, -8)\r\n-        66.2\u00b11\u03bcs       31.4\u00b10.1\u03bcs     0.47  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 1, 'd')\r\n-        88.1\u00b13\u03bcs         41.3\u00b11\u03bcs     0.47  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 1, 'd')\r\n-      78.7\u00b10.7\u03bcs       35.7\u00b10.5\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'Q')\r\n-        79.1\u00b11\u03bcs       35.8\u00b10.7\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'l')\r\n-        79.8\u00b11\u03bcs       35.8\u00b10.5\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'L')\r\n-        79.4\u00b11\u03bcs       35.5\u00b10.5\u03bcs     0.45  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'q')\r\n-         434\u00b11\u03bcs          194\u00b14\u03bcs     0.45  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 4, 'd')\r\n-        80.5\u00b11\u03bcs       35.6\u00b10.5\u03bcs     0.44  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'L')\r\n-      12.6\u00b10.2\u03bcs      5.57\u00b10.02\u03bcs     0.44  bench_reduce.MinMax.time_min(<class 'numpy.int64'> (1))\r\n-         443\u00b12\u03bcs          196\u00b13\u03bcs     0.44  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 4, 'd')\r\n-      12.6\u00b10.2\u03bcs      5.57\u00b10.01\u03bcs     0.44  bench_reduce.MinMax.time_min(<class 'numpy.uint64'>)\r\n-      13.2\u00b10.3\u03bcs      5.82\u00b10.02\u03bcs     0.44  bench_reduce.MinMax.time_max(<class 'numpy.uint64'>)\r\n-      81.0\u00b10.5\u03bcs       35.6\u00b10.6\u03bcs     0.44  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'Q')\r\n-      81.5\u00b10.6\u03bcs       35.7\u00b10.4\u03bcs     0.44  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'l')\r\n-        81.4\u00b11\u03bcs       35.6\u00b10.8\u03bcs     0.44  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'q')\r\n-      12.8\u00b10.2\u03bcs      5.54\u00b10.02\u03bcs     0.43  bench_reduce.MinMax.time_min(<class 'numpy.int64'> (0))\r\n-      12.9\u00b10.3\u03bcs      5.61\u00b10.05\u03bcs     0.43  bench_reduce.MinMax.time_max(<class 'numpy.int64'> (1))\r\n-      13.2\u00b10.3\u03bcs      5.66\u00b10.02\u03bcs     0.43  bench_reduce.MinMax.time_max(<class 'numpy.int64'> (0))\r\n-         505\u00b11\u03bcs          216\u00b11\u03bcs     0.43  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 4, 'f')\r\n-         506\u00b13\u03bcs          213\u00b13\u03bcs     0.42  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 2, 'f')\r\n-      61.7\u00b10.9\u03bcs       25.9\u00b10.1\u03bcs     0.42  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 1, 'd')\r\n-         505\u00b11\u03bcs          209\u00b11\u03bcs     0.41  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 1, 'f')\r\n-         432\u00b12\u03bcs         177\u00b110\u03bcs     0.41  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 4, 'd')\r\n-         520\u00b19\u03bcs          209\u00b15\u03bcs     0.40  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 4, 'f')\r\n-         508\u00b16\u03bcs          200\u00b14\u03bcs     0.39  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 4, 'f')\r\n-         436\u00b12\u03bcs          170\u00b13\u03bcs     0.39  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 2, 'd')\r\n-         503\u00b11\u03bcs          195\u00b12\u03bcs     0.39  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 2, 'f')\r\n-         438\u00b14\u03bcs          170\u00b15\u03bcs     0.39  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 2, 'd')\r\n-      36.4\u00b10.2\u03bcs       14.1\u00b10.1\u03bcs     0.39  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 4, 'F')\r\n-         432\u00b13\u03bcs          167\u00b13\u03bcs     0.39  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 4, 'd')\r\n-         432\u00b13\u03bcs          167\u00b13\u03bcs     0.39  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 4, 'd')\r\n-         505\u00b12\u03bcs          195\u00b14\u03bcs     0.39  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 2, 1, 'f')\r\n-        81.6\u00b12\u03bcs       31.4\u00b10.3\u03bcs     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 1, 'd')\r\n-         509\u00b13\u03bcs          195\u00b12\u03bcs     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 4, 1, 'f')\r\n-         436\u00b12\u03bcs          167\u00b15\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 4, 'd')\r\n-         436\u00b13\u03bcs          166\u00b13\u03bcs     0.38  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 4, 'd')\r\n-         218\u00b12\u03bcs         83.0\u00b12\u03bcs     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 4, 'f')\r\n-      12.0\u00b10.2\u03bcs      4.56\u00b10.02\u03bcs     0.38  bench_reduce.MinMax.time_max(<class 'numpy.uint32'>)\r\n-       215\u00b10.9\u03bcs       81.0\u00b10.3\u03bcs     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 1, 'f')\r\n-         220\u00b12\u03bcs         82.5\u00b11\u03bcs     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 4, 'f')\r\n-      12.0\u00b10.2\u03bcs      4.50\u00b10.05\u03bcs     0.38  bench_reduce.MinMax.time_min(<class 'numpy.int32'>)\r\n-         505\u00b12\u03bcs          189\u00b11\u03bcs     0.38  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'reciprocal'>, 1, 2, 'f')\r\n-      36.3\u00b10.1\u03bcs      13.6\u00b10.08\u03bcs     0.37  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 2, 'F')\r\n-     2.71\u00b10.02ms         1.01\u00b10ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'd')\r\n-     2.76\u00b10.03ms      1.03\u00b10.01ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 4, 'd')\r\n-         425\u00b13\u03bcs          159\u00b16\u03bcs     0.37  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 4, 'd')\r\n-         439\u00b11\u03bcs         163\u00b110\u03bcs     0.37  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 4, 'd')\r\n-       218\u00b10.9\u03bcs       80.8\u00b10.6\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 2, 'f')\r\n-         217\u00b11\u03bcs       80.5\u00b10.5\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 1, 'f')\r\n-     2.74\u00b10.08ms      1.01\u00b10.04ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 4, 'd')\r\n-     2.74\u00b10.03ms      1.01\u00b10.01ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 2, 'd')\r\n-      12.2\u00b10.2\u03bcs      4.48\u00b10.06\u03bcs     0.37  bench_reduce.MinMax.time_min(<class 'numpy.uint32'>)\r\n-       217\u00b10.5\u03bcs       79.9\u00b10.5\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 2, 'f')\r\n-         219\u00b12\u03bcs       80.6\u00b10.4\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 2, 4, 'f')\r\n-     2.78\u00b10.03ms      1.02\u00b10.01ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 2, 'd')\r\n-     2.76\u00b10.04ms      1.02\u00b10.01ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'd')\r\n-     2.74\u00b10.04ms         1.01\u00b10ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'd')\r\n-       216\u00b10.5\u03bcs       79.2\u00b10.4\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 2, 'f')\r\n-     2.72\u00b10.04ms         996\u00b120\u03bcs     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'd')\r\n-     2.87\u00b10.09ms      1.05\u00b10.03ms     0.37  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'd')\r\n-         219\u00b15\u03bcs       79.4\u00b10.4\u03bcs     0.36  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 4, 1, 'f')\r\n-         432\u00b12\u03bcs          156\u00b13\u03bcs     0.36  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 4, 'd')\r\n-      36.2\u00b10.2\u03bcs      13.0\u00b10.08\u03bcs     0.36  bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc('multiply', 1, 'F')\r\n-      12.2\u00b10.2\u03bcs      4.39\u00b10.02\u03bcs     0.36  bench_reduce.MinMax.time_max(<class 'numpy.int32'>)\r\n-         448\u00b16\u03bcs         160\u00b110\u03bcs     0.36  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 4, 'd')\r\n-        455\u00b110\u03bcs         161\u00b110\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 4, 'd')\r\n-        443\u00b110\u03bcs          156\u00b14\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 4, 1, 'd')\r\n-         428\u00b12\u03bcs          150\u00b11\u03bcs     0.35  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 2, 'd')\r\n-         430\u00b11\u03bcs          148\u00b15\u03bcs     0.34  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 2, 'd')\r\n-         433\u00b11\u03bcs          149\u00b17\u03bcs     0.34  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 4, 1, 'd')\r\n-       430\u00b10.8\u03bcs          145\u00b14\u03bcs     0.34  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 2, 'd')\r\n-       103\u00b10.3\u03bcs      34.5\u00b10.06\u03bcs     0.34  bench_ufunc.CustomScalar.time_divide_scalar2_inplace(<class 'numpy.float32'>)\r\n-         435\u00b15\u03bcs          146\u00b12\u03bcs     0.34  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 2, 'd')\r\n-         103\u00b11\u03bcs       34.5\u00b10.1\u03bcs     0.34  bench_ufunc.CustomScalar.time_divide_scalar2(<class 'numpy.float32'>)\r\n-       426\u00b10.3\u03bcs          143\u00b15\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 2, 'd')\r\n-     8.84\u00b10.04\u03bcs      2.96\u00b10.04\u03bcs     0.33  bench_reduce.MinMax.time_min(<class 'numpy.uint8'>)\r\n-         430\u00b13\u03bcs          143\u00b13\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 2, 1, 'd')\r\n-         428\u00b11\u03bcs          142\u00b17\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 4, 1, 'd')\r\n-        442\u00b110\u03bcs          147\u00b16\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 4, 'd')\r\n-         438\u00b13\u03bcs          145\u00b16\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 2, 'd')\r\n-     8.83\u00b10.03\u03bcs      2.92\u00b10.03\u03bcs     0.33  bench_reduce.MinMax.time_max(<class 'numpy.uint8'>)\r\n-       158\u00b10.5\u03bcs         52.2\u00b11\u03bcs     0.33  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-      75.5\u00b10.9\u03bcs       24.9\u00b10.4\u03bcs     0.33  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'I')\r\n-       156\u00b10.2\u03bcs       51.5\u00b10.6\u03bcs     0.33  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-         433\u00b14\u03bcs          142\u00b18\u03bcs     0.33  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 2, 'd')\r\n-      77.3\u00b10.5\u03bcs       25.2\u00b10.4\u03bcs     0.33  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'I')\r\n-      76.2\u00b10.4\u03bcs       24.8\u00b10.5\u03bcs     0.33  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'i')\r\n-         433\u00b14\u03bcs          141\u00b16\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 2, 1, 'd')\r\n-        76.0\u00b12\u03bcs       24.7\u00b10.6\u03bcs     0.32  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'i')\r\n-         447\u00b19\u03bcs        145\u00b10.5\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 4, 'd')\r\n-         447\u00b19\u03bcs          145\u00b14\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 4, 'd')\r\n-       437\u00b10.9\u03bcs          141\u00b17\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 2, 'd')\r\n-       159\u00b10.5\u03bcs       51.4\u00b10.4\u03bcs     0.32  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-         432\u00b14\u03bcs          139\u00b15\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 4, 1, 'd')\r\n-       158\u00b10.4\u03bcs       50.6\u00b10.1\u03bcs     0.32  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-      81.3\u00b10.6\u03bcs       25.7\u00b10.3\u03bcs     0.32  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'absolute'>, 1, 1, 'd')\r\n-         443\u00b19\u03bcs          140\u00b15\u03bcs     0.32  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 4, 'd')\r\n-         432\u00b14\u03bcs         130\u00b110\u03bcs     0.30  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 4, 1, 'd')\r\n-       157\u00b10.1\u03bcs       46.8\u00b10.4\u03bcs     0.30  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-         158\u00b11\u03bcs       46.4\u00b10.3\u03bcs     0.29  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-       157\u00b10.4\u03bcs       45.5\u00b10.5\u03bcs     0.29  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-       157\u00b10.4\u03bcs       45.5\u00b10.3\u03bcs     0.29  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-         434\u00b16\u03bcs          123\u00b13\u03bcs     0.28  bench_ufunc_strides.Binary.time_ufunc('maximum', 4, 1, 1, 'd')\r\n-       426\u00b10.9\u03bcs          120\u00b13\u03bcs     0.28  bench_ufunc_strides.Binary.time_ufunc('minimum', 4, 1, 1, 'd')\r\n-         199\u00b12\u03bcs         55.5\u00b12\u03bcs     0.28  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 4, 'f')\r\n-         199\u00b12\u03bcs         55.2\u00b11\u03bcs     0.28  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 4, 'f')\r\n-         198\u00b12\u03bcs         54.9\u00b11\u03bcs     0.28  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 4, 'f')\r\n-         439\u00b19\u03bcs          120\u00b13\u03bcs     0.27  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 4, 'd')\r\n-         198\u00b12\u03bcs         54.2\u00b11\u03bcs     0.27  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 4, 'f')\r\n-         199\u00b12\u03bcs         54.5\u00b13\u03bcs     0.27  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 4, 'f')\r\n-         441\u00b19\u03bcs          119\u00b12\u03bcs     0.27  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 4, 'd')\r\n-         437\u00b12\u03bcs          118\u00b16\u03bcs     0.27  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 4, 1, 'd')\r\n-      40.3\u00b10.3\u03bcs      10.9\u00b10.03\u03bcs     0.27  bench_ufunc.CustomScalar.time_add_scalar2(<class 'numpy.float32'>)\r\n-       424\u00b10.8\u03bcs          114\u00b14\u03bcs     0.27  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 2, 'd')\r\n-       195\u00b10.7\u03bcs       51.7\u00b10.4\u03bcs     0.27  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 4, 'f')\r\n-         430\u00b13\u03bcs          113\u00b13\u03bcs     0.26  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 2, 'd')\r\n-       195\u00b10.7\u03bcs       50.7\u00b10.4\u03bcs     0.26  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 4, 'f')\r\n-     13.6\u00b10.04\u03bcs      3.52\u00b10.01\u03bcs     0.26  bench_reduce.MinMax.time_max(<class 'numpy.uint16'>)\r\n-         195\u00b11\u03bcs       50.3\u00b10.3\u03bcs     0.26  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 4, 'f')\r\n-         200\u00b12\u03bcs         51.0\u00b11\u03bcs     0.26  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 4, 'f')\r\n-         197\u00b13\u03bcs       50.3\u00b10.3\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 4, 'f')\r\n-     13.7\u00b10.09\u03bcs      3.48\u00b10.03\u03bcs     0.25  bench_reduce.MinMax.time_min(<class 'numpy.uint16'>)\r\n-         200\u00b13\u03bcs         50.4\u00b11\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 4, 'f')\r\n-         201\u00b13\u03bcs         50.5\u00b11\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 4, 'f')\r\n-         200\u00b13\u03bcs         50.0\u00b11\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 4, 'f')\r\n-         200\u00b13\u03bcs       49.4\u00b10.7\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 4, 'f')\r\n-         200\u00b13\u03bcs       49.1\u00b10.6\u03bcs     0.25  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 4, 'f')\r\n-        1.95\u00b10ms          477\u00b13\u03bcs     0.24  bench_reduce.AddReduceSeparate.time_reduce(0, 'float32')\r\n-         597\u00b14\u03bcs          146\u00b11\u03bcs     0.24  bench_ufunc.CustomInplace.time_float_add_temp\r\n-     12.5\u00b10.05\u03bcs      2.98\u00b10.02\u03bcs     0.24  bench_reduce.MinMax.time_min(<class 'numpy.int8'>)\r\n-         584\u00b11\u03bcs          137\u00b11\u03bcs     0.24  bench_ufunc.CustomInplace.time_float_add\r\n-     15.0\u00b10.05\u03bcs      3.52\u00b10.02\u03bcs     0.24  bench_reduce.MinMax.time_min(<class 'numpy.int16'>)\r\n-     12.6\u00b10.08\u03bcs      2.95\u00b10.02\u03bcs     0.23  bench_reduce.MinMax.time_max(<class 'numpy.int8'>)\r\n-     15.0\u00b10.08\u03bcs      3.51\u00b10.02\u03bcs     0.23  bench_reduce.MinMax.time_max(<class 'numpy.int16'>)\r\n-         913\u00b18\u03bcs          203\u00b17\u03bcs     0.22  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 4, 'd')\r\n-         881\u00b13\u03bcs          195\u00b14\u03bcs     0.22  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 4, 'd')\r\n-       197\u00b10.8\u03bcs       42.7\u00b10.4\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 2, 'f')\r\n-         198\u00b12\u03bcs       42.8\u00b10.5\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 2, 'f')\r\n-         199\u00b13\u03bcs       43.1\u00b10.1\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 1, 'f')\r\n-         197\u00b12\u03bcs       42.7\u00b10.7\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 4, 1, 'f')\r\n-         199\u00b13\u03bcs       43.0\u00b10.6\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 4, 1, 'f')\r\n-         197\u00b11\u03bcs       42.5\u00b10.4\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 2, 'f')\r\n-       196\u00b10.5\u03bcs       42.3\u00b10.4\u03bcs     0.22  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 4, 1, 'f')\r\n-         200\u00b13\u03bcs       43.0\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 4, 2, 'f')\r\n-         199\u00b11\u03bcs       42.6\u00b10.5\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 1, 'f')\r\n-       199\u00b10.8\u03bcs       42.4\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 4, 2, 'f')\r\n-         195\u00b11\u03bcs       41.4\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 2, 'f')\r\n-       196\u00b10.9\u03bcs       41.4\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 2, 'f')\r\n-         195\u00b11\u03bcs       41.2\u00b10.5\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 2, 'f')\r\n-         195\u00b12\u03bcs       41.2\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 2, 'f')\r\n-         196\u00b11\u03bcs       41.3\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 2, 'f')\r\n-       195\u00b10.5\u03bcs       41.0\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 2, 1, 'f')\r\n-       194\u00b10.6\u03bcs       40.9\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 2, 'f')\r\n-       195\u00b10.8\u03bcs       41.0\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 2, 1, 'f')\r\n-       195\u00b10.2\u03bcs       40.9\u00b10.5\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 1, 'f')\r\n-       196\u00b10.6\u03bcs       40.9\u00b10.4\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 2, 1, 'f')\r\n-         196\u00b11\u03bcs       40.8\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 2, 'f')\r\n-         197\u00b12\u03bcs       40.9\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 2, 'f')\r\n-       195\u00b10.4\u03bcs       40.4\u00b10.1\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 1, 1, 'f')\r\n-         197\u00b11\u03bcs       40.8\u00b10.1\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 2, 'f')\r\n-       198\u00b10.9\u03bcs       40.9\u00b10.3\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 2, 'f')\r\n-       196\u00b10.4\u03bcs       40.4\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'ceil'>, 1, 1, 'f')\r\n-         196\u00b12\u03bcs       40.3\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'trunc'>, 1, 1, 'f')\r\n-         196\u00b12\u03bcs       40.3\u00b10.2\u03bcs     0.21  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 1, 1, 'f')\r\n-         198\u00b12\u03bcs       40.7\u00b10.2\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'rint'>, 2, 1, 'f')\r\n-         199\u00b13\u03bcs       40.6\u00b10.5\u03bcs     0.20  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'square'>, 1, 1, 'f')\r\n-         894\u00b11\u03bcs          173\u00b16\u03bcs     0.19  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 4, 'd')\r\n-         880\u00b12\u03bcs          166\u00b12\u03bcs     0.19  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 2, 'd')\r\n-         878\u00b11\u03bcs          164\u00b14\u03bcs     0.19  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 4, 'd')\r\n-         901\u00b15\u03bcs          167\u00b14\u03bcs     0.19  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 4, 'd')\r\n-         899\u00b15\u03bcs          167\u00b17\u03bcs     0.19  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 2, 'd')\r\n-         424\u00b12\u03bcs         78.8\u00b12\u03bcs     0.19  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 2, 'd')\r\n-         428\u00b11\u03bcs         78.3\u00b12\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 2, 'd')\r\n-         887\u00b16\u03bcs          159\u00b15\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 4, 'd')\r\n-         423\u00b11\u03bcs         74.6\u00b12\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 2, 1, 'd')\r\n-         878\u00b12\u03bcs          155\u00b16\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 4, 'd')\r\n-         427\u00b12\u03bcs         75.2\u00b11\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 2, 'd')\r\n-         425\u00b13\u03bcs         74.6\u00b12\u03bcs     0.18  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 2, 1, 'd')\r\n-       422\u00b10.8\u03bcs       73.6\u00b10.5\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 2, 'd')\r\n-         906\u00b16\u03bcs          156\u00b16\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 4, 'd')\r\n-         434\u00b14\u03bcs         74.7\u00b12\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 2, 1, 'd')\r\n-        914\u00b120\u03bcs          157\u00b19\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 4, 'd')\r\n-         877\u00b11\u03bcs          151\u00b15\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 1, 'd')\r\n-        898\u00b110\u03bcs          154\u00b15\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 4, 'd')\r\n-        931\u00b120\u03bcs         159\u00b110\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 4, 'd')\r\n-         429\u00b15\u03bcs         73.5\u00b13\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 2, 1, 'd')\r\n-         881\u00b11\u03bcs         149\u00b110\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 4, 'd')\r\n-         895\u00b13\u03bcs          152\u00b14\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 4, 1, 'd')\r\n-         896\u00b13\u03bcs          150\u00b15\u03bcs     0.17  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 2, 'd')\r\n-      24.4\u00b10.2\u03bcs      4.06\u00b10.03\u03bcs     0.17  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint32'>, 43)\r\n-         904\u00b15\u03bcs          149\u00b14\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 2, 'd')\r\n-         880\u00b12\u03bcs          144\u00b14\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 2, 'd')\r\n-      24.9\u00b10.2\u03bcs      4.02\u00b10.01\u03bcs     0.16  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint32'>, 8)\r\n-         879\u00b12\u03bcs          141\u00b11\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 2, 'd')\r\n-         421\u00b11\u03bcs         67.1\u00b11\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('minimum', 2, 1, 1, 'd')\r\n-         420\u00b12\u03bcs         66.5\u00b11\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 2, 'd')\r\n-         890\u00b12\u03bcs          141\u00b12\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 1, 'd')\r\n-      82.5\u00b10.3\u03bcs      12.9\u00b10.07\u03bcs     0.16  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'h')\r\n-         875\u00b12\u03bcs          136\u00b11\u03bcs     0.16  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 2, 'd')\r\n-      83.1\u00b10.7\u03bcs       12.9\u00b10.2\u03bcs     0.16  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'h')\r\n-       427\u00b10.8\u03bcs         66.1\u00b11\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('maximum', 2, 1, 1, 'd')\r\n-         427\u00b12\u03bcs         65.3\u00b11\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 2, 'd')\r\n-        935\u00b120\u03bcs          141\u00b14\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 4, 'd')\r\n-        921\u00b120\u03bcs          139\u00b11\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 4, 'd')\r\n-      86.9\u00b10.6\u03bcs       13.1\u00b10.1\u03bcs     0.15  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'H')\r\n-        937\u00b120\u03bcs          141\u00b13\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 4, 'd')\r\n-         898\u00b13\u03bcs          135\u00b13\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 2, 1, 'd')\r\n-        917\u00b120\u03bcs          138\u00b13\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 4, 'd')\r\n-         892\u00b12\u03bcs          134\u00b15\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 2, 'd')\r\n-      86.3\u00b10.7\u03bcs      12.8\u00b10.08\u03bcs     0.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'H')\r\n-         884\u00b16\u03bcs          131\u00b15\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 2, 1, 'd')\r\n-      65.9\u00b10.4\u03bcs       9.76\u00b10.1\u03bcs     0.15  bench_reduce.FMinMax.time_min(<class 'numpy.float64'>)\r\n-         898\u00b14\u03bcs          133\u00b15\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 2, 'd')\r\n-         883\u00b13\u03bcs          130\u00b14\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 2, 'd')\r\n-         881\u00b14\u03bcs          129\u00b13\u03bcs     0.15  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 4, 1, 'd')\r\n-      67.1\u00b10.5\u03bcs       9.62\u00b10.1\u03bcs     0.14  bench_reduce.FMinMax.time_max(<class 'numpy.float64'>)\r\n-         892\u00b11\u03bcs          119\u00b13\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 4, 1, 1, 'd')\r\n-         660\u00b16\u03bcs         87.8\u00b11\u03bcs     0.13  bench_lib.Nan.time_nanmax(200000, 0)\r\n-         672\u00b15\u03bcs         88.5\u00b11\u03bcs     0.13  bench_lib.Nan.time_nanmin(200000, 0)\r\n-        921\u00b130\u03bcs          121\u00b13\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 4, 'd')\r\n-         671\u00b15\u03bcs         88.2\u00b12\u03bcs     0.13  bench_lib.Nan.time_nanmax(200000, 2.0)\r\n-         874\u00b11\u03bcs          115\u00b12\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 1, 1, 'd')\r\n-         669\u00b13\u03bcs         87.5\u00b11\u03bcs     0.13  bench_lib.Nan.time_nanmax(200000, 0.1)\r\n-        667\u00b110\u03bcs         87.3\u00b12\u03bcs     0.13  bench_lib.Nan.time_nanmin(200000, 0.1)\r\n-         672\u00b15\u03bcs         87.7\u00b12\u03bcs     0.13  bench_lib.Nan.time_nanmin(200000, 2.0)\r\n-         877\u00b13\u03bcs          114\u00b19\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 4, 1, 'd')\r\n-        931\u00b120\u03bcs          120\u00b12\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 4, 'd')\r\n-         895\u00b16\u03bcs          114\u00b13\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 2, 'd')\r\n-         880\u00b14\u03bcs          112\u00b12\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 2, 'd')\r\n-         891\u00b12\u03bcs          112\u00b19\u03bcs     0.13  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 4, 1, 'd')\r\n-      79.7\u00b10.1\u03bcs       9.53\u00b10.2\u03bcs     0.12  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-       418\u00b10.9\u03bcs       45.2\u00b10.9\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('minimum', 1, 1, 1, 'd')\r\n-         427\u00b13\u03bcs       45.2\u00b10.8\u03bcs     0.11  bench_ufunc_strides.Binary.time_ufunc('maximum', 1, 1, 1, 'd')\r\n-      68.5\u00b10.3\u03bcs      7.19\u00b10.07\u03bcs     0.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'b')\r\n-      68.3\u00b10.4\u03bcs      7.14\u00b10.08\u03bcs     0.10  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'b')\r\n-       156\u00b10.2\u03bcs       15.9\u00b10.2\u03bcs     0.10  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       157\u00b10.3\u03bcs       15.8\u00b10.1\u03bcs     0.10  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-      70.5\u00b10.2\u03bcs      6.45\u00b10.04\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 43)\r\n-      70.6\u00b10.2\u03bcs      6.40\u00b10.06\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -43)\r\n-      75.7\u00b10.5\u03bcs      6.48\u00b10.04\u03bcs     0.09  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, -8)\r\n-         186\u00b12\u03bcs       15.9\u00b10.2\u03bcs     0.09  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-      75.9\u00b10.4\u03bcs      6.42\u00b10.06\u03bcs     0.08  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int32'>, 8)\r\n-       189\u00b10.3\u03bcs      15.8\u00b10.09\u03bcs     0.08  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-        901\u00b120\u03bcs         74.7\u00b11\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 2, 1, 'd')\r\n-         881\u00b15\u03bcs       71.7\u00b10.7\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 2, 1, 'd')\r\n-      89.1\u00b10.2\u03bcs       7.22\u00b10.1\u03bcs     0.08  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 1, 1, 'B')\r\n-     1.10\u00b10.01ms         88.9\u00b11\u03bcs     0.08  bench_lib.Nan.time_nanmax(200000, 90.0)\r\n-      88.6\u00b10.1\u03bcs      7.08\u00b10.07\u03bcs     0.08  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 1, 1, 'B')\r\n-        1.10\u00b10ms         87.3\u00b11\u03bcs     0.08  bench_lib.Nan.time_nanmin(200000, 90.0)\r\n-         881\u00b15\u03bcs         69.3\u00b11\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 2, 'd')\r\n-         893\u00b12\u03bcs       69.9\u00b10.8\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 2, 'd')\r\n-         872\u00b15\u03bcs         68.1\u00b11\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 2, 'd')\r\n-         901\u00b14\u03bcs         70.3\u00b11\u03bcs     0.08  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 2, 'd')\r\n-         890\u00b19\u03bcs       65.1\u00b10.6\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 2, 'd')\r\n-         889\u00b12\u03bcs       64.8\u00b10.8\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 2, 'd')\r\n-     41.6\u00b10.07\u03bcs      3.02\u00b10.01\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint16'>, 8)\r\n-         880\u00b15\u03bcs         63.7\u00b12\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 2, 1, 'd')\r\n-      41.6\u00b10.2\u03bcs      2.99\u00b10.02\u03bcs     0.07  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint16'>, 43)\r\n-         903\u00b15\u03bcs         62.9\u00b11\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 2, 1, 'd')\r\n-         876\u00b14\u03bcs       60.5\u00b10.2\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmin', 2, 1, 1, 'd')\r\n-         907\u00b16\u03bcs         61.8\u00b11\u03bcs     0.07  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 1, 1, 'd')\r\n-       156\u00b10.4\u03bcs       9.44\u00b10.2\u03bcs     0.06  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-       157\u00b10.7\u03bcs       9.41\u00b10.2\u03bcs     0.06  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-       157\u00b10.7\u03bcs      9.32\u00b10.09\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n-       157\u00b10.6\u03bcs      9.25\u00b10.03\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-        1.53\u00b10ms         87.5\u00b12\u03bcs     0.06  bench_lib.Nan.time_nanmax(200000, 50.0)\r\n-     1.55\u00b10.01ms         87.0\u00b11\u03bcs     0.06  bench_lib.Nan.time_nanmin(200000, 50.0)\r\n-      69.9\u00b10.2\u03bcs      3.75\u00b10.02\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -43)\r\n-      70.3\u00b10.4\u03bcs      3.77\u00b10.04\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 43)\r\n-      41.3\u00b10.4\u03bcs         2.18\u00b10\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint8'>, 8)\r\n-      41.3\u00b10.3\u03bcs      2.18\u00b10.01\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.uint8'>, 43)\r\n-        885\u00b110\u03bcs       45.9\u00b10.6\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmin', 1, 1, 1, 'd')\r\n-         892\u00b15\u03bcs       45.3\u00b10.5\u03bcs     0.05  bench_ufunc_strides.Binary.time_ufunc('fmax', 1, 1, 1, 'd')\r\n-      74.5\u00b10.8\u03bcs      3.76\u00b10.03\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, -8)\r\n-      74.0\u00b10.1\u03bcs      3.73\u00b10.05\u03bcs     0.05  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int16'>, 8)\r\n-      71.3\u00b10.3\u03bcs      2.43\u00b10.02\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 43)\r\n-      71.3\u00b10.4\u03bcs      2.42\u00b10.01\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -43)\r\n-      75.6\u00b10.6\u03bcs      2.42\u00b10.04\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, 8)\r\n-      75.3\u00b10.7\u03bcs      2.39\u00b10.01\u03bcs     0.03  bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int(<class 'numpy.int8'>, -8)\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE DECREASED.\r\n```\r\n</details>\r\n",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/20913.improvement.rst",
                "patch": "@@ -0,0 +1,12 @@\n+IBM zSystems Vector Extension Facility (SIMD)\n+---------------------------------------------\n+\n+Added support for SIMD extensions of zSystem (z13, z14, z15),\n+through the universal intrinsics interface. This support leads\n+to performance improvements for all SIMD kernels implemented\n+using the universal intrinsics, including the following operations:\n+\n+rint, floor, trunc, ceil, sqrt, absolute, square, reciprocal, tanh, sin, cos,\n+equal, not_equal, greater, greater_equal, less, less_equal,\n+maximum, minimum, fmax, fmin, argmax, argmin,\n+add, subtract, multiply, divide."
            },
            {
                "filename": "numpy/core/src/_simd/_simd.dispatch.c.src",
                "patch": "@@ -18,7 +18,7 @@\n  * #esfx      = u16,s8, u32, s16, u32, s32, u64, s64, f32, f64#\n  * #size      = 8,  8,  16,  16,  32,  32,  64,  64,  32,  64#\n  * #expand_sup= 1,  0,  1,   0,   0,   0,   0,   0,   0,   0#\n- * #simd_sup  = 1,  1,  1,   1,   1,   1,   1,   1,   1,   NPY_SIMD_F64#\n+ * #simd_sup  = 1,  1,  1,   1,   1,   1,   1,   1,   NPY_SIMD_F32, NPY_SIMD_F64#\n  * #fp_only   = 0,  0,  0,   0,   0,   0,   0,   0,   1,   1#\n  * #sat_sup   = 1,  1,  1,   1,   0,   0,   0,   0,   0,   0#\n  * #mul_sup   = 1,  1,  1,   1,   1,   1,   0,   0,   1,   1#\n@@ -252,7 +252,7 @@ SIMD_IMPL_INTRIN_3(select_@sfx@, v@sfx@, v@bsfx@, v@sfx@, v@sfx@)\n \n /**begin repeat1\n  * #sfx_to     = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n- * #simd_sup2  = 1,  1,  1,   1,   1,   1,   1,   1,   1,   NPY_SIMD_F64#\n+ * #simd_sup2  = 1,  1,  1,   1,   1,   1,   1,   1,   NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n #if @simd_sup2@\n SIMD_IMPL_INTRIN_1(reinterpret_@sfx_to@_@sfx@, v@sfx_to@, v@sfx@)\n@@ -442,15 +442,19 @@ SIMD_IMPL_INTRIN_0N(cleanup)\n  * Operators\n  ***************************/\n // check special cases\n-SIMD_IMPL_INTRIN_1(notnan_f32, vb32, vf32)\n+#if NPY_SIMD_F32\n+    SIMD_IMPL_INTRIN_1(notnan_f32, vb32, vf32)\n+#endif\n #if NPY_SIMD_F64\n     SIMD_IMPL_INTRIN_1(notnan_f64, vb64, vf64)\n #endif\n /***************************\n  * Conversions\n  ***************************/\n // round to nearest integer (assume even)\n-SIMD_IMPL_INTRIN_1(round_s32_f32, vs32, vf32)\n+#if NPY_SIMD_F32\n+    SIMD_IMPL_INTRIN_1(round_s32_f32, vs32, vf32)\n+#endif\n #if NPY_SIMD_F64\n     SIMD_IMPL_INTRIN_2(round_s32_f64, vs32, vf64, vf64)\n #endif\n@@ -492,10 +496,10 @@ static PyMethodDef simd__intrinsics_methods[] = {\n /**begin repeat\n  * #sfx       = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n  * #bsfx      = b8, b8, b16, b16, b32, b32, b64, b64, b32, b64#\n- * #esfx      = u16,s8, u32, s16, u32, s32, u64, s64, f32, f64#\n  * #size      = 8,  8,  16,  16,  32,  32,  64,  64,  32,  64#\n+ * #esfx      = u16, s8, u32,s16, u32, s32, u64, s64, f32, f64#\n  * #expand_sup= 1,  0,  1,   0,   0,   0,   0,   0,   0,   0#\n- * #simd_sup  = 1,  1,  1,   1,   1,   1,   1,   1,   1,   NPY_SIMD_F64#\n+ * #simd_sup  = 1,  1,  1,   1,   1,   1,   1,   1,   NPY_SIMD_F32, NPY_SIMD_F64#\n  * #fp_only   = 0,  0,  0,   0,   0,   0,   0,   0,   1,   1#\n  * #sat_sup   = 1,  1,  1,   1,   0,   0,   0,   0,   0,   0#\n  * #mul_sup   = 1,  1,  1,   1,   1,   1,   0,   0,   1,   1#\n@@ -547,7 +551,7 @@ SIMD_INTRIN_DEF(lut16_@sfx@)\n  ***************************/\n /**begin repeat1\n  * #sfx_to     = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n- * #simd_sup2  = 1,  1,  1,   1,   1,   1,   1,   1,   1,   NPY_SIMD_F64#\n+ * #simd_sup2  = 1,  1,  1,   1,   1,   1,   1,   1,   NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n #if @simd_sup2@\n SIMD_INTRIN_DEF(reinterpret_@sfx_to@_@sfx@)\n@@ -698,15 +702,19 @@ SIMD_INTRIN_DEF(cleanup)\n  * Operators\n  ***************************/\n // check special cases\n-SIMD_INTRIN_DEF(notnan_f32)\n+#if NPY_SIMD_F32\n+    SIMD_INTRIN_DEF(notnan_f32)\n+#endif\n #if NPY_SIMD_F64\n     SIMD_INTRIN_DEF(notnan_f64)\n #endif\n /***************************\n  * Conversions\n  ***************************/\n // round to nearest integer (assume even)\n-SIMD_INTRIN_DEF(round_s32_f32)\n+#if NPY_SIMD_F32\n+    SIMD_INTRIN_DEF(round_s32_f32)\n+#endif\n #if NPY_SIMD_F64\n     SIMD_INTRIN_DEF(round_s32_f64)\n #endif\n@@ -777,12 +785,18 @@ NPY_CPU_DISPATCH_CURFX(simd_create_module)(void)\n     if (PyModule_AddIntConstant(m, \"simd_f64\", NPY_SIMD_F64)) {\n         goto err;\n     }\n+    if (PyModule_AddIntConstant(m, \"simd_f32\", NPY_SIMD_F32)) {\n+        goto err;\n+    }\n     if (PyModule_AddIntConstant(m, \"simd_fma3\", NPY_SIMD_FMA3)) {\n         goto err;\n     }\n     if (PyModule_AddIntConstant(m, \"simd_width\", NPY_SIMD_WIDTH)) {\n         goto err;\n     }\n+    if (PyModule_AddIntConstant(m, \"simd_bigendian\", NPY_SIMD_BIGENDIAN)) {\n+        goto err;\n+    }\n #if NPY_SIMD\n     if (PySIMDVectorType_Init(m)) {\n         goto err;"
            },
            {
                "filename": "numpy/core/src/_simd/_simd_convert.inc",
                "patch": "@@ -20,6 +20,10 @@ simd_scalar_from_number(PyObject *obj, simd_data_type dtype)\n         }\n     } else {\n         data.u64 = PyLong_AsUnsignedLongLongMask(obj);\n+    #if NPY_SIMD_BIGENDIAN\n+        int leftb = (sizeof(npyv_lanetype_u64) - info->lane_size) * 8;\n+        data.u64 <<= leftb;\n+    #endif\n     }\n     return data;\n }\n@@ -36,7 +40,9 @@ simd_scalar_to_number(simd_data data, simd_data_type dtype)\n         return PyFloat_FromDouble(data.f64);\n     }\n     int leftb = (sizeof(npyv_lanetype_u64) - info->lane_size) * 8;\n+#if !NPY_SIMD_BIGENDIAN\n     data.u64 <<= leftb;\n+#endif\n     if (info->is_signed) {\n         return PyLong_FromLongLong(data.s64 >> leftb);\n     }"
            },
            {
                "filename": "numpy/core/src/_simd/_simd_inc.h.src",
                "patch": "@@ -27,22 +27,27 @@ typedef union\n     /**end repeat**/\n     // vectors\n     /**begin repeat\n-     * #sfx  = u8, u16, u32, u64, s8, s16, s32, s64, f32, b8, b16, b32, b64#\n+     * #sfx  = u8, u16, u32, u64, s8, s16, s32, s64, b8, b16, b32, b64#\n      */\n     npyv_@sfx@ v@sfx@;\n     /**end repeat**/\n     // multi-vectors x2\n     /**begin repeat\n-     * #sfx = u8, u16, u32, u64, s8, s16, s32, s64, f32#\n+     * #sfx = u8, u16, u32, u64, s8, s16, s32, s64#\n      */\n     npyv_@sfx@x2 v@sfx@x2;\n     /**end repeat**/\n     // multi-vectors x3\n     /**begin repeat\n-     * #sfx = u8, u16, u32, u64, s8, s16, s32, s64, f32#\n+     * #sfx = u8, u16, u32, u64, s8, s16, s32, s64#\n      */\n     npyv_@sfx@x3 v@sfx@x3;\n     /**end repeat**/\n+#if NPY_SIMD_F32\n+    npyv_f32    vf32;\n+    npyv_f32x2  vf32x2;\n+    npyv_f32x3  vf32x3;\n+#endif\n #if NPY_SIMD_F64\n     npyv_f64    vf64;\n     npyv_f64x2  vf64x2;"
            },
            {
                "filename": "numpy/core/src/common/npy_cpu_dispatch.h",
                "patch": "@@ -22,7 +22,7 @@\n  * which is explicitly disabling the module ccompiler_opt.\n  */\n #ifndef NPY_DISABLE_OPTIMIZATION\n-    #if defined(__powerpc64__) && !defined(__cplusplus) && defined(bool)\n+    #if (defined(__s390x__) || defined(__powerpc64__)) && !defined(__cplusplus) && defined(bool)\n         /**\n          * \"altivec.h\" header contains the definitions(bool, vector, pixel),\n          * usually in c++ we undefine them after including the header.\n@@ -34,7 +34,7 @@\n         typedef bool npy__dispatch_bkbool;\n     #endif\n     #include \"npy_cpu_dispatch_config.h\"\n-    #ifdef NPY_HAVE_VSX\n+    #if defined(NPY_HAVE_VSX) || defined(NPY_HAVE_VX)\n         #undef bool\n         #undef vector\n         #undef pixel"
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/avx2.h",
                "patch": "@@ -3,12 +3,14 @@\n #endif\n #define NPY_SIMD 256\n #define NPY_SIMD_WIDTH 32\n+#define NPY_SIMD_F32 1\n #define NPY_SIMD_F64 1\n #ifdef NPY_HAVE_FMA3\n     #define NPY_SIMD_FMA3 1 // native support\n #else\n     #define NPY_SIMD_FMA3 0 // fast emulated\n #endif\n+#define NPY_SIMD_BIGENDIAN 0\n // Enough limit to allow us to use _mm256_i32gather_*\n #define NPY_SIMD_MAXLOAD_STRIDE32 (0x7fffffff / 8)\n "
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/avx512.h",
                "patch": "@@ -3,8 +3,10 @@\n #endif\n #define NPY_SIMD 512\n #define NPY_SIMD_WIDTH 64\n+#define NPY_SIMD_F32 1\n #define NPY_SIMD_F64 1\n #define NPY_SIMD_FMA3 1 // native support\n+#define NPY_SIMD_BIGENDIAN 0\n // Enough limit to allow us to use _mm512_i32gather_* and _mm512_i32scatter_*\n #define NPY_SIMD_MAXLOAD_STRIDE32  (0x7fffffff / 16)\n #define NPY_SIMD_MAXSTORE_STRIDE32 (0x7fffffff / 16)"
            },
            {
                "filename": "numpy/core/src/common/simd/emulate_maskop.h",
                "patch": "@@ -36,7 +36,9 @@ NPYV_IMPL_EMULATE_MASK_ADDSUB(u32, b32)\n NPYV_IMPL_EMULATE_MASK_ADDSUB(s32, b32)\n NPYV_IMPL_EMULATE_MASK_ADDSUB(u64, b64)\n NPYV_IMPL_EMULATE_MASK_ADDSUB(s64, b64)\n-NPYV_IMPL_EMULATE_MASK_ADDSUB(f32, b32)\n+#if NPY_SIMD_F32\n+    NPYV_IMPL_EMULATE_MASK_ADDSUB(f32, b32)\n+#endif\n #if NPY_SIMD_F64\n     NPYV_IMPL_EMULATE_MASK_ADDSUB(f64, b64)\n #endif"
            },
            {
                "filename": "numpy/core/src/common/simd/intdiv.h",
                "patch": "@@ -206,7 +206,7 @@ NPY_FINLINE npyv_u8x3 npyv_divisor_u8(npy_uint8 d)\n     divisor.val[0] = npyv_setall_u16(m);\n     divisor.val[1] = npyv_set_u8(sh1);\n     divisor.val[2] = npyv_set_u8(sh2);\n-#elif defined(NPY_HAVE_VSX2)\n+#elif defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX)\n     divisor.val[0] = npyv_setall_u8(m);\n     divisor.val[1] = npyv_setall_u8(sh1);\n     divisor.val[2] = npyv_setall_u8(sh2);\n@@ -247,7 +247,7 @@ NPY_FINLINE npyv_s8x3 npyv_divisor_s8(npy_int8 d)\n     npyv_s8x3 divisor;\n     divisor.val[0] = npyv_setall_s8(m);\n     divisor.val[2] = npyv_setall_s8(d < 0 ? -1 : 0);\n-    #ifdef NPY_HAVE_VSX2\n+    #if defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX)\n         divisor.val[1] = npyv_setall_s8(sh);\n     #elif defined(NPY_HAVE_NEON)\n         divisor.val[1] = npyv_setall_s8(-sh);\n@@ -283,7 +283,7 @@ NPY_FINLINE npyv_u16x3 npyv_divisor_u16(npy_uint16 d)\n #ifdef NPY_HAVE_SSE2 // SSE/AVX2/AVX512\n     divisor.val[1] = npyv_set_u16(sh1);\n     divisor.val[2] = npyv_set_u16(sh2);\n-#elif defined(NPY_HAVE_VSX2)\n+#elif defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX)\n     divisor.val[1] = npyv_setall_u16(sh1);\n     divisor.val[2] = npyv_setall_u16(sh2);\n #elif defined(NPY_HAVE_NEON)\n@@ -315,7 +315,7 @@ NPY_FINLINE npyv_s16x3 npyv_divisor_s16(npy_int16 d)\n     divisor.val[2] = npyv_setall_s16(d < 0 ? -1 : 0); // sign of divisor\n #ifdef NPY_HAVE_SSE2 // SSE/AVX2/AVX512\n     divisor.val[1] = npyv_set_s16(sh);\n-#elif defined(NPY_HAVE_VSX2)\n+#elif defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX)\n     divisor.val[1] = npyv_setall_s16(sh);\n #elif defined(NPY_HAVE_NEON)\n     divisor.val[1] = npyv_setall_s16(-sh);\n@@ -350,7 +350,7 @@ NPY_FINLINE npyv_u32x3 npyv_divisor_u32(npy_uint32 d)\n #ifdef NPY_HAVE_SSE2 // SSE/AVX2/AVX512\n     divisor.val[1] = npyv_set_u32(sh1);\n     divisor.val[2] = npyv_set_u32(sh2);\n-#elif defined(NPY_HAVE_VSX2)\n+#elif defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX)\n     divisor.val[1] = npyv_setall_u32(sh1);\n     divisor.val[2] = npyv_setall_u32(sh2);\n #elif defined(NPY_HAVE_NEON)\n@@ -387,7 +387,7 @@ NPY_FINLINE npyv_s32x3 npyv_divisor_s32(npy_int32 d)\n     divisor.val[2] = npyv_setall_s32(d < 0 ? -1 : 0); // sign of divisor\n #ifdef NPY_HAVE_SSE2 // SSE/AVX2/AVX512\n     divisor.val[1] = npyv_set_s32(sh);\n-#elif defined(NPY_HAVE_VSX2)\n+#elif defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX)\n     divisor.val[1] = npyv_setall_s32(sh);\n #elif defined(NPY_HAVE_NEON)\n     divisor.val[1] = npyv_setall_s32(-sh);\n@@ -400,7 +400,7 @@ NPY_FINLINE npyv_s32x3 npyv_divisor_s32(npy_int32 d)\n NPY_FINLINE npyv_u64x3 npyv_divisor_u64(npy_uint64 d)\n {\n     npyv_u64x3 divisor;\n-#if defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_NEON)\n+#if defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX) || defined(NPY_HAVE_NEON)\n     divisor.val[0] = npyv_setall_u64(d);\n #else\n     npy_uint64 l, l2, sh1, sh2, m;\n@@ -435,7 +435,7 @@ NPY_FINLINE npyv_u64x3 npyv_divisor_u64(npy_uint64 d)\n NPY_FINLINE npyv_s64x3 npyv_divisor_s64(npy_int64 d)\n {\n     npyv_s64x3 divisor;\n-#if defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_NEON)\n+#if defined(NPY_HAVE_VSX2) || defined(NPY_HAVE_VX) || defined(NPY_HAVE_NEON)\n     divisor.val[0] = npyv_setall_s64(d);\n     divisor.val[1] = npyv_cvt_s64_b64(\n         npyv_cmpeq_s64(npyv_setall_s64(-1), divisor.val[0])"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/neon.h",
                "patch": "@@ -4,7 +4,7 @@\n \n #define NPY_SIMD 128\n #define NPY_SIMD_WIDTH 16\n-\n+#define NPY_SIMD_F32 1\n #ifdef __aarch64__\n     #define NPY_SIMD_F64 1\n #else\n@@ -15,6 +15,7 @@\n #else\n     #define NPY_SIMD_FMA3 0  // HW emulated\n #endif\n+#define NPY_SIMD_BIGENDIAN 0\n \n typedef uint8x16_t  npyv_u8;\n typedef int8x16_t   npyv_s8;"
            },
            {
                "filename": "numpy/core/src/common/simd/simd.h",
                "patch": "@@ -34,7 +34,7 @@ typedef double     npyv_lanetype_f64;\n  * They had bad impact on the generated instructions,\n  * sometimes the compiler deal with them without the respect\n  * of 32-bit mode which lead to crush due to execute 64-bit\n- * instructions and other times generate bad emulated instructions. \n+ * instructions and other times generate bad emulated instructions.\n  */\n     #undef _mm512_set1_epi64\n     #undef _mm256_set1_epi64x\n@@ -54,20 +54,30 @@ typedef double     npyv_lanetype_f64;\n     #include \"sse/sse.h\"\n #endif\n \n-// TODO: Add support for VSX(2.06) and BE Mode\n-#if defined(NPY_HAVE_VSX2) && defined(__LITTLE_ENDIAN__)\n-    #include \"vsx/vsx.h\"\n+// TODO: Add support for VSX(2.06) and BE Mode for VSX\n+#if defined(NPY_HAVE_VX) || (defined(NPY_HAVE_VSX2) && defined(__LITTLE_ENDIAN__))\n+    #include \"vec/vec.h\"\n #endif\n \n #ifdef NPY_HAVE_NEON\n     #include \"neon/neon.h\"\n #endif\n \n #ifndef NPY_SIMD\n+    /// SIMD width in bits or 0 if there's no SIMD extension available.\n     #define NPY_SIMD 0\n+    /// SIMD width in bytes or 0 if there's no SIMD extension available.\n     #define NPY_SIMD_WIDTH 0\n+    /// 1 if the enabled SIMD extension supports single-precision otherwise 0.\n+    #define NPY_SIMD_F32 0\n+    /// 1 if the enabled SIMD extension supports double-precision otherwise 0.\n     #define NPY_SIMD_F64 0\n+    /// 1 if the enabled SIMD extension supports native FMA otherwise 0.\n+    /// note: we still emulate(fast) FMA intrinsics even if they\n+    /// aren't supported but they shouldn't be used if the precision is matters.\n     #define NPY_SIMD_FMA3 0\n+    /// 1 if the enabled SIMD extension is running on big-endian mode otherwise 0.\n+    #define NPY_SIMD_BIGENDIAN 0\n #endif\n \n // enable emulated mask operations for all SIMD extension except for AVX512"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/sse.h",
                "patch": "@@ -4,12 +4,15 @@\n \n #define NPY_SIMD 128\n #define NPY_SIMD_WIDTH 16\n+#define NPY_SIMD_F32 1\n #define NPY_SIMD_F64 1\n #if defined(NPY_HAVE_FMA3) || defined(NPY_HAVE_FMA4)\n     #define NPY_SIMD_FMA3 1  // native support\n #else\n     #define NPY_SIMD_FMA3 0  // fast emulated\n #endif\n+#define NPY_SIMD_BIGENDIAN 0\n+\n typedef __m128i npyv_u8;\n typedef __m128i npyv_s8;\n typedef __m128i npyv_u16;"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/arithmetic.h",
                "patch": "@@ -2,8 +2,8 @@\n     #error \"Not a standalone header\"\n #endif\n \n-#ifndef _NPY_SIMD_VSX_ARITHMETIC_H\n-#define _NPY_SIMD_VSX_ARITHMETIC_H\n+#ifndef _NPY_SIMD_VEC_ARITHMETIC_H\n+#define _NPY_SIMD_VEC_ARITHMETIC_H\n \n /***************************\n  * Addition\n@@ -17,15 +17,32 @@\n #define npyv_add_s32 vec_add\n #define npyv_add_u64 vec_add\n #define npyv_add_s64 vec_add\n+#if NPY_SIMD_F32\n #define npyv_add_f32 vec_add\n+#endif\n #define npyv_add_f64 vec_add\n \n // saturated\n-#define npyv_adds_u8  vec_adds\n-#define npyv_adds_s8  vec_adds\n-#define npyv_adds_u16 vec_adds\n-#define npyv_adds_s16 vec_adds\n+#ifdef NPY_HAVE_VX\n+    #define NPYV_IMPL_VX_ADDS(SFX, PSFX) \\\n+        NPY_FINLINE npyv_##SFX npyv_adds_##SFX(npyv_##SFX a, npyv_##SFX b)\\\n+        {                                                                 \\\n+            return vec_pack##PSFX(                                        \\\n+                vec_add(vec_unpackh(a), vec_unpackh(b)),                  \\\n+                vec_add(vec_unpackl(a), vec_unpackl(b))                   \\\n+            );                                                            \\\n+        }\n \n+    NPYV_IMPL_VX_ADDS(u8, su)\n+    NPYV_IMPL_VX_ADDS(s8, s)\n+    NPYV_IMPL_VX_ADDS(u16, su)\n+    NPYV_IMPL_VX_ADDS(s16, s)\n+#else // VSX\n+    #define npyv_adds_u8  vec_adds\n+    #define npyv_adds_s8  vec_adds\n+    #define npyv_adds_u16 vec_adds\n+    #define npyv_adds_s16 vec_adds\n+#endif\n /***************************\n  * Subtraction\n  ***************************/\n@@ -38,21 +55,39 @@\n #define npyv_sub_s32 vec_sub\n #define npyv_sub_u64 vec_sub\n #define npyv_sub_s64 vec_sub\n+#if NPY_SIMD_F32\n #define npyv_sub_f32 vec_sub\n+#endif\n #define npyv_sub_f64 vec_sub\n \n // saturated\n-#define npyv_subs_u8  vec_subs\n-#define npyv_subs_s8  vec_subs\n-#define npyv_subs_u16 vec_subs\n-#define npyv_subs_s16 vec_subs\n+#ifdef NPY_HAVE_VX\n+    #define NPYV_IMPL_VX_SUBS(SFX, PSFX)                                  \\\n+        NPY_FINLINE npyv_##SFX npyv_subs_##SFX(npyv_##SFX a, npyv_##SFX b)\\\n+        {                                                                 \\\n+            return vec_pack##PSFX(                                        \\\n+                vec_sub(vec_unpackh(a), vec_unpackh(b)),                  \\\n+                vec_sub(vec_unpackl(a), vec_unpackl(b))                   \\\n+            );                                                            \\\n+        }\n+\n+    NPYV_IMPL_VX_SUBS(u8, su)\n+    NPYV_IMPL_VX_SUBS(s8, s)\n+    NPYV_IMPL_VX_SUBS(u16, su)\n+    NPYV_IMPL_VX_SUBS(s16, s)\n+#else // VSX\n+    #define npyv_subs_u8  vec_subs\n+    #define npyv_subs_s8  vec_subs\n+    #define npyv_subs_u16 vec_subs\n+    #define npyv_subs_s16 vec_subs\n+#endif\n \n /***************************\n  * Multiplication\n  ***************************/\n // non-saturated\n // up to GCC 6 vec_mul only supports precisions and llong\n-#if defined(__GNUC__) && __GNUC__ < 7\n+#if defined(NPY_HAVE_VSX) && defined(__GNUC__) && __GNUC__ < 7\n     #define NPYV_IMPL_VSX_MUL(T_VEC, SFX, ...)              \\\n         NPY_FINLINE T_VEC npyv_mul_##SFX(T_VEC a, T_VEC b)  \\\n         {                                                   \\\n@@ -91,7 +126,9 @@\n     #define npyv_mul_u32 vec_mul\n     #define npyv_mul_s32 vec_mul\n #endif\n+#if NPY_SIMD_F32\n #define npyv_mul_f32 vec_mul\n+#endif\n #define npyv_mul_f64 vec_mul\n \n /***************************\n@@ -101,13 +138,17 @@\n // divide each unsigned 8-bit element by a precomputed divisor\n NPY_FINLINE npyv_u8 npyv_divc_u8(npyv_u8 a, const npyv_u8x3 divisor)\n {\n+#ifdef NPY_HAVE_VX\n+    npyv_u8  mulhi    = vec_mulh(a, divisor.val[0]);\n+#else // VSX\n     const npyv_u8 mergeo_perm = {\n         1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31\n     };\n     // high part of unsigned multiplication\n     npyv_u16 mul_even = vec_mule(a, divisor.val[0]);\n     npyv_u16 mul_odd  = vec_mulo(a, divisor.val[0]);\n     npyv_u8  mulhi    = (npyv_u8)vec_perm(mul_even, mul_odd, mergeo_perm);\n+#endif\n     // floor(a/d)     = (mulhi + ((a-mulhi) >> sh1)) >> sh2\n     npyv_u8 q         = vec_sub(a, mulhi);\n             q         = vec_sr(q, divisor.val[1]);\n@@ -118,30 +159,38 @@ NPY_FINLINE npyv_u8 npyv_divc_u8(npyv_u8 a, const npyv_u8x3 divisor)\n // divide each signed 8-bit element by a precomputed divisor\n NPY_FINLINE npyv_s8 npyv_divc_s8(npyv_s8 a, const npyv_s8x3 divisor)\n {\n+#ifdef NPY_HAVE_VX\n+    npyv_s8  mulhi    = vec_mulh(a, divisor.val[0]);\n+#else\n     const npyv_u8 mergeo_perm = {\n         1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31\n     };\n     // high part of signed multiplication\n     npyv_s16 mul_even = vec_mule(a, divisor.val[0]);\n     npyv_s16 mul_odd  = vec_mulo(a, divisor.val[0]);\n     npyv_s8  mulhi    = (npyv_s8)vec_perm(mul_even, mul_odd, mergeo_perm);\n+#endif\n     // q              = ((a + mulhi) >> sh1) - XSIGN(a)\n     // trunc(a/d)     = (q ^ dsign) - dsign\n-    npyv_s8 q         = vec_sra(vec_add(a, mulhi), (npyv_u8)divisor.val[1]);\n-            q         = vec_sub(q, vec_sra(a, npyv_setall_u8(7)));\n+    npyv_s8 q         = vec_sra_s8(vec_add(a, mulhi), (npyv_u8)divisor.val[1]);\n+            q         = vec_sub(q, vec_sra_s8(a, npyv_setall_u8(7)));\n             q         = vec_sub(vec_xor(q, divisor.val[2]), divisor.val[2]);\n     return  q;\n }\n // divide each unsigned 16-bit element by a precomputed divisor\n NPY_FINLINE npyv_u16 npyv_divc_u16(npyv_u16 a, const npyv_u16x3 divisor)\n {\n+#ifdef NPY_HAVE_VX\n+    npyv_u16 mulhi    = vec_mulh(a, divisor.val[0]);\n+#else // VSX\n     const npyv_u8 mergeo_perm = {\n         2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31\n     };\n     // high part of unsigned multiplication\n     npyv_u32 mul_even = vec_mule(a, divisor.val[0]);\n     npyv_u32 mul_odd  = vec_mulo(a, divisor.val[0]);\n     npyv_u16 mulhi    = (npyv_u16)vec_perm(mul_even, mul_odd, mergeo_perm);\n+#endif\n     // floor(a/d)     = (mulhi + ((a-mulhi) >> sh1)) >> sh2\n     npyv_u16 q        = vec_sub(a, mulhi);\n              q        = vec_sr(q, divisor.val[1]);\n@@ -152,37 +201,41 @@ NPY_FINLINE npyv_u16 npyv_divc_u16(npyv_u16 a, const npyv_u16x3 divisor)\n // divide each signed 16-bit element by a precomputed divisor (round towards zero)\n NPY_FINLINE npyv_s16 npyv_divc_s16(npyv_s16 a, const npyv_s16x3 divisor)\n {\n+#ifdef NPY_HAVE_VX\n+    npyv_s16 mulhi    = vec_mulh(a, divisor.val[0]);\n+#else // VSX\n     const npyv_u8 mergeo_perm = {\n         2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31\n     };\n     // high part of signed multiplication\n     npyv_s32 mul_even = vec_mule(a, divisor.val[0]);\n     npyv_s32 mul_odd  = vec_mulo(a, divisor.val[0]);\n     npyv_s16 mulhi    = (npyv_s16)vec_perm(mul_even, mul_odd, mergeo_perm);\n+#endif\n     // q              = ((a + mulhi) >> sh1) - XSIGN(a)\n     // trunc(a/d)     = (q ^ dsign) - dsign\n-    npyv_s16 q        = vec_sra(vec_add(a, mulhi), (npyv_u16)divisor.val[1]);\n-             q        = vec_sub(q, vec_sra(a, npyv_setall_u16(15)));\n+    npyv_s16 q        = vec_sra_s16(vec_add(a, mulhi), (npyv_u16)divisor.val[1]);\n+             q        = vec_sub(q, vec_sra_s16(a, npyv_setall_u16(15)));\n              q        = vec_sub(vec_xor(q, divisor.val[2]), divisor.val[2]);\n     return   q;\n }\n // divide each unsigned 32-bit element by a precomputed divisor\n NPY_FINLINE npyv_u32 npyv_divc_u32(npyv_u32 a, const npyv_u32x3 divisor)\n {\n-#if defined(NPY_HAVE_VSX4)\n+#if defined(NPY_HAVE_VSX4) || defined(NPY_HAVE_VX)\n     // high part of unsigned multiplication\n     npyv_u32 mulhi    = vec_mulh(a, divisor.val[0]);\n-#else\n-#if defined(__GNUC__) && __GNUC__ < 8\n-    // Doubleword integer wide multiplication supported by GCC 8+\n-    npyv_u64 mul_even, mul_odd;\n-    __asm__ (\"vmulouw %0,%1,%2\" : \"=v\" (mul_even) : \"v\" (a), \"v\" (divisor.val[0]));\n-    __asm__ (\"vmuleuw %0,%1,%2\" : \"=v\" (mul_odd)  : \"v\" (a), \"v\" (divisor.val[0]));\n-#else\n-    // Doubleword integer wide multiplication supported by GCC 8+\n-    npyv_u64 mul_even = vec_mule(a, divisor.val[0]);\n-    npyv_u64 mul_odd  = vec_mulo(a, divisor.val[0]);\n-#endif\n+#else // VSX\n+    #if defined(__GNUC__) && __GNUC__ < 8\n+        // Doubleword integer wide multiplication supported by GCC 8+\n+        npyv_u64 mul_even, mul_odd;\n+        __asm__ (\"vmulouw %0,%1,%2\" : \"=v\" (mul_even) : \"v\" (a), \"v\" (divisor.val[0]));\n+        __asm__ (\"vmuleuw %0,%1,%2\" : \"=v\" (mul_odd)  : \"v\" (a), \"v\" (divisor.val[0]));\n+    #else\n+        // Doubleword integer wide multiplication supported by GCC 8+\n+        npyv_u64 mul_even = vec_mule(a, divisor.val[0]);\n+        npyv_u64 mul_odd  = vec_mulo(a, divisor.val[0]);\n+    #endif\n     // high part of unsigned multiplication\n     npyv_u32 mulhi    = vec_mergeo((npyv_u32)mul_even, (npyv_u32)mul_odd);\n #endif\n@@ -196,27 +249,27 @@ NPY_FINLINE npyv_u32 npyv_divc_u32(npyv_u32 a, const npyv_u32x3 divisor)\n // divide each signed 32-bit element by a precomputed divisor (round towards zero)\n NPY_FINLINE npyv_s32 npyv_divc_s32(npyv_s32 a, const npyv_s32x3 divisor)\n {\n-#if defined(NPY_HAVE_VSX4)\n+#if defined(NPY_HAVE_VSX4) || defined(NPY_HAVE_VX)\n     // high part of signed multiplication\n     npyv_s32 mulhi    = vec_mulh(a, divisor.val[0]);\n #else\n-#if defined(__GNUC__) && __GNUC__ < 8\n-    // Doubleword integer wide multiplication supported by GCC8+\n-    npyv_s64 mul_even, mul_odd;\n-    __asm__ (\"vmulosw %0,%1,%2\" : \"=v\" (mul_even) : \"v\" (a), \"v\" (divisor.val[0]));\n-    __asm__ (\"vmulesw %0,%1,%2\" : \"=v\" (mul_odd)  : \"v\" (a), \"v\" (divisor.val[0]));\n-#else\n-    // Doubleword integer wide multiplication supported by GCC8+\n-    npyv_s64 mul_even = vec_mule(a, divisor.val[0]);\n-    npyv_s64 mul_odd  = vec_mulo(a, divisor.val[0]);\n-#endif\n+    #if defined(__GNUC__) && __GNUC__ < 8\n+        // Doubleword integer wide multiplication supported by GCC8+\n+        npyv_s64 mul_even, mul_odd;\n+        __asm__ (\"vmulosw %0,%1,%2\" : \"=v\" (mul_even) : \"v\" (a), \"v\" (divisor.val[0]));\n+        __asm__ (\"vmulesw %0,%1,%2\" : \"=v\" (mul_odd)  : \"v\" (a), \"v\" (divisor.val[0]));\n+    #else\n+        // Doubleword integer wide multiplication supported by GCC8+\n+        npyv_s64 mul_even = vec_mule(a, divisor.val[0]);\n+        npyv_s64 mul_odd  = vec_mulo(a, divisor.val[0]);\n+    #endif\n     // high part of signed multiplication\n     npyv_s32 mulhi    = vec_mergeo((npyv_s32)mul_even, (npyv_s32)mul_odd);\n #endif\n     // q              = ((a + mulhi) >> sh1) - XSIGN(a)\n     // trunc(a/d)     = (q ^ dsign) - dsign\n-    npyv_s32 q        = vec_sra(vec_add(a, mulhi), (npyv_u32)divisor.val[1]);\n-             q        = vec_sub(q, vec_sra(a, npyv_setall_u32(31)));\n+    npyv_s32 q        = vec_sra_s32(vec_add(a, mulhi), (npyv_u32)divisor.val[1]);\n+             q        = vec_sub(q, vec_sra_s32(a, npyv_setall_u32(31)));\n              q        = vec_sub(vec_xor(q, divisor.val[2]), divisor.val[2]);\n     return   q;\n }\n@@ -240,45 +293,67 @@ NPY_FINLINE npyv_s64 npyv_divc_s64(npyv_s64 a, const npyv_s64x3 divisor)\n /***************************\n  * Division\n  ***************************/\n-#define npyv_div_f32 vec_div\n+#if NPY_SIMD_F32\n+    #define npyv_div_f32 vec_div\n+#endif\n #define npyv_div_f64 vec_div\n \n /***************************\n  * FUSED\n  ***************************/\n // multiply and add, a*b + c\n-#define npyv_muladd_f32 vec_madd\n #define npyv_muladd_f64 vec_madd\n // multiply and subtract, a*b - c\n-#define npyv_mulsub_f32 vec_msub\n #define npyv_mulsub_f64 vec_msub\n-// negate multiply and add, -(a*b) + c\n-#define npyv_nmuladd_f32 vec_nmsub // equivalent to -(a*b - c)\n-#define npyv_nmuladd_f64 vec_nmsub\n-// negate multiply and subtract, -(a*b) - c\n-#define npyv_nmulsub_f32 vec_nmadd // equivalent to -(a*b + c)\n-#define npyv_nmulsub_f64 vec_nmadd\n-\n+#if NPY_SIMD_F32\n+    #define npyv_muladd_f32 vec_madd\n+    #define npyv_mulsub_f32 vec_msub\n+#endif\n+#if defined(NPY_HAVE_VXE) || defined(NPY_HAVE_VSX)\n+    // negate multiply and add, -(a*b) + c\n+    #define npyv_nmuladd_f32 vec_nmsub // equivalent to -(a*b - c)\n+    #define npyv_nmuladd_f64 vec_nmsub\n+    // negate multiply and subtract, -(a*b) - c\n+    #define npyv_nmulsub_f64 vec_nmadd\n+    #define npyv_nmulsub_f32 vec_nmadd // equivalent to -(a*b + c)\n+#else\n+    NPY_FINLINE npyv_f64 npyv_nmuladd_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+    { return vec_neg(vec_msub(a, b, c)); }\n+    NPY_FINLINE npyv_f64 npyv_nmulsub_f64(npyv_f64 a, npyv_f64 b, npyv_f64 c)\n+    { return vec_neg(vec_madd(a, b, c)); }\n+#endif\n /***************************\n  * Summation\n  ***************************/\n // reduce sum across vector\n NPY_FINLINE npy_uint64 npyv_sum_u64(npyv_u64 a)\n {\n+#ifdef NPY_HAVE_VX\n+    const npyv_u64 zero = npyv_zero_u64();\n+    return vec_extract((npyv_u64)vec_sum_u128(a, zero), 1);\n+#else\n     return vec_extract(vec_add(a, vec_mergel(a, a)), 0);\n+#endif\n }\n \n NPY_FINLINE npy_uint32 npyv_sum_u32(npyv_u32 a)\n {\n+#ifdef NPY_HAVE_VX\n+    const npyv_u32 zero = npyv_zero_u32();\n+    return vec_extract((npyv_u32)vec_sum_u128(a, zero), 3);\n+#else\n     const npyv_u32 rs = vec_add(a, vec_sld(a, a, 8));\n     return vec_extract(vec_add(rs, vec_sld(rs, rs, 4)), 0);\n+#endif\n }\n \n+#if NPY_SIMD_F32\n NPY_FINLINE float npyv_sum_f32(npyv_f32 a)\n {\n     npyv_f32 sum = vec_add(a, npyv_combineh_f32(a, a));\n     return vec_extract(sum, 0) + vec_extract(sum, 1);\n }\n+#endif\n \n NPY_FINLINE double npyv_sum_f64(npyv_f64 a)\n {\n@@ -288,19 +363,30 @@ NPY_FINLINE double npyv_sum_f64(npyv_f64 a)\n // expand the source vector and performs sum reduce\n NPY_FINLINE npy_uint16 npyv_sumup_u8(npyv_u8 a)\n {\n+#ifdef NPY_HAVE_VX\n+    const npyv_u8 zero = npyv_zero_u8();\n+    npyv_u32 sum4 = vec_sum4(a, zero);\n+    return (npy_uint16)npyv_sum_u32(sum4);\n+#else\n     const npyv_u32 zero = npyv_zero_u32();\n     npyv_u32 four = vec_sum4s(a, zero);\n     npyv_s32 one  = vec_sums((npyv_s32)four, (npyv_s32)zero);\n     return (npy_uint16)vec_extract(one, 3);\n+#endif\n }\n \n NPY_FINLINE npy_uint32 npyv_sumup_u16(npyv_u16 a)\n {\n+#ifdef NPY_HAVE_VX\n+    npyv_u64 sum = vec_sum2(a, npyv_zero_u16());\n+    return (npy_uint32)npyv_sum_u64(sum);\n+#else // VSX\n     const npyv_s32 zero = npyv_zero_s32();\n     npyv_u32x2 eight = npyv_expand_u32_u16(a);\n     npyv_u32   four  = vec_add(eight.val[0], eight.val[1]);\n     npyv_s32   one   = vec_sums((npyv_s32)four, zero);\n     return (npy_uint32)vec_extract(one, 3);\n+#endif\n }\n \n-#endif // _NPY_SIMD_VSX_ARITHMETIC_H\n+#endif // _NPY_SIMD_VEC_ARITHMETIC_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/conversion.h",
                "patch": "@@ -0,0 +1,228 @@\n+#ifndef NPY_SIMD\n+    #error \"Not a standalone header\"\n+#endif\n+\n+#ifndef _NPY_SIMD_VEC_CVT_H\n+#define _NPY_SIMD_VEC_CVT_H\n+\n+// convert boolean vectors to integer vectors\n+#define npyv_cvt_u8_b8(BL)   ((npyv_u8)  BL)\n+#define npyv_cvt_s8_b8(BL)   ((npyv_s8)  BL)\n+#define npyv_cvt_u16_b16(BL) ((npyv_u16) BL)\n+#define npyv_cvt_s16_b16(BL) ((npyv_s16) BL)\n+#define npyv_cvt_u32_b32(BL) ((npyv_u32) BL)\n+#define npyv_cvt_s32_b32(BL) ((npyv_s32) BL)\n+#define npyv_cvt_u64_b64(BL) ((npyv_u64) BL)\n+#define npyv_cvt_s64_b64(BL) ((npyv_s64) BL)\n+#if NPY_SIMD_F32\n+    #define npyv_cvt_f32_b32(BL) ((npyv_f32) BL)\n+#endif\n+#define npyv_cvt_f64_b64(BL) ((npyv_f64) BL)\n+\n+// convert integer vectors to boolean vectors\n+#define npyv_cvt_b8_u8(A)   ((npyv_b8)  A)\n+#define npyv_cvt_b8_s8(A)   ((npyv_b8)  A)\n+#define npyv_cvt_b16_u16(A) ((npyv_b16) A)\n+#define npyv_cvt_b16_s16(A) ((npyv_b16) A)\n+#define npyv_cvt_b32_u32(A) ((npyv_b32) A)\n+#define npyv_cvt_b32_s32(A) ((npyv_b32) A)\n+#define npyv_cvt_b64_u64(A) ((npyv_b64) A)\n+#define npyv_cvt_b64_s64(A) ((npyv_b64) A)\n+#if NPY_SIMD_F32\n+    #define npyv_cvt_b32_f32(A) ((npyv_b32) A)\n+#endif\n+#define npyv_cvt_b64_f64(A) ((npyv_b64) A)\n+\n+//expand\n+NPY_FINLINE npyv_u16x2 npyv_expand_u16_u8(npyv_u8 data)\n+{\n+    npyv_u16x2 r;\n+#ifdef NPY_HAVE_VX\n+    r.val[0] = vec_unpackh(data);\n+    r.val[1] = vec_unpackl(data);\n+#else\n+    npyv_u8 zero = npyv_zero_u8();\n+    r.val[0] = (npyv_u16)vec_mergeh(data, zero);\n+    r.val[1] = (npyv_u16)vec_mergel(data, zero);\n+#endif\n+    return r;\n+}\n+\n+NPY_FINLINE npyv_u32x2 npyv_expand_u32_u16(npyv_u16 data)\n+{\n+    npyv_u32x2 r;\n+#ifdef NPY_HAVE_VX\n+    r.val[0] = vec_unpackh(data);\n+    r.val[1] = vec_unpackl(data);\n+#else\n+    npyv_u16 zero = npyv_zero_u16();\n+    r.val[0] = (npyv_u32)vec_mergeh(data, zero);\n+    r.val[1] = (npyv_u32)vec_mergel(data, zero);\n+#endif\n+    return r;\n+}\n+\n+// pack two 16-bit boolean into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b) {\n+    return vec_pack(a, b);\n+}\n+\n+// pack four 32-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32 c, npyv_b32 d) {\n+    npyv_b16 ab = vec_pack(a, b);\n+    npyv_b16 cd = vec_pack(c, d);\n+    return npyv_pack_b8_b16(ab, cd);\n+}\n+\n+// pack eight 64-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n+                 npyv_b64 e, npyv_b64 f, npyv_b64 g, npyv_b64 h) {\n+    npyv_b32 ab = vec_pack(a, b);\n+    npyv_b32 cd = vec_pack(c, d);\n+    npyv_b32 ef = vec_pack(e, f);\n+    npyv_b32 gh = vec_pack(g, h);\n+    return npyv_pack_b8_b32(ab, cd, ef, gh);\n+}\n+\n+// convert boolean vector to integer bitfield\n+#if defined(NPY_HAVE_VXE) || defined(NPY_HAVE_VSX2)\n+    NPY_FINLINE npy_uint64 npyv_tobits_b8(npyv_b8 a)\n+    {\n+        const npyv_u8 qperm = npyv_set_u8(120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 0);\n+        npyv_u16 r = (npyv_u16)vec_vbpermq((npyv_u8)a, qperm);\n+    #ifdef NPY_HAVE_VXE\n+        return vec_extract(r, 3);\n+    #else\n+        return vec_extract(r, 4);\n+    #endif\n+    }\n+    NPY_FINLINE npy_uint64 npyv_tobits_b16(npyv_b16 a)\n+    {\n+        const npyv_u8 qperm = npyv_setf_u8(128, 112, 96, 80, 64, 48, 32, 16, 0);\n+        npyv_u8 r = (npyv_u8)vec_vbpermq((npyv_u8)a, qperm);\n+    #ifdef NPY_HAVE_VXE\n+        return vec_extract(r, 6);\n+    #else\n+        return vec_extract(r, 8);\n+    #endif\n+    }\n+    NPY_FINLINE npy_uint64 npyv_tobits_b32(npyv_b32 a)\n+    {\n+    #ifdef NPY_HAVE_VXE\n+        const npyv_u8 qperm = npyv_setf_u8(128, 128, 128, 128, 128, 96, 64, 32, 0);\n+    #else\n+        const npyv_u8 qperm = npyv_setf_u8(128, 96, 64, 32, 0);\n+    #endif\n+        npyv_u8 r = (npyv_u8)vec_vbpermq((npyv_u8)a, qperm);\n+    #ifdef NPY_HAVE_VXE\n+        return vec_extract(r, 6);\n+    #else\n+        return vec_extract(r, 8);\n+    #endif\n+    }\n+    NPY_FINLINE npy_uint64 npyv_tobits_b64(npyv_b64 a)\n+    {\n+    #ifdef NPY_HAVE_VXE\n+        const npyv_u8 qperm = npyv_setf_u8(128, 128, 128, 128, 128, 128, 128, 64, 0);\n+    #else\n+        const npyv_u8 qperm = npyv_setf_u8(128, 64, 0);\n+    #endif\n+        npyv_u8 r = (npyv_u8)vec_vbpermq((npyv_u8)a, qperm);\n+    #ifdef NPY_HAVE_VXE\n+        return vec_extract(r, 6);\n+    #else\n+        return vec_extract(r, 8);\n+    #endif\n+    }\n+#else\n+    NPY_FINLINE npy_uint64 npyv_tobits_b8(npyv_b8 a)\n+    {\n+        const npyv_u8 scale = npyv_set_u8(1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128);\n+        npyv_u8 seq_scale = vec_and((npyv_u8)a, scale);\n+        npyv_u64 sum = vec_sum2(vec_sum4(seq_scale, npyv_zero_u8()), npyv_zero_u32());\n+        return vec_extract(sum, 0) + ((int)vec_extract(sum, 1) << 8);\n+    }\n+    NPY_FINLINE npy_uint64 npyv_tobits_b16(npyv_b16 a)\n+    {\n+        const npyv_u16 scale = npyv_set_u16(1, 2, 4, 8, 16, 32, 64, 128);\n+        npyv_u16 seq_scale = vec_and((npyv_u16)a, scale);\n+        npyv_u64 sum = vec_sum2(seq_scale, npyv_zero_u16());\n+        return vec_extract(vec_sum_u128(sum, npyv_zero_u64()), 15);\n+    }\n+    NPY_FINLINE npy_uint64 npyv_tobits_b32(npyv_b32 a)\n+    {\n+        const npyv_u32 scale = npyv_set_u32(1, 2, 4, 8);\n+        npyv_u32 seq_scale = vec_and((npyv_u32)a, scale);\n+        return vec_extract(vec_sum_u128(seq_scale, npyv_zero_u32()), 15);\n+    }\n+    NPY_FINLINE npy_uint64 npyv_tobits_b64(npyv_b64 a)\n+    {\n+        const npyv_u64 scale = npyv_set_u64(1, 2);\n+        npyv_u64 seq_scale = vec_and((npyv_u64)a, scale);\n+        return vec_extract(vec_sum_u128(seq_scale, npyv_zero_u64()), 15);\n+    }\n+#endif\n+// truncate compatible with all compilers(internal use for now)\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_s32 npyv__trunc_s32_f32(npyv_f32 a)\n+    {\n+    #ifdef NPY_HAVE_VXE2\n+        return vec_signed(a);\n+    #elif defined(NPY_HAVE_VXE)\n+        return vec_packs(vec_signed(npyv_doublee(a)), vec_signed(npyv_doublee(vec_mergel(a, a))));\n+    // VSX\n+    #elif defined(__IBMC__)\n+        return vec_cts(a, 0);\n+    #elif defined(__clang__)\n+        /**\n+         * old versions of CLANG doesn't support %x<n> in the inline asm template\n+         * which fixes register number when using any of the register constraints wa, wd, wf.\n+         * therefore, we count on built-in functions.\n+         */\n+        return __builtin_convertvector(a, npyv_s32);\n+    #else // gcc\n+        npyv_s32 ret;\n+        __asm__ (\"xvcvspsxws %x0,%x1\" : \"=wa\" (ret) : \"wa\" (a));\n+        return ret;\n+    #endif\n+    }\n+#endif\n+\n+NPY_FINLINE npyv_s32 npyv__trunc_s32_f64(npyv_f64 a, npyv_f64 b)\n+{\n+#ifdef NPY_HAVE_VX\n+    return vec_packs(vec_signed(a), vec_signed(b));\n+// VSX\n+#elif defined(__IBMC__)\n+    const npyv_u8 seq_even = npyv_set_u8(0, 1, 2, 3, 8, 9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27);\n+    // unfortunately, XLC missing asm register vsx fixer\n+    // hopefully, xlc can optimize around big-endian compatibility\n+    npyv_s32 lo_even = vec_cts(a, 0);\n+    npyv_s32 hi_even = vec_cts(b, 0);\n+    return vec_perm(lo_even, hi_even, seq_even);\n+#else\n+    const npyv_u8 seq_odd = npyv_set_u8(4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31);\n+    #ifdef __clang__\n+        // __builtin_convertvector doesn't support this conversion on wide range of versions\n+        // fortunately, almost all versions have direct builtin of 'xvcvdpsxws'\n+        npyv_s32 lo_odd = __builtin_vsx_xvcvdpsxws(a);\n+        npyv_s32 hi_odd = __builtin_vsx_xvcvdpsxws(b);\n+    #else // gcc\n+        npyv_s32 lo_odd, hi_odd;\n+        __asm__ (\"xvcvdpsxws %x0,%x1\" : \"=wa\" (lo_odd) : \"wa\" (a));\n+        __asm__ (\"xvcvdpsxws %x0,%x1\" : \"=wa\" (hi_odd) : \"wa\" (b));\n+    #endif\n+    return vec_perm(lo_odd, hi_odd, seq_odd);\n+#endif\n+}\n+\n+// round to nearest integer (assuming even)\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_s32 npyv_round_s32_f32(npyv_f32 a)\n+    { return npyv__trunc_s32_f32(vec_rint(a)); }\n+#endif\n+NPY_FINLINE npyv_s32 npyv_round_s32_f64(npyv_f64 a, npyv_f64 b)\n+{ return npyv__trunc_s32_f64(vec_rint(a), vec_rint(b)); }\n+\n+#endif // _NPY_SIMD_VEC_CVT_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/math.h",
                "patch": "@@ -2,45 +2,67 @@\n     #error \"Not a standalone header\"\n #endif\n \n-#ifndef _NPY_SIMD_VSX_MATH_H\n-#define _NPY_SIMD_VSX_MATH_H\n+#ifndef _NPY_SIMD_VEC_MATH_H\n+#define _NPY_SIMD_VEC_MATH_H\n /***************************\n  * Elementary\n  ***************************/\n // Square root\n-#define npyv_sqrt_f32 vec_sqrt\n+#if NPY_SIMD_F32\n+    #define npyv_sqrt_f32 vec_sqrt\n+#endif\n #define npyv_sqrt_f64 vec_sqrt\n \n // Reciprocal\n-NPY_FINLINE npyv_f32 npyv_recip_f32(npyv_f32 a)\n-{\n-    const npyv_f32 one = npyv_setall_f32(1.0f);\n-    return vec_div(one, a);\n-}\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_f32 npyv_recip_f32(npyv_f32 a)\n+    {\n+        const npyv_f32 one = npyv_setall_f32(1.0f);\n+        return vec_div(one, a);\n+    }\n+#endif\n NPY_FINLINE npyv_f64 npyv_recip_f64(npyv_f64 a)\n {\n     const npyv_f64 one = npyv_setall_f64(1.0);\n     return vec_div(one, a);\n }\n \n // Absolute\n-#define npyv_abs_f32 vec_abs\n+#if NPY_SIMD_F32\n+    #define npyv_abs_f32 vec_abs\n+#endif\n #define npyv_abs_f64 vec_abs\n \n // Square\n-NPY_FINLINE npyv_f32 npyv_square_f32(npyv_f32 a)\n-{ return vec_mul(a, a); }\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_f32 npyv_square_f32(npyv_f32 a)\n+    { return vec_mul(a, a); }\n+#endif\n NPY_FINLINE npyv_f64 npyv_square_f64(npyv_f64 a)\n { return vec_mul(a, a); }\n \n // Maximum, natively mapping with no guarantees to handle NaN.\n-#define npyv_max_f32 vec_max\n+#if NPY_SIMD_F32\n+    #define npyv_max_f32 vec_max\n+#endif\n #define npyv_max_f64 vec_max\n // Maximum, supports IEEE floating-point arithmetic (IEC 60559),\n // - If one of the two vectors contains NaN, the equivalent element of the other vector is set\n // - Only if both corresponded elements are NaN, NaN is set.\n-#define npyv_maxp_f32 vec_max\n-#define npyv_maxp_f64 vec_max\n+#if NPY_SIMD_F32\n+    #define npyv_maxp_f32 vec_max\n+#endif\n+#if defined(NPY_HAVE_VXE) || defined(NPY_HAVE_VSX)\n+    #define npyv_maxp_f64 vec_max\n+#else\n+    // vfmindb & vfmaxdb appears in zarch12\n+    NPY_FINLINE npyv_f64 npyv_maxp_f64(npyv_f64 a, npyv_f64 b)\n+    {\n+        npyv_b64 nn_a = npyv_notnan_f64(a);\n+        npyv_b64 nn_b = npyv_notnan_f64(b);\n+        return vec_max(vec_sel(b, a, nn_a), vec_sel(a, b, nn_b));\n+    }\n+#endif\n // Maximum, integer operations\n #define npyv_max_u8 vec_max\n #define npyv_max_s8 vec_max\n@@ -52,13 +74,27 @@ NPY_FINLINE npyv_f64 npyv_square_f64(npyv_f64 a)\n #define npyv_max_s64 vec_max\n \n // Minimum, natively mapping with no guarantees to handle NaN.\n-#define npyv_min_f32 vec_min\n+#if NPY_SIMD_F32\n+    #define npyv_min_f32 vec_min\n+#endif\n #define npyv_min_f64 vec_min\n // Minimum, supports IEEE floating-point arithmetic (IEC 60559),\n // - If one of the two vectors contains NaN, the equivalent element of the other vector is set\n // - Only if both corresponded elements are NaN, NaN is set.\n-#define npyv_minp_f32 vec_min\n-#define npyv_minp_f64 vec_min\n+#if NPY_SIMD_F32\n+    #define npyv_minp_f32 vec_min\n+#endif\n+#if defined(NPY_HAVE_VXE) || defined(NPY_HAVE_VSX)\n+    #define npyv_minp_f64 vec_min\n+#else\n+    // vfmindb & vfmaxdb appears in zarch12\n+    NPY_FINLINE npyv_f64 npyv_minp_f64(npyv_f64 a, npyv_f64 b)\n+    {\n+        npyv_b64 nn_a = npyv_notnan_f64(a);\n+        npyv_b64 nn_b = npyv_notnan_f64(b);\n+        return vec_min(vec_sel(b, a, nn_a), vec_sel(a, b, nn_b));\n+    }\n+#endif\n // Minimum, integer operations\n #define npyv_min_u8 vec_min\n #define npyv_min_s8 vec_min\n@@ -70,19 +106,18 @@ NPY_FINLINE npyv_f64 npyv_square_f64(npyv_f64 a)\n #define npyv_min_s64 vec_min\n \n // round to nearest int even\n-#define npyv_rint_f32 vec_rint\n #define npyv_rint_f64 vec_rint\n-\n // ceil\n-#define npyv_ceil_f32 vec_ceil\n #define npyv_ceil_f64 vec_ceil\n-\n // trunc\n-#define npyv_trunc_f32 vec_trunc\n #define npyv_trunc_f64 vec_trunc\n-\n // floor\n-#define npyv_floor_f32 vec_floor\n #define npyv_floor_f64 vec_floor\n+#if NPY_SIMD_F32\n+    #define npyv_rint_f32 vec_rint\n+    #define npyv_ceil_f32 vec_ceil\n+    #define npyv_trunc_f32 vec_trunc\n+    #define npyv_floor_f32 vec_floor\n+#endif\n \n-#endif // _NPY_SIMD_VSX_MATH_H\n+#endif // _NPY_SIMD_VEC_MATH_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/memory.h",
                "patch": "@@ -2,8 +2,8 @@\n     #error \"Not a standalone header\"\n #endif\n \n-#ifndef _NPY_SIMD_VSX_MEMORY_H\n-#define _NPY_SIMD_VSX_MEMORY_H\n+#ifndef _NPY_SIMD_VEC_MEMORY_H\n+#define _NPY_SIMD_VEC_MEMORY_H\n \n #include \"misc.h\"\n \n@@ -19,19 +19,32 @@\n      * CLANG fails to load unaligned addresses via vec_xl, vec_xst\n      * so we failback to vec_vsx_ld, vec_vsx_st\n      */\n-    #if (defined(__GNUC__) && !defined(vec_xl)) || (defined(__clang__) && !defined(__IBMC__))\n+    #if defined (NPY_HAVE_VSX2) && ( \\\n+        (defined(__GNUC__) && !defined(vec_xl)) || (defined(__clang__) && !defined(__IBMC__)) \\\n+    )\n         #define npyv__load(T_VEC, PTR) vec_vsx_ld(0, PTR)\n-    #else\n+    #else // VX\n         #define npyv__load(T_VEC, PTR) vec_xl(0, PTR)\n     #endif\n #endif\n // unaligned store\n-#if (defined(__GNUC__) && !defined(vec_xl)) || (defined(__clang__) && !defined(__IBMC__))\n+#if defined (NPY_HAVE_VSX2) && ( \\\n+    (defined(__GNUC__) && !defined(vec_xl)) || (defined(__clang__) && !defined(__IBMC__)) \\\n+)\n     #define npyv__store(PTR, VEC) vec_vsx_st(VEC, 0, PTR)\n-#else\n+#else // VX\n     #define npyv__store(PTR, VEC) vec_xst(VEC, 0, PTR)\n #endif\n \n+// aligned load/store\n+#if defined (NPY_HAVE_VSX)\n+    #define npyv__loada(PTR) vec_ld(0, PTR)\n+    #define npyv__storea(PTR, VEC) vec_st(VEC, 0, PTR)\n+#else // VX\n+    #define npyv__loada(PTR) vec_xl(0, PTR)\n+    #define npyv__storea(PTR, VEC) vec_xst(VEC, 0, PTR)\n+#endif\n+\n // avoid aliasing rules\n #ifdef __cplusplus\n     template<typename T_PTR>\n@@ -45,12 +58,16 @@\n // load lower part\n NPY_FINLINE npyv_u64 npyv__loadl(const void *ptr)\n {\n+#ifdef NPY_HAVE_VSX\n     #if defined(__clang__) && !defined(__IBMC__)\n         // vec_promote doesn't support doubleword on clang\n         return npyv_setall_u64(*npyv__ptr2u64(ptr));\n     #else\n         return vec_promote(*npyv__ptr2u64(ptr), 0);\n     #endif\n+#else // VX\n+    return vec_load_len((const unsigned long long*)ptr, 7);\n+#endif\n }\n // store lower part\n #define npyv__storel(PTR, VEC) \\\n@@ -62,36 +79,38 @@ NPY_FINLINE npyv_u64 npyv__loadl(const void *ptr)\n /****************************\n  * load/store\n  ****************************/\n-#define NPYV_IMPL_VSX_MEM(SFX, DW_CAST)                                                 \\\n+#define NPYV_IMPL_VEC_MEM(SFX, DW_CAST)                                                 \\\n     NPY_FINLINE npyv_##SFX npyv_load_##SFX(const npyv_lanetype_##SFX *ptr)              \\\n     { return (npyv_##SFX)npyv__load(npyv_##SFX, (const npyv_lanetype_##DW_CAST*)ptr); } \\\n     NPY_FINLINE npyv_##SFX npyv_loada_##SFX(const npyv_lanetype_##SFX *ptr)             \\\n-    { return (npyv_##SFX)vec_ld(0, (const npyv_lanetype_u32*)ptr); }                    \\\n+    { return (npyv_##SFX)npyv__loada((const npyv_lanetype_u32*)ptr); }                  \\\n     NPY_FINLINE npyv_##SFX npyv_loads_##SFX(const npyv_lanetype_##SFX *ptr)             \\\n     { return npyv_loada_##SFX(ptr); }                                                   \\\n     NPY_FINLINE npyv_##SFX npyv_loadl_##SFX(const npyv_lanetype_##SFX *ptr)             \\\n     { return (npyv_##SFX)npyv__loadl(ptr); }                                            \\\n     NPY_FINLINE void npyv_store_##SFX(npyv_lanetype_##SFX *ptr, npyv_##SFX vec)         \\\n     { npyv__store((npyv_lanetype_##DW_CAST*)ptr, (npyv_##DW_CAST)vec); }                \\\n     NPY_FINLINE void npyv_storea_##SFX(npyv_lanetype_##SFX *ptr, npyv_##SFX vec)        \\\n-    { vec_st((npyv_u32)vec, 0, (npyv_lanetype_u32*)ptr); }                              \\\n+    { npyv__storea((npyv_lanetype_##DW_CAST*)ptr, (npyv_##DW_CAST)vec); }               \\\n     NPY_FINLINE void npyv_stores_##SFX(npyv_lanetype_##SFX *ptr, npyv_##SFX vec)        \\\n     { npyv_storea_##SFX(ptr, vec); }                                                    \\\n     NPY_FINLINE void npyv_storel_##SFX(npyv_lanetype_##SFX *ptr, npyv_##SFX vec)        \\\n     { npyv__storel(ptr, vec); }                                                         \\\n     NPY_FINLINE void npyv_storeh_##SFX(npyv_lanetype_##SFX *ptr, npyv_##SFX vec)        \\\n     { npyv__storeh(ptr, vec); }\n \n-NPYV_IMPL_VSX_MEM(u8,  u8)\n-NPYV_IMPL_VSX_MEM(s8,  s8)\n-NPYV_IMPL_VSX_MEM(u16, u16)\n-NPYV_IMPL_VSX_MEM(s16, s16)\n-NPYV_IMPL_VSX_MEM(u32, u32)\n-NPYV_IMPL_VSX_MEM(s32, s32)\n-NPYV_IMPL_VSX_MEM(u64, f64)\n-NPYV_IMPL_VSX_MEM(s64, f64)\n-NPYV_IMPL_VSX_MEM(f32, f32)\n-NPYV_IMPL_VSX_MEM(f64, f64)\n+NPYV_IMPL_VEC_MEM(u8,  u8)\n+NPYV_IMPL_VEC_MEM(s8,  s8)\n+NPYV_IMPL_VEC_MEM(u16, u16)\n+NPYV_IMPL_VEC_MEM(s16, s16)\n+NPYV_IMPL_VEC_MEM(u32, u32)\n+NPYV_IMPL_VEC_MEM(s32, s32)\n+NPYV_IMPL_VEC_MEM(u64, f64)\n+NPYV_IMPL_VEC_MEM(s64, f64)\n+#if NPY_SIMD_F32\n+NPYV_IMPL_VEC_MEM(f32, f32)\n+#endif\n+NPYV_IMPL_VEC_MEM(f64, f64)\n \n /***************************\n  * Non-contiguous Load\n@@ -106,8 +125,10 @@ NPY_FINLINE npyv_u32 npyv_loadn_u32(const npy_uint32 *ptr, npy_intp stride)\n }\n NPY_FINLINE npyv_s32 npyv_loadn_s32(const npy_int32 *ptr, npy_intp stride)\n { return (npyv_s32)npyv_loadn_u32((const npy_uint32*)ptr, stride); }\n+#if NPY_SIMD_F32\n NPY_FINLINE npyv_f32 npyv_loadn_f32(const float *ptr, npy_intp stride)\n { return (npyv_f32)npyv_loadn_u32((const npy_uint32*)ptr, stride); }\n+#endif\n //// 64\n NPY_FINLINE npyv_u64 npyv_loadn_u64(const npy_uint64 *ptr, npy_intp stride)\n { return npyv_set_u64(ptr[0], ptr[stride]); }\n@@ -128,8 +149,10 @@ NPY_FINLINE void npyv_storen_u32(npy_uint32 *ptr, npy_intp stride, npyv_u32 a)\n }\n NPY_FINLINE void npyv_storen_s32(npy_int32 *ptr, npy_intp stride, npyv_s32 a)\n { npyv_storen_u32((npy_uint32*)ptr, stride, (npyv_u32)a); }\n+#if NPY_SIMD_F32\n NPY_FINLINE void npyv_storen_f32(float *ptr, npy_intp stride, npyv_f32 a)\n { npyv_storen_u32((npy_uint32*)ptr, stride, (npyv_u32)a); }\n+#endif\n //// 64\n NPY_FINLINE void npyv_storen_u64(npy_uint64 *ptr, npy_intp stride, npyv_u64 a)\n {\n@@ -149,6 +172,14 @@ NPY_FINLINE npyv_s32 npyv_load_till_s32(const npy_int32 *ptr, npy_uintp nlane, n\n {\n     assert(nlane > 0);\n     npyv_s32 vfill = npyv_setall_s32(fill);\n+#ifdef NPY_HAVE_VX\n+    const unsigned blane = (unsigned short)nlane;\n+    const npyv_u32 steps = npyv_set_u32(0, 1, 2, 3);\n+    const npyv_u32 vlane = npyv_setall_u32((unsigned)blane);\n+    const npyv_b32 mask  = vec_cmpgt(vlane, steps);\n+    npyv_s32 a = vec_load_len(ptr, blane*4-1);\n+    return vec_sel(vfill, a, mask);\n+#else\n     switch(nlane) {\n     case 1:\n         return vec_insert(ptr[0], vfill, 0);\n@@ -164,10 +195,18 @@ NPY_FINLINE npyv_s32 npyv_load_till_s32(const npy_int32 *ptr, npy_uintp nlane, n\n     default:\n         return npyv_load_s32(ptr);\n     }\n+#endif\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s32 npyv_load_tillz_s32(const npy_int32 *ptr, npy_uintp nlane)\n-{ return npyv_load_till_s32(ptr, nlane, 0); }\n+{\n+#ifdef NPY_HAVE_VX\n+    unsigned blane = ((unsigned short)nlane)*4 - 1;\n+    return vec_load_len(ptr, blane);\n+#else\n+    return npyv_load_till_s32(ptr, nlane, 0);\n+#endif\n+}\n //// 64\n NPY_FINLINE npyv_s64 npyv_load_till_s64(const npy_int64 *ptr, npy_uintp nlane, npy_int64 fill)\n {\n@@ -179,7 +218,14 @@ NPY_FINLINE npyv_s64 npyv_load_till_s64(const npy_int64 *ptr, npy_uintp nlane, n\n }\n // fill zero to rest lanes\n NPY_FINLINE npyv_s64 npyv_load_tillz_s64(const npy_int64 *ptr, npy_uintp nlane)\n-{  return npyv_load_till_s64(ptr, nlane, 0); }\n+{\n+#ifdef NPY_HAVE_VX\n+    unsigned blane = (unsigned short)nlane;\n+    return vec_load_len((const signed long long*)ptr, blane*8-1);\n+#else\n+    return npyv_load_till_s64(ptr, nlane, 0);\n+#endif\n+}\n /*********************************\n  * Non-contiguous partial load\n  *********************************/\n@@ -226,6 +272,10 @@ NPY_FINLINE npyv_s64 npyv_loadn_tillz_s64(const npy_int64 *ptr, npy_intp stride,\n NPY_FINLINE void npyv_store_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a)\n {\n     assert(nlane > 0);\n+#ifdef NPY_HAVE_VX\n+    unsigned blane = (unsigned short)nlane;\n+    vec_store_len(a, ptr, blane*4-1);\n+#else\n     switch(nlane) {\n     case 1:\n         *ptr = vec_extract(a, 0);\n@@ -240,16 +290,22 @@ NPY_FINLINE void npyv_store_till_s32(npy_int32 *ptr, npy_uintp nlane, npyv_s32 a\n     default:\n         npyv_store_s32(ptr, a);\n     }\n+#endif\n }\n //// 64\n NPY_FINLINE void npyv_store_till_s64(npy_int64 *ptr, npy_uintp nlane, npyv_s64 a)\n {\n     assert(nlane > 0);\n+#ifdef NPY_HAVE_VX\n+    unsigned blane = (unsigned short)nlane;\n+    vec_store_len(a, (signed long long*)ptr, blane*8-1);\n+#else\n     if (nlane == 1) {\n         npyv_storel_s64(ptr, a);\n         return;\n     }\n     npyv_store_s64(ptr, a);\n+#endif\n }\n /*********************************\n  * Non-contiguous partial store\n@@ -283,7 +339,7 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n /*****************************************************************\n  * Implement partial load/store for u32/f32/u64/f64... via casting\n  *****************************************************************/\n-#define NPYV_IMPL_VSX_REST_PARTIAL_TYPES(F_SFX, T_SFX)                                      \\\n+#define NPYV_IMPL_VEC_REST_PARTIAL_TYPES(F_SFX, T_SFX)                                      \\\n     NPY_FINLINE npyv_##F_SFX npyv_load_till_##F_SFX                                         \\\n     (const npyv_lanetype_##F_SFX *ptr, npy_uintp nlane, npyv_lanetype_##F_SFX fill)         \\\n     {                                                                                       \\\n@@ -338,39 +394,47 @@ NPY_FINLINE void npyv_storen_till_s64(npy_int64 *ptr, npy_intp stride, npy_uintp\n         );                                                                                  \\\n     }\n \n-NPYV_IMPL_VSX_REST_PARTIAL_TYPES(u32, s32)\n-NPYV_IMPL_VSX_REST_PARTIAL_TYPES(f32, s32)\n-NPYV_IMPL_VSX_REST_PARTIAL_TYPES(u64, s64)\n-NPYV_IMPL_VSX_REST_PARTIAL_TYPES(f64, s64)\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES(u32, s32)\n+#if NPY_SIMD_F32\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES(f32, s32)\n+#endif\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES(u64, s64)\n+NPYV_IMPL_VEC_REST_PARTIAL_TYPES(f64, s64)\n \n /*********************************\n  * Lookup table\n  *********************************/\n // uses vector as indexes into a table\n // that contains 32 elements of float32.\n-NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n {\n     const unsigned i0 = vec_extract(idx, 0);\n     const unsigned i1 = vec_extract(idx, 1);\n     const unsigned i2 = vec_extract(idx, 2);\n     const unsigned i3 = vec_extract(idx, 3);\n-    npyv_f32 r = vec_promote(table[i0], 0);\n+    npyv_u32 r = vec_promote(table[i0], 0);\n              r = vec_insert(table[i1], r, 1);\n              r = vec_insert(table[i2], r, 2);\n              r = vec_insert(table[i3], r, 3);\n     return r;\n }\n-NPY_FINLINE npyv_u32 npyv_lut32_u32(const npy_uint32 *table, npyv_u32 idx)\n-{ return npyv_reinterpret_u32_f32(npyv_lut32_f32((const float*)table, idx)); }\n NPY_FINLINE npyv_s32 npyv_lut32_s32(const npy_int32 *table, npyv_u32 idx)\n-{ return npyv_reinterpret_s32_f32(npyv_lut32_f32((const float*)table, idx)); }\n-\n+{ return (npyv_s32)npyv_lut32_u32((const npy_uint32*)table, idx); }\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_f32 npyv_lut32_f32(const float *table, npyv_u32 idx)\n+    { return (npyv_f32)npyv_lut32_u32((const npy_uint32*)table, idx); }\n+#endif\n // uses vector as indexes into a table\n // that contains 16 elements of float64.\n NPY_FINLINE npyv_f64 npyv_lut16_f64(const double *table, npyv_u64 idx)\n {\n+#ifdef NPY_HAVE_VX\n+    const unsigned i0 = vec_extract((npyv_u32)idx, 1);\n+    const unsigned i1 = vec_extract((npyv_u32)idx, 3);\n+#else\n     const unsigned i0 = vec_extract((npyv_u32)idx, 0);\n     const unsigned i1 = vec_extract((npyv_u32)idx, 2);\n+#endif\n     npyv_f64 r = vec_promote(table[i0], 0);\n              r = vec_insert(table[i1], r, 1);\n     return r;\n@@ -380,4 +444,4 @@ NPY_FINLINE npyv_u64 npyv_lut16_u64(const npy_uint64 *table, npyv_u64 idx)\n NPY_FINLINE npyv_s64 npyv_lut16_s64(const npy_int64 *table, npyv_u64 idx)\n { return npyv_reinterpret_s64_f64(npyv_lut16_f64((const double*)table, idx)); }\n \n-#endif // _NPY_SIMD_VSX_MEMORY_H\n+#endif // _NPY_SIMD_VEC_MEMORY_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/misc.h",
                "patch": "@@ -2,8 +2,8 @@\n     #error \"Not a standalone header\"\n #endif\n \n-#ifndef _NPY_SIMD_VSX_MISC_H\n-#define _NPY_SIMD_VSX_MISC_H\n+#ifndef _NPY_SIMD_VEC_MISC_H\n+#define _NPY_SIMD_VEC_MISC_H\n \n // vector with zero lanes\n #define npyv_zero_u8()  ((npyv_u8)   npyv_setall_s32(0))\n@@ -14,26 +14,30 @@\n #define npyv_zero_s32() npyv_setall_s32(0)\n #define npyv_zero_u64() ((npyv_u64) npyv_setall_s32(0))\n #define npyv_zero_s64() ((npyv_s64) npyv_setall_s32(0))\n-#define npyv_zero_f32() npyv_setall_f32(0.0f)\n+#if NPY_SIMD_F32\n+    #define npyv_zero_f32() npyv_setall_f32(0.0f)\n+#endif\n #define npyv_zero_f64() npyv_setall_f64(0.0)\n \n // vector with a specific value set to all lanes\n // the safest way to generate vsplti* and vsplt* instructions\n-#define NPYV_IMPL_VSX_SPLTB(T_VEC, V) ((T_VEC){V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V})\n-#define NPYV_IMPL_VSX_SPLTH(T_VEC, V) ((T_VEC){V, V, V, V, V, V, V, V})\n-#define NPYV_IMPL_VSX_SPLTW(T_VEC, V) ((T_VEC){V, V, V, V})\n-#define NPYV_IMPL_VSX_SPLTD(T_VEC, V) ((T_VEC){V, V})\n-\n-#define npyv_setall_u8(VAL)  NPYV_IMPL_VSX_SPLTB(npyv_u8,  (unsigned char)VAL)\n-#define npyv_setall_s8(VAL)  NPYV_IMPL_VSX_SPLTB(npyv_s8,  (signed char)VAL)\n-#define npyv_setall_u16(VAL) NPYV_IMPL_VSX_SPLTH(npyv_u16, (unsigned short)VAL)\n-#define npyv_setall_s16(VAL) NPYV_IMPL_VSX_SPLTH(npyv_s16, (short)VAL)\n-#define npyv_setall_u32(VAL) NPYV_IMPL_VSX_SPLTW(npyv_u32, (unsigned int)VAL)\n-#define npyv_setall_s32(VAL) NPYV_IMPL_VSX_SPLTW(npyv_s32, (int)VAL)\n-#define npyv_setall_f32(VAL) NPYV_IMPL_VSX_SPLTW(npyv_f32, VAL)\n-#define npyv_setall_u64(VAL) NPYV_IMPL_VSX_SPLTD(npyv_u64, (npy_uint64)VAL)\n-#define npyv_setall_s64(VAL) NPYV_IMPL_VSX_SPLTD(npyv_s64, (npy_int64)VAL)\n-#define npyv_setall_f64(VAL) NPYV_IMPL_VSX_SPLTD(npyv_f64, VAL)\n+#define NPYV_IMPL_VEC_SPLTB(T_VEC, V) ((T_VEC){V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V})\n+#define NPYV_IMPL_VEC_SPLTH(T_VEC, V) ((T_VEC){V, V, V, V, V, V, V, V})\n+#define NPYV_IMPL_VEC_SPLTW(T_VEC, V) ((T_VEC){V, V, V, V})\n+#define NPYV_IMPL_VEC_SPLTD(T_VEC, V) ((T_VEC){V, V})\n+\n+#define npyv_setall_u8(VAL)  NPYV_IMPL_VEC_SPLTB(npyv_u8,  (unsigned char)VAL)\n+#define npyv_setall_s8(VAL)  NPYV_IMPL_VEC_SPLTB(npyv_s8,  (signed char)VAL)\n+#define npyv_setall_u16(VAL) NPYV_IMPL_VEC_SPLTH(npyv_u16, (unsigned short)VAL)\n+#define npyv_setall_s16(VAL) NPYV_IMPL_VEC_SPLTH(npyv_s16, (short)VAL)\n+#define npyv_setall_u32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_u32, (unsigned int)VAL)\n+#define npyv_setall_s32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_s32, (int)VAL)\n+#if NPY_SIMD_F32\n+    #define npyv_setall_f32(VAL) NPYV_IMPL_VEC_SPLTW(npyv_f32, VAL)\n+#endif\n+#define npyv_setall_u64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_u64, (npy_uint64)VAL)\n+#define npyv_setall_s64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_s64, (npy_int64)VAL)\n+#define npyv_setall_f64(VAL) NPYV_IMPL_VEC_SPLTD(npyv_f64, VAL)\n \n // vector with specific values set to each lane and\n // set a specific value to all remained lanes\n@@ -45,7 +49,9 @@\n #define npyv_setf_s32(FILL, ...) ((npyv_s32){NPYV__SET_FILL_4(int, FILL, __VA_ARGS__)})\n #define npyv_setf_u64(FILL, ...) ((npyv_u64){NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__)})\n #define npyv_setf_s64(FILL, ...) ((npyv_s64){NPYV__SET_FILL_2(npy_int64, FILL, __VA_ARGS__)})\n-#define npyv_setf_f32(FILL, ...) ((npyv_f32){NPYV__SET_FILL_4(float, FILL, __VA_ARGS__)})\n+#if NPY_SIMD_F32\n+    #define npyv_setf_f32(FILL, ...) ((npyv_f32){NPYV__SET_FILL_4(float, FILL, __VA_ARGS__)})\n+#endif\n #define npyv_setf_f64(FILL, ...) ((npyv_f64){NPYV__SET_FILL_2(double, FILL, __VA_ARGS__)})\n \n // vector with specific values set to each lane and\n@@ -58,7 +64,9 @@\n #define npyv_set_s32(...) npyv_setf_s32(0, __VA_ARGS__)\n #define npyv_set_u64(...) npyv_setf_u64(0, __VA_ARGS__)\n #define npyv_set_s64(...) npyv_setf_s64(0, __VA_ARGS__)\n-#define npyv_set_f32(...) npyv_setf_f32(0, __VA_ARGS__)\n+#if NPY_SIMD_F32\n+    #define npyv_set_f32(...) npyv_setf_f32(0, __VA_ARGS__)\n+#endif\n #define npyv_set_f64(...) npyv_setf_f64(0, __VA_ARGS__)\n \n // Per lane select\n@@ -70,7 +78,9 @@\n #define npyv_select_s32 npyv_select_u8\n #define npyv_select_u64 npyv_select_u8\n #define npyv_select_s64 npyv_select_u8\n-#define npyv_select_f32 npyv_select_u8\n+#if NPY_SIMD_F32\n+    #define npyv_select_f32 npyv_select_u8\n+#endif\n #define npyv_select_f64 npyv_select_u8\n \n // Reinterpret\n@@ -82,7 +92,9 @@\n #define npyv_reinterpret_u8_s32 npyv_reinterpret_u8_s8\n #define npyv_reinterpret_u8_u64 npyv_reinterpret_u8_s8\n #define npyv_reinterpret_u8_s64 npyv_reinterpret_u8_s8\n-#define npyv_reinterpret_u8_f32 npyv_reinterpret_u8_s8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_u8_f32 npyv_reinterpret_u8_s8\n+#endif\n #define npyv_reinterpret_u8_f64 npyv_reinterpret_u8_s8\n \n #define npyv_reinterpret_s8_s8(X) X\n@@ -93,7 +105,9 @@\n #define npyv_reinterpret_s8_s32 npyv_reinterpret_s8_u8\n #define npyv_reinterpret_s8_u64 npyv_reinterpret_s8_u8\n #define npyv_reinterpret_s8_s64 npyv_reinterpret_s8_u8\n-#define npyv_reinterpret_s8_f32 npyv_reinterpret_s8_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_s8_f32 npyv_reinterpret_s8_u8\n+#endif\n #define npyv_reinterpret_s8_f64 npyv_reinterpret_s8_u8\n \n #define npyv_reinterpret_u16_u16(X) X\n@@ -104,7 +118,9 @@\n #define npyv_reinterpret_u16_s32 npyv_reinterpret_u16_u8\n #define npyv_reinterpret_u16_u64 npyv_reinterpret_u16_u8\n #define npyv_reinterpret_u16_s64 npyv_reinterpret_u16_u8\n-#define npyv_reinterpret_u16_f32 npyv_reinterpret_u16_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_u16_f32 npyv_reinterpret_u16_u8\n+#endif\n #define npyv_reinterpret_u16_f64 npyv_reinterpret_u16_u8\n \n #define npyv_reinterpret_s16_s16(X) X\n@@ -115,7 +131,9 @@\n #define npyv_reinterpret_s16_s32 npyv_reinterpret_s16_u8\n #define npyv_reinterpret_s16_u64 npyv_reinterpret_s16_u8\n #define npyv_reinterpret_s16_s64 npyv_reinterpret_s16_u8\n-#define npyv_reinterpret_s16_f32 npyv_reinterpret_s16_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_s16_f32 npyv_reinterpret_s16_u8\n+#endif\n #define npyv_reinterpret_s16_f64 npyv_reinterpret_s16_u8\n \n #define npyv_reinterpret_u32_u32(X) X\n@@ -126,7 +144,9 @@\n #define npyv_reinterpret_u32_s32 npyv_reinterpret_u32_u8\n #define npyv_reinterpret_u32_u64 npyv_reinterpret_u32_u8\n #define npyv_reinterpret_u32_s64 npyv_reinterpret_u32_u8\n-#define npyv_reinterpret_u32_f32 npyv_reinterpret_u32_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_u32_f32 npyv_reinterpret_u32_u8\n+#endif\n #define npyv_reinterpret_u32_f64 npyv_reinterpret_u32_u8\n \n #define npyv_reinterpret_s32_s32(X) X\n@@ -137,7 +157,9 @@\n #define npyv_reinterpret_s32_u32 npyv_reinterpret_s32_u8\n #define npyv_reinterpret_s32_u64 npyv_reinterpret_s32_u8\n #define npyv_reinterpret_s32_s64 npyv_reinterpret_s32_u8\n-#define npyv_reinterpret_s32_f32 npyv_reinterpret_s32_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_s32_f32 npyv_reinterpret_s32_u8\n+#endif\n #define npyv_reinterpret_s32_f64 npyv_reinterpret_s32_u8\n \n #define npyv_reinterpret_u64_u64(X) X\n@@ -148,7 +170,9 @@\n #define npyv_reinterpret_u64_u32 npyv_reinterpret_u64_u8\n #define npyv_reinterpret_u64_s32 npyv_reinterpret_u64_u8\n #define npyv_reinterpret_u64_s64 npyv_reinterpret_u64_u8\n-#define npyv_reinterpret_u64_f32 npyv_reinterpret_u64_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_u64_f32 npyv_reinterpret_u64_u8\n+#endif\n #define npyv_reinterpret_u64_f64 npyv_reinterpret_u64_u8\n \n #define npyv_reinterpret_s64_s64(X) X\n@@ -159,19 +183,23 @@\n #define npyv_reinterpret_s64_u32 npyv_reinterpret_s64_u8\n #define npyv_reinterpret_s64_s32 npyv_reinterpret_s64_u8\n #define npyv_reinterpret_s64_u64 npyv_reinterpret_s64_u8\n-#define npyv_reinterpret_s64_f32 npyv_reinterpret_s64_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_s64_f32 npyv_reinterpret_s64_u8\n+#endif\n #define npyv_reinterpret_s64_f64 npyv_reinterpret_s64_u8\n \n-#define npyv_reinterpret_f32_f32(X) X\n-#define npyv_reinterpret_f32_u8(X) ((npyv_f32)X)\n-#define npyv_reinterpret_f32_s8  npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_u16 npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_s16 npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_u32 npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_s32 npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_u64 npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_s64 npyv_reinterpret_f32_u8\n-#define npyv_reinterpret_f32_f64 npyv_reinterpret_f32_u8\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_f32_f32(X) X\n+    #define npyv_reinterpret_f32_u8(X) ((npyv_f32)X)\n+    #define npyv_reinterpret_f32_s8  npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_u16 npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_s16 npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_u32 npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_s32 npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_u64 npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_s64 npyv_reinterpret_f32_u8\n+    #define npyv_reinterpret_f32_f64 npyv_reinterpret_f32_u8\n+#endif\n \n #define npyv_reinterpret_f64_f64(X) X\n #define npyv_reinterpret_f64_u8(X) ((npyv_f64)X)\n@@ -182,9 +210,10 @@\n #define npyv_reinterpret_f64_s32 npyv_reinterpret_f64_u8\n #define npyv_reinterpret_f64_u64 npyv_reinterpret_f64_u8\n #define npyv_reinterpret_f64_s64 npyv_reinterpret_f64_u8\n-#define npyv_reinterpret_f64_f32 npyv_reinterpret_f64_u8\n-\n+#if NPY_SIMD_F32\n+    #define npyv_reinterpret_f64_f32 npyv_reinterpret_f64_u8\n+#endif\n // Only required by AVX2/AVX512\n #define npyv_cleanup() ((void)0)\n \n-#endif // _NPY_SIMD_VSX_MISC_H\n+#endif // _NPY_SIMD_VEC_MISC_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/operators.h",
                "patch": "@@ -2,20 +2,20 @@\n     #error \"Not a standalone header\"\n #endif\n \n-#ifndef _NPY_SIMD_VSX_OPERATORS_H\n-#define _NPY_SIMD_VSX_OPERATORS_H\n+#ifndef _NPY_SIMD_VEC_OPERATORS_H\n+#define _NPY_SIMD_VEC_OPERATORS_H\n \n /***************************\n  * Shifting\n  ***************************/\n \n // Left\n #define npyv_shl_u16(A, C) vec_sl(A, npyv_setall_u16(C))\n-#define npyv_shl_s16(A, C) vec_sl(A, npyv_setall_u16(C))\n+#define npyv_shl_s16(A, C) vec_sl_s16(A, npyv_setall_u16(C))\n #define npyv_shl_u32(A, C) vec_sl(A, npyv_setall_u32(C))\n-#define npyv_shl_s32(A, C) vec_sl(A, npyv_setall_u32(C))\n+#define npyv_shl_s32(A, C) vec_sl_s32(A, npyv_setall_u32(C))\n #define npyv_shl_u64(A, C) vec_sl(A, npyv_setall_u64(C))\n-#define npyv_shl_s64(A, C) vec_sl(A, npyv_setall_u64(C))\n+#define npyv_shl_s64(A, C) vec_sl_s64(A, npyv_setall_u64(C))\n \n // Left by an immediate constant\n #define npyv_shli_u16 npyv_shl_u16\n@@ -27,11 +27,11 @@\n \n // Right\n #define npyv_shr_u16(A, C) vec_sr(A,  npyv_setall_u16(C))\n-#define npyv_shr_s16(A, C) vec_sra(A, npyv_setall_u16(C))\n+#define npyv_shr_s16(A, C) vec_sra_s16(A, npyv_setall_u16(C))\n #define npyv_shr_u32(A, C) vec_sr(A,  npyv_setall_u32(C))\n-#define npyv_shr_s32(A, C) vec_sra(A, npyv_setall_u32(C))\n+#define npyv_shr_s32(A, C) vec_sra_s32(A, npyv_setall_u32(C))\n #define npyv_shr_u64(A, C) vec_sr(A,  npyv_setall_u64(C))\n-#define npyv_shr_s64(A, C) vec_sra(A, npyv_setall_u64(C))\n+#define npyv_shr_s64(A, C) vec_sra_s64(A, npyv_setall_u64(C))\n \n // Right by an immediate constant\n #define npyv_shri_u16 npyv_shr_u16\n@@ -44,15 +44,15 @@\n /***************************\n  * Logical\n  ***************************/\n-#define NPYV_IMPL_VSX_BIN_CAST(INTRIN, SFX, CAST) \\\n+#define NPYV_IMPL_VEC_BIN_CAST(INTRIN, SFX, CAST) \\\n     NPY_FINLINE npyv_##SFX npyv_##INTRIN##_##SFX(npyv_##SFX a, npyv_##SFX b) \\\n     { return (npyv_##SFX)vec_##INTRIN((CAST)a, (CAST)b); }\n \n // Up to GCC 6 logical intrinsics don't support bool long long\n #if defined(__GNUC__) && __GNUC__ <= 6\n-    #define NPYV_IMPL_VSX_BIN_B64(INTRIN) NPYV_IMPL_VSX_BIN_CAST(INTRIN, b64, npyv_u64)\n+    #define NPYV_IMPL_VEC_BIN_B64(INTRIN) NPYV_IMPL_VEC_BIN_CAST(INTRIN, b64, npyv_u64)\n #else\n-    #define NPYV_IMPL_VSX_BIN_B64(INTRIN) NPYV_IMPL_VSX_BIN_CAST(INTRIN, b64, npyv_b64)\n+    #define NPYV_IMPL_VEC_BIN_B64(INTRIN) NPYV_IMPL_VEC_BIN_CAST(INTRIN, b64, npyv_b64)\n #endif\n // AND\n #define npyv_and_u8  vec_and\n@@ -63,12 +63,14 @@\n #define npyv_and_s32 vec_and\n #define npyv_and_u64 vec_and\n #define npyv_and_s64 vec_and\n-#define npyv_and_f32 vec_and\n+#if NPY_SIMD_F32\n+    #define npyv_and_f32 vec_and\n+#endif\n #define npyv_and_f64 vec_and\n #define npyv_and_b8  vec_and\n #define npyv_and_b16 vec_and\n #define npyv_and_b32 vec_and\n-NPYV_IMPL_VSX_BIN_B64(and)\n+NPYV_IMPL_VEC_BIN_B64(and)\n \n // OR\n #define npyv_or_u8  vec_or\n@@ -79,12 +81,14 @@ NPYV_IMPL_VSX_BIN_B64(and)\n #define npyv_or_s32 vec_or\n #define npyv_or_u64 vec_or\n #define npyv_or_s64 vec_or\n-#define npyv_or_f32 vec_or\n+#if NPY_SIMD_F32\n+    #define npyv_or_f32 vec_or\n+#endif\n #define npyv_or_f64 vec_or\n #define npyv_or_b8  vec_or\n #define npyv_or_b16 vec_or\n #define npyv_or_b32 vec_or\n-NPYV_IMPL_VSX_BIN_B64(or)\n+NPYV_IMPL_VEC_BIN_B64(or)\n \n // XOR\n #define npyv_xor_u8  vec_xor\n@@ -95,30 +99,32 @@ NPYV_IMPL_VSX_BIN_B64(or)\n #define npyv_xor_s32 vec_xor\n #define npyv_xor_u64 vec_xor\n #define npyv_xor_s64 vec_xor\n-#define npyv_xor_f32 vec_xor\n+#if NPY_SIMD_F32\n+    #define npyv_xor_f32 vec_xor\n+#endif\n #define npyv_xor_f64 vec_xor\n #define npyv_xor_b8  vec_xor\n #define npyv_xor_b16 vec_xor\n #define npyv_xor_b32 vec_xor\n-NPYV_IMPL_VSX_BIN_B64(xor)\n+NPYV_IMPL_VEC_BIN_B64(xor)\n \n // NOT\n // note: we implement npyv_not_b*(boolean types) for internal use*/\n-#define NPYV_IMPL_VSX_NOT_INT(VEC_LEN)                                 \\\n+#define NPYV_IMPL_VEC_NOT_INT(VEC_LEN)                                 \\\n     NPY_FINLINE npyv_u##VEC_LEN npyv_not_u##VEC_LEN(npyv_u##VEC_LEN a) \\\n     { return vec_nor(a, a); }                                          \\\n     NPY_FINLINE npyv_s##VEC_LEN npyv_not_s##VEC_LEN(npyv_s##VEC_LEN a) \\\n     { return vec_nor(a, a); }                                          \\\n     NPY_FINLINE npyv_b##VEC_LEN npyv_not_b##VEC_LEN(npyv_b##VEC_LEN a) \\\n     { return vec_nor(a, a); }\n \n-NPYV_IMPL_VSX_NOT_INT(8)\n-NPYV_IMPL_VSX_NOT_INT(16)\n-NPYV_IMPL_VSX_NOT_INT(32)\n+NPYV_IMPL_VEC_NOT_INT(8)\n+NPYV_IMPL_VEC_NOT_INT(16)\n+NPYV_IMPL_VEC_NOT_INT(32)\n \n-// up to gcc5 vec_nor doesn't support bool long long\n-#if defined(__GNUC__) && __GNUC__ > 5\n-    NPYV_IMPL_VSX_NOT_INT(64)\n+// on ppc64, up to gcc5 vec_nor doesn't support bool long long\n+#if defined(NPY_HAVE_VSX) && defined(__GNUC__) && __GNUC__ > 5\n+    NPYV_IMPL_VEC_NOT_INT(64)\n #else\n     NPY_FINLINE npyv_u64 npyv_not_u64(npyv_u64 a)\n     { return vec_nor(a, a); }\n@@ -128,8 +134,10 @@ NPYV_IMPL_VSX_NOT_INT(32)\n     { return (npyv_b64)vec_nor((npyv_u64)a, (npyv_u64)a); }\n #endif\n \n-NPY_FINLINE npyv_f32 npyv_not_f32(npyv_f32 a)\n-{ return vec_nor(a, a); }\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_f32 npyv_not_f32(npyv_f32 a)\n+    { return vec_nor(a, a); }\n+#endif\n NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n { return vec_nor(a, a); }\n \n@@ -152,7 +160,9 @@ NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n #define npyv_cmpeq_s32 vec_cmpeq\n #define npyv_cmpeq_u64 vec_cmpeq\n #define npyv_cmpeq_s64 vec_cmpeq\n-#define npyv_cmpeq_f32 vec_cmpeq\n+#if NPY_SIMD_F32\n+    #define npyv_cmpeq_f32 vec_cmpeq\n+#endif\n #define npyv_cmpeq_f64 vec_cmpeq\n \n // Int Not Equal\n@@ -177,7 +187,9 @@ NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n     #define npyv_cmpneq_s32(A, B) npyv_not_b32(vec_cmpeq(A, B))\n     #define npyv_cmpneq_u64(A, B) npyv_not_b64(vec_cmpeq(A, B))\n     #define npyv_cmpneq_s64(A, B) npyv_not_b64(vec_cmpeq(A, B))\n-    #define npyv_cmpneq_f32(A, B) npyv_not_b32(vec_cmpeq(A, B))\n+    #if NPY_SIMD_F32\n+        #define npyv_cmpneq_f32(A, B) npyv_not_b32(vec_cmpeq(A, B))\n+    #endif\n     #define npyv_cmpneq_f64(A, B) npyv_not_b64(vec_cmpeq(A, B))\n #endif\n \n@@ -190,12 +202,14 @@ NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n #define npyv_cmpgt_s32 vec_cmpgt\n #define npyv_cmpgt_u64 vec_cmpgt\n #define npyv_cmpgt_s64 vec_cmpgt\n-#define npyv_cmpgt_f32 vec_cmpgt\n+#if NPY_SIMD_F32\n+    #define npyv_cmpgt_f32 vec_cmpgt\n+#endif\n #define npyv_cmpgt_f64 vec_cmpgt\n \n // Greater than or equal\n-// up to gcc5 vec_cmpge only supports single and double precision\n-#if defined(__GNUC__) && __GNUC__ > 5\n+// On ppc64le, up to gcc5 vec_cmpge only supports single and double precision\n+#if defined(NPY_HAVE_VX) || (defined(__GNUC__) && __GNUC__ > 5)\n     #define npyv_cmpge_u8  vec_cmpge\n     #define npyv_cmpge_s8  vec_cmpge\n     #define npyv_cmpge_u16 vec_cmpge\n@@ -214,7 +228,9 @@ NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n     #define npyv_cmpge_u64(A, B) npyv_not_b64(vec_cmpgt(B, A))\n     #define npyv_cmpge_s64(A, B) npyv_not_b64(vec_cmpgt(B, A))\n #endif\n-#define npyv_cmpge_f32 vec_cmpge\n+#if NPY_SIMD_F32\n+    #define npyv_cmpge_f32 vec_cmpge\n+#endif\n #define npyv_cmpge_f64 vec_cmpge\n \n // Less than\n@@ -226,7 +242,9 @@ NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n #define npyv_cmplt_s32(A, B) npyv_cmpgt_s32(B, A)\n #define npyv_cmplt_u64(A, B) npyv_cmpgt_u64(B, A)\n #define npyv_cmplt_s64(A, B) npyv_cmpgt_s64(B, A)\n-#define npyv_cmplt_f32(A, B) npyv_cmpgt_f32(B, A)\n+#if NPY_SIMD_F32\n+    #define npyv_cmplt_f32(A, B) npyv_cmpgt_f32(B, A)\n+#endif\n #define npyv_cmplt_f64(A, B) npyv_cmpgt_f64(B, A)\n \n // Less than or equal\n@@ -238,13 +256,17 @@ NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n #define npyv_cmple_s32(A, B) npyv_cmpge_s32(B, A)\n #define npyv_cmple_u64(A, B) npyv_cmpge_u64(B, A)\n #define npyv_cmple_s64(A, B) npyv_cmpge_s64(B, A)\n-#define npyv_cmple_f32(A, B) npyv_cmpge_f32(B, A)\n+#if NPY_SIMD_F32\n+    #define npyv_cmple_f32(A, B) npyv_cmpge_f32(B, A)\n+#endif\n #define npyv_cmple_f64(A, B) npyv_cmpge_f64(B, A)\n \n // check special cases\n-NPY_FINLINE npyv_b32 npyv_notnan_f32(npyv_f32 a)\n-{ return vec_cmpeq(a, a); }\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_b32 npyv_notnan_f32(npyv_f32 a)\n+    { return vec_cmpeq(a, a); }\n+#endif\n NPY_FINLINE npyv_b64 npyv_notnan_f64(npyv_f64 a)\n { return vec_cmpeq(a, a); }\n \n-#endif // _NPY_SIMD_VSX_OPERATORS_H\n+#endif // _NPY_SIMD_VEC_OPERATORS_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/reorder.h",
                "patch": "@@ -2,8 +2,8 @@\n     #error \"Not a standalone header\"\n #endif\n \n-#ifndef _NPY_SIMD_VSX_REORDER_H\n-#define _NPY_SIMD_VSX_REORDER_H\n+#ifndef _NPY_SIMD_VEC_REORDER_H\n+#define _NPY_SIMD_VEC_REORDER_H\n \n // combine lower part of two vectors\n #define npyv__combinel(A, B) vec_mergeh((npyv_u64)(A), (npyv_u64)(B))\n@@ -15,7 +15,9 @@\n #define npyv_combinel_s32(A, B) ((npyv_s32)npyv__combinel(A, B))\n #define npyv_combinel_u64       vec_mergeh\n #define npyv_combinel_s64       vec_mergeh\n-#define npyv_combinel_f32(A, B) ((npyv_f32)npyv__combinel(A, B))\n+#if NPY_SIMD_F32\n+    #define npyv_combinel_f32(A, B) ((npyv_f32)npyv__combinel(A, B))\n+#endif\n #define npyv_combinel_f64       vec_mergeh\n \n // combine higher part of two vectors\n@@ -28,14 +30,16 @@\n #define npyv_combineh_s32(A, B) ((npyv_s32)npyv__combineh(A, B))\n #define npyv_combineh_u64       vec_mergel\n #define npyv_combineh_s64       vec_mergel\n-#define npyv_combineh_f32(A, B) ((npyv_f32)npyv__combineh(A, B))\n+#if NPY_SIMD_F32\n+    #define npyv_combineh_f32(A, B) ((npyv_f32)npyv__combineh(A, B))\n+#endif\n #define npyv_combineh_f64       vec_mergel\n \n /*\n  * combine: combine two vectors from lower and higher parts of two other vectors\n  * zip: interleave two vectors\n */\n-#define NPYV_IMPL_VSX_COMBINE_ZIP(T_VEC, SFX)                  \\\n+#define NPYV_IMPL_VEC_COMBINE_ZIP(T_VEC, SFX)                  \\\n     NPY_FINLINE T_VEC##x2 npyv_combine_##SFX(T_VEC a, T_VEC b) \\\n     {                                                          \\\n         T_VEC##x2 r;                                           \\\n@@ -51,16 +55,18 @@\n         return r;                                              \\\n     }\n \n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_u8,  u8)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_s8,  s8)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_u16, u16)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_s16, s16)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_u32, u32)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_s32, s32)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_u64, u64)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_s64, s64)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_f32, f32)\n-NPYV_IMPL_VSX_COMBINE_ZIP(npyv_f64, f64)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_u8,  u8)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_s8,  s8)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_u16, u16)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_s16, s16)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_u32, u32)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_s32, s32)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_u64, u64)\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_s64, s64)\n+#if NPY_SIMD_F32\n+    NPYV_IMPL_VEC_COMBINE_ZIP(npyv_f32, f32)\n+#endif\n+NPYV_IMPL_VEC_COMBINE_ZIP(npyv_f64, f64)\n \n // Reverse elements of each 64-bit lane\n NPY_FINLINE npyv_u8 npyv_rev64_u8(npyv_u8 a)\n@@ -100,7 +106,9 @@ NPY_FINLINE npyv_u32 npyv_rev64_u32(npyv_u32 a)\n }\n NPY_FINLINE npyv_s32 npyv_rev64_s32(npyv_s32 a)\n { return (npyv_s32)npyv_rev64_u32((npyv_u32)a); }\n-NPY_FINLINE npyv_f32 npyv_rev64_f32(npyv_f32 a)\n-{ return (npyv_f32)npyv_rev64_u32((npyv_u32)a); }\n+#if NPY_SIMD_F32\n+    NPY_FINLINE npyv_f32 npyv_rev64_f32(npyv_f32 a)\n+    { return (npyv_f32)npyv_rev64_u32((npyv_u32)a); }\n+#endif\n \n-#endif // _NPY_SIMD_VSX_REORDER_H\n+#endif // _NPY_SIMD_VEC_REORDER_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/utils.h",
                "patch": "@@ -0,0 +1,84 @@\n+#ifndef NPY_SIMD\n+    #error \"Not a standalone header\"\n+#endif\n+\n+#ifndef _NPY_SIMD_VEC_UTILS_H\n+#define _NPY_SIMD_VEC_UTILS_H\n+\n+// the following intrinsics may not some|all by zvector API on gcc/clang\n+#ifdef NPY_HAVE_VX\n+    #ifndef vec_neg\n+        #define vec_neg(a) (-(a)) // Vector Negate\n+    #endif\n+    #ifndef vec_add\n+        #define vec_add(a, b) ((a) + (b)) // Vector Add\n+    #endif\n+    #ifndef vec_sub\n+        #define vec_sub(a, b) ((a) - (b)) // Vector Subtract\n+    #endif\n+    #ifndef vec_mul\n+        #define vec_mul(a, b) ((a) * (b)) // Vector Multiply\n+    #endif\n+    #ifndef vec_div\n+        #define vec_div(a, b) ((a) / (b)) // Vector Divide\n+    #endif\n+    #ifndef vec_neg\n+        #define vec_neg(a) (-(a))\n+    #endif\n+    #ifndef vec_and\n+        #define vec_and(a, b) ((a) & (b)) // Vector AND\n+    #endif\n+    #ifndef vec_or\n+        #define vec_or(a, b) ((a) | (b)) // Vector OR\n+    #endif\n+    #ifndef vec_xor\n+        #define vec_xor(a, b) ((a) ^ (b)) // Vector XOR\n+    #endif\n+    #ifndef vec_sl\n+        #define vec_sl(a, b) ((a) << (b)) // Vector Shift Left\n+    #endif\n+    #ifndef vec_sra\n+        #define vec_sra(a, b) ((a) >> (b)) // Vector Shift Right\n+    #endif\n+    #ifndef vec_sr\n+        #define vec_sr(a, b) ((a) >> (b)) // Vector Shift Right Algebraic\n+    #endif\n+    #ifndef vec_slo\n+        #define vec_slo(a, b) vec_slb(a, (b) << 64) // Vector Shift Left by Octet\n+    #endif\n+    #ifndef vec_sro\n+        #define vec_sro(a, b) vec_srb(a, (b) << 64) // Vector Shift Right by Octet\n+    #endif\n+    // vec_doublee maps to wrong intrin \"vfll\".\n+    // see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100871\n+    #if defined(__GNUC__) && !defined(__clang__)\n+        #define npyv_doublee __builtin_s390_vflls\n+    #else\n+        #define npyv_doublee vec_doublee\n+    #endif\n+    // compatibility with vsx\n+    #ifndef vec_vbpermq\n+        #define vec_vbpermq vec_bperm_u128\n+    #endif\n+    // zvector requires second operand to signed while vsx api expected to be\n+    // unsigned, the following macros are set to remove this conflict\n+    #define vec_sl_s8(a, b)   vec_sl(a, (npyv_s8)(b))\n+    #define vec_sl_s16(a, b)  vec_sl(a, (npyv_s16)(b))\n+    #define vec_sl_s32(a, b)  vec_sl(a, (npyv_s32)(b))\n+    #define vec_sl_s64(a, b)  vec_sl(a, (npyv_s64)(b))\n+    #define vec_sra_s8(a, b)  vec_sra(a, (npyv_s8)(b))\n+    #define vec_sra_s16(a, b) vec_sra(a, (npyv_s16)(b))\n+    #define vec_sra_s32(a, b) vec_sra(a, (npyv_s32)(b))\n+    #define vec_sra_s64(a, b) vec_sra(a, (npyv_s64)(b))\n+#else\n+    #define vec_sl_s8 vec_sl\n+    #define vec_sl_s16 vec_sl\n+    #define vec_sl_s32 vec_sl\n+    #define vec_sl_s64 vec_sl\n+    #define vec_sra_s8 vec_sra\n+    #define vec_sra_s16 vec_sra\n+    #define vec_sra_s32 vec_sra\n+    #define vec_sra_s64 vec_sra\n+#endif\n+\n+#endif // _NPY_SIMD_VEC_UTILS_H"
            },
            {
                "filename": "numpy/core/src/common/simd/vec/vec.h",
                "patch": "@@ -1,7 +1,22 @@\n+/**\n+ * branch /vec(altivec-like) provides the SIMD operations for\n+ * both IBM VSX(Power) and VX(ZArch).\n+*/\n #ifndef _NPY_SIMD_H_\n     #error \"Not a standalone header\"\n #endif\n \n+#if !defined(NPY_HAVE_VX) && !defined(NPY_HAVE_VSX2)\n+    #error \"require minimum support VX(zarch11) or VSX2(Power8/ISA2.07)\"\n+#endif\n+\n+#if defined(NPY_HAVE_VSX) && !defined(__LITTLE_ENDIAN__)\n+    #error \"VSX support doesn't cover big-endian mode yet, only zarch.\"\n+#endif\n+#if defined(NPY_HAVE_VX) && defined(__LITTLE_ENDIAN__)\n+    #error \"VX(zarch) support doesn't cover little-endian mode.\"\n+#endif\n+\n #if defined(__GNUC__) && __GNUC__ <= 7\n     /**\n       * GCC <= 7 produces ambiguous warning caused by -Werror=maybe-uninitialized,\n@@ -15,8 +30,19 @@\n #define NPY_SIMD 128\n #define NPY_SIMD_WIDTH 16\n #define NPY_SIMD_F64 1\n+#if defined(NPY_HAVE_VXE) || defined(NPY_HAVE_VSX)\n+    #define NPY_SIMD_F32 1\n+#else\n+    #define NPY_SIMD_F32 0\n+#endif\n #define NPY_SIMD_FMA3 1 // native support\n \n+#ifdef NPY_HAVE_VX\n+    #define NPY_SIMD_BIGENDIAN 1\n+#else\n+    #define NPY_SIMD_BIGENDIAN 0\n+#endif\n+\n typedef __vector unsigned char      npyv_u8;\n typedef __vector signed char        npyv_s8;\n typedef __vector unsigned short     npyv_u16;\n@@ -25,7 +51,9 @@ typedef __vector unsigned int       npyv_u32;\n typedef __vector signed int         npyv_s32;\n typedef __vector unsigned long long npyv_u64;\n typedef __vector signed long long   npyv_s64;\n+#if NPY_SIMD_F32\n typedef __vector float              npyv_f32;\n+#endif\n typedef __vector double             npyv_f64;\n \n typedef struct { npyv_u8  val[2]; } npyv_u8x2;\n@@ -36,7 +64,9 @@ typedef struct { npyv_u32 val[2]; } npyv_u32x2;\n typedef struct { npyv_s32 val[2]; } npyv_s32x2;\n typedef struct { npyv_u64 val[2]; } npyv_u64x2;\n typedef struct { npyv_s64 val[2]; } npyv_s64x2;\n+#if NPY_SIMD_F32\n typedef struct { npyv_f32 val[2]; } npyv_f32x2;\n+#endif\n typedef struct { npyv_f64 val[2]; } npyv_f64x2;\n \n typedef struct { npyv_u8  val[3]; } npyv_u8x3;\n@@ -47,7 +77,9 @@ typedef struct { npyv_u32 val[3]; } npyv_u32x3;\n typedef struct { npyv_s32 val[3]; } npyv_s32x3;\n typedef struct { npyv_u64 val[3]; } npyv_u64x3;\n typedef struct { npyv_s64 val[3]; } npyv_s64x3;\n+#if NPY_SIMD_F32\n typedef struct { npyv_f32 val[3]; } npyv_f32x3;\n+#endif\n typedef struct { npyv_f64 val[3]; } npyv_f64x3;\n \n #define npyv_nlanes_u8  16\n@@ -67,6 +99,7 @@ typedef struct { npyv_f64 val[3]; } npyv_f64x3;\n #define npyv_b32 __vector __bool int\n #define npyv_b64 __vector __bool long long\n \n+#include \"utils.h\"\n #include \"memory.h\"\n #include \"misc.h\"\n #include \"reorder.h\""
            },
            {
                "filename": "numpy/core/src/multiarray/argfunc.dispatch.c.src",
                "patch": "@@ -4,6 +4,7 @@\n  ** sse2 sse42 xop avx2 avx512_skx\n  ** vsx2\n  ** neon asimd\n+ ** vx vxe\n  **/\n \n #define NPY_NO_DEPRECATED_API NPY_API_VERSION\n@@ -123,7 +124,7 @@ simd_@func@_@sfx@(npyv_lanetype_@sfx@ *ip, npy_intp len)\n  * #bsfx = b32, b32, b64, b64, b32, b64#\n  * #is_fp = 0*4, 1*2#\n  * #is_idx32 = 1*2, 0*2, 1, 0#\n- * #chk_simd = NPY_SIMD*5, NPY_SIMD_F64#\n+ * #chk_simd = NPY_SIMD*4, NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n #if @chk_simd@\n /**begin repeat1\n@@ -298,6 +299,9 @@ scalar_loop:\n         #if NPY_BITSOF_@BTYPE@ == 64 && !NPY_SIMD_F64\n             #undef TO_SIMD_SFX\n         #endif\n+        #if NPY_BITSOF_@BTYPE@ == 32 && !NPY_SIMD_F32\n+            #undef TO_SIMD_SFX\n+        #endif\n     #elif @is_unsigned@\n         #define TO_SIMD_SFX(X) X##_u@len@\n     #else"
            },
            {
                "filename": "numpy/core/src/multiarray/einsum_sumprod.c.src",
                "patch": "@@ -68,7 +68,7 @@\n  *            0*3#\n  * #NPYV_CHK = 0*5,\n  *             0*5,\n- *             0, NPY_SIMD, NPY_SIMD_F64, 0,\n+ *             0, NPY_SIMD_F32, NPY_SIMD_F64, 0,\n  *             0*3#\n  */\n "
            },
            {
                "filename": "numpy/core/src/umath/loops_arithm_fp.dispatch.c.src",
                "patch": "@@ -1,6 +1,7 @@\n /*@targets\n  ** $maxopt baseline\n  ** sse2 avx2 avx512f\n+ ** vx vxe\n  **/\n #define _UMATHMODULE\n #define _MULTIARRAYMODULE\n@@ -364,7 +365,7 @@ sse2_binary_scalar2_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_i\n  *  #type = npy_float, npy_double#\n  *  #TYPE = FLOAT, DOUBLE#\n  *  #sfx = f32, f64#\n- *  #CHK =    , _F64#\n+ *  #CHK = _F32, _F64#\n  */\n #if NPY_SIMD@CHK@\n /**begin repeat1\n@@ -444,7 +445,7 @@ simd_binary_scalar2_@kind@_@TYPE@(@type@ * op, @type@ * ip1, @type@ * ip2, npy_i\n  *  #type = npy_float, npy_double, npy_longdouble#\n  *  #TYPE = FLOAT, DOUBLE, LONGDOUBLE#\n  *  #vector = 1, 1, 0#\n- *  #VECTOR = NPY_SIMD, NPY_SIMD_F64, 0 #\n+ *  #VECTOR = NPY_SIMD_F32, NPY_SIMD_F64, 0 #\n  */\n /**begin repeat1\n  * Arithmetic"
            },
            {
                "filename": "numpy/core/src/umath/loops_arithmetic.dispatch.c.src",
                "patch": "@@ -3,6 +3,7 @@\n  ** sse2 sse41 avx2 avx512f avx512_skx\n  ** vsx2 vsx4\n  ** neon\n+ ** vx\n  **/\n #define _UMATHMODULE\n #define _MULTIARRAYMODULE"
            },
            {
                "filename": "numpy/core/src/umath/loops_comparison.dispatch.c.src",
                "patch": "@@ -3,6 +3,7 @@\n  ** sse2 sse42 avx2 avx512f avx512_skx\n  ** vsx2 vsx3\n  ** neon\n+ ** vx vxe\n  **/\n #define _UMATHMODULE\n #define _MULTIARRAYMODULE\n@@ -22,7 +23,7 @@\n  * #sfx    = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n  * #len    =  8,  8,  16,  16,  32,  32,  64,  64,  32,  64#\n  * #signed =  0,  1,   0,   1,   0,   1,   0,   1,   0,   0#\n- * #VECTOR = NPY_SIMD*9, NPY_SIMD_F64#\n+ * #VECTOR = NPY_SIMD*8, NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n /**begin repeat1\n  * #kind = equal, not_equal, less, less_equal#\n@@ -298,7 +299,7 @@ static void simd_binary_scalar2_@kind@_b8(char **args, npy_intp len)\n  * #bool = 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0#\n  * #fp = 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1#\n  * #signed = 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0#\n- * #VECTOR = NPY_SIMD*10, NPY_SIMD_F64#\n+ * #VECTOR = NPY_SIMD*9, NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n /**begin repeat1\n  * #kind = equal, not_equal, less, less_equal#"
            },
            {
                "filename": "numpy/core/src/umath/loops_hyperbolic.dispatch.c.src",
                "patch": "@@ -3,6 +3,7 @@\n  ** (avx2 fma3) AVX512_SKX\n  ** vsx2 vsx4\n  ** neon_vfpv4\n+ ** vx vxe\n  **/\n #include \"numpy/npy_math.h\"\n #include \"simd/simd.h\"\n@@ -240,6 +241,8 @@ simd_tanh_f64(const double *src, npy_intp ssrc, double *dst, npy_intp sdst, npy_\n     }\n }\n #endif // NPY_SIMD_F64\n+\n+#if NPY_SIMD_F32\n static void\n simd_tanh_f32(const float *src, npy_intp ssrc, float *dst, npy_intp sdst, npy_intp len)\n {\n@@ -335,14 +338,15 @@ simd_tanh_f32(const float *src, npy_intp ssrc, float *dst, npy_intp sdst, npy_in\n         }\n     }\n }\n+#endif // NPY_SIMD_F32\n #endif // NPY_SIMD_FMA3\n \n /**begin repeat\n  * #TYPE = FLOAT, DOUBLE#\n  * #type = float, double#\n  * #sfx  = f32,   f64#\n  * #ssfx = f,     #\n- * #simd = NPY_SIMD_FMA3, NPY_SIMD_FMA3 && NPY_SIMD_F64#\n+ * #simd = NPY_SIMD_FMA3 && NPY_SIMD_F32, NPY_SIMD_FMA3 && NPY_SIMD_F64#\n  */\n /**begin repeat1\n  *  #func = tanh#"
            },
            {
                "filename": "numpy/core/src/umath/loops_minmax.dispatch.c.src",
                "patch": "@@ -3,6 +3,7 @@\n  ** neon asimd\n  ** sse2 avx2 avx512_skx\n  ** vsx2\n+ ** vx vxe\n  **/\n #define _UMATHMODULE\n #define _MULTIARRAYMODULE\n@@ -144,7 +145,7 @@ NPY_FINLINE @type@ scalar_@op@_@c_sfx@(@type@ a, @type@ b) {\n /**begin repeat\n  * #sfx = f32, f64#\n  * #bsfx = b32, b64#\n- * #simd_chk = NPY_SIMD, NPY_SIMD_F64#\n+ * #simd_chk = NPY_SIMD_F32, NPY_SIMD_F64#\n  * #scalar_sfx = f, d#\n  */\n #if @simd_chk@\n@@ -196,7 +197,7 @@ NPY_FINLINE @type@ scalar_@op@_@c_sfx@(@type@ a, @type@ b) {\n  ******************************************************************************/\n /**begin repeat\n  * #sfx = s8, u8, s16, u16, s32, u32, s64, u64, f32, f64#\n- * #simd_chk = NPY_SIMD*9, NPY_SIMD_F64#\n+ * #simd_chk = NPY_SIMD*8, NPY_SIMD_F32, NPY_SIMD_F64#\n  * #is_fp = 0*8, 1, 1#\n  * #scalar_sfx = i*8, f, d#\n  */\n@@ -395,6 +396,9 @@ simd_binary_@intrin@_@sfx@(const npyv_lanetype_@sfx@ *ip1, npy_intp sip1,\n #elif NPY_SIMD && NPY_BITSOF_@BTYPE@ == @len@\n     #if @is_fp@\n         #define TO_SIMD_SFX(X) X##_f@len@\n+        #if NPY_BITSOF_@BTYPE@ == 32 && !NPY_SIMD_F32\n+            #undef TO_SIMD_SFX\n+        #endif\n         #if NPY_BITSOF_@BTYPE@ == 64 && !NPY_SIMD_F64\n             #undef TO_SIMD_SFX\n         #endif"
            },
            {
                "filename": "numpy/core/src/umath/loops_trigonometric.dispatch.c.src",
                "patch": "@@ -3,6 +3,7 @@\n  ** (avx2 fma3) avx512f\n  ** vsx2 vsx3 vsx4\n  ** neon_vfpv4\n+ ** vxe vxe2\n  **/\n #include \"numpy/npy_math.h\"\n #include \"simd/simd.h\"\n@@ -13,7 +14,7 @@\n  * - use vectorized version of Payne-Hanek style reduction for large elements or\n  *   when there's no native FUSED support instead of fallback to libc\n  */\n-#if NPY_SIMD_FMA3 // native support\n+#if NPY_SIMD_F32 && NPY_SIMD_FMA3 // native support\n /*\n  * Vectorized Cody-Waite range reduction technique\n  * Performs the reduction step x* = x - y*C in three steps:\n@@ -210,7 +211,7 @@ NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(FLOAT_@func@)\n     const npy_intp sdst = steps[1] / lsize;\n     npy_intp len = dimensions[0];\n     assert(len <= 1 || (steps[0] % lsize == 0 && steps[1] % lsize == 0));\n-#if NPY_SIMD_FMA3\n+#if NPY_SIMD_F32 && NPY_SIMD_FMA3\n     if (is_mem_overlap(src, steps[0], dst, steps[1], len) ||\n         !npyv_loadable_stride_f32(ssrc) || !npyv_storable_stride_f32(sdst)\n     ) {"
            },
            {
                "filename": "numpy/core/src/umath/loops_unary_fp.dispatch.c.src",
                "patch": "@@ -3,6 +3,7 @@\n  ** sse2 sse41\n  ** vsx2\n  ** neon asimd\n+ ** vx vxe\n  **/\n /**\n  * Force use SSE only on x86, even if AVX2 or AVX512F are enabled\n@@ -18,7 +19,7 @@\n /**********************************************************\n  ** Scalars\n  **********************************************************/\n-#if !NPY_SIMD\n+#if !NPY_SIMD_F32\n NPY_FINLINE float c_recip_f32(float a)\n { return 1.0f / a; }\n NPY_FINLINE float c_abs_f32(float a)\n@@ -29,7 +30,7 @@ NPY_FINLINE float c_abs_f32(float a)\n }\n NPY_FINLINE float c_square_f32(float a)\n { return a * a; }\n-#endif // !NPY_SIMD\n+#endif // !NPY_SIMD_F32\n \n #if !NPY_SIMD_F64\n NPY_FINLINE double c_recip_f64(double a)\n@@ -147,7 +148,7 @@ NPY_FINLINE double c_square_f64(double a)\n /**begin repeat\n  * #TYPE = FLOAT, DOUBLE#\n  * #sfx  = f32, f64#\n- * #VCHK = NPY_SIMD, NPY_SIMD_F64#\n+ * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n #if @VCHK@\n /**begin repeat1\n@@ -259,7 +260,7 @@ static void simd_@TYPE@_@kind@_@STYPE@_@DTYPE@\n /**begin repeat\n  * #TYPE = FLOAT, DOUBLE#\n  * #sfx  = f32, f64#\n- * #VCHK = NPY_SIMD, NPY_SIMD_F64#\n+ * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#\n  */\n /**begin repeat1\n  * #kind  = rint, floor, ceil, trunc, sqrt, absolute, square, reciprocal#"
            },
            {
                "filename": "numpy/core/tests/test_simd.py",
                "patch": "@@ -85,16 +85,13 @@ def _to_unsigned(self, vector):\n             return getattr(self.npyv, cvt_intrin.format(sfx[1:], sfx))(vector)\n \n     def _pinfinity(self):\n-        v = self.npyv.setall_u32(0x7f800000)\n-        return self.npyv.reinterpret_f32_u32(v)[0]\n+        return float(\"inf\")\n \n     def _ninfinity(self):\n-        v = self.npyv.setall_u32(0xff800000)\n-        return self.npyv.reinterpret_f32_u32(v)[0]\n+        return -float(\"inf\")\n \n     def _nan(self):\n-        v = self.npyv.setall_u32(0x7fc00000)\n-        return self.npyv.reinterpret_f32_u32(v)[0]\n+        return float(\"nan\")\n \n     def _cpu_features(self):\n         target = self.target_name\n@@ -170,8 +167,9 @@ def test_tobits(self):\n         for data in (self._data(), self._data(reverse=True)):\n             vdata = self._load_b(data)\n             data_bits = data2bits(data)\n-            tobits = bin(self.tobits(vdata))\n-            assert tobits == bin(data_bits)\n+            tobits = self.tobits(vdata)\n+            bin_tobits = bin(tobits)\n+            assert bin_tobits == bin(data_bits)\n \n     def test_pack(self):\n         \"\"\"\n@@ -718,9 +716,11 @@ def test_misc(self):\n         # We're testing the sanity of _simd's type-vector,\n         # reinterpret* intrinsics itself are tested via compiler\n         # during the build of _simd module\n-        sfxes = [\"u8\", \"s8\", \"u16\", \"s16\", \"u32\", \"s32\", \"u64\", \"s64\", \"f32\"]\n+        sfxes = [\"u8\", \"s8\", \"u16\", \"s16\", \"u32\", \"s32\", \"u64\", \"s64\"]\n         if self.npyv.simd_f64:\n             sfxes.append(\"f64\")\n+        if self.npyv.simd_f32:\n+            sfxes.append(\"f32\")\n         for sfx in sfxes:\n             vec_name = getattr(self, \"reinterpret_\" + sfx)(vdata_a).__name__\n             assert vec_name == \"npyv_\" + sfx\n@@ -1049,8 +1049,13 @@ def test_mask_conditional(self):\n         skip = f\"target '{pretty_name}' isn't supported by current machine\"\n     elif not npyv.simd:\n         skip = f\"target '{pretty_name}' isn't supported by NPYV\"\n-    elif not npyv.simd_f64:\n-        skip_sfx[\"f64\"] = f\"target '{pretty_name}' doesn't support double-precision\"\n+    else:\n+        if not npyv.simd_f32:\n+            skip_sfx[\"f32\"] = f\"target '{pretty_name}' \"\\\n+                               \"doesn't support single-precision\"\n+        if not npyv.simd_f64:\n+            skip_sfx[\"f64\"] = f\"target '{pretty_name}' doesn't\"\\\n+                               \"support double-precision\"\n \n     for sfxes, cls in tests_registry.items():\n         for sfx in sfxes:"
            },
            {
                "filename": "numpy/core/tests/test_simd_module.py",
                "patch": "@@ -12,7 +12,9 @@\n \n unsigned_sfx = [\"u8\", \"u16\", \"u32\", \"u64\"]\n signed_sfx = [\"s8\", \"s16\", \"s32\", \"s64\"]\n-fp_sfx = [\"f32\"]\n+fp_sfx = []\n+if npyv and npyv.simd_f32:\n+    fp_sfx.append(\"f32\")\n if npyv and npyv.simd_f64:\n     fp_sfx.append(\"f64\")\n "
            },
            {
                "filename": "tools/travis-test.sh",
                "patch": "@@ -40,7 +40,7 @@ setup_base()\n   # any specified features will be ignored if they're not supported by compiler or platform\n   # note: it almost the same default value of --simd-test execpt adding policy `$werror` to treat all\n   # warnings as errors\n-  simd_test=\"\\$werror BASELINE SSE2 SSE42 XOP FMA4 (FMA3 AVX2) AVX512F AVX512_SKX VSX VSX2 VSX3 NEON ASIMD\"\n+  simd_test=\"\\$werror BASELINE SSE2 SSE42 XOP FMA4 (FMA3 AVX2) AVX512F AVX512_SKX VSX VSX2 VSX3 NEON ASIMD VX VXE VXE2\"\n   # We used to use 'setup.py install' here, but that has the terrible\n   # behaviour that if a copy of the package is already installed in the\n   # install location, then the new copy just gets dropped on top of it."
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 13207,
        "body": "For calls of the form `np.min(arr_1, arr_2)`, the runtime does not scale with `sizeof(type)` as we would generally expect:\r\n```\r\n[100.00%] \u00b7\u00b7\u00b7 bench_ufunc.MinMax.time_minimum                                                                                                                             ok\r\n[100.00%] \u00b7\u00b7\u00b7 ================= ============\r\n                    dtype\r\n              ----------------- ------------\r\n                     bool        6.54\u00b10.2\u03bcs    <----|\r\n                    uint8         88.9\u00b12\u03bcs     <----|--- these are the same size\r\n                     int8         85.1\u00b11\u03bcs     <----|\r\n                    int32         75.9\u00b12\u03bcs\r\n                    uint64        82.3\u00b11\u03bcs\r\n                    int64         82.6\u00b11\u03bcs\r\n                   float16        795\u00b120\u03bcs\r\n                   float32        102\u00b13\u03bcs\r\n                   float64        86.7\u00b11\u03bcs\r\n                  complex128      178\u00b14\u03bcs\r\n                datetime64[ns]    101\u00b12\u03bcs\r\n               timedelta64[ns]    101\u00b12\u03bcs\r\n              ================= ============\r\n```\r\nNotably, the integer types all take about the same amount of time to run irrespective of the size of the data, which indicates we're not constrained on memory bandwidth. Using the existing `BINARY_LOOP_FAST` macro and enabling `-O3` is sufficient to get some broad speedups:\r\n\r\n```\r\nasv compare HEAD^ HEAD -s --sort ratio\r\n\r\nBenchmarks that have improved:\r\n\r\n       before           after         ratio\r\n     [db5fcc8e]       [34a8d2f9]\r\n     <maximum_speedup~1>       <maximum_speedup>\r\n-        99.4\u00b12\u03bcs         78.3\u00b13\u03bcs     0.79  bench_ufunc.MinMax.time_fmin('datetime64[ns]')\r\n-        97.3\u00b11\u03bcs         75.7\u00b13\u03bcs     0.78  bench_ufunc.MinMax.time_minimum('datetime64[ns]')\r\n-        99.1\u00b12\u03bcs         76.3\u00b11\u03bcs     0.77  bench_ufunc.MinMax.time_maximum('timedelta64[ns]')\r\n-        99.2\u00b11\u03bcs         75.3\u00b13\u03bcs     0.76  bench_ufunc.MinMax.time_minimum('timedelta64[ns]')\r\n-        83.1\u00b13\u03bcs         62.3\u00b11\u03bcs     0.75  bench_ufunc.MinMax.time_maximum('float64')\r\n-        105\u00b110\u03bcs         78.3\u00b13\u03bcs     0.75  bench_ufunc.MinMax.time_fmax('timedelta64[ns]')\r\n-        82.1\u00b12\u03bcs         61.1\u00b13\u03bcs     0.74  bench_ufunc.MinMax.time_minimum('float64')\r\n-         102\u00b12\u03bcs        71.3\u00b110\u03bcs     0.70  bench_ufunc.MinMax.time_fmax('float64')\r\n-        83.5\u00b13\u03bcs         31.4\u00b12\u03bcs     0.38  bench_ufunc.MinMax.time_maximum('float32')\r\n-        73.2\u00b13\u03bcs       25.4\u00b10.5\u03bcs     0.35  bench_ufunc.MinMax.time_maximum('int32')\r\n-      76.8\u00b10.7\u03bcs       26.5\u00b10.7\u03bcs     0.35  bench_ufunc.MinMax.time_fmax('int32')\r\n-        76.6\u00b14\u03bcs       25.7\u00b10.5\u03bcs     0.34  bench_ufunc.MinMax.time_minimum('int32')\r\n-        77.9\u00b14\u03bcs       25.2\u00b10.7\u03bcs     0.32  bench_ufunc.MinMax.time_fmin('int32')\r\n-        98.2\u00b12\u03bcs         31.7\u00b11\u03bcs     0.32  bench_ufunc.MinMax.time_minimum('float32')\r\n-        98.7\u00b13\u03bcs       31.4\u00b10.8\u03bcs     0.32  bench_ufunc.MinMax.time_fmax('float32')\r\n-        98.8\u00b12\u03bcs       30.3\u00b10.8\u03bcs     0.31  bench_ufunc.MinMax.time_fmin('float32')\r\n-        78.5\u00b12\u03bcs       7.08\u00b10.2\u03bcs     0.09  bench_ufunc.MinMax.time_minimum('int8')\r\n-       95.7\u00b120\u03bcs         8.59\u00b11\u03bcs     0.09  bench_ufunc.MinMax.time_fmin('int8')\r\n-      88.5\u00b10.6\u03bcs      7.18\u00b10.09\u03bcs     0.08  bench_ufunc.MinMax.time_maximum('int8')\r\n-        89.7\u00b12\u03bcs       7.11\u00b10.3\u03bcs     0.08  bench_ufunc.MinMax.time_fmax('int8')\r\n-        81.6\u00b11\u03bcs       5.37\u00b10.2\u03bcs     0.07  bench_ufunc.MinMax.time_fmax('uint8')\r\n-      80.5\u00b10.5\u03bcs       5.23\u00b10.2\u03bcs     0.06  bench_ufunc.MinMax.time_maximum('uint8')\r\n-        87.1\u00b11\u03bcs       5.38\u00b10.1\u03bcs     0.06  bench_ufunc.MinMax.time_minimum('uint8')\r\n-       92.1\u00b120\u03bcs       5.37\u00b10.4\u03bcs     0.06  bench_ufunc.MinMax.time_fmin('uint8')\r\n\r\nBenchmarks that have stayed the same:\r\n\r\n       before           after         ratio\r\n     [db5fcc8e]       [34a8d2f9]\r\n     <maximum_speedup~1>       <maximum_speedup>\r\n         907\u00b120\u03bcs         975\u00b120\u03bcs     1.07  bench_ufunc.MinMax.time_maximum('float16')\r\n          171\u00b17\u03bcs         183\u00b110\u03bcs     1.07  bench_ufunc.MinMax.time_minimum('complex128')\r\n          817\u00b19\u03bcs         872\u00b170\u03bcs     1.07  bench_ufunc.MinMax.time_minimum('float16')\r\n       6.01\u00b10.1\u03bcs       6.31\u00b10.2\u03bcs     1.05  bench_ufunc.MinMax.time_minimum('bool')\r\n         78.9\u00b12\u03bcs         82.2\u00b12\u03bcs     1.04  bench_ufunc.MinMax.time_minimum('uint64')\r\n        861\u00b1200\u03bcs         895\u00b120\u03bcs     1.04  bench_ufunc.MinMax.time_fmin('float16')\r\n       11.1\u00b10.3\u03bcs       11.5\u00b10.8\u03bcs     1.03  bench_reduce.MinMax.time_min(<class 'numpy.float64'>)\r\n       11.0\u00b10.4\u03bcs       11.3\u00b10.1\u03bcs     1.03  bench_reduce.MinMax.time_max(<class 'numpy.float64'>)\r\n       7.55\u00b10.2\u03bcs       7.75\u00b10.1\u03bcs     1.03  bench_reduce.MinMax.time_min(<class 'numpy.float32'>)\r\n         953\u00b110\u03bcs         969\u00b110\u03bcs     1.02  bench_ufunc.MinMax.time_fmax('float16')\r\n          206\u00b15\u03bcs          207\u00b17\u03bcs     1.01  bench_ufunc.MinMax.time_fmax('complex128')\r\n       5.83\u00b10.3\u03bcs       5.86\u00b10.1\u03bcs     1.00  bench_ufunc.MinMax.time_fmax('bool')\r\n       17.0\u00b10.3\u03bcs       17.1\u00b10.6\u03bcs     1.00  bench_reduce.MinMax.time_min(<class 'numpy.int64'>)\r\n       7.80\u00b10.4\u03bcs       7.83\u00b10.3\u03bcs     1.00  bench_reduce.MinMax.time_max(<class 'numpy.float32'>)\r\n       17.2\u00b10.3\u03bcs       17.2\u00b10.4\u03bcs     1.00  bench_reduce.MinMax.time_max(<class 'numpy.int64'>)\r\n      5.93\u00b10.06\u03bcs       5.90\u00b10.2\u03bcs     1.00  bench_ufunc.MinMax.time_maximum('bool')\r\n          176\u00b13\u03bcs          175\u00b18\u03bcs     0.99  bench_ufunc.MinMax.time_maximum('complex128')\r\n       6.63\u00b10.8\u03bcs       6.44\u00b10.1\u03bcs     0.97  bench_ufunc.MinMax.time_fmin('bool')\r\n          206\u00b14\u03bcs          198\u00b13\u03bcs     0.96  bench_ufunc.MinMax.time_fmin('complex128')\r\n         83.7\u00b16\u03bcs         80.0\u00b12\u03bcs     0.96  bench_ufunc.MinMax.time_fmin('uint64')\r\n       72.4\u00b10.8\u03bcs         68.3\u00b12\u03bcs     0.94  bench_ufunc.MinMax.time_minimum('int64')\r\n         73.9\u00b12\u03bcs         69.2\u00b12\u03bcs     0.94  bench_ufunc.MinMax.time_fmax('int64')\r\n         76.4\u00b11\u03bcs         71.2\u00b12\u03bcs     0.93  bench_ufunc.MinMax.time_maximum('int64')\r\n       76.4\u00b10.8\u03bcs         70.0\u00b15\u03bcs     0.92  bench_ufunc.MinMax.time_fmin('int64')\r\n         73.5\u00b12\u03bcs         66.9\u00b12\u03bcs     0.91  bench_ufunc.MinMax.time_fmax('uint64')\r\n       77.8\u00b10.6\u03bcs         69.9\u00b12\u03bcs    ~0.90  bench_ufunc.MinMax.time_maximum('uint64')\r\n         94.1\u00b15\u03bcs         82.4\u00b18\u03bcs    ~0.88  bench_ufunc.MinMax.time_fmin('timedelta64[ns]')\r\n         98.1\u00b13\u03bcs         82.8\u00b15\u03bcs    ~0.84  bench_ufunc.MinMax.time_fmax('datetime64[ns]')\r\n         84.9\u00b13\u03bcs         66.2\u00b16\u03bcs    ~0.78  bench_ufunc.MinMax.time_fmin('float64')\r\n          100\u00b12\u03bcs         74.7\u00b14\u03bcs    ~0.74  bench_ufunc.MinMax.time_maximum('datetime64[ns]')\r\n```\r\nPlease note that this does not impact the more common `np.min(arr)` reduction call - I will have a separate PR addressing those functions in the near future.",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -60,6 +60,31 @@ def time_ufunc_types(self, ufuncname):\n         [self.f(*arg) for arg in self.args]\n \n \n+class UFuncBenchmark(Benchmark):\n+    params = ['bool', 'uint8', 'int8', 'int32', 'uint64', 'int64', 'float16',\n+              'float32', 'float64', 'complex128', 'datetime64[ns]',\n+              'timedelta64[ns]']\n+    param_names = ['dtype']\n+\n+    def setup(self, dtype):\n+        self.arr = np.full(100000, 1, dtype=dtype)\n+        self.val = self.arr[0]\n+\n+\n+class MinMax(UFuncBenchmark):\n+    def time_fmax(self, dtype):\n+        np.fmax(self.arr, self.arr)\n+\n+    def time_fmin(self, dtype):\n+        np.fmin(self.arr, self.arr)\n+\n+    def time_maximum(self, dtype):\n+        np.maximum(self.arr, self.arr)\n+\n+    def time_minimum(self, dtype):\n+        np.minimum(self.arr, self.arr)\n+\n+\n class Custom(Benchmark):\n     def setup(self):\n         self.b = np.ones(20000, dtype=bool)"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -814,7 +814,7 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n  * #OP =  >, <#\n  **/\n \n-NPY_NO_EXPORT void\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n @TYPE@_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func))\n {\n     if (IS_BINARY_REDUCE) {\n@@ -825,11 +825,8 @@ NPY_NO_EXPORT void\n         *((@type@ *)iop1) = io1;\n     }\n     else {\n-        BINARY_LOOP {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            const @type@ in2 = *(@type@ *)ip2;\n-            *((@type@ *)op1) = (in1 @OP@ in2) ? in1 : in2;\n-        }\n+        BINARY_LOOP_FAST(@type@, @type@,\n+                         *out = (in1 @OP@ in2) ? in1 : in2);\n     }\n }\n \n@@ -1826,7 +1823,7 @@ NPY_NO_EXPORT void\n  * #kind = maximum, minimum#\n  * #OP =  >=, <=#\n  **/\n-NPY_NO_EXPORT void\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n @TYPE@_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func))\n {\n     /*  */\n@@ -1841,13 +1838,9 @@ NPY_NO_EXPORT void\n         }\n     }\n     else {\n-        BINARY_LOOP {\n-            @type@ in1 = *(@type@ *)ip1;\n-            const @type@ in2 = *(@type@ *)ip2;\n-            /* Order of operations important for MSVC 2015 */\n-            in1 = (in1 @OP@ in2 || npy_isnan(in1)) ? in1 : in2;\n-            *((@type@ *)op1) = in1;\n-        }\n+        BINARY_LOOP_FAST(@type@, @type@,\n+                         /* Order of operations important for MSVC 2015 */\n+                         *out = (in1 @OP@ in2 || npy_isnan(in1)) ? in1 : in2);\n     }\n     npy_clear_floatstatus_barrier((char*)dimensions);\n }\n@@ -1857,7 +1850,7 @@ NPY_NO_EXPORT void\n  * #kind = fmax, fmin#\n  * #OP =  >=, <=#\n  **/\n-NPY_NO_EXPORT void\n+NPY_NO_EXPORT NPY_GCC_OPT_3 void\n @TYPE@_@kind@(char **args, npy_intp *dimensions, npy_intp *steps, void *NPY_UNUSED(func))\n {\n     /*  */\n@@ -1870,12 +1863,10 @@ NPY_NO_EXPORT void\n         *((@type@ *)iop1) = io1;\n     }\n     else {\n-        BINARY_LOOP {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            const @type@ in2 = *(@type@ *)ip2;\n-            /* Order of operations important for MSVC 2015 */\n-            *((@type@ *)op1) = (in1 @OP@ in2 || npy_isnan(in2)) ? in1 : in2;\n-        }\n+        BINARY_LOOP_FAST(@type@, @type@,\n+                         /* Order of operations important for MSVC 2015 */\n+                         *out = (in1 @OP@ in2 || npy_isnan(in2)) ? in1 : in2;\n+                         );\n     }\n     npy_clear_floatstatus_barrier((char*)dimensions);\n }"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 16378,
        "body": "Fix for #16377. Previously, this would error:\r\n\r\n```python\r\nnp.array_equal(list(\"abc\"), list(\"abc\"), equal_nan=True)\r\n```\r\n\r\nThis fixes that bug, and tests for it. \r\n\r\nThis is a little messy, but I'm not sure if there is a nicer way to check if a ufunc could be applied to a given type.\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/numeric.py",
                "patch": "@@ -2369,21 +2369,33 @@ def array_equal(a1, a2, equal_nan=False):\n     >>> np.array_equal(a, b, equal_nan=True)\n     True\n     \"\"\"\n+    import builtins\n+\n     try:\n         a1, a2 = asarray(a1), asarray(a2)\n     except Exception:\n         return False\n     if a1.shape != a2.shape:\n         return False\n-    if not equal_nan:\n-        return bool(asarray(a1 == a2).all())\n-    # Handling NaN values if equal_nan is True\n-    a1nan, a2nan = isnan(a1), isnan(a2)\n-    # NaN's occur at different locations\n-    if not (a1nan == a2nan).all():\n+\n+    common_type_char = np.sctype2char(np.promote_types(a1.dtype, a2.dtype))\n+    comp = asarray(a1 == a2)\n+\n+    if bool(comp.all()):\n+        return True\n+    elif not equal_nan:\n         return False\n-    # Shapes of a1, a2 and masks are guaranteed to be consistent by this point\n-    return bool(asarray(a1[~a1nan] == a2[~a1nan]).all())\n+    elif common_type_char == \"O\":\n+        # When isnan won't work, but there could be nan values\n+        _isnan = np.vectorize(functools.partial(operator.is_, nan), otypes=\"?\")\n+    elif not builtins.any(t.startswith(common_type_char) for t in isnan.types):\n+        # When there can't be nan values\n+        return False\n+    else:\n+        # When we can use isnan\n+        _isnan = isnan\n+    # Check if all failed comparisons were due to nans\n+    return all(comp | (_isnan(a1) & _isnan(a2)))\n \n \n def _array_equiv_dispatcher(a1, a2):"
            },
            {
                "filename": "numpy/core/tests/test_numeric.py",
                "patch": "@@ -1475,6 +1475,14 @@ def test_array_equal_equal_nan(self):\n         a.real, b.imag = np.nan, np.nan\n         assert_(not np.array_equal(a, b, equal_nan=False))\n         assert_(np.array_equal(a, b, equal_nan=True))\n+        # String values (checking for issue #16377)\n+        a = np.array(list(\"abc\"))\n+        assert_(np.array_equal(a, a, equal_nan=False))\n+        assert_(np.array_equal(a, a, equal_nan=True))\n+        # Object arrays\n+        a = np.array([\"a\", None, np.nan], dtype=object)\n+        assert_(not np.array_equal(a, a, equal_nan=False))\n+        assert_(np.array_equal(a, a, equal_nan=True))\n \n     def test_none_compares_elementwise(self):\n         a = np.array([None, 1, None], dtype=object)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21527,
        "body": "* Add fast path for lookup of the dtype of a Python float and long\r\n* Remove check on `Py_None`\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/array_coercion.c",
                "patch": "@@ -15,6 +15,7 @@\n #include \"common_dtype.h\"\n #include \"dtypemeta.h\"\n \n+#include \"abstractdtypes.h\"\n #include \"array_coercion.h\"\n #include \"ctors.h\"\n #include \"common.h\"\n@@ -204,29 +205,33 @@ _PyArray_MapPyTypeToDType(\n  * Lookup the DType for a registered known python scalar type.\n  *\n  * @param pytype Python Type to look up\n- * @return DType, None if it a known non-scalar, or NULL if an unknown object.\n+ * @return DType, None if it is a known non-scalar, or NULL if an unknown object.\n  */\n static NPY_INLINE PyArray_DTypeMeta *\n npy_discover_dtype_from_pytype(PyTypeObject *pytype)\n {\n     PyObject *DType;\n \n     if (pytype == &PyArray_Type) {\n-        Py_INCREF(Py_None);\n-        return (PyArray_DTypeMeta *)Py_None;\n+        DType = Py_None;\n     }\n-\n-    DType = PyDict_GetItem(_global_pytype_to_type_dict, (PyObject *)pytype);\n-    if (DType == NULL) {\n-        /* the python type is not known */\n-        return NULL;\n+    else if (pytype == &PyFloat_Type) {\n+        DType = (PyObject *)&PyArray_PyFloatAbstractDType;\n+    }\n+    else if (pytype == &PyLong_Type) {\n+        DType = (PyObject *)&PyArray_PyIntAbstractDType;\n     }\n+    else {\n+        DType = PyDict_GetItem(_global_pytype_to_type_dict,\n+                               (PyObject *)pytype);\n \n-    Py_INCREF(DType);\n-    if (DType == Py_None) {\n-        return (PyArray_DTypeMeta *)Py_None;\n+        if (DType == NULL) {\n+            /* the python type is not known */\n+            return NULL;\n+        }\n     }\n-    assert(PyObject_TypeCheck(DType, (PyTypeObject *)&PyArrayDTypeMeta_Type));\n+    Py_INCREF(DType);\n+    assert(DType == Py_None || PyObject_TypeCheck(DType, (PyTypeObject *)&PyArrayDTypeMeta_Type));\n     return (PyArray_DTypeMeta *)DType;\n }\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21531,
        "body": "This PR reduces the overhead introduced in #17582 when the default allocation strategy is used. Approaches to reduce the overhead are discussed in #21488\r\n\r\nFixes #21488\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "doc/source/reference/c-api/data_memory.rst",
                "patch": "@@ -55,6 +55,8 @@ functions may change during the lifetime of the process, each ``ndarray``\n carries with it the functions used at the time of its instantiation, and these\n will be used to reallocate or free the data memory of the instance.\n \n+For details see: :ref:`NEP 49 \u2014 Data allocation strategies <NEP49>`.\n+\n .. c:type:: PyDataMem_Handler\n \n     A struct to hold function pointers used to manipulate memory\n@@ -87,6 +89,9 @@ will be used to reallocate or free the data memory of the instance.\n    return ``NULL`` if an error has occurred. We wrap the user-provided functions\n    so they will still call the python and numpy memory management callback\n    hooks.\n+   The handlers are stored in a Python context variable\n+   (see https://docs.python.org/3/library/contextvars.html),\n+   so there can be multiple handlers in a Python session.\n     \n .. c:function:: PyObject * PyDataMem_GetHandler()\n "
            },
            {
                "filename": "numpy/core/src/multiarray/alloc.c",
                "patch": "@@ -430,11 +430,25 @@ int uo_index=0;   /* user_override index */\n \n /* Wrappers for the default or any user-assigned PyDataMem_Handler */\n \n+int default_allocator_policy = 1;\n+\n+static inline PyDataMem_Handler *\n+_PyDataMem_GetHandler_Internal(PyObject *mem_handler)\n+{\n+    if (PyDataMem_DefaultHandler == mem_handler)\n+        // fast path for default allocator\n+        return &default_handler;\n+\n+    PyDataMem_Handler *handler = (PyDataMem_Handler *)PyCapsule_GetPointer(\n+            mem_handler, \"mem_handler\");\n+    return handler;\n+}\n+\n NPY_NO_EXPORT void *\n PyDataMem_UserNEW(size_t size, PyObject *mem_handler)\n {\n     void *result;\n-    PyDataMem_Handler *handler = (PyDataMem_Handler *) PyCapsule_GetPointer(mem_handler, \"mem_handler\");\n+    PyDataMem_Handler *handler = _PyDataMem_GetHandler_Internal(mem_handler);\n     if (handler == NULL) {\n         return NULL;\n     }\n@@ -457,7 +471,7 @@ NPY_NO_EXPORT void *\n PyDataMem_UserNEW_ZEROED(size_t nmemb, size_t size, PyObject *mem_handler)\n {\n     void *result;\n-    PyDataMem_Handler *handler = (PyDataMem_Handler *) PyCapsule_GetPointer(mem_handler, \"mem_handler\");\n+    PyDataMem_Handler *handler = _PyDataMem_GetHandler_Internal(mem_handler);\n     if (handler == NULL) {\n         return NULL;\n     }\n@@ -479,7 +493,7 @@ PyDataMem_UserNEW_ZEROED(size_t nmemb, size_t size, PyObject *mem_handler)\n NPY_NO_EXPORT void\n PyDataMem_UserFREE(void *ptr, size_t size, PyObject *mem_handler)\n {\n-    PyDataMem_Handler *handler = (PyDataMem_Handler *) PyCapsule_GetPointer(mem_handler, \"mem_handler\");\n+    PyDataMem_Handler *handler = _PyDataMem_GetHandler_Internal(mem_handler);\n     if (handler == NULL) {\n         WARN_NO_RETURN(PyExc_RuntimeWarning,\n                      \"Could not get pointer to 'mem_handler' from PyCapsule\");\n@@ -502,7 +516,7 @@ NPY_NO_EXPORT void *\n PyDataMem_UserRENEW(void *ptr, size_t size, PyObject *mem_handler)\n {\n     void *result;\n-    PyDataMem_Handler *handler = (PyDataMem_Handler *) PyCapsule_GetPointer(mem_handler, \"mem_handler\");\n+    PyDataMem_Handler *handler = _PyDataMem_GetHandler_Internal(mem_handler);\n     if (handler == NULL) {\n         return NULL;\n     }\n@@ -535,6 +549,13 @@ PyDataMem_UserRENEW(void *ptr, size_t size, PyObject *mem_handler)\n NPY_NO_EXPORT PyObject *\n PyDataMem_SetHandler(PyObject *handler)\n {\n+    /*\n+     * Once the user sets an allocation policy, we cannot guarantee\n+     * the default allocator without checking the context until then,\n+     * this a is a micro-optimization to avoid the `PyContextVar_Get`\n+     */\n+    default_allocator_policy = 0;\n+\n     PyObject *old_handler;\n     PyObject *token;\n     if (PyContextVar_Get(current_handler, NULL, &old_handler)) {\n@@ -560,6 +581,11 @@ NPY_NO_EXPORT PyObject *\n PyDataMem_GetHandler()\n {\n     PyObject *handler;\n+\n+    if (default_allocator_policy) {\n+        Py_INCREF(PyDataMem_DefaultHandler);\n+        return PyDataMem_DefaultHandler;\n+    }\n     if (PyContextVar_Get(current_handler, NULL, &handler)) {\n         return NULL;\n     }"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21483,
        "body": "This PR moves the comparison functions (eq, ne, lt, le, ge and gt) to a new dispatchable source file to make use of the universal intrinsics. This optimization benefits all architectures.\r\n\r\nThe following universal intrinsics were added in this PR:\r\n```\r\nnpyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b);\r\nnpyv_b8 npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32  c, npyv_b32 d);\r\nnpyv_b8 npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64  c, npyv_b64 d, \r\n                         npyv_b64 e, npyv_b64 f, npyv_b64  g, npyv_b64 h);\r\n```\r\n\r\n## Benchmark:\r\n\r\n### X86\r\n\r\n<details>\r\n  <summary>CPU</summary>\r\n  \r\n```\r\nCPU\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                32\r\nOn-line CPU(s) list:   0-31\r\nThread(s) per core:    2\r\nCore(s) per socket:    8\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 85\r\nModel name:            Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz\r\nStepping:              7\r\nCPU MHz:               2100.128\r\nCPU max MHz:           2100.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              4200.00\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              1024K\r\nL3 cache:              11264K\r\nNUMA node0 CPU(s):     0-7,16-23\r\nNUMA node1 CPU(s):     8-15,24-31\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology non\r\nstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cd\r\np_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt cl\r\nwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts pku ospke avx512_vnni md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>OS</summary>\r\n\r\n```\r\nCentOS Linux 7 (Core)\"\r\ngcc (Spack GCC) 9.4.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n  <summary>SSE</summary>\r\n\r\n```\r\nexport NPY_DISABLE_CPU_FEATURES=\"SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_SKX\"\r\n        before          after\r\n+     10.1\u00b10.04\u03bcs      11.6\u00b10.05\u03bcs     1.15  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float32'>)\r\n+     8.43\u00b10.03\u03bcs      9.16\u00b10.03\u03bcs     1.09  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint16'>)\r\n+      41.3\u00b10.1\u03bcs         43.7\u00b11\u03bcs     1.06  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint64'>)\r\n-      18.3\u00b10.2\u03bcs      16.9\u00b10.09\u03bcs     0.92  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float64'>)\r\n-      18.1\u00b10.1\u03bcs      16.1\u00b10.03\u03bcs     0.89  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float64'>)\r\n-      30.2\u00b10.4\u03bcs       26.3\u00b10.2\u03bcs     0.87  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float64'>)\r\n-     50.9\u00b10.09\u03bcs       44.2\u00b10.5\u03bcs     0.87  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint64'>)\r\n-     8.23\u00b10.08\u03bcs      6.97\u00b10.01\u03bcs     0.85  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int16'>)\r\n-      8.88\u00b10.1\u03bcs      7.50\u00b10.04\u03bcs     0.85  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint16'>)\r\n-      50.8\u00b10.2\u03bcs       42.1\u00b10.2\u03bcs     0.83  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int64'>)\r\n-     18.1\u00b10.02\u03bcs       14.9\u00b10.3\u03bcs     0.82  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int32'>)\r\n-     8.15\u00b10.09\u03bcs      6.47\u00b10.04\u03bcs     0.79  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int16'>)\r\n-     50.5\u00b10.05\u03bcs      39.3\u00b10.05\u03bcs     0.78  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int64'>)\r\n-     10.2\u00b10.02\u03bcs      7.80\u00b10.03\u03bcs     0.77  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint16'>)\r\n-      50.7\u00b10.1\u03bcs       38.6\u00b10.1\u03bcs     0.76  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int64'>)\r\n-      20.3\u00b10.2\u03bcs      14.7\u00b10.03\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint32'>)\r\n-      19.9\u00b10.3\u03bcs       12.3\u00b10.1\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint32'>)\r\n-     19.3\u00b10.04\u03bcs      11.7\u00b10.07\u03bcs     0.60  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint32'>)\r\n-     19.0\u00b10.01\u03bcs      10.1\u00b10.01\u03bcs     0.53  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int32'>)\r\n-     19.1\u00b10.02\u03bcs      9.69\u00b10.04\u03bcs     0.51  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int32'>)\r\n-     77.7\u00b10.05\u03bcs      6.92\u00b10.01\u03bcs     0.09  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.bool_'>)\r\n-     82.7\u00b10.05\u03bcs      6.79\u00b10.02\u03bcs     0.08  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.bool_'>)\r\n-     77.7\u00b10.04\u03bcs      6.38\u00b10.04\u03bcs     0.08  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.bool_'>)\r\n\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>AVX2</summary>\r\n\r\n```\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX512F AVX512_SKX\"\r\n        before          after\r\n+     4.80\u00b10.03\u03bcs      5.29\u00b10.02\u03bcs     1.10  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint8'>)\r\n-     9.95\u00b10.01\u03bcs      9.44\u00b10.01\u03bcs     0.95  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float32'>)\r\n-     10.0\u00b10.04\u03bcs      9.42\u00b10.01\u03bcs     0.94  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float32'>)\r\n-     14.6\u00b10.08\u03bcs       13.1\u00b10.2\u03bcs     0.90  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint32'>)\r\n-     7.84\u00b10.09\u03bcs      6.98\u00b10.04\u03bcs     0.89  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint16'>)\r\n-     14.8\u00b10.07\u03bcs      13.0\u00b10.05\u03bcs     0.88  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float32'>)\r\n-     7.31\u00b10.03\u03bcs      6.32\u00b10.04\u03bcs     0.86  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int16'>)\r\n-      7.94\u00b10.1\u03bcs      6.85\u00b10.06\u03bcs     0.86  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint16'>)\r\n-     7.29\u00b10.02\u03bcs      6.28\u00b10.08\u03bcs     0.86  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int16'>)\r\n-     18.2\u00b10.08\u03bcs      15.1\u00b10.04\u03bcs     0.83  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float64'>)\r\n-     18.6\u00b10.07\u03bcs      15.2\u00b10.07\u03bcs     0.82  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float64'>)\r\n-      29.3\u00b10.2\u03bcs       23.8\u00b10.3\u03bcs     0.81  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float64'>)\r\n-     11.4\u00b10.04\u03bcs      9.26\u00b10.04\u03bcs     0.81  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int32'>)\r\n-      31.0\u00b10.4\u03bcs       24.7\u00b10.1\u03bcs     0.79  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int64'>)\r\n-      11.7\u00b10.2\u03bcs      9.25\u00b10.06\u03bcs     0.79  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int32'>)\r\n-      34.6\u00b10.8\u03bcs       27.1\u00b10.2\u03bcs     0.78  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint64'>)\r\n-      12.7\u00b10.2\u03bcs      9.83\u00b10.03\u03bcs     0.77  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint32'>)\r\n-      12.7\u00b10.2\u03bcs      9.67\u00b10.02\u03bcs     0.76  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint32'>)\r\n-     28.2\u00b10.01\u03bcs      17.8\u00b10.03\u03bcs     0.63  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int64'>)\r\n-      28.4\u00b10.1\u03bcs      17.7\u00b10.03\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint64'>)\r\n-      28.4\u00b10.1\u03bcs      17.7\u00b10.02\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint64'>)\r\n-      28.3\u00b10.1\u03bcs      17.6\u00b10.02\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int64'>)\r\n-     82.7\u00b10.03\u03bcs      6.03\u00b10.01\u03bcs     0.07  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.bool_'>)\r\n-     77.7\u00b10.02\u03bcs      5.42\u00b10.02\u03bcs     0.07  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.bool_'>)\r\n-      77.8\u00b10.2\u03bcs      5.37\u00b10.06\u03bcs     0.07  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.bool_'>)\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>AVX512F</summary>\r\n\r\n```\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX512_SKX\"\r\n        before          after\r\n+     18.5\u00b10.09\u03bcs       19.5\u00b10.2\u03bcs     1.06  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float64'>)\r\n-      7.19\u00b10.1\u03bcs       6.83\u00b10.2\u03bcs     0.95  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int16'>)\r\n-      5.02\u00b10.1\u03bcs       4.72\u00b10.2\u03bcs     0.94  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint8'>)\r\n-      5.04\u00b10.2\u03bcs       4.69\u00b10.2\u03bcs     0.93  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint8'>)\r\n-     4.99\u00b10.08\u03bcs       4.63\u00b10.1\u03bcs     0.93  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int8'>)\r\n-     4.69\u00b10.02\u03bcs      4.32\u00b10.06\u03bcs     0.92  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint8'>)\r\n-      7.39\u00b10.1\u03bcs       6.73\u00b10.2\u03bcs     0.91  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int16'>)\r\n-     7.85\u00b10.06\u03bcs       7.01\u00b10.3\u03bcs     0.89  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint16'>)\r\n-     8.26\u00b10.02\u03bcs       7.26\u00b10.1\u03bcs     0.88  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int16'>)\r\n-      11.6\u00b10.2\u03bcs       10.1\u00b10.1\u03bcs     0.87  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int32'>)\r\n-      7.92\u00b10.1\u03bcs       6.79\u00b10.3\u03bcs     0.86  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint16'>)\r\n-     12.1\u00b10.07\u03bcs       10.2\u00b10.1\u03bcs     0.84  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int32'>)\r\n-     8.36\u00b10.03\u03bcs       7.00\u00b10.2\u03bcs     0.84  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint16'>)\r\n-      12.5\u00b10.1\u03bcs       10.0\u00b10.2\u03bcs     0.80  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint32'>)\r\n-      13.1\u00b10.3\u03bcs       10.1\u00b10.1\u03bcs     0.77  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint32'>)\r\n-      30.4\u00b10.6\u03bcs       22.9\u00b10.9\u03bcs     0.75  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float64'>)\r\n-      14.2\u00b10.4\u03bcs      10.1\u00b10.04\u03bcs     0.71  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint32'>)\r\n-      14.0\u00b10.2\u03bcs      9.94\u00b10.09\u03bcs     0.71  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int32'>)\r\n-      31.0\u00b10.9\u03bcs       21.8\u00b10.6\u03bcs     0.70  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int64'>)\r\n-      28.4\u00b10.1\u03bcs       19.7\u00b10.3\u03bcs     0.70  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int64'>)\r\n-      28.4\u00b10.1\u03bcs       19.7\u00b10.1\u03bcs     0.69  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint64'>)\r\n-      28.4\u00b10.1\u03bcs       19.6\u00b10.1\u03bcs     0.69  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int64'>)\r\n-      28.6\u00b10.1\u03bcs       19.7\u00b10.2\u03bcs     0.69  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint64'>)\r\n-      14.8\u00b10.1\u03bcs      10.0\u00b10.06\u03bcs     0.68  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float32'>)\r\n-      34.3\u00b10.8\u03bcs       21.8\u00b10.4\u03bcs     0.63  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint64'>)\r\n-      82.5\u00b10.2\u03bcs      6.08\u00b10.09\u03bcs     0.07  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.bool_'>)\r\n-     77.7\u00b10.07\u03bcs       5.61\u00b10.2\u03bcs     0.07  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.bool_'>)\r\n-      77.9\u00b10.5\u03bcs       5.60\u00b10.2\u03bcs     0.07  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.bool_'>)\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>AVX512BW</summary>\r\n\r\n```\r\nunset NPY_DISABLE_CPU_FEATURES\r\n        before          after\r\n-     4.87\u00b10.04\u03bcs      4.54\u00b10.08\u03bcs     0.93  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int8'>)\r\n-     4.92\u00b10.02\u03bcs      4.56\u00b10.04\u03bcs     0.93  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint8'>)\r\n-     4.88\u00b10.01\u03bcs      4.52\u00b10.05\u03bcs     0.93  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int8'>)\r\n-     4.95\u00b10.04\u03bcs      4.54\u00b10.03\u03bcs     0.92  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint8'>)\r\n-     4.72\u00b10.01\u03bcs      4.21\u00b10.07\u03bcs     0.89  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int8'>)\r\n-     4.73\u00b10.02\u03bcs      4.21\u00b10.03\u03bcs     0.89  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint8'>)\r\n-     7.09\u00b10.07\u03bcs      5.82\u00b10.05\u03bcs     0.82  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int16'>)\r\n-     7.04\u00b10.06\u03bcs      5.62\u00b10.07\u03bcs     0.80  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int16'>)\r\n-     8.26\u00b10.03\u03bcs      6.24\u00b10.04\u03bcs     0.76  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int16'>)\r\n-     7.60\u00b10.06\u03bcs      5.67\u00b10.04\u03bcs     0.75  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint16'>)\r\n-     9.94\u00b10.03\u03bcs       7.38\u00b10.4\u03bcs     0.74  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float32'>)\r\n-     8.38\u00b10.05\u03bcs      6.22\u00b10.04\u03bcs     0.74  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint16'>)\r\n-     7.64\u00b10.07\u03bcs      5.62\u00b10.05\u03bcs     0.74  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint16'>)\r\n-     10.1\u00b10.05\u03bcs       7.32\u00b10.4\u03bcs     0.73  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float32'>)\r\n-     13.3\u00b10.04\u03bcs       8.81\u00b10.7\u03bcs     0.66  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int32'>)\r\n-     14.9\u00b10.08\u03bcs       9.44\u00b10.2\u03bcs     0.63  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float32'>)\r\n-      11.8\u00b10.3\u03bcs       7.36\u00b10.3\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int32'>)\r\n-      11.7\u00b10.2\u03bcs       7.27\u00b10.4\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int32'>)\r\n-      18.2\u00b10.3\u03bcs       11.3\u00b10.2\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float64'>)\r\n-      29.9\u00b10.5\u03bcs       18.4\u00b10.6\u03bcs     0.62  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float64'>)\r\n-      31.6\u00b10.6\u03bcs         19.3\u00b11\u03bcs     0.61  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int64'>)\r\n-     18.5\u00b10.02\u03bcs       11.1\u00b10.2\u03bcs     0.60  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float64'>)\r\n-      14.2\u00b10.3\u03bcs       8.43\u00b10.8\u03bcs     0.59  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint32'>)\r\n-     12.6\u00b10.08\u03bcs       7.41\u00b10.5\u03bcs     0.59  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint32'>)\r\n-      33.2\u00b10.2\u03bcs       19.5\u00b10.6\u03bcs     0.59  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint64'>)\r\n-     13.0\u00b10.06\u03bcs       7.33\u00b10.4\u03bcs     0.56  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint32'>)\r\n-      28.3\u00b10.1\u03bcs       11.1\u00b10.3\u03bcs     0.39  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint64'>)\r\n-     28.2\u00b10.02\u03bcs       11.1\u00b10.3\u03bcs     0.39  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int64'>)\r\n-      28.2\u00b10.1\u03bcs       11.0\u00b10.3\u03bcs     0.39  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int64'>)\r\n-     28.5\u00b10.05\u03bcs       11.1\u00b10.3\u03bcs     0.39  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint64'>)\r\n-     77.7\u00b10.04\u03bcs      4.52\u00b10.07\u03bcs     0.06  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.bool_'>)\r\n-     77.6\u00b10.01\u03bcs      4.47\u00b10.04\u03bcs     0.06  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.bool_'>)\r\n-      82.7\u00b10.1\u03bcs      4.24\u00b10.04\u03bcs     0.05  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.bool_'>)\r\n```\r\n</details>\r\n\r\n---\r\n\r\n### Power little-endian (Power9/VSX3)\r\n\r\n<details>\r\n  <summary>CPU</summary>\r\n  \r\n```\r\nMachine\r\nArchitecture:                    ppc64le\r\nByte Order:                      Little Endian\r\nCPU(s):                          160\r\nOn-line CPU(s) list:             0-159\r\nThread(s) per core:              4\r\nCore(s) per socket:              20\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nModel:                           2.2 (pvr 004e 1202)\r\nModel name:                      POWER9, altivec supported\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     3800.0000\r\nCPU min MHz:                     2166.0000\r\nL1d cache:                       1.3 MiB\r\nL1i cache:                       1.3 MiB\r\nL2 cache:                        10 MiB\r\nL3 cache:                        200 MiB\r\nNUMA node0 CPU(s):               0-79\r\nNUMA node8 CPU(s):               80-159\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>OS</summary>\r\n\r\n```\r\ngcc (GCC) 11.2.1 20210921\r\nUbuntu 20.04.3 LTS\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n  <summary>VSX3</summary>\r\n\r\n```\r\n        before          after\r\n-      17.2\u00b10.4\u03bcs       14.9\u00b10.5\u03bcs     0.86  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int32'>)\r\n-      11.9\u00b10.2\u03bcs       9.93\u00b10.3\u03bcs     0.83  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int16'>)\r\n-      18.2\u00b10.5\u03bcs       14.6\u00b10.3\u03bcs     0.80  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint32'>)\r\n-      12.6\u00b10.4\u03bcs       9.72\u00b10.3\u03bcs     0.77  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint16'>)\r\n-      20.5\u00b10.3\u03bcs       15.7\u00b10.4\u03bcs     0.76  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int32'>)\r\n-      18.6\u00b10.2\u03bcs       14.0\u00b10.1\u03bcs     0.75  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int32'>)\r\n-     20.7\u00b10.05\u03bcs       15.2\u00b10.3\u03bcs     0.73  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint32'>)\r\n-      30.0\u00b10.2\u03bcs       21.7\u00b10.2\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int64'>)\r\n-      30.4\u00b10.4\u03bcs       22.0\u00b10.4\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int64'>)\r\n-      12.8\u00b10.1\u03bcs       9.29\u00b10.4\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int16'>)\r\n-      30.2\u00b10.2\u03bcs       21.8\u00b10.2\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint64'>)\r\n-      35.0\u00b10.3\u03bcs       25.0\u00b10.6\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.int64'>)\r\n-      30.4\u00b10.5\u03bcs       21.8\u00b10.2\u03bcs     0.72  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.uint64'>)\r\n-      10.3\u00b10.3\u03bcs       6.96\u00b10.2\u03bcs     0.68  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.int8'>)\r\n-        15.9\u00b12\u03bcs       10.1\u00b10.3\u03bcs     0.63  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.int16'>)\r\n-        43.2\u00b18\u03bcs      24.6\u00b10.08\u03bcs     0.57  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.uint64'>)\r\n-        28.2\u00b16\u03bcs       14.3\u00b10.7\u03bcs     0.51  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.uint32'>)\r\n-      56.5\u00b10.8\u03bcs       24.6\u00b10.3\u03bcs     0.44  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float64'>)\r\n-      57.0\u00b10.3\u03bcs       22.1\u00b10.5\u03bcs     0.39  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float64'>)\r\n-      57.3\u00b10.7\u03bcs       21.7\u00b10.2\u03bcs     0.38  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float64'>)\r\n-      70.8\u00b10.5\u03bcs       15.3\u00b10.2\u03bcs     0.22  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.float32'>)\r\n-      68.7\u00b10.5\u03bcs       14.1\u00b10.4\u03bcs     0.21  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.float32'>)\r\n-        112\u00b130\u03bcs       14.1\u00b10.5\u03bcs     0.13  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.float32'>)\r\n-        72.5\u00b19\u03bcs       8.25\u00b10.2\u03bcs     0.11  bench_ufunc.CustomComparison.time_less_than_binary(<class 'numpy.bool_'>)\r\n-      74.2\u00b10.2\u03bcs       8.04\u00b10.1\u03bcs     0.11  bench_ufunc.CustomComparison.time_less_than_scalar2(<class 'numpy.bool_'>)\r\n-        75.2\u00b11\u03bcs      7.78\u00b10.07\u03bcs     0.10  bench_ufunc.CustomComparison.time_less_than_scalar1(<class 'numpy.bool_'>)\r\n\r\n```\r\n</details>\r\n\r\n---\r\n\r\n### AArch64\r\n\r\nAs I do not have access to an ARM processor, I didn't execute the benchmark to check performance. But I was able to test the NEON code I added in this PR on a small ARM processor.\r\n\r\ncc: @mattip and @seiko2plus ",
        "changed_files": [
            {
                "filename": ".gitignore",
                "patch": "@@ -225,6 +225,7 @@ numpy/core/src/umath/loops_exponent_log.dispatch.c\n numpy/core/src/umath/loops_umath_fp.dispatch.c\n numpy/core/src/umath/loops_hyperbolic.dispatch.c\n numpy/core/src/umath/loops_modulo.dispatch.c\n+numpy/core/src/umath/loops_comparison.dispatch.c\n # npysort module\n numpy/core/src/npysort/x86-qsort.dispatch.c\n numpy/core/src/npysort/x86-qsort.dispatch.*.cpp"
            },
            {
                "filename": "benchmarks/benchmarks/bench_ufunc.py",
                "patch": "@@ -170,8 +170,25 @@ def time_divide_scalar2(self, dtype):\n     def time_divide_scalar2_inplace(self, dtype):\n         np.divide(self.d, 1, out=self.d)\n \n+\n+class CustomComparison(Benchmark):\n+    params = (np.int8,  np.int16,  np.int32,  np.int64, np.uint8, np.uint16,\n+              np.uint32, np.uint64, np.float32, np.float64, np.bool_)\n+    param_names = ['dtype']\n+\n+    def setup(self, dtype):\n+        self.x = np.ones(50000, dtype=dtype)\n+        self.y = np.ones(50000, dtype=dtype)\n+        self.s = np.ones(1, dtype=dtype)\n+\n+    def time_less_than_binary(self, dtype):\n+        (self.x < self.y)\n+\n+    def time_less_than_scalar1(self, dtype):\n+        (self.s < self.x)\n+\n     def time_less_than_scalar2(self, dtype):\n-        (self.d < 1)\n+        (self.x < self.s)\n \n \n class CustomScalarFloorDivideInt(Benchmark):"
            },
            {
                "filename": "doc/release/upcoming_changes/21483.performance.rst",
                "patch": "@@ -0,0 +1,7 @@\n+Faster comparison operators\n+----------------------------\n+The comparison functions (``numpy.equal``, ``numpy.not_equal``, ``numpy.less``,\n+``numpy.less_equal``, ``numpy.greater`` and ``numpy.greater_equal``) are now\n+much faster as they are now vectorized with universal intrinsics. For a CPU\n+with SIMD extension AVX512BW, the performance gain is up to 2.57x, 1.65x and\n+19.15x for integer, float and boolean data types, respectively (with N=50000)."
            },
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -445,47 +445,47 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.greater'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'greater_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.greater_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'less':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.less'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'less_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.less_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'not_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.not_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -1070,6 +1070,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops_exponent_log.dispatch.c.src'),\n             join('src', 'umath', 'loops_hyperbolic.dispatch.c.src'),\n             join('src', 'umath', 'loops_modulo.dispatch.c.src'),\n+            join('src', 'umath', 'loops_comparison.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),\n             join('src', 'umath', 'matmul.c.src'),\n             join('src', 'umath', 'clip.h'),"
            },
            {
                "filename": "numpy/core/src/_simd/_simd.dispatch.c.src",
                "patch": "@@ -31,6 +31,7 @@\n  * #intdiv_sup= 1,  1,  1,   1,   1,   1,   1,   1,   0,   0#\n  * #shl_imm   = 0,  0,  15,  15,  31,  31,  63,  63,  0,   0#\n  * #shr_imm   = 0,  0,  16,  16,  32,  32,  64,  64,  0,   0#\n+ * #bitw8b_sup= 1,  0,  0,   0,   0,   0,   0,   0,   0,   0#\n  */\n #if @simd_sup@\n /***************************\n@@ -332,6 +333,13 @@ SIMD_IMPL_INTRIN_1(not_@sfx@, v@sfx@, v@sfx@)\n SIMD_IMPL_INTRIN_2(@intrin@_@sfx@, v@bsfx@, v@sfx@, v@sfx@)\n /**end repeat1**/\n \n+#if @bitw8b_sup@\n+SIMD_IMPL_INTRIN_2(andc_@sfx@, v@sfx@, v@sfx@, v@sfx@)\n+SIMD_IMPL_INTRIN_2(andc_@bsfx@, v@bsfx@, v@bsfx@, v@bsfx@)\n+SIMD_IMPL_INTRIN_2(orc_@bsfx@, v@bsfx@, v@bsfx@, v@bsfx@)\n+SIMD_IMPL_INTRIN_2(xnor_@bsfx@, v@bsfx@, v@bsfx@, v@bsfx@)\n+#endif\n+\n /***************************\n  * Conversion\n  ***************************/\n@@ -472,6 +480,10 @@ SIMD_IMPL_INTRIN_1(not_@bsfx@, v@bsfx@, v@bsfx@)\n SIMD_IMPL_INTRIN_1(tobits_@bsfx@, u64, v@bsfx@)\n /**end repeat**/\n \n+SIMD_IMPL_INTRIN_2(pack_b8_b16, vb8, vb16, vb16)\n+SIMD_IMPL_INTRIN_4(pack_b8_b32, vb8, vb32, vb32, vb32, vb32)\n+SIMD_IMPL_INTRIN_8(pack_b8_b64, vb8, vb64, vb64, vb64, vb64,\n+                                     vb64, vb64, vb64, vb64)\n \n //#########################################################################\n //## Attach module functions\n@@ -496,6 +508,7 @@ static PyMethodDef simd__intrinsics_methods[] = {\n  * #intdiv_sup= 1,  1,  1,   1,   1,   1,   1,   1,   0,   0#\n  * #shl_imm   = 0,  0,  15,  15,  31,  31,  63,  63,  0,   0#\n  * #shr_imm   = 0,  0,  16,  16,  32,  32,  64,  64,  0,   0#\n+ * #bitw8b_sup= 1,  0,  0,   0,   0,   0,   0,   0,   0,   0#\n  */\n #if @simd_sup@\n \n@@ -577,6 +590,13 @@ SIMD_INTRIN_DEF(@intrin@_@sfx@)\n SIMD_INTRIN_DEF(@intrin@_@sfx@)\n /**end repeat1**/\n \n+#if @bitw8b_sup@\n+SIMD_INTRIN_DEF(andc_@sfx@)\n+SIMD_INTRIN_DEF(andc_@bsfx@)\n+SIMD_INTRIN_DEF(orc_@bsfx@)\n+SIMD_INTRIN_DEF(xnor_@bsfx@)\n+#endif\n+\n /***************************\n  * Conversion\n  ***************************/\n@@ -716,6 +736,11 @@ SIMD_INTRIN_DEF(not_@bsfx@)\n SIMD_INTRIN_DEF(tobits_@bsfx@)\n /**end repeat**/\n \n+// Pack multiple vectors into one\n+SIMD_INTRIN_DEF(pack_b8_b16)\n+SIMD_INTRIN_DEF(pack_b8_b32)\n+SIMD_INTRIN_DEF(pack_b8_b64)\n+\n /************************************************************************/\n {NULL, NULL, 0, NULL}\n }; // PyMethodDef"
            },
            {
                "filename": "numpy/core/src/_simd/_simd_easyintrin.inc",
                "patch": "@@ -153,6 +153,50 @@\n         return simd_arg_to_obj(&ret);                     \\\n     }\n \n+#define SIMD_IMPL_INTRIN_8(NAME, RET, IN0, IN1, IN2, IN3, \\\n+                                      IN4, IN5, IN6, IN7) \\\n+    static PyObject *simd__intrin_##NAME                  \\\n+    (PyObject* NPY_UNUSED(self), PyObject *args)          \\\n+    {                                                     \\\n+        simd_arg arg1 = {.dtype = simd_data_##IN0};       \\\n+        simd_arg arg2 = {.dtype = simd_data_##IN1};       \\\n+        simd_arg arg3 = {.dtype = simd_data_##IN2};       \\\n+        simd_arg arg4 = {.dtype = simd_data_##IN3};       \\\n+        simd_arg arg5 = {.dtype = simd_data_##IN4};       \\\n+        simd_arg arg6 = {.dtype = simd_data_##IN5};       \\\n+        simd_arg arg7 = {.dtype = simd_data_##IN6};       \\\n+        simd_arg arg8 = {.dtype = simd_data_##IN7};       \\\n+        if (!PyArg_ParseTuple(                            \\\n+            args, \"O&O&O&O&O&O&O&O&:\"NPY_TOSTRING(NAME),  \\\n+            simd_arg_converter, &arg1,                    \\\n+            simd_arg_converter, &arg2,                    \\\n+            simd_arg_converter, &arg3,                    \\\n+            simd_arg_converter, &arg4,                    \\\n+            simd_arg_converter, &arg5,                    \\\n+            simd_arg_converter, &arg6,                    \\\n+            simd_arg_converter, &arg7,                    \\\n+            simd_arg_converter, &arg8                     \\\n+        )) return NULL;                                   \\\n+        simd_data data = {.RET = npyv_##NAME(             \\\n+            arg1.data.IN0, arg2.data.IN1,                 \\\n+            arg3.data.IN2, arg4.data.IN3,                 \\\n+            arg5.data.IN4, arg6.data.IN5,                 \\\n+            arg7.data.IN6, arg8.data.IN7                  \\\n+        )};                                               \\\n+        simd_arg_free(&arg1);                             \\\n+        simd_arg_free(&arg2);                             \\\n+        simd_arg_free(&arg3);                             \\\n+        simd_arg_free(&arg4);                             \\\n+        simd_arg_free(&arg5);                             \\\n+        simd_arg_free(&arg6);                             \\\n+        simd_arg_free(&arg7);                             \\\n+        simd_arg_free(&arg8);                             \\\n+        simd_arg ret = {                                  \\\n+            .data = data, .dtype = simd_data_##RET        \\\n+        };                                                \\\n+        return simd_arg_to_obj(&ret);                     \\\n+    }\n+\n /**\n  * Helper macros for repeating and expand a certain macro.\n  * Mainly used for converting a scalar to an immediate constant."
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/conversion.h",
                "patch": "@@ -58,6 +58,36 @@ NPY_FINLINE npyv_u32x2 npyv_expand_u32_u16(npyv_u16 data) {\n     return r;\n }\n \n+// pack two 16-bit boolean into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b) {\n+    __m256i ab = _mm256_packs_epi16(a, b);\n+    return npyv256_shuffle_odd(ab);\n+}\n+\n+// pack four 32-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32 c, npyv_b32 d) {\n+    __m256i ab = _mm256_packs_epi32(a, b);\n+    __m256i cd = _mm256_packs_epi32(c, d);\n+    __m256i abcd = npyv_pack_b8_b16(ab, cd);\n+    return _mm256_shuffle_epi32(abcd, _MM_SHUFFLE(3, 1, 2, 0));\n+}\n+\n+// pack eight 64-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n+                 npyv_b64 e, npyv_b64 f, npyv_b64 g, npyv_b64 h) {\n+    __m256i ab = _mm256_packs_epi32(a, b);\n+    __m256i cd = _mm256_packs_epi32(c, d);\n+    __m256i ef = _mm256_packs_epi32(e, f);\n+    __m256i gh = _mm256_packs_epi32(g, h);\n+    __m256i abcd = _mm256_packs_epi32(ab, cd);\n+    __m256i efgh = _mm256_packs_epi32(ef, gh);\n+    __m256i all  = npyv256_shuffle_odd(_mm256_packs_epi16(abcd, efgh));\n+    __m256i rev128 = _mm256_alignr_epi8(all, all, 8);\n+    return _mm256_unpacklo_epi16(all, rev128);\n+}\n+\n // round to nearest integer (assuming even)\n #define npyv_round_s32_f32 _mm256_cvtps_epi32\n NPY_FINLINE npyv_s32 npyv_round_s32_f64(npyv_f64 a, npyv_f64 b)"
            },
            {
                "filename": "numpy/core/src/common/simd/avx2/operators.h",
                "patch": "@@ -114,6 +114,12 @@ NPY_FINLINE __m256i npyv_shr_s64(__m256i a, int c)\n #define npyv_not_b32 npyv_not_u8\n #define npyv_not_b64 npyv_not_u8\n \n+// ANDC, ORC and XNOR\n+#define npyv_andc_u8(A, B) _mm256_andnot_si256(B, A)\n+#define npyv_andc_b8(A, B) _mm256_andnot_si256(B, A)\n+#define npyv_orc_b8(A, B) npyv_or_b8(npyv_not_b8(B), A)\n+#define npyv_xnor_b8 _mm256_cmpeq_epi8\n+\n /***************************\n  * Comparison\n  ***************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/conversion.h",
                "patch": "@@ -90,6 +90,48 @@ NPY_FINLINE npyv_u32x2 npyv_expand_u32_u16(npyv_u16 data)\n     return r;\n }\n \n+// pack two 16-bit boolean into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b) {\n+#ifdef NPY_HAVE_AVX512BW\n+    return _mm512_kunpackd((__mmask64)b, (__mmask64)a);\n+#else\n+    const __m512i idx = _mm512_setr_epi64(0, 2, 4, 6, 1, 3, 5, 7);\n+    return _mm512_permutexvar_epi64(idx, npyv512_packs_epi16(a, b));\n+#endif\n+}\n+\n+// pack four 32-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32 c, npyv_b32 d) {\n+#ifdef NPY_HAVE_AVX512BW\n+    __mmask32 ab = _mm512_kunpackw((__mmask32)b, (__mmask32)a);\n+    __mmask32 cd = _mm512_kunpackw((__mmask32)d, (__mmask32)c);\n+    return npyv_pack_b8_b16(ab, cd);\n+#else\n+    const __m512i idx = _mm512_setr_epi32(\n+        0, 4, 1, 5, 2, 6, 3, 7, 8, 12, 9, 13, 10, 14, 11, 15);\n+    __m256i ta = npyv512_pack_lo_hi(npyv_cvt_u32_b32(a));\n+    __m256i tb = npyv512_pack_lo_hi(npyv_cvt_u32_b32(b));\n+    __m256i tc = npyv512_pack_lo_hi(npyv_cvt_u32_b32(c));\n+    __m256i td = npyv512_pack_lo_hi(npyv_cvt_u32_b32(d));\n+    __m256i ab = _mm256_packs_epi16(ta, tb);\n+    __m256i cd = _mm256_packs_epi16(tc, td);\n+    __m512i abcd = npyv512_combine_si256(ab, cd);\n+    return _mm512_permutexvar_epi32(idx, abcd);\n+#endif\n+}\n+\n+// pack eight 64-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n+                 npyv_b64 e, npyv_b64 f, npyv_b64 g, npyv_b64 h) {\n+    __mmask16 ab = _mm512_kunpackb((__mmask16)b, (__mmask16)a);\n+    __mmask16 cd = _mm512_kunpackb((__mmask16)d, (__mmask16)c);\n+    __mmask16 ef = _mm512_kunpackb((__mmask16)f, (__mmask16)e);\n+    __mmask16 gh = _mm512_kunpackb((__mmask16)h, (__mmask16)g);\n+    return npyv_pack_b8_b32(ab, cd, ef, gh);\n+}\n+\n // convert boolean vectors to integer bitfield\n NPY_FINLINE npy_uint64 npyv_tobits_b8(npyv_b8 a)\n {"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/operators.h",
                "patch": "@@ -140,6 +140,9 @@\n     #define npyv_not_f64(A) _mm512_castsi512_pd(npyv_not_u64(_mm512_castpd_si512(A)))\n #endif\n \n+// ANDC\n+#define npyv_andc_u8(A, B) _mm512_andnot_si512(B, A)\n+\n /***************************\n  * Logical (boolean)\n  ***************************/\n@@ -152,6 +155,9 @@\n     #define npyv_xor_b16 _kxor_mask32\n     #define npyv_not_b8  _knot_mask64\n     #define npyv_not_b16 _knot_mask32\n+    #define npyv_andc_b8(A, B) _kandn_mask64(B, A)\n+    #define npyv_orc_b8(A, B) npyv_or_b8(npyv_not_b8(B), A)\n+    #define npyv_xnor_b8 _kxnor_mask64\n #elif defined(NPY_HAVE_AVX512BW)\n     NPY_FINLINE npyv_b8  npyv_and_b8(npyv_b8 a, npyv_b8 b)\n     { return a & b; }\n@@ -169,6 +175,12 @@\n     { return ~a; }\n     NPY_FINLINE npyv_b16 npyv_not_b16(npyv_b16 a)\n     { return ~a; }\n+    NPY_FINLINE npyv_b8  npyv_andc_b8(npyv_b8 a, npyv_b8 b)\n+    { return a & (~b); }\n+    NPY_FINLINE npyv_b8  npyv_orc_b8(npyv_b8 a, npyv_b8 b)\n+    { return a | (~b); }\n+    NPY_FINLINE npyv_b8  npyv_xnor_b8(npyv_b8 a, npyv_b8 b)\n+    { return ~(a ^ b); }\n #else\n     #define npyv_and_b8  _mm512_and_si512\n     #define npyv_and_b16 _mm512_and_si512\n@@ -178,6 +190,9 @@\n     #define npyv_xor_b16 _mm512_xor_si512\n     #define npyv_not_b8  npyv_not_u8\n     #define npyv_not_b16 npyv_not_u8\n+    #define npyv_andc_b8(A, B) _mm512_andnot_si512(B, A)\n+    #define npyv_orc_b8(A, B) npyv_or_b8(npyv_not_b8(B), A)\n+    #define npyv_xnor_b8(A, B) npyv_not_b8(npyv_xor_b8(A, B))\n #endif\n \n #define npyv_and_b32 _mm512_kand"
            },
            {
                "filename": "numpy/core/src/common/simd/avx512/utils.h",
                "patch": "@@ -87,4 +87,16 @@\n         ));                                                  \\\n     }\n \n+#ifndef NPY_HAVE_AVX512BW\n+    NPYV_IMPL_AVX512_FROM_AVX2_2ARG(npyv512_packs_epi16,  _mm256_packs_epi16)\n+#else\n+    #define npyv512_packs_epi16 _mm512_packs_epi16\n+#endif\n+\n+NPY_FINLINE __m256i npyv512_pack_lo_hi(__m512i a) {\n+    __m256i lo = npyv512_lower_si256(a);\n+    __m256i hi = npyv512_higher_si256(a);\n+    return _mm256_packs_epi32(lo, hi);\n+}\n+\n #endif // _NPY_SIMD_AVX512_UTILS_H"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/conversion.h",
                "patch": "@@ -86,6 +86,30 @@ NPY_FINLINE npyv_u32x2 npyv_expand_u32_u16(npyv_u16 data) {\n     return r;\n }\n \n+// pack two 16-bit boolean into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b) {\n+    return vcombine_u8(vmovn_u16(a), vmovn_u16(b));\n+}\n+\n+// pack four 32-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32 c, npyv_b32 d) {\n+    npyv_b16 ab = vcombine_u16(vmovn_u32(a), vmovn_u32(b));\n+    npyv_b16 cd = vcombine_u16(vmovn_u32(c), vmovn_u32(d));\n+    return npyv_pack_b8_b16(ab, cd);\n+}\n+\n+// pack eight 64-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n+                 npyv_b64 e, npyv_b64 f, npyv_b64 g, npyv_b64 h) {\n+    npyv_b32 ab = vcombine_u32(vmovn_u64(a), vmovn_u64(b));\n+    npyv_b32 cd = vcombine_u32(vmovn_u64(c), vmovn_u64(d));\n+    npyv_b32 ef = vcombine_u32(vmovn_u64(e), vmovn_u64(f));\n+    npyv_b32 gh = vcombine_u32(vmovn_u64(g), vmovn_u64(h));\n+    return npyv_pack_b8_b32(ab, cd, ef, gh);\n+}\n+\n // round to nearest integer\n #if NPY_SIMD_F64\n     #define npyv_round_s32_f32 vcvtnq_s32_f32"
            },
            {
                "filename": "numpy/core/src/common/simd/neon/operators.h",
                "patch": "@@ -116,6 +116,12 @@\n #define npyv_not_b32  vmvnq_u32\n #define npyv_not_b64  npyv_not_u64\n \n+// ANDC, ORC and XNOR\n+#define npyv_andc_u8 vbicq_u8\n+#define npyv_andc_b8 vbicq_u8\n+#define npyv_orc_b8 vornq_u8\n+#define npyv_xnor_b8 vceqq_u8\n+\n /***************************\n  * Comparison\n  ***************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/conversion.h",
                "patch": "@@ -59,6 +59,30 @@ NPY_FINLINE npyv_u32x2 npyv_expand_u32_u16(npyv_u16 data) {\n     return r;\n }\n \n+// pack two 16-bit boolean into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b) {\n+    return _mm_packs_epi16(a, b);\n+}\n+\n+// pack four 32-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32 c, npyv_b32 d) {\n+    npyv_b16 ab = _mm_packs_epi32(a, b);\n+    npyv_b16 cd = _mm_packs_epi32(c, d);\n+    return npyv_pack_b8_b16(ab, cd);\n+}\n+\n+// pack eight 64-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n+                 npyv_b64 e, npyv_b64 f, npyv_b64 g, npyv_b64 h) {\n+    npyv_b32 ab = _mm_packs_epi32(a, b);\n+    npyv_b32 cd = _mm_packs_epi32(c, d);\n+    npyv_b32 ef = _mm_packs_epi32(e, f);\n+    npyv_b32 gh = _mm_packs_epi32(g, h);\n+    return npyv_pack_b8_b32(ab, cd, ef, gh);\n+}\n+\n // round to nearest integer (assuming even)\n #define npyv_round_s32_f32 _mm_cvtps_epi32\n NPY_FINLINE npyv_s32 npyv_round_s32_f64(npyv_f64 a, npyv_f64 b)"
            },
            {
                "filename": "numpy/core/src/common/simd/sse/operators.h",
                "patch": "@@ -115,6 +115,12 @@ NPY_FINLINE __m128i npyv_shr_s64(__m128i a, int c)\n #define npyv_not_b32 npyv_not_u8\n #define npyv_not_b64 npyv_not_u8\n \n+// ANDC, ORC and XNOR\n+#define npyv_andc_u8(A, B) _mm_andnot_si128(B, A)\n+#define npyv_andc_b8(A, B) _mm_andnot_si128(B, A)\n+#define npyv_orc_b8(A, B) npyv_or_b8(npyv_not_b8(B), A)\n+#define npyv_xnor_b8 _mm_cmpeq_epi8\n+\n /***************************\n  * Comparison\n  ***************************/"
            },
            {
                "filename": "numpy/core/src/common/simd/vsx/conversion.h",
                "patch": "@@ -48,6 +48,29 @@ NPY_FINLINE npyv_u32x2 npyv_expand_u32_u16(npyv_u16 data)\n     return r;\n }\n \n+// pack two 16-bit boolean into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b16(npyv_b16 a, npyv_b16 b) {\n+    return vec_pack(a, b);\n+}\n+\n+// pack four 32-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8 npyv_pack_b8_b32(npyv_b32 a, npyv_b32 b, npyv_b32 c, npyv_b32 d) {\n+    npyv_b16 ab = vec_pack(a, b);\n+    npyv_b16 cd = vec_pack(c, d);\n+    return npyv_pack_b8_b16(ab, cd);\n+}\n+\n+// pack eight 64-bit boolean vectors into one 8-bit boolean vector\n+NPY_FINLINE npyv_b8\n+npyv_pack_b8_b64(npyv_b64 a, npyv_b64 b, npyv_b64 c, npyv_b64 d,\n+                 npyv_b64 e, npyv_b64 f, npyv_b64 g, npyv_b64 h) {\n+    npyv_b32 ab = vec_pack(a, b);\n+    npyv_b32 cd = vec_pack(c, d);\n+    npyv_b32 ef = vec_pack(e, f);\n+    npyv_b32 gh = vec_pack(g, h);\n+    return npyv_pack_b8_b32(ab, cd, ef, gh);\n+}\n+\n // convert boolean vector to integer bitfield\n NPY_FINLINE npy_uint64 npyv_tobits_b8(npyv_b8 a)\n {"
            },
            {
                "filename": "numpy/core/src/common/simd/vsx/operators.h",
                "patch": "@@ -133,6 +133,12 @@ NPY_FINLINE npyv_f32 npyv_not_f32(npyv_f32 a)\n NPY_FINLINE npyv_f64 npyv_not_f64(npyv_f64 a)\n { return vec_nor(a, a); }\n \n+// ANDC, ORC and XNOR\n+#define npyv_andc_u8 vec_andc\n+#define npyv_andc_b8 vec_andc\n+#define npyv_orc_b8 vec_orc\n+#define npyv_xnor_b8 vec_eqv\n+\n /***************************\n  * Comparison\n  ***************************/"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -400,23 +400,6 @@ PyUFunc_On_Om(char **args, npy_intp const *dimensions, npy_intp const *steps, vo\n  *****************************************************************************\n  */\n \n-/**begin repeat\n- * #kind = equal, not_equal, greater, greater_equal, less, less_equal#\n- * #OP =  ==, !=, >, >=, <, <=#\n- **/\n-\n-NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n-{\n-    BINARY_LOOP {\n-        npy_bool in1 = *((npy_bool *)ip1) != 0;\n-        npy_bool in2 = *((npy_bool *)ip2) != 0;\n-        *((npy_bool *)op1)= in1 @OP@ in2;\n-    }\n-}\n-/**end repeat**/\n-\n-\n /**begin repeat\n  * #kind = logical_and, logical_or#\n  * #OP =  &&, ||#\n@@ -688,9 +671,8 @@ void\n \n \n /**begin repeat2\n- * #kind = equal, not_equal, greater, greater_equal, less, less_equal,\n- *         logical_and, logical_or#\n- * #OP =  ==, !=, >, >=, <, <=, &&, ||#\n+ * #kind = logical_and, logical_or#\n+ * #OP = &&, ||#\n  */\n \n #if @CHK@\n@@ -1408,19 +1390,16 @@ TIMEDELTA_mm_qm_divmod(char **args, npy_intp const *dimensions, npy_intp const *\n  *  #C = F, , L#\n  */\n /**begin repeat1\n- * #kind = equal, not_equal, less, less_equal, greater, greater_equal,\n- *        logical_and, logical_or#\n- * #OP = ==, !=, <, <=, >, >=, &&, ||#\n+ * #kind = logical_and, logical_or#\n+ * #OP = &&, ||#\n  */\n NPY_NO_EXPORT void\n @TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    if (!run_binary_simd_@kind@_@TYPE@(args, dimensions, steps)) {\n-        BINARY_LOOP {\n-            const @type@ in1 = *(@type@ *)ip1;\n-            const @type@ in2 = *(@type@ *)ip2;\n-            *((npy_bool *)op1) = in1 @OP@ in2;\n-        }\n+    BINARY_LOOP {\n+        const @type@ in1 = *(@type@ *)ip1;\n+        const @type@ in2 = *(@type@ *)ip2;\n+        *((npy_bool *)op1) = in1 @OP@ in2;\n     }\n     npy_clear_floatstatus_barrier((char*)dimensions);\n }\n@@ -1654,6 +1633,22 @@ LONGDOUBLE_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps\n }\n /**end repeat**/\n \n+/**begin repeat\n+ * #kind = equal, not_equal, less, less_equal, greater, greater_equal#\n+ * #OP = ==, !=, <, <=, >, >=#\n+ */\n+NPY_NO_EXPORT void\n+LONGDOUBLE_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    BINARY_LOOP {\n+        const npy_longdouble in1 = *(npy_longdouble *)ip1;\n+        const npy_longdouble in2 = *(npy_longdouble *)ip2;\n+        *((npy_bool *)op1) = in1 @OP@ in2;\n+    }\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+}\n+/**end repeat**/\n+\n NPY_NO_EXPORT void\n LONGDOUBLE_reciprocal(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data))\n {"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -28,9 +28,19 @@\n  *****************************************************************************\n  */\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_comparison.dispatch.h\"\n+#endif\n+\n+/**begin repeat\n+ * #kind = equal, not_equal, greater, greater_equal, less, less_equal#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void BOOL_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat**/\n+\n /**begin repeat\n- * #kind = equal, not_equal, greater, greater_equal, less, less_equal,\n- *         logical_and, logical_or, absolute, logical_not#\n+ * #kind = logical_and, logical_or, absolute, logical_not#\n  **/\n NPY_NO_EXPORT void\n BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n@@ -60,8 +70,8 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n  * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n            BYTE,  SHORT,  INT,  LONG,  LONGLONG#\n  */\n- NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_divide,\n-     (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_divide,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n /**end repeat**/\n \n #ifndef NPY_DISABLE_OPTIMIZATION\n@@ -72,14 +82,28 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n  * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n            BYTE,  SHORT,  INT,  LONG,  LONGLONG#\n  */\n- NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_divmod,\n-     (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**begin repeat1\n+ * #kind = divmod, fmod, remainder#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat1**/\n+/**end repeat**/\n \n- NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_fmod,\n-     (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_comparison.dispatch.h\"\n+#endif\n \n- NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_remainder,\n-     (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**begin repeat\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n+           BYTE,  SHORT,  INT,  LONG,  LONGLONG#\n+ */\n+/**begin repeat1\n+ * #kind = equal, not_equal, greater, greater_equal, less, less_equal#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@,\n+    (char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)))\n+/**end repeat1**/\n /**end repeat**/\n \n /**begin repeat\n@@ -136,9 +160,8 @@ NPY_NO_EXPORT void\n /**end repeat3**/\n \n /**begin repeat3\n- * #kind = equal, not_equal, greater, greater_equal, less, less_equal,\n- *         logical_and, logical_or#\n- * #OP =  ==, !=, >, >=, <, <=, &&, ||#\n+ * #kind = logical_and, logical_or#\n+ * #OP = &&, ||#\n  */\n NPY_NO_EXPORT void\n @S@@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n@@ -232,9 +255,6 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@func@,\n /**end repeat1**/\n /**end repeat**/\n \n-/**end repeat1**/\n-/**end repeat**/\n-\n // SVML\n #ifndef NPY_DISABLE_OPTIMIZATION\n     #include \"loops_umath_fp.dispatch.h\"\n@@ -300,14 +320,28 @@ NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@, (\n /**end repeat1**/\n /**end repeat**/\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"loops_comparison.dispatch.h\"\n+#endif\n+/**begin repeat\n+ *  #TYPE = FLOAT, DOUBLE#\n+ */\n+/**begin repeat1\n+ * #kind = equal, not_equal, less, less_equal, greater, greater_equal#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT void @TYPE@_@kind@, (\n+  char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func)\n+))\n+/**end repeat1**/\n+/**end repeat**/\n+\n /**begin repeat\n  * Float types\n  *  #TYPE = HALF, FLOAT, DOUBLE, LONGDOUBLE#\n  *  #c = f, f, , l#\n  *  #C = F, F, , L#\n  */\n \n-\n /**begin repeat1\n  * Arithmetic\n  * # kind = add, subtract, multiply, divide#\n@@ -318,9 +352,8 @@ NPY_NO_EXPORT void\n /**end repeat1**/\n \n /**begin repeat1\n- * #kind = equal, not_equal, less, less_equal, greater, greater_equal,\n- *        logical_and, logical_or#\n- * #OP = ==, !=, <, <=, >, >=, &&, ||#\n+ * #kind = logical_and, logical_or#\n+ * #OP = &&, ||#\n  */\n NPY_NO_EXPORT void\n @TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n@@ -407,6 +440,16 @@ NPY_NO_EXPORT void\n @TYPE@_ldexp_long(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat**/\n \n+/**begin repeat\n+ * #TYPE = HALF, LONGDOUBLE#\n+ */\n+/**begin repeat1\n+ * #kind = equal, not_equal, less, less_equal, greater, greater_equal#\n+ */\n+NPY_NO_EXPORT void\n+@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+/**end repeat1**/\n+/**end repeat**/\n \n /*\n  *****************************************************************************"
            },
            {
                "filename": "numpy/core/src/umath/loops_comparison.dispatch.c.src",
                "patch": "@@ -0,0 +1,451 @@\n+/*@targets\n+ ** $maxopt baseline\n+ ** sse2 sse42 avx2 avx512f avx512_skx\n+ ** vsx2 vsx3\n+ ** neon\n+ **/\n+#define _UMATHMODULE\n+#define _MULTIARRAYMODULE\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"simd/simd.h\"\n+#include \"loops_utils.h\"\n+#include \"loops.h\"\n+#include \"lowlevel_strided_loops.h\"\n+// Provides the various *_LOOP macros\n+#include \"fast_loop_macros.h\"\n+\n+/********************************************************************************\n+ ** Defining the SIMD kernels\n+ ********************************************************************************/\n+/**begin repeat\n+ * #sfx    = u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n+ * #len    =  8,  8,  16,  16,  32,  32,  64,  64,  32,  64#\n+ * #signed =  0,  1,   0,   1,   0,   1,   0,   1,   0,   0#\n+ * #VECTOR = NPY_SIMD*9, NPY_SIMD_F64#\n+ */\n+/**begin repeat1\n+ * #kind = equal, not_equal, less, less_equal#\n+ * #eq = 1, 0, 0, 0#\n+ * #neq = 0, 1, 0, 0#\n+ * #OP = ==, !=, <, <=#\n+ * #VOP = cmpeq, cmpneq, cmplt, cmple#\n+ */\n+#if @VECTOR@ && !((@eq@ || @neq@) && @signed@)\n+static void simd_binary_@kind@_@sfx@(char **args, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ *src1 = (npyv_lanetype_@sfx@ *) args[0];\n+    npyv_lanetype_@sfx@ *src2 = (npyv_lanetype_@sfx@ *) args[1];\n+    npyv_lanetype_u8 *dst     = (npyv_lanetype_u8 *) args[2];\n+    const npyv_u8 truemask    = npyv_setall_u8(0x1);\n+    const int vstep           = npyv_nlanes_u8;\n+\n+    // Unroll the loop to get a resultant vector with 'vsteps' elements.\n+    for (; len >= vstep;\n+         len -= vstep, src1 += vstep, src2 += vstep, dst += vstep) {\n+#if @len@ >= 8\n+        npyv_@sfx@  a1 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 0);\n+        npyv_@sfx@  b1 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 0);\n+        npyv_b@len@ c1 = npyv_@VOP@_@sfx@(a1, b1);\n+#if @len@ >= 16\n+        npyv_@sfx@  a2 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 1);\n+        npyv_@sfx@  b2 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 1);\n+        npyv_b@len@ c2 = npyv_@VOP@_@sfx@(a2, b2);\n+#if @len@ >= 32\n+        npyv_@sfx@  a3 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 2);\n+        npyv_@sfx@  b3 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 2);\n+        npyv_@sfx@  a4 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 3);\n+        npyv_@sfx@  b4 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 3);\n+        npyv_b@len@ c3 = npyv_@VOP@_@sfx@(a3, b3);\n+        npyv_b@len@ c4 = npyv_@VOP@_@sfx@(a4, b4);\n+#if @len@ == 64\n+        npyv_@sfx@  a5 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 4);\n+        npyv_@sfx@  b5 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 4);\n+        npyv_@sfx@  a6 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 5);\n+        npyv_@sfx@  b6 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 5);\n+        npyv_@sfx@  a7 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 6);\n+        npyv_@sfx@  b7 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 6);\n+        npyv_@sfx@  a8 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 7);\n+        npyv_@sfx@  b8 = npyv_load_@sfx@(src2 + npyv_nlanes_@sfx@ * 7);\n+        npyv_b@len@ c5 = npyv_@VOP@_@sfx@(a5, b5);\n+        npyv_b@len@ c6 = npyv_@VOP@_@sfx@(a6, b6);\n+        npyv_b@len@ c7 = npyv_@VOP@_@sfx@(a7, b7);\n+        npyv_b@len@ c8 = npyv_@VOP@_@sfx@(a8, b8);\n+#endif // @len@ >= 64\n+#endif // @len@ >= 32\n+#endif // @len@ >= 16\n+#endif // @len@ >= 8\n+\n+        // Pack the 'c' vectors into a single vector 'r'\n+#if @len@ == 8\n+        npyv_u8 r = npyv_cvt_u8_b8(c1);\n+#elif @len@ == 16\n+        npyv_u8 r = npyv_cvt_u8_b8(npyv_pack_b8_b16(c1, c2));\n+#elif @len@ == 32\n+        npyv_u8 r = npyv_cvt_u8_b8(npyv_pack_b8_b32(c1, c2, c3, c4));\n+#elif @len@ == 64\n+        npyv_u8 r =\n+            npyv_cvt_u8_b8(npyv_pack_b8_b64(c1, c2, c3, c4, c5, c6, c7, c8));\n+#endif\n+        npyv_store_u8(dst, npyv_and_u8(r, truemask));\n+    }\n+\n+    for (; len > 0; --len, ++src1, ++src2, ++dst) {\n+        const npyv_lanetype_@sfx@ a = *src1;\n+        const npyv_lanetype_@sfx@ b = *src2;\n+        *dst = a @OP@ b;\n+    }\n+}\n+\n+static void simd_binary_scalar1_@kind@_@sfx@(char **args, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ scalar = *(npyv_lanetype_@sfx@ *) args[0];\n+    npyv_lanetype_@sfx@ *src   = (npyv_lanetype_@sfx@ *) args[1];\n+    npyv_lanetype_u8 *dst      = (npyv_lanetype_u8 *) args[2];\n+    const npyv_@sfx@ a         = npyv_setall_@sfx@(scalar);\n+    const npyv_u8 truemask     = npyv_setall_u8(0x1);\n+    const int vstep            = npyv_nlanes_u8;\n+\n+    for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+#if @len@ >= 8\n+        npyv_@sfx@  b1 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 0);\n+        npyv_b@len@ c1 = npyv_@VOP@_@sfx@(a, b1);\n+#if @len@ >= 16\n+        npyv_@sfx@  b2 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 1);\n+        npyv_b@len@ c2 = npyv_@VOP@_@sfx@(a, b2);\n+#if @len@ >= 32\n+        npyv_@sfx@  b3 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 2);\n+        npyv_@sfx@  b4 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 3);\n+        npyv_b@len@ c3 = npyv_@VOP@_@sfx@(a, b3);\n+        npyv_b@len@ c4 = npyv_@VOP@_@sfx@(a, b4);\n+#if @len@ == 64\n+        npyv_@sfx@  b5 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 4);\n+        npyv_@sfx@  b6 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 5);\n+        npyv_@sfx@  b7 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 6);\n+        npyv_@sfx@  b8 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 7);\n+        npyv_b@len@ c5 = npyv_@VOP@_@sfx@(a, b5);\n+        npyv_b@len@ c6 = npyv_@VOP@_@sfx@(a, b6);\n+        npyv_b@len@ c7 = npyv_@VOP@_@sfx@(a, b7);\n+        npyv_b@len@ c8 = npyv_@VOP@_@sfx@(a, b8);\n+#endif // @len@ >= 64\n+#endif // @len@ >= 32\n+#endif // @len@ >= 16\n+#endif // @len@ >= 8\n+\n+#if @len@ == 8\n+        npyv_u8 r = npyv_cvt_u8_b8(c1);\n+#elif @len@ == 16\n+        npyv_u8 r = npyv_cvt_u8_b8(npyv_pack_b8_b16(c1, c2));\n+#elif @len@ == 32\n+        npyv_u8 r = npyv_cvt_u8_b8(npyv_pack_b8_b32(c1, c2, c3, c4));\n+#elif @len@ == 64\n+        npyv_u8 r =\n+            npyv_cvt_u8_b8(npyv_pack_b8_b64(c1, c2, c3, c4, c5, c6, c7, c8));\n+#endif\n+        npyv_store_u8(dst, npyv_and_u8(r, truemask));\n+    }\n+\n+    for (; len > 0; --len, ++src, ++dst) {\n+        const npyv_lanetype_@sfx@ b = *src;\n+        *dst = scalar @OP@ b;\n+    }\n+}\n+\n+static void simd_binary_scalar2_@kind@_@sfx@(char **args, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ *src   = (npyv_lanetype_@sfx@ *) args[0];\n+    npyv_lanetype_@sfx@ scalar = *(npyv_lanetype_@sfx@ *) args[1];\n+    npyv_lanetype_u8 *dst      = (npyv_lanetype_u8 *) args[2];\n+    const npyv_@sfx@ b         = npyv_setall_@sfx@(scalar);\n+    const npyv_u8 truemask     = npyv_setall_u8(0x1);\n+    const int vstep            = npyv_nlanes_u8;\n+\n+    for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+#if @len@ >= 8\n+        npyv_@sfx@  a1 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 0);\n+        npyv_b@len@ c1 = npyv_@VOP@_@sfx@(a1, b);\n+#if @len@ >= 16\n+        npyv_@sfx@  a2 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 1);\n+        npyv_b@len@ c2 = npyv_@VOP@_@sfx@(a2, b);\n+#if @len@ >= 32\n+        npyv_@sfx@  a3 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 2);\n+        npyv_@sfx@  a4 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 3);\n+        npyv_b@len@ c3 = npyv_@VOP@_@sfx@(a3, b);\n+        npyv_b@len@ c4 = npyv_@VOP@_@sfx@(a4, b);\n+#if @len@ == 64\n+        npyv_@sfx@  a5 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 4);\n+        npyv_@sfx@  a6 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 5);\n+        npyv_@sfx@  a7 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 6);\n+        npyv_@sfx@  a8 = npyv_load_@sfx@(src + npyv_nlanes_@sfx@ * 7);\n+        npyv_b@len@ c5 = npyv_@VOP@_@sfx@(a5, b);\n+        npyv_b@len@ c6 = npyv_@VOP@_@sfx@(a6, b);\n+        npyv_b@len@ c7 = npyv_@VOP@_@sfx@(a7, b);\n+        npyv_b@len@ c8 = npyv_@VOP@_@sfx@(a8, b);\n+#endif // @len@ >= 64\n+#endif // @len@ >= 32\n+#endif // @len@ >= 16\n+#endif // @len@ >= 8\n+\n+#if @len@ == 8\n+        npyv_u8 r = npyv_cvt_u8_b8(c1);\n+#elif @len@ == 16\n+        npyv_u8 r = npyv_cvt_u8_b8(npyv_pack_b8_b16(c1, c2));\n+#elif @len@ == 32\n+        npyv_u8 r = npyv_cvt_u8_b8(npyv_pack_b8_b32(c1, c2, c3, c4));\n+#elif @len@ == 64\n+        npyv_u8 r =\n+            npyv_cvt_u8_b8(npyv_pack_b8_b64(c1, c2, c3, c4, c5, c6, c7, c8));\n+#endif\n+        npyv_store_u8(dst, npyv_and_u8(r, truemask));\n+    }\n+\n+    for (; len > 0; --len, ++src, ++dst) {\n+        const npyv_lanetype_@sfx@ a = *src;\n+        *dst = a @OP@ scalar;\n+    }\n+}\n+#endif\n+\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #kind = equal, not_equal, less, less_equal#\n+ * #eq = 1, 0, 0, 0#\n+ * #neq = 0, 1, 0, 0#\n+ * #OP = ==, !=, <, <=#\n+ * #VOP = xnor, xor, andc, orc#\n+ */\n+\n+#if NPY_SIMD\n+static void simd_binary_@kind@_b8(char **args, npy_intp len)\n+{\n+    npyv_lanetype_u8 *src1 = (npyv_lanetype_u8 *) args[0];\n+    npyv_lanetype_u8 *src2 = (npyv_lanetype_u8 *) args[1];\n+    npyv_lanetype_u8 *dst  = (npyv_lanetype_u8 *) args[2];\n+    const npyv_u8 truemask = npyv_setall_u8(0x1);\n+    const npyv_u8 vzero    = npyv_setall_u8(0x0);\n+    const int vstep        = npyv_nlanes_u8;\n+\n+    for (; len >= vstep;\n+         len -= vstep, src1 += vstep, src2 += vstep, dst += vstep) {\n+        // Whatever element in src != 0x0 is converted to 0xFF\n+        npyv_b8 a = npyv_cmpeq_u8(npyv_load_u8(src1), vzero);\n+        npyv_b8 b = npyv_cmpeq_u8(npyv_load_u8(src2), vzero);\n+        npyv_b8 c = npyv_@VOP@_b8(a, b);\n+        npyv_store_u8(dst, npyv_andc_u8(npyv_cvt_u8_b8(c), truemask));\n+    }\n+\n+    for (; len > 0; --len, ++src1, ++src2, ++dst) {\n+        const npyv_lanetype_u8 a = *src1 != 0;\n+        const npyv_lanetype_u8 b = *src2 != 0;\n+        *dst = a @OP@ b;\n+    }\n+}\n+\n+static void simd_binary_scalar1_@kind@_b8(char **args, npy_intp len)\n+{\n+    npyv_lanetype_u8 scalar = *(npyv_lanetype_u8 *) args[0];\n+    npyv_lanetype_u8 *src   = (npyv_lanetype_u8 *) args[1];\n+    npyv_lanetype_u8 *dst   = (npyv_lanetype_u8 *) args[2];\n+    const npyv_u8 vzero     = npyv_setall_u8(0x0);\n+    const npyv_u8 vscalar   = npyv_setall_u8(scalar);\n+    const npyv_b8 a         = npyv_cmpeq_u8(vscalar, vzero);\n+    const npyv_u8 truemask  = npyv_setall_u8(0x1);\n+    const int vstep         = npyv_nlanes_u8;\n+\n+    for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+        npyv_b8 b = npyv_cmpeq_u8(npyv_load_u8(src), vzero);\n+        npyv_b8 c = npyv_@VOP@_b8(a, b);\n+        npyv_store_u8(dst, npyv_andc_u8(npyv_cvt_u8_b8(c), truemask));\n+    }\n+\n+    for (; len > 0; --len, ++src, ++dst) {\n+        const npyv_lanetype_u8 b = *src != 0;\n+        *dst = scalar @OP@ b;\n+    }\n+}\n+\n+static void simd_binary_scalar2_@kind@_b8(char **args, npy_intp len)\n+{\n+    npyv_lanetype_u8 *src   = (npyv_lanetype_u8 *) args[0];\n+    npyv_lanetype_u8 scalar = *(npyv_lanetype_u8 *) args[1];\n+    npyv_lanetype_u8 *dst   = (npyv_lanetype_u8 *) args[2];\n+    const npyv_u8 vzero     = npyv_setall_u8(0x0);\n+    const npyv_u8 vscalar   = npyv_setall_u8(scalar);\n+    const npyv_b8 b         = npyv_cmpeq_u8(vscalar, vzero);\n+    const npyv_u8 truemask  = npyv_setall_u8(0x1);\n+    const int vstep         = npyv_nlanes_u8;\n+\n+    for (; len >= vstep; len -= vstep, src += vstep, dst += vstep) {\n+        npyv_b8 a = npyv_cmpeq_u8(npyv_load_u8(src), vzero);\n+        npyv_b8 c = npyv_@VOP@_b8(a, b);\n+        npyv_store_u8(dst, npyv_andc_u8(npyv_cvt_u8_b8(c), truemask));\n+    }\n+\n+    for (; len > 0; --len, ++src, ++dst) {\n+        const npyv_lanetype_u8 a = *src != 0;\n+        *dst = a @OP@ scalar;\n+    }\n+}\n+#endif\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #type = npy_ubyte*2, npy_byte, npy_ushort, npy_short, npy_uint, npy_int,\n+           npy_ulonglong, npy_longlong, npy_float, npy_double#\n+ * #sfx = b8, u8, s8, u16, s16, u32, s32, u64, s64, f32, f64#\n+ * #bool = 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0#\n+ * #fp = 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1#\n+ * #signed = 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0#\n+ * #VECTOR = NPY_SIMD*10, NPY_SIMD_F64#\n+ */\n+/**begin repeat1\n+ * #kind = equal, not_equal, less, less_equal#\n+ * #eq   = 1, 0, 0, 0#\n+ * #neq  = 0, 1, 0, 0#\n+ * #OP = ==, !=, <, <=#\n+ */\n+#if !((@eq@ || @neq@) && @signed@)\n+static NPY_INLINE void\n+run_binary_simd_@kind@_@sfx@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+{\n+#if @VECTOR@\n+    /* argument one scalar */\n+    if (IS_BLOCKABLE_BINARY_SCALAR1_BOOL(sizeof(@type@), NPY_SIMD_WIDTH)) {\n+        simd_binary_scalar1_@kind@_@sfx@(args, dimensions[0]);\n+        return;\n+    }\n+    /* argument two scalar */\n+    else if (IS_BLOCKABLE_BINARY_SCALAR2_BOOL(sizeof(@type@), NPY_SIMD_WIDTH)) {\n+        simd_binary_scalar2_@kind@_@sfx@(args, dimensions[0]);\n+        return;\n+    }\n+    else if (IS_BLOCKABLE_BINARY_BOOL(sizeof(@type@), NPY_SIMD_WIDTH)) {\n+        simd_binary_@kind@_@sfx@(args, dimensions[0]);\n+        return;\n+    }\n+#endif\n+\n+    BINARY_LOOP {\n+#if @bool@\n+        npy_bool in1 = *((npy_bool *)ip1) != 0;\n+        npy_bool in2 = *((npy_bool *)ip2) != 0;\n+#else\n+        const @type@ in1 = *(@type@ *)ip1;\n+        const @type@ in2 = *(@type@ *)ip2;\n+#endif\n+        *((npy_bool *)op1) = in1 @OP@ in2;\n+    }\n+}\n+#endif\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/********************************************************************************\n+ ** Defining ufunc inner functions\n+ ********************************************************************************/\n+\n+/*\n+ * In order to reduce the size of the binary generated from this source, the\n+ * following rules are applied: 1) each data type implements its function\n+ * 'greater' as a call to the function 'less' but with the arguments swapped,\n+ * the same applies to the function 'greater_equal', which is implemented\n+ * with a call to the function 'less_equal', and 2) for the integer datatypes\n+ * of the same size (eg 8-bit), a single kernel of the functions 'equal' and\n+ * 'not_equal' is used to implement both signed and unsigned types.\n+ */\n+\n+/**begin repeat\n+ * Signed and Unsigned types\n+ *  #TYPE  = UBYTE,     USHORT,     UINT,     ULONG,     ULONGLONG,\n+ *           BYTE,      SHORT,      INT,      LONG,      LONGLONG#\n+ *  #STYPE = BYTE,      SHORT,      INT,      LONG,      LONGLONG,\n+ *           BYTE,      SHORT,      INT,      LONG,      LONGLONG#\n+ *  #signed = 0, 0, 0, 0, 0, 1, 1, 1, 1, 1#\n+ */\n+#undef TO_SIMD_SFX\n+#undef TO_SIMD_UTYPE\n+#if 0\n+/**begin repeat1\n+ * #len = 8, 16, 32, 64#\n+ */\n+#elif NPY_BITSOF_@STYPE@ == @len@\n+    #define TO_SIMD_UTYPE(X) X##_u@len@\n+    #if @signed@\n+        #define TO_SIMD_SFX(X) X##_s@len@\n+    #else\n+        #define TO_SIMD_SFX(X) X##_u@len@\n+    #endif\n+/**end repeat1**/\n+#endif\n+\n+/**begin repeat1\n+ * #kind = greater, greater_equal#\n+ * #kind_to = less, less_equal#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    char *nargs[3] = {args[1], args[0], args[2]};\n+    npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};\n+    TO_SIMD_SFX(run_binary_simd_@kind_to@)(nargs, dimensions, nsteps);\n+}\n+/**end repeat1**/\n+\n+/**begin repeat1\n+ * #kind = less, less_equal#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    TO_SIMD_SFX(run_binary_simd_@kind@)(args, dimensions, steps);\n+}\n+/**end repeat1**/\n+\n+/**begin repeat1\n+ * #kind = equal, not_equal#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    TO_SIMD_UTYPE(run_binary_simd_@kind@)(args, dimensions, steps);\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/**begin repeat\n+ * Boolean & Float types\n+ * #TYPE = BOOL, FLOAT, DOUBLE#\n+ * #sfx = b8, f32, f64#\n+ * #fp = 0, 1, 1#\n+ */\n+/**begin repeat1\n+ * #kind = greater, greater_equal#\n+ * #kind_to = less, less_equal#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    char *nargs[3] = {args[1], args[0], args[2]};\n+    npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};\n+    run_binary_simd_@kind_to@_@sfx@(nargs, dimensions, nsteps);\n+#if @fp@\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+#endif\n+}\n+/**end repeat1**/\n+\n+/**begin repeat1\n+ * #kind = equal, not_equal, less, less_equal#\n+ */\n+NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)\n+(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+{\n+    run_binary_simd_@kind@_@sfx@(args, dimensions, steps);\n+#if @fp@\n+    npy_clear_floatstatus_barrier((char*)dimensions);\n+#endif\n+}\n+/**end repeat1**/\n+/**end repeat**/"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -158,55 +158,6 @@ run_@name@_simd_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp\n \n /**end repeat1**/\n \n-/**begin repeat1\n- * #kind = equal, not_equal, less, less_equal, greater, greater_equal,\n- *         logical_and, logical_or#\n- * #simd = 1, 1, 1, 1, 1, 1, 0, 0#\n- */\n-\n-#if @vector@ && @simd@ && defined NPY_HAVE_SSE2_INTRINSICS\n-\n-/* prototypes */\n-static void\n-sse2_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n-                          npy_intp n);\n-static void\n-sse2_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n-                                  npy_intp n);\n-static void\n-sse2_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n-                                  npy_intp n);\n-\n-#endif\n-\n-static NPY_INLINE int\n-run_binary_simd_@kind@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n-{\n-#if @vector@ && @simd@ && defined NPY_HAVE_SSE2_INTRINSICS\n-    @type@ * ip1 = (@type@ *)args[0];\n-    @type@ * ip2 = (@type@ *)args[1];\n-    npy_bool * op = (npy_bool *)args[2];\n-    npy_intp n = dimensions[0];\n-    /* argument one scalar */\n-    if (IS_BLOCKABLE_BINARY_SCALAR1_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_scalar1_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-    /* argument two scalar */\n-    else if (IS_BLOCKABLE_BINARY_SCALAR2_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_scalar2_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-    else if (IS_BLOCKABLE_BINARY_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_@kind@_@TYPE@(op, ip1, ip2, n);\n-        return 1;\n-    }\n-#endif\n-    return 0;\n-}\n-\n-/**end repeat1**/\n-\n /**begin repeat1\n  * #kind = isnan, isfinite, isinf, signbit#\n  */\n@@ -476,101 +427,6 @@ sse2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n)\n \n /**end repeat1**/\n \n-/**begin repeat1\n- * #kind = equal, not_equal, less, less_equal, greater, greater_equal#\n- * #OP = ==, !=, <, <=, >, >=#\n- * #VOP = cmpeq, cmpneq, cmplt, cmple, cmpgt, cmpge#\n-*/\n-\n-/* sets invalid fpu flag on QNaN for consistency with packed compare */\n-NPY_FINLINE int\n-sse2_ordered_cmp_@kind@_@TYPE@(const @type@ a, const @type@ b)\n-{\n-    @vtype@ one = @vpre@_set1_@vsuf@(1);\n-    @type@ tmp;\n-    @vtype@ v = @vpre@_@VOP@_@vsufs@(@vpre@_load_@vsufs@(&a),\n-                                     @vpre@_load_@vsufs@(&b));\n-    v = @vpre@_and_@vsuf@(v, one);\n-    @vpre@_store_@vsufs@(&tmp, v);\n-    return tmp;\n-}\n-\n-static void\n-sse2_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {\n-        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[i]);\n-    }\n-    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {\n-        @vtype@ a1 = @vpre@_load_@vsuf@(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ b1 = @vpre@_load_@vsuf@(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ c1 = @vpre@_load_@vsuf@(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ d1 = @vpre@_load_@vsuf@(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ a2 = @vpre@_loadu_@vsuf@(&ip2[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ b2 = @vpre@_loadu_@vsuf@(&ip2[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ c2 = @vpre@_loadu_@vsuf@(&ip2[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ d2 = @vpre@_loadu_@vsuf@(&ip2[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ r1 = @vpre@_@VOP@_@vsuf@(a1, a2);\n-        @vtype@ r2 = @vpre@_@VOP@_@vsuf@(b1, b2);\n-        @vtype@ r3 = @vpre@_@VOP@_@vsuf@(c1, c2);\n-        @vtype@ r4 = @vpre@_@VOP@_@vsuf@(d1, d2);\n-        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[i]);\n-    }\n-}\n-\n-\n-static void\n-sse2_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-    @vtype@ s = @vpre@_set1_@vsuf@(ip1[0]);\n-    LOOP_BLOCK_ALIGN_VAR(ip2, @type@, VECTOR_SIZE_BYTES) {\n-        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[0], ip2[i]);\n-    }\n-    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {\n-        @vtype@ a = @vpre@_load_@vsuf@(&ip2[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ b = @vpre@_load_@vsuf@(&ip2[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ c = @vpre@_load_@vsuf@(&ip2[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ d = @vpre@_load_@vsuf@(&ip2[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ r1 = @vpre@_@VOP@_@vsuf@(s, a);\n-        @vtype@ r2 = @vpre@_@VOP@_@vsuf@(s, b);\n-        @vtype@ r3 = @vpre@_@VOP@_@vsuf@(s, c);\n-        @vtype@ r4 = @vpre@_@VOP@_@vsuf@(s, d);\n-        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[0], ip2[i]);\n-    }\n-}\n-\n-\n-static void\n-sse2_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n-{\n-    @vtype@ s = @vpre@_set1_@vsuf@(ip2[0]);\n-    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {\n-        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[0]);\n-    }\n-    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {\n-        @vtype@ a = @vpre@_load_@vsuf@(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ b = @vpre@_load_@vsuf@(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ c = @vpre@_load_@vsuf@(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ d = @vpre@_load_@vsuf@(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);\n-        @vtype@ r1 = @vpre@_@VOP@_@vsuf@(a, s);\n-        @vtype@ r2 = @vpre@_@VOP@_@vsuf@(b, s);\n-        @vtype@ r3 = @vpre@_@VOP@_@vsuf@(c, s);\n-        @vtype@ r4 = @vpre@_@VOP@_@vsuf@(d, s);\n-        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);\n-    }\n-    LOOP_BLOCKED_END {\n-        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[0]);\n-    }\n-}\n-/**end repeat1**/\n-\n-\n static void\n sse2_negative_@TYPE@(@type@ * op, @type@ * ip, const npy_intp n)\n {"
            },
            {
                "filename": "numpy/core/tests/test_simd.py",
                "patch": "@@ -126,7 +126,8 @@ def test_operators_logical(self):\n         \"\"\"\n         Logical operations for boolean types.\n         Test intrinsics:\n-            npyv_xor_##SFX, npyv_and_##SFX, npyv_or_##SFX, npyv_not_##SFX\n+            npyv_xor_##SFX, npyv_and_##SFX, npyv_or_##SFX, npyv_not_##SFX,\n+            npyv_andc_b8, npvy_orc_b8, nvpy_xnor_b8\n         \"\"\"\n         data_a = self._data()\n         data_b = self._data(reverse=True)\n@@ -148,6 +149,22 @@ def test_operators_logical(self):\n         vnot = getattr(self, \"not\")(vdata_a)\n         assert vnot == data_b\n \n+        # among the boolean types, andc, orc and xnor only support b8\n+        if self.sfx not in (\"b8\"):\n+            return\n+\n+        data_andc = [(a & ~b) & 0xFF for a, b in zip(data_a, data_b)]\n+        vandc = getattr(self, \"andc\")(vdata_a, vdata_b)\n+        assert data_andc == vandc\n+\n+        data_orc = [(a | ~b) & 0xFF for a, b in zip(data_a, data_b)]\n+        vorc = getattr(self, \"orc\")(vdata_a, vdata_b)\n+        assert data_orc == vorc\n+\n+        data_xnor = [~(a ^ b) & 0xFF for a, b in zip(data_a, data_b)]\n+        vxnor = getattr(self, \"xnor\")(vdata_a, vdata_b)\n+        assert data_xnor == vxnor\n+\n     def test_tobits(self):\n         data2bits = lambda data: sum([int(x != 0) << i for i, x in enumerate(data, 0)])\n         for data in (self._data(), self._data(reverse=True)):\n@@ -156,6 +173,37 @@ def test_tobits(self):\n             tobits = bin(self.tobits(vdata))\n             assert tobits == bin(data_bits)\n \n+    def test_pack(self):\n+        \"\"\"\n+        Pack multiple vectors into one\n+        Test intrinsics:\n+            npyv_pack_b8_b16\n+            npyv_pack_b8_b32\n+            npyv_pack_b8_b64\n+        \"\"\"\n+        if self.sfx not in (\"b16\", \"b32\", \"b64\"):\n+            return\n+        # create the vectors\n+        data = self._data()\n+        rdata = self._data(reverse=True)\n+        vdata = self._load_b(data)\n+        vrdata = self._load_b(rdata)\n+        pack_simd = getattr(self.npyv, f\"pack_b8_{self.sfx}\")\n+        # for scalar execution, concatenate the elements of the multiple lists\n+        # into a single list (spack) and then iterate over the elements of\n+        # the created list applying a mask to capture the first byte of them.\n+        if self.sfx == \"b16\":\n+            spack = [(i & 0xFF) for i in (list(rdata) + list(data))]\n+            vpack = pack_simd(vrdata, vdata)\n+        elif self.sfx == \"b32\":\n+            spack = [(i & 0xFF) for i in (2*list(rdata) + 2*list(data))]\n+            vpack = pack_simd(vrdata, vrdata, vdata, vdata)\n+        elif self.sfx == \"b64\":\n+            spack = [(i & 0xFF) for i in (4*list(rdata) + 4*list(data))]\n+            vpack = pack_simd(vrdata, vrdata, vrdata, vrdata,\n+                               vdata,  vdata,  vdata,  vdata)\n+        assert vpack == spack\n+\n class _SIMD_INT(_Test_Utility):\n     \"\"\"\n     To test all integer vector types at once\n@@ -792,6 +840,12 @@ def test_operators_logical(self):\n         vnot = cast(getattr(self, \"not\")(vdata_a))\n         assert vnot == data_not\n \n+        if self.sfx not in (\"u8\"):\n+            return\n+        data_andc = [a & ~b for a, b in zip(data_cast_a, data_cast_b)]\n+        vandc = cast(getattr(self, \"andc\")(vdata_a, vdata_b))\n+        assert vandc == data_andc\n+\n     def test_conversion_boolean(self):\n         bsfx = \"b\" + self.sfx[1:]\n         to_boolean = getattr(self.npyv, \"cvt_%s_%s\" % (bsfx, self.sfx))"
            },
            {
                "filename": "numpy/core/tests/test_umath.py",
                "patch": "@@ -185,6 +185,52 @@ def __array_wrap__(self, arr, context):\n \n \n class TestComparisons:\n+    import operator\n+\n+    @pytest.mark.parametrize('dtype', np.sctypes['uint'] + np.sctypes['int'] +\n+                             np.sctypes['float'] + [np.bool_])\n+    @pytest.mark.parametrize('py_comp,np_comp', [\n+        (operator.lt, np.less),\n+        (operator.le, np.less_equal),\n+        (operator.gt, np.greater),\n+        (operator.ge, np.greater_equal),\n+        (operator.eq, np.equal),\n+        (operator.ne, np.not_equal)\n+    ])\n+    def test_comparison_functions(self, dtype, py_comp, np_comp):\n+        # Initialize input arrays\n+        if dtype == np.bool_:\n+            a = np.random.choice(a=[False, True], size=1000)\n+            b = np.random.choice(a=[False, True], size=1000)\n+            scalar = True\n+        else:\n+            a = np.random.randint(low=1, high=10, size=1000).astype(dtype)\n+            b = np.random.randint(low=1, high=10, size=1000).astype(dtype)\n+            scalar = 5\n+        np_scalar = np.dtype(dtype).type(scalar)\n+        a_lst = a.tolist()\n+        b_lst = b.tolist()\n+\n+        # (Binary) Comparison (x1=array, x2=array)\n+        comp_b = np_comp(a, b)\n+        comp_b_list = [py_comp(x, y) for x, y in zip(a_lst, b_lst)]\n+\n+        # (Scalar1) Comparison (x1=scalar, x2=array)\n+        comp_s1 = np_comp(np_scalar, b)\n+        comp_s1_list = [py_comp(scalar, x) for x in b_lst]\n+\n+        # (Scalar2) Comparison (x1=array, x2=scalar)\n+        comp_s2 = np_comp(a, np_scalar)\n+        comp_s2_list = [py_comp(x, scalar) for x in a_lst]\n+\n+        # Sequence: Binary, Scalar1 and Scalar2\n+        assert_(comp_b.tolist() == comp_b_list,\n+            f\"Failed comparision ({py_comp.__name__})\")\n+        assert_(comp_s1.tolist() == comp_s1_list,\n+            f\"Failed comparision ({py_comp.__name__})\")\n+        assert_(comp_s2.tolist() == comp_s2_list,\n+            f\"Failed comparision ({py_comp.__name__})\")\n+\n     def test_ignore_object_identity_in_equal(self):\n         # Check comparing identical objects whose comparison\n         # is not a simple boolean, e.g., arrays that are compared elementwise."
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21465,
        "body": "* The calculation in `_count_reduce_items` is faster with a plain python in than a numpy scalar `np.inp`. The elements of `arr.shape` are plain integers.\r\n* The usage of the result (division in `np.mean`) also might be faster with `int` instead of `np.intp`. (although in the long run in might be better to make the division with `np.intp` just as fast as with `int`)\r\n* \r\nMicrobenchmark\r\n```\r\nimport numpy as np\r\nimport time\r\n\r\nx=np.random.rand(10,20)   \r\nniter=20_0000\r\n\r\nt0=time.perf_counter()\r\nfor ii in range(niter):\r\n    y=np.mean(x, axis=1)\r\ndt=time.perf_counter()-t0\r\nprint(dt)\r\n```\r\nResults in:\r\n```\r\nmain: 1.78\r\nPR: 1.39\r\n```\r\n\r\nAddresses one of the items in #21455\r\n\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/_methods.py",
                "patch": "@@ -71,9 +71,10 @@ def _count_reduce_items(arr, axis, keepdims=False, where=True):\n             axis = tuple(range(arr.ndim))\n         elif not isinstance(axis, tuple):\n             axis = (axis,)\n-        items = nt.intp(1)\n+        items = 1\n         for ax in axis:\n             items *= arr.shape[mu.normalize_axis_index(ax, arr.ndim)]\n+        items = nt.intp(items)\n     else:\n         # TODO: Optimize case when `where` is broadcast along a non-reduction\n         # axis and full sum is more excessive than needed."
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21470,
        "body": "\r\nThe method `PyUFuncOverride_GetNonDefaultArrayUfunc` is expensive on numpy scalars because these objects do not have a `__array_ufunc__` set and for a missing attribute lookup cpython generates an exception that is later cleared by numpy. This is a performance bottleneck, see #21455.\r\n\r\nAn issue has been submitted to cpython (https://github.com/python/cpython/issues/92216). But even if this is addressed in cpython, it will take untill python 3.12+ before this will be useable by numpy.\r\n\r\nAs an alternative solution, this PR adds a fast path to `PyUFuncOverride_GetNonDefaultArrayUfunc` to determine whether an object is a numpy scalar. Some remarks:\r\n\r\n- Currently the check is only on `PyDoubleArrType_Type` and `PyIntArrType_Type`. Perhaps we can check all types easily? Or select a different subset of all numpy scalar types.\r\n- Subclasses of the numpy scalars are not handled\r\n- An alternative would be to add `__array_ufunc__` to the numpy scalars, but this is not backwards compatible (https://github.com/numpy/numpy/pull/21423#issuecomment-1115803712)\r\n\r\nMicrobenchmark on `np.sqrt(np.float64(1.1))`:\r\n```\r\nmain: 0.54\r\nPR: 0.40\r\n```\r\n<details>\r\n<summary>Full benchmark</summary>\r\n\r\n```\r\nimport numpy as np\r\nimport math\r\nimport time\r\nprint(np.__version__)\r\n\r\nw=np.float64(1.1)\r\n#w=1.1\r\n\r\nniter=600_000\r\nfor kk in range(3):\r\n    t0=time.perf_counter()\r\n    for ii in range(niter):\r\n        _=np.sqrt(w)\r\n    dt=time.perf_counter()-t0\r\n    print(f'loop {kk}: {dt}')\r\n```\r\n</details>\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/common/get_attr_string.h",
                "patch": "@@ -42,7 +42,7 @@ _is_basic_python_type(PyTypeObject *tp)\n  * on the type object, rather than on the instance itself.\n  *\n  * Assumes that the special method is a numpy-specific one, so does not look\n- * at builtin types, nor does it look at a base ndarray.\n+ * at builtin types. It does check base ndarray and numpy scalar types.\n  *\n  * In future, could be made more like _Py_LookupSpecial\n  */"
            },
            {
                "filename": "numpy/core/src/common/ufunc_override.c",
                "patch": "@@ -5,6 +5,7 @@\n #include \"get_attr_string.h\"\n #include \"npy_import.h\"\n #include \"ufunc_override.h\"\n+#include \"scalartypes.h\"\n \n /*\n  * Check whether an object has __array_ufunc__ defined on its class and it\n@@ -30,6 +31,11 @@ PyUFuncOverride_GetNonDefaultArrayUfunc(PyObject *obj)\n     if (PyArray_CheckExact(obj)) {\n         return NULL;\n     }\n+   /* Fast return for numpy scalar types */\n+    if (is_anyscalar_exact(obj)) {\n+        return NULL;\n+    }\n+\n     /*\n      * Does the class define __array_ufunc__? (Note that LookupSpecial has fast\n      * return for basic python types, so no need to worry about those here)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20020,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nUsing `ndenumerate` on a masked array does not yield `ma.masked` for masked values, but rather the actual value of the underlying data array. This makes sense from a performance perspective, but is potentially undesirable. Rather than filling the masked array before using `ndenumerate` and incurring a copy, we can use a simple `ndenumerate` wrapper specifically for masked arrays. This MR suggests such a specialization of `ndenumerate`.\r\n\r\nAn 'upcoming changes' message is still missing as this is this functionality is proposed without prior discussion and I want to know what people think first.\r\n\r\nAt least in 2011 I could find someone else wanting this functionality too: https://stackoverflow.com/questions/8620798/numpy-ndenumerate-for-masked-arrays/8621173.",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/20020.new_function.rst",
                "patch": "@@ -0,0 +1,4 @@\n+`ndenumerate` specialization for masked arrays\n+----------------------------------------------\n+The masked array module now provides the `numpy.ma.ndenumerate` function,\n+an alternative to `numpy.ndenumerate` that skips masked values by default."
            },
            {
                "filename": "doc/source/reference/routines.ma.rst",
                "patch": "@@ -190,6 +190,7 @@ Finding masked data\n .. autosummary::\n    :toctree: generated/\n \n+   ma.ndenumerate\n    ma.flatnotmasked_contiguous\n    ma.flatnotmasked_edges\n    ma.notmasked_contiguous"
            },
            {
                "filename": "numpy/ma/__init__.pyi",
                "patch": "@@ -216,6 +216,7 @@ from numpy.ma.extras import (\n     masked_all_like as masked_all_like,\n     median as median,\n     mr_ as mr_,\n+    ndenumerate as ndenumerate,\n     notmasked_contiguous as notmasked_contiguous,\n     notmasked_edges as notmasked_edges,\n     polyfit as polyfit,"
            },
            {
                "filename": "numpy/ma/extras.py",
                "patch": "@@ -10,12 +10,12 @@\n \"\"\"\n __all__ = [\n     'apply_along_axis', 'apply_over_axes', 'atleast_1d', 'atleast_2d',\n-    'atleast_3d', 'average', 'clump_masked', 'clump_unmasked',\n-    'column_stack', 'compress_cols', 'compress_nd', 'compress_rowcols',\n-    'compress_rows', 'count_masked', 'corrcoef', 'cov', 'diagflat', 'dot',\n-    'dstack', 'ediff1d', 'flatnotmasked_contiguous', 'flatnotmasked_edges',\n-    'hsplit', 'hstack', 'isin', 'in1d', 'intersect1d', 'mask_cols', 'mask_rowcols',\n-    'mask_rows', 'masked_all', 'masked_all_like', 'median', 'mr_',\n+    'atleast_3d', 'average', 'clump_masked', 'clump_unmasked', 'column_stack',\n+    'compress_cols', 'compress_nd', 'compress_rowcols', 'compress_rows',\n+    'count_masked', 'corrcoef', 'cov', 'diagflat', 'dot', 'dstack', 'ediff1d',\n+    'flatnotmasked_contiguous', 'flatnotmasked_edges', 'hsplit', 'hstack',\n+    'isin', 'in1d', 'intersect1d', 'mask_cols', 'mask_rowcols', 'mask_rows',\n+    'masked_all', 'masked_all_like', 'median', 'mr_', 'ndenumerate',\n     'notmasked_contiguous', 'notmasked_edges', 'polyfit', 'row_stack',\n     'setdiff1d', 'setxor1d', 'stack', 'unique', 'union1d', 'vander', 'vstack',\n     ]\n@@ -1520,6 +1520,74 @@ def __init__(self):\n #---- Find unmasked data ---\n #####--------------------------------------------------------------------------\n \n+def ndenumerate(a, compressed=True):\n+    \"\"\"\n+    Multidimensional index iterator.\n+\n+    Return an iterator yielding pairs of array coordinates and values,\n+    skipping elements that are masked. With `compressed=False`,\n+    `ma.masked` is yielded as the value of masked elements. This\n+    behavior differs from that of `numpy.ndenumerate`, which yields the\n+    value of the underlying data array.\n+\n+    Notes\n+    -----\n+    .. versionadded:: 1.23.0\n+\n+    Parameters\n+    ----------\n+    a : array_like\n+        An array with (possibly) masked elements.\n+    compressed : bool, optional\n+        If True (default), masked elements are skipped.\n+\n+    See Also\n+    --------\n+    numpy.ndenumerate : Equivalent function ignoring any mask.\n+\n+    Examples\n+    --------\n+    >>> a = np.ma.arange(9).reshape((3, 3))\n+    >>> a[1, 0] = np.ma.masked\n+    >>> a[1, 2] = np.ma.masked\n+    >>> a[2, 1] = np.ma.masked\n+    >>> a\n+    masked_array(\n+      data=[[0, 1, 2],\n+            [--, 4, --],\n+            [6, --, 8]],\n+      mask=[[False, False, False],\n+            [ True, False,  True],\n+            [False,  True, False]],\n+      fill_value=999999)\n+    >>> for index, x in np.ma.ndenumerate(a):\n+    ...     print(index, x)\n+    (0, 0) 0\n+    (0, 1) 1\n+    (0, 2) 2\n+    (1, 1) 4\n+    (2, 0) 6\n+    (2, 2) 8\n+\n+    >>> for index, x in np.ma.ndenumerate(a, compressed=False):\n+    ...     print(index, x)\n+    (0, 0) 0\n+    (0, 1) 1\n+    (0, 2) 2\n+    (1, 0) --\n+    (1, 1) 4\n+    (1, 2) --\n+    (2, 0) 6\n+    (2, 1) --\n+    (2, 2) 8\n+    \"\"\"\n+    for it, mask in zip(np.ndenumerate(a), getmaskarray(a).flat):\n+        if not mask:\n+            yield it\n+        elif not compressed:\n+            yield it[0], masked\n+\n+\n def flatnotmasked_edges(a):\n     \"\"\"\n     Find the indices of the first and last unmasked values."
            },
            {
                "filename": "numpy/ma/extras.pyi",
                "patch": "@@ -74,6 +74,7 @@ class mr_class(MAxisConcatenator):\n \n mr_: mr_class\n \n+def ndenumerate(a, compressed=...): ...\n def flatnotmasked_edges(a): ...\n def notmasked_edges(a, axis=...): ...\n def flatnotmasked_contiguous(a): ..."
            },
            {
                "filename": "numpy/ma/tests/test_extras.py",
                "patch": "@@ -28,7 +28,7 @@\n     ediff1d, apply_over_axes, apply_along_axis, compress_nd, compress_rowcols,\n     mask_rowcols, clump_masked, clump_unmasked, flatnotmasked_contiguous,\n     notmasked_contiguous, notmasked_edges, masked_all, masked_all_like, isin,\n-    diagflat, stack, vstack\n+    diagflat, ndenumerate, stack, vstack\n     )\n \n \n@@ -1648,6 +1648,44 @@ def test_shape_scalar(self):\n         assert_equal(b.mask.shape, b.data.shape)\n \n \n+class TestNDEnumerate:\n+\n+    def test_ndenumerate_nomasked(self):\n+        ordinary = np.ndarray(6).reshape((1, 3, 2))\n+        empty_mask = np.zeros_like(ordinary, dtype=bool)\n+        with_mask = masked_array(ordinary, mask=empty_mask)\n+        assert_equal(list(np.ndenumerate(ordinary)),\n+                     list(ndenumerate(ordinary)))\n+        assert_equal(list(ndenumerate(ordinary)),\n+                     list(ndenumerate(with_mask)))\n+        assert_equal(list(ndenumerate(with_mask)),\n+                     list(ndenumerate(with_mask, compressed=False)))\n+\n+    def test_ndenumerate_allmasked(self):\n+        a = masked_all(())\n+        b = masked_all((100,))\n+        c = masked_all((2, 3, 4))\n+        assert_equal(list(ndenumerate(a)), [])\n+        assert_equal(list(ndenumerate(b)), [])\n+        assert_equal(list(ndenumerate(b, compressed=False)),\n+                     list(zip(np.ndindex((100,)), 100 * [masked])))\n+        assert_equal(list(ndenumerate(c)), [])\n+        assert_equal(list(ndenumerate(c, compressed=False)),\n+                     list(zip(np.ndindex((2, 3, 4)), 2 * 3 * 4 * [masked])))\n+\n+    def test_ndenumerate_mixedmasked(self):\n+        a = masked_array(np.arange(12).reshape((3, 4)),\n+                         mask=[[1, 1, 1, 1],\n+                               [1, 1, 0, 1],\n+                               [0, 0, 0, 0]])\n+        items = [((1, 2), 6),\n+                 ((2, 0), 8), ((2, 1), 9), ((2, 2), 10), ((2, 3), 11)]\n+        assert_equal(list(ndenumerate(a)), items)\n+        assert_equal(len(list(ndenumerate(a, compressed=False))), a.size)\n+        for coordinate, value in ndenumerate(a, compressed=False):\n+            assert_equal(a[coordinate], value)\n+\n+\n class TestStack:\n \n     def test_stack_1d(self):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21188,
        "body": "This tries to redo the scalar math logic to take some more care about subclasses, but most of all introduce logic to defer to the `other` if `self` can be cast to it safely (i.e. it is the correct promotion).\r\n\r\nThis makes things much faster and more reliable, since we now use defer much less to `ufuncs` indirectly.  This ensures that integer overflows are reported more reliably.\r\n\r\nAnother major point about it, is that this reorganizes the coercion of Python int, float, complex (and bool).  This should help a bit with switching to \"weak\" Python scalars.\r\n    \r\nFurther, it contains a commit to make all macros to inline functions and move the floating point overflow flag handling to a return value.  Checking floating point flags is not insanely slow, but it is pretty slow on the scale of integer operations here (~30% on my computer).\r\n\r\nIt should fix some bugs around subclasses, but then subclassing scalars should still be pretty prone to issues (similar to subclassing arrays I guess).  Complicated subclass cases may still end up in the generic (array) path, but we catch a few more ahead of time.\r\n\r\n---\r\n\r\nThis is the 2-3 approach I have been thinking about.  Another one would be to have (more or less) a single function dealing with any scalar inputs.  It does \"inline\" the casting logic here.  I do not like that, but it seemed somewhat straight forward \u2013 it would be nice to create `npy_cast_@from@_to_@to@` or so functions, to use here more generically.\r\n\r\nThe alternative would be logic with the existing \"cast\" functionality, but it should be slower, and while verbose (and the current macros are ugly), I am not sure that would actually end up much better.\r\n\r\nThe `PyArray_ScalarFromObject` seemed only useful for the old scalar paths, so I added a deprecation:  Neither were even documented, and both would probably need some work to transition to the new DTypes well.\r\n`PyArray_CastScalarDirect` may also be a target, but it is still used in at least one place, so likely should just fix it.\r\n\r\n---\r\n\r\nBenchmarks:\r\n```\r\n       before           after         ratio\r\n     [732ed25e]       [3b60effc]\r\n     <main>           <scalar-math-rewrite>\r\n-        861\u00b110ns          819\u00b16ns     0.95  bench_scalar.ScalarMath.time_abs('int32')\r\n-         862\u00b13ns          819\u00b15ns     0.95  bench_scalar.ScalarMath.time_abs('float16')\r\n-        929\u00b110ns         882\u00b110ns     0.95  bench_scalar.ScalarMath.time_addition('complex128')\r\n-     1.05\u00b10.02\u03bcs          996\u00b14ns     0.95  bench_scalar.ScalarMath.time_addition('float16')\r\n-         859\u00b14ns          813\u00b13ns     0.95  bench_scalar.ScalarMath.time_abs('int16')\r\n-         890\u00b18ns          841\u00b12ns     0.94  bench_scalar.ScalarMath.time_abs('complex64')\r\n-         924\u00b14ns          871\u00b14ns     0.94  bench_scalar.ScalarMath.time_multiplication('float64')\r\n-         957\u00b12ns        895\u00b10.8ns     0.93  bench_scalar.ScalarMath.time_abs('complex256')\r\n-        932\u00b110ns        871\u00b10.4ns     0.93  bench_scalar.ScalarMath.time_addition('float64')\r\n-         922\u00b15ns        855\u00b10.8ns     0.93  bench_scalar.ScalarMath.time_abs('longfloat')\r\n-        893\u00b110ns          826\u00b17ns     0.93  bench_scalar.ScalarMath.time_abs('complex128')\r\n-     1.09\u00b10.07\u03bcs          987\u00b18ns     0.90  bench_scalar.ScalarMath.time_multiplication('float16')\r\n-        687\u00b110ns          532\u00b12ns     0.77  bench_scalar.ScalarMath.time_add_int32_other('int32')\r\n-     4.63\u00b10.05\u03bcs       3.58\u00b10.1\u03bcs     0.77  bench_scalar.ScalarMath.time_addition_pyint('int16')\r\n-     5.58\u00b10.06\u03bcs      4.24\u00b10.07\u03bcs     0.76  bench_scalar.ScalarMath.time_addition_pyint('float32')\r\n-     5.62\u00b10.08\u03bcs       4.22\u00b10.1\u03bcs     0.75  bench_scalar.ScalarMath.time_addition_pyint('complex64')\r\n-     1.03\u00b10.01\u03bcs          755\u00b19ns     0.73  bench_scalar.ScalarMath.time_add_int32_other('int16')\r\n-     5.76\u00b10.03\u03bcs       4.21\u00b10.1\u03bcs     0.73  bench_scalar.ScalarMath.time_addition_pyint('float16')\r\n-         922\u00b12ns          662\u00b12ns     0.72  bench_scalar.ScalarMath.time_addition('int16')\r\n-         921\u00b13ns        653\u00b10.5ns     0.71  bench_scalar.ScalarMath.time_multiplication('int64')\r\n-         929\u00b11ns          656\u00b16ns     0.71  bench_scalar.ScalarMath.time_addition('int64')\r\n-         916\u00b18ns          645\u00b14ns     0.70  bench_scalar.ScalarMath.time_multiplication('int16')\r\n-        953\u00b160ns          657\u00b15ns     0.69  bench_scalar.ScalarMath.time_multiplication('int32')\r\n-         934\u00b18ns        642\u00b10.9ns     0.69  bench_scalar.ScalarMath.time_addition('int32')\r\n-     3.54\u00b10.01\u03bcs         1.76\u00b10\u03bcs     0.50  bench_scalar.ScalarMath.time_power_of_two('longfloat')\r\n-     3.41\u00b10.04\u03bcs      1.59\u00b10.01\u03bcs     0.47  bench_scalar.ScalarMath.time_power_of_two('float64')\r\n-      3.70\u00b10.1\u03bcs         1.65\u00b10\u03bcs     0.45  bench_scalar.ScalarMath.time_power_of_two('complex256')\r\n-     2.36\u00b10.05\u03bcs         1.04\u00b10\u03bcs     0.44  bench_scalar.ScalarMath.time_power_of_two('int64')\r\n-     2.83\u00b10.05\u03bcs         1.19\u00b10\u03bcs     0.42  bench_scalar.ScalarMath.time_addition_pyint('longfloat')\r\n-     2.87\u00b10.05\u03bcs         1.20\u00b10\u03bcs     0.42  bench_scalar.ScalarMath.time_addition_pyint('complex256')\r\n-     3.17\u00b10.08\u03bcs         1.25\u00b10\u03bcs     0.39  bench_scalar.ScalarMath.time_power_of_two('complex128')\r\n-     2.64\u00b10.02\u03bcs          992\u00b15ns     0.38  bench_scalar.ScalarMath.time_addition_pyint('complex128')\r\n-     2.17\u00b10.04\u03bcs          807\u00b12ns     0.37  bench_scalar.ScalarMath.time_addition_pyint('int64')\r\n-     2.75\u00b10.02\u03bcs          993\u00b15ns     0.36  bench_scalar.ScalarMath.time_addition_pyint('float64')\r\n-      14.1\u00b10.2\u03bcs      1.24\u00b10.03\u03bcs     0.09  bench_scalar.ScalarMath.time_add_int32_other('longfloat')\r\n-      14.6\u00b10.3\u03bcs      1.23\u00b10.01\u03bcs     0.08  bench_scalar.ScalarMath.time_add_int32_other('complex256')\r\n-      14.1\u00b10.2\u03bcs      1.10\u00b10.01\u03bcs     0.08  bench_scalar.ScalarMath.time_add_int32_other('float64')\r\n-      14.5\u00b10.1\u03bcs      1.11\u00b10.01\u03bcs     0.08  bench_scalar.ScalarMath.time_add_int32_other('complex128')\r\n-     13.9\u00b10.07\u03bcs          973\u00b16ns     0.07  bench_scalar.ScalarMath.time_add_int32_other('int64')\r\n```\r\n(Some corner cases may be significantly slower, mainly certain `scalar + 0d_array` ops, but I am not sure I want to worry about those much.)\r\n\r\n---\r\n\r\n**This PR became quite big, I may split it up.  At this time it relies on gh-21178.   It is currently missing some additional tests for the subclass behavior at least, but I would like to check code-coverage on that front as well.**",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_scalar.py",
                "patch": "@@ -10,6 +10,8 @@ class ScalarMath(Benchmark):\n     param_names = [\"type\"]\n     def setup(self, typename):\n         self.num = np.dtype(typename).type(2)\n+        self.int32 = np.int32(2)\n+        self.int32arr = np.array(2, dtype=np.int32)\n \n     def time_addition(self, typename):\n         n = self.num\n@@ -31,3 +33,35 @@ def time_abs(self, typename):\n         n = self.num\n         res = abs(abs(abs(abs(abs(abs(abs(abs(abs(abs(n))))))))))\n \n+    def time_add_int32_other(self, typename):\n+        # Some mixed cases are fast, some are slow, this documents these\n+        # differences.  (When writing, it was fast if the type of the result\n+        # is one of the inputs.)\n+        int32 = self.int32\n+        other = self.num\n+        int32 + other\n+        int32 + other\n+        int32 + other\n+        int32 + other\n+        int32 + other\n+\n+    def time_add_int32arr_and_other(self, typename):\n+        # `arr + scalar` hits the normal ufunc (array) paths.\n+        int32 = self.int32arr\n+        other = self.num\n+        int32 + other\n+        int32 + other\n+        int32 + other\n+        int32 + other\n+        int32 + other\n+\n+    def time_add_other_and_int32arr(self, typename):\n+        # `scalar + arr` at some point hit scalar paths in some cases, and\n+        # these paths could be optimized more easily\n+        int32 = self.int32arr\n+        other = self.num\n+        other + int32\n+        other + int32\n+        other + int32\n+        other + int32\n+        other + int32"
            },
            {
                "filename": "doc/release/upcoming_changes/21188.performance.rst",
                "patch": "@@ -0,0 +1,8 @@\n+Faster operations on NumPy scalars\n+----------------------------------\n+Many operations on NumPy scalars are now significantly faster, although\n+rare operations (e.g. with 0-D arrays rather than scalars) may be slower\n+in some cases.\n+However, even with these improvements users who want the best performance\n+for their scalars, may want to convert a known NumPy scalar into a Python\n+one using `scalar.item()`."
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -183,6 +183,8 @@ def set_sig(sig):\n                                 'attribute_optimize_unroll_loops'),\n                                 ('__attribute__((optimize(\"O3\")))',\n                                  'attribute_optimize_opt_3'),\n+                                ('__attribute__((optimize(\"O2\")))',\n+                                 'attribute_optimize_opt_2'),\n                                 ('__attribute__((nonnull (1)))',\n                                  'attribute_nonnull'),\n                                 ('__attribute__((target (\"avx\")))',"
            },
            {
                "filename": "numpy/core/src/multiarray/abstractdtypes.c",
                "patch": "@@ -259,6 +259,7 @@ NPY_NO_EXPORT PyArray_DTypeMeta PyArray_PyIntAbstractDType = {{{\n         .tp_name = \"numpy._IntegerAbstractDType\",\n     },},\n     .flags = NPY_DT_ABSTRACT,\n+    .type_num = -1,\n     .dt_slots = &pyintabstractdtype_slots,\n };\n \n@@ -276,6 +277,7 @@ NPY_NO_EXPORT PyArray_DTypeMeta PyArray_PyFloatAbstractDType = {{{\n         .tp_name = \"numpy._FloatAbstractDType\",\n     },},\n     .flags = NPY_DT_ABSTRACT,\n+    .type_num = -1,\n     .dt_slots = &pyfloatabstractdtype_slots,\n };\n \n@@ -293,5 +295,6 @@ NPY_NO_EXPORT PyArray_DTypeMeta PyArray_PyComplexAbstractDType = {{{\n         .tp_name = \"numpy._ComplexAbstractDType\",\n     },},\n     .flags = NPY_DT_ABSTRACT,\n+    .type_num = -1,\n     .dt_slots = &pycomplexabstractdtype_slots,\n };"
            },
            {
                "filename": "numpy/core/src/multiarray/array_coercion.c",
                "patch": "@@ -230,6 +230,16 @@ npy_discover_dtype_from_pytype(PyTypeObject *pytype)\n     return (PyArray_DTypeMeta *)DType;\n }\n \n+/*\n+ * Note: This function never fails, but will return `NULL` for unknown scalars\n+ *       and `None` for known array-likes (e.g. tuple, list, ndarray).\n+ */\n+NPY_NO_EXPORT PyObject *\n+PyArray_DiscoverDTypeFromScalarType(PyTypeObject *pytype)\n+{\n+    return (PyObject *)npy_discover_dtype_from_pytype(pytype);\n+}\n+\n \n /**\n  * Find the correct DType class for the given python type. If flags is NULL"
            },
            {
                "filename": "numpy/core/src/multiarray/array_coercion.h",
                "patch": "@@ -19,6 +19,9 @@ NPY_NO_EXPORT int\n _PyArray_MapPyTypeToDType(\n         PyArray_DTypeMeta *DType, PyTypeObject *pytype, npy_bool userdef);\n \n+NPY_NO_EXPORT PyObject *\n+PyArray_DiscoverDTypeFromScalarType(PyTypeObject *pytype);\n+\n NPY_NO_EXPORT int\n PyArray_Pack(PyArray_Descr *descr, char *item, PyObject *value);\n "
            },
            {
                "filename": "numpy/core/src/multiarray/scalarapi.c",
                "patch": "@@ -312,6 +312,14 @@ PyArray_FromScalar(PyObject *scalar, PyArray_Descr *outcode)\n NPY_NO_EXPORT PyObject *\n PyArray_ScalarFromObject(PyObject *object)\n {\n+    if (DEPRECATE(\n+            \"PyArray_ScalarFromObject() is deprecated and scheduled for \"\n+            \"removal. If you are using this (undocumented) function, \"\n+            \"please notify the NumPy developers to look for solutions.\"\n+            \"(Deprecated in NumPy 1.23)\") < 0) {\n+        return NULL;\n+    }\n+\n     PyObject *ret = NULL;\n \n     if (PyArray_IsZeroDim(object)) {"
            },
            {
                "filename": "numpy/core/src/umath/scalarmath.c.src",
                "patch": "@@ -26,6 +26,13 @@\n #include \"binop_override.h\"\n #include \"npy_longdouble.h\"\n \n+#include \"array_coercion.h\"\n+#include \"common.h\"\n+#include \"can_cast_table.h\"\n+\n+/* TODO: Used for some functions, should possibly move these to npy_math.h */\n+#include \"loops.h\"\n+\n /* Basic operations:\n  *\n  *  BINARY:\n@@ -45,47 +52,45 @@\n  *  #name = byte, short, int, long, longlong#\n  *  #type = npy_byte, npy_short, npy_int, npy_long, npy_longlong#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_add(@type@ a, @type@ b, @type@ *out) {\n     *out = a + b;\n     if ((*out^a) >= 0 || (*out^b) >= 0) {\n-        return;\n+        return 0;\n     }\n-    npy_set_floatstatus_overflow();\n-    return;\n+    return NPY_FPE_OVERFLOW;\n }\n-static void\n+\n+static NPY_INLINE int\n @name@_ctype_subtract(@type@ a, @type@ b, @type@ *out) {\n     *out = a - b;\n     if ((*out^a) >= 0 || (*out^~b) >= 0) {\n-        return;\n+        return 0;\n     }\n-    npy_set_floatstatus_overflow();\n-    return;\n+    return NPY_FPE_OVERFLOW;\n }\n /**end repeat**/\n \n /**begin repeat\n  *  #name = ubyte, ushort, uint, ulong, ulonglong#\n  *  #type = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_add(@type@ a, @type@ b, @type@ *out) {\n     *out = a + b;\n     if (*out >= a && *out >= b) {\n-        return;\n+        return 0;\n     }\n-    npy_set_floatstatus_overflow();\n-    return;\n+    return NPY_FPE_OVERFLOW;\n }\n-static void\n+\n+static NPY_INLINE int\n @name@_ctype_subtract(@type@ a, @type@ b, @type@ *out) {\n     *out = a - b;\n     if (a >= b) {\n-        return;\n+        return 0;\n     }\n-    npy_set_floatstatus_overflow();\n-    return;\n+    return NPY_FPE_OVERFLOW;\n }\n /**end repeat**/\n \n@@ -108,18 +113,19 @@ static void\n  * #neg = (1,0)*4#\n  */\n #if NPY_SIZEOF_@SIZE@ > NPY_SIZEOF_@SIZENAME@\n-static void\n+static NPY_INLINE int\n @name@_ctype_multiply(@type@ a, @type@ b, @type@ *out) {\n     @big@ temp;\n     temp = ((@big@) a) * ((@big@) b);\n     *out = (@type@) temp;\n #if @neg@\n-    if (temp > NPY_MAX_@NAME@ || temp < NPY_MIN_@NAME@)\n+    if (temp > NPY_MAX_@NAME@ || temp < NPY_MIN_@NAME@) {\n #else\n-        if (temp > NPY_MAX_@NAME@)\n+    if (temp > NPY_MAX_@NAME@) {\n #endif\n-            npy_set_floatstatus_overflow();\n-    return;\n+        return NPY_FPE_OVERFLOW;\n+    }\n+    return 0;\n }\n #endif\n /**end repeat**/\n@@ -133,12 +139,12 @@ static void\n  * #SIZE = INT*2, LONG*2, LONGLONG*2#\n  */\n #if NPY_SIZEOF_LONGLONG == NPY_SIZEOF_@SIZE@\n-static void\n+static NPY_INLINE int\n @name@_ctype_multiply(@type@ a, @type@ b, @type@ *out) {\n     if (npy_mul_with_overflow_@name@(out, a, b)) {\n-        npy_set_floatstatus_overflow();\n+        return NPY_FPE_OVERFLOW;\n     }\n-    return;\n+    return 0;\n }\n #endif\n /**end repeat**/\n@@ -151,16 +157,16 @@ static void\n  *         npy_long, npy_ulong, npy_longlong, npy_ulonglong#\n  * #neg = (1,0)*5#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_divide(@type@ a, @type@ b, @type@ *out) {\n     if (b == 0) {\n-        npy_set_floatstatus_divbyzero();\n         *out = 0;\n+        return NPY_FPE_DIVIDEBYZERO;\n     }\n #if @neg@\n     else if (b == -1 && a < 0 && a == -a) {\n-        npy_set_floatstatus_overflow();\n         *out = a / b;\n+        return NPY_FPE_OVERFLOW;\n     }\n #endif\n     else {\n@@ -174,17 +180,20 @@ static void\n #else\n         *out = a / b;\n #endif\n+        return 0;\n     }\n }\n \n #define @name@_ctype_floor_divide @name@_ctype_divide\n \n-static void\n+static NPY_INLINE int\n @name@_ctype_remainder(@type@ a, @type@ b, @type@ *out) {\n     if (a == 0 || b == 0) {\n-        if (b == 0) npy_set_floatstatus_divbyzero();\n         *out = 0;\n-        return;\n+        if (b == 0) {\n+            return NPY_FPE_DIVIDEBYZERO;\n+        }\n+        return 0;\n     }\n #if @neg@\n     else if ((a > 0) == (b > 0)) {\n@@ -198,17 +207,23 @@ static void\n #else\n     *out = a % b;\n #endif\n+    return 0;\n }\n /**end repeat**/\n \n /**begin repeat\n  *\n  * #name = byte, ubyte, short, ushort, int, uint, long,\n  *         ulong, longlong, ulonglong#\n- * #otyp = npy_float*4, npy_double*6#\n  */\n-#define @name@_ctype_true_divide(a, b, out)     \\\n-    *(out) = ((@otyp@) (a)) / ((@otyp@) (b));\n+\n+static NPY_INLINE int\n+@name@_ctype_true_divide(npy_@name@ a, npy_@name@ b, npy_double *out)\n+{\n+    *out = (npy_double)a / (npy_double)b;\n+    return 0;\n+}\n+\n /**end repeat**/\n \n /* b will always be positive in this call */\n@@ -221,17 +236,17 @@ static void\n  * #upc = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n  *        LONG, ULONG, LONGLONG, ULONGLONG#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_power(@type@ a, @type@ b, @type@ *out) {\n     @type@ tmp;\n \n     if (b == 0) {\n         *out = 1;\n-        return;\n+        return 0;\n     }\n     if (a == 1) {\n         *out = 1;\n-        return;\n+        return 0;\n     }\n \n     tmp = b & 1 ? a : 1;\n@@ -244,6 +259,7 @@ static void\n         b >>= 1;\n     }\n     *out = tmp;\n+    return 0;\n }\n /**end repeat**/\n \n@@ -261,12 +277,28 @@ static void\n  * #op = &, ^, |#\n  */\n \n-#define @name@_ctype_@oper@(arg1, arg2, out) *(out) = (arg1) @op@ (arg2)\n+static NPY_INLINE int\n+@name@_ctype_@oper@(@type@ arg1, @type@ arg2, @type@ *out)\n+{\n+    *out = arg1 @op@ arg2;\n+    return 0;\n+}\n \n /**end repeat1**/\n \n-#define @name@_ctype_lshift(arg1, arg2, out) *(out) = npy_lshift@suffix@(arg1, arg2)\n-#define @name@_ctype_rshift(arg1, arg2, out) *(out) = npy_rshift@suffix@(arg1, arg2)\n+static NPY_INLINE int\n+@name@_ctype_lshift(@type@ arg1, @type@ arg2, @type@ *out)\n+{\n+    *out = npy_lshift@suffix@(arg1, arg2);\n+    return 0;\n+}\n+\n+static NPY_INLINE int\n+@name@_ctype_rshift(@type@ arg1, @type@ arg2, @type@ *out)\n+{\n+    *out = npy_rshift@suffix@(arg1, arg2);\n+    return 0;\n+}\n \n /**end repeat**/\n \n@@ -275,135 +307,162 @@ static void\n  * #type = npy_float, npy_double, npy_longdouble#\n  * #c = f, , l#\n  */\n-#define @name@_ctype_add(a, b, outp) *(outp) = (a) + (b)\n-#define @name@_ctype_subtract(a, b, outp) *(outp) = (a) - (b)\n-#define @name@_ctype_multiply(a, b, outp) *(outp) = (a) * (b)\n-#define @name@_ctype_divide(a, b, outp) *(outp) = (a) / (b)\n+\n+/**begin repeat1\n+ * #OP = +, -, *, /#\n+ * #oper = add, subtract, multiply, divide#\n+ */\n+\n+static NPY_INLINE int\n+@name@_ctype_@oper@(@type@ a, @type@ b, @type@ *out)\n+{\n+    *out = a @OP@ b;\n+    return 0;\n+}\n+\n+/**end repeat1**/\n+\n #define @name@_ctype_true_divide @name@_ctype_divide\n \n \n-static void\n+static NPY_INLINE int\n @name@_ctype_floor_divide(@type@ a, @type@ b, @type@ *out) {\n     *out = npy_floor_divide@c@(a, b);\n+    return 0;\n }\n \n \n-static void\n+static NPY_INLINE int\n @name@_ctype_remainder(@type@ a, @type@ b, @type@ *out) {\n     *out = npy_remainder@c@(a, b);\n+    return 0;\n }\n \n \n-static void\n+static NPY_INLINE int\n @name@_ctype_divmod(@type@ a, @type@ b, @type@ *out1, @type@ *out2) {\n     *out1 = npy_divmod@c@(a, b, out2);\n+    return 0;\n }\n \n \n /**end repeat**/\n \n-#define half_ctype_add(a, b, outp) *(outp) = \\\n-        npy_float_to_half(npy_half_to_float(a) + npy_half_to_float(b))\n-#define half_ctype_subtract(a, b, outp) *(outp) = \\\n-        npy_float_to_half(npy_half_to_float(a) - npy_half_to_float(b))\n-#define half_ctype_multiply(a, b, outp) *(outp) = \\\n-        npy_float_to_half(npy_half_to_float(a) * npy_half_to_float(b))\n-#define half_ctype_divide(a, b, outp) *(outp) = \\\n-        npy_float_to_half(npy_half_to_float(a) / npy_half_to_float(b))\n+/**begin repeat\n+ * #OP = +, -, *, /#\n+ * #oper = add, subtract, multiply, divide#\n+ */\n+\n+static NPY_INLINE int\n+half_ctype_@oper@(npy_half a, npy_half b, npy_half *out)\n+{\n+    float res = npy_half_to_float(a) @OP@ npy_half_to_float(b);\n+    *out = npy_float_to_half(res);\n+    return 0;\n+}\n+\n+/**end repeat**/\n #define half_ctype_true_divide half_ctype_divide\n \n \n-static void\n-half_ctype_floor_divide(npy_half a, npy_half b, npy_half *out) {\n+static NPY_INLINE int\n+half_ctype_floor_divide(npy_half a, npy_half b, npy_half *out)\n+{\n     npy_half mod;\n \n     if (!b) {\n-        *out = a / b;\n-    } else {\n+        float res = npy_half_to_float(a) / npy_half_to_float(b);\n+        *out = npy_float_to_half(res);\n+    }\n+    else {\n         *out = npy_half_divmod(a, b, &mod);\n     }\n+    return 0;\n }\n \n \n-static void\n-half_ctype_remainder(npy_half a, npy_half b, npy_half *out) {\n+static NPY_INLINE int\n+half_ctype_remainder(npy_half a, npy_half b, npy_half *out)\n+{\n     npy_half_divmod(a, b, out);\n+    return 0;\n }\n \n \n-static void\n-half_ctype_divmod(npy_half a, npy_half b, npy_half *out1, npy_half *out2) {\n+static NPY_INLINE int\n+half_ctype_divmod(npy_half a, npy_half b, npy_half *out1, npy_half *out2)\n+{\n     *out1 = npy_half_divmod(a, b, out2);\n+    return 0;\n }\n \n /**begin repeat\n  * #name = cfloat, cdouble, clongdouble#\n+ * #type = npy_cfloat, npy_cdouble, npy_clongdouble#\n+ * #TYPE = CFLOAT, CDOUBLE, CLONGDOUBLE#\n  * #rname = float, double, longdouble#\n  * #rtype = npy_float, npy_double, npy_longdouble#\n  * #c = f,,l#\n  */\n-#define @name@_ctype_add(a, b, outp) do{        \\\n-    (outp)->real = (a).real + (b).real;         \\\n-    (outp)->imag = (a).imag + (b).imag;         \\\n-    } while(0)\n-#define @name@_ctype_subtract(a, b, outp) do{   \\\n-    (outp)->real = (a).real - (b).real;         \\\n-    (outp)->imag = (a).imag - (b).imag;         \\\n-    } while(0)\n-#define @name@_ctype_multiply(a, b, outp) do{                   \\\n-    (outp)->real = (a).real * (b).real - (a).imag * (b).imag;   \\\n-    (outp)->imag = (a).real * (b).imag + (a).imag * (b).real;   \\\n-    } while(0)\n-/* Algorithm identical to that in loops.c.src, for consistency */\n-#define @name@_ctype_divide(a, b, outp) do{                         \\\n-    @rtype@ in1r = (a).real;                                        \\\n-    @rtype@ in1i = (a).imag;                                        \\\n-    @rtype@ in2r = (b).real;                                        \\\n-    @rtype@ in2i = (b).imag;                                        \\\n-    @rtype@ in2r_abs = npy_fabs@c@(in2r);                           \\\n-    @rtype@ in2i_abs = npy_fabs@c@(in2i);                           \\\n-    if (in2r_abs >= in2i_abs) {                                     \\\n-        if (in2r_abs == 0 && in2i_abs == 0) {                       \\\n-            /* divide by zero should yield a complex inf or nan */  \\\n-            (outp)->real = in1r/in2r_abs;                           \\\n-            (outp)->imag = in1i/in2i_abs;                           \\\n-        }                                                           \\\n-        else {                                                      \\\n-            @rtype@ rat = in2i/in2r;                                \\\n-            @rtype@ scl = 1.0@c@/(in2r + in2i*rat);                 \\\n-            (outp)->real = (in1r + in1i*rat)*scl;                   \\\n-            (outp)->imag = (in1i - in1r*rat)*scl;                   \\\n-        }                                                           \\\n-    }                                                               \\\n-    else {                                                          \\\n-        @rtype@ rat = in2r/in2i;                                    \\\n-        @rtype@ scl = 1.0@c@/(in2i + in2r*rat);                     \\\n-        (outp)->real = (in1r*rat + in1i)*scl;                       \\\n-        (outp)->imag = (in1i*rat - in1r)*scl;                       \\\n-    }                                                               \\\n-    } while(0)\n+static NPY_INLINE int\n+@name@_ctype_add(@type@ a, @type@ b, @type@ *out)\n+{\n+    out->real = a.real + b.real;\n+    out->imag = a.imag + b.imag;\n+    return 0;\n+}\n+\n+static NPY_INLINE int\n+@name@_ctype_subtract(@type@ a, @type@ b, @type@ *out)\n+{\n+    out->real = a.real - b.real;\n+    out->imag = a.imag - b.imag;\n+    return 0;\n+}\n+\n+\n+/*\n+ * TODO: Mark as  to work around FPEs not being issues on clang 12.\n+ *       This should be removed when possible.\n+ */\n+static NPY_INLINE int\n+@name@_ctype_multiply( @type@ a, @type@ b, @type@ *out)\n+{\n+    out->real = a.real * b.real - a.imag * b.imag;\n+    out->imag = a.real * b.imag + a.imag * b.real;\n+    return 0;\n+}\n+\n+/* Use the ufunc loop directly to avoid duplicating the complicated logic */\n+static NPY_INLINE int\n+@name@_ctype_divide(@type@ a, @type@ b, @type@ *out)\n+{\n+    char *args[3] = {(char *)&a, (char *)&b, (char *)out};\n+    npy_intp steps[3];\n+    npy_intp size = 1;\n+    @TYPE@_divide(args, &size, steps, NULL);\n+    return 0;\n+}\n \n #define @name@_ctype_true_divide @name@_ctype_divide\n \n-#define @name@_ctype_floor_divide(a, b, outp) do {      \\\n-    @rname@_ctype_floor_divide(                         \\\n-        ((a).real*(b).real + (a).imag*(b).imag),        \\\n-        ((b).real*(b).real + (b).imag*(b).imag),        \\\n-        &((outp)->real));                               \\\n-    (outp)->imag = 0;                                   \\\n-    } while(0)\n /**end repeat**/\n \n \n \n /**begin repeat\n  * #name = byte, ubyte, short, ushort, int, uint, long, ulong,\n- *         longlong, ulonglong, cfloat, cdouble, clongdouble#\n+ *         longlong, ulonglong#\n  */\n-#define @name@_ctype_divmod(a, b, out, out2) {  \\\n-    @name@_ctype_floor_divide(a, b, out);       \\\n-    @name@_ctype_remainder(a, b, out2);         \\\n-    }\n+\n+static NPY_INLINE int\n+@name@_ctype_divmod(npy_@name@ a, npy_@name@ b, npy_@name@ *out, npy_@name@ *out2)\n+{\n+    int res = @name@_ctype_floor_divide(a, b, out);\n+    res |= @name@_ctype_remainder(a, b, out2);\n+    return res;\n+}\n+\n /**end repeat**/\n \n \n@@ -413,20 +472,22 @@ half_ctype_divmod(npy_half a, npy_half b, npy_half *out1, npy_half *out2) {\n  * #c = f,,l#\n  */\n \n-static void\n+static NPY_INLINE int\n @name@_ctype_power(@type@ a, @type@ b, @type@ *out)\n {\n     *out = npy_pow@c@(a, b);\n+    return 0;\n }\n \n /**end repeat**/\n-static void\n+static NPY_INLINE int\n half_ctype_power(npy_half a, npy_half b, npy_half *out)\n {\n     const npy_float af = npy_half_to_float(a);\n     const npy_float bf = npy_half_to_float(b);\n     const npy_float outf = npy_powf(af,bf);\n     *out = npy_float_to_half(outf);\n+    return 0;\n }\n \n /**begin repeat\n@@ -438,32 +499,36 @@ half_ctype_power(npy_half a, npy_half b, npy_half *out)\n  *         npy_float, npy_double, npy_longdouble#\n  * #uns = (0,1)*5,0*3#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_negative(@type@ a, @type@ *out)\n {\n+    *out = -a;\n #if @uns@\n-    npy_set_floatstatus_overflow();\n+    return NPY_FPE_OVERFLOW;\n+#else\n+    return 0;\n #endif\n-    *out = -a;\n }\n /**end repeat**/\n \n-static void\n+static NPY_INLINE int\n half_ctype_negative(npy_half a, npy_half *out)\n {\n     *out = a^0x8000u;\n+    return 0;\n }\n \n \n /**begin repeat\n  * #name = cfloat, cdouble, clongdouble#\n  * #type = npy_cfloat, npy_cdouble, npy_clongdouble#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_negative(@type@ a, @type@ *out)\n {\n     out->real = -a.real;\n     out->imag = -a.imag;\n+    return 0;\n }\n /**end repeat**/\n \n@@ -475,10 +540,11 @@ static void\n  *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n  *         npy_half, npy_float, npy_double, npy_longdouble#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_positive(@type@ a, @type@ *out)\n {\n     *out = a;\n+    return 0;\n }\n /**end repeat**/\n \n@@ -487,17 +553,19 @@ static void\n  * #type = npy_cfloat, npy_cdouble, npy_clongdouble#\n  * #c = f,,l#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_positive(@type@ a, @type@ *out)\n {\n     out->real = a.real;\n     out->imag = a.imag;\n+    return 0;\n }\n \n-static void\n+static NPY_INLINE int\n @name@_ctype_power(@type@ a, @type@ b, @type@ *out)\n {\n     *out = npy_cpow@c@(a, b);\n+    return 0;\n }\n /**end repeat**/\n \n@@ -515,10 +583,11 @@ static void\n  * #name = byte, short, int, long, longlong#\n  * #type = npy_byte, npy_short, npy_int, npy_long, npy_longlong#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_absolute(@type@ a, @type@ *out)\n {\n     *out = (a < 0 ? -a : a);\n+    return 0;\n }\n /**end repeat**/\n \n@@ -527,17 +596,19 @@ static void\n  * #type = npy_float, npy_double, npy_longdouble#\n  * #c = f,,l#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_absolute(@type@ a, @type@ *out)\n {\n     *out = npy_fabs@c@(a);\n+    return 0;\n }\n /**end repeat**/\n \n-static void\n+static NPY_INLINE int\n half_ctype_absolute(npy_half a, npy_half *out)\n {\n     *out = a&0x7fffu;\n+    return 0;\n }\n \n /**begin repeat\n@@ -546,10 +617,11 @@ half_ctype_absolute(npy_half a, npy_half *out)\n  * #rtype = npy_float, npy_double, npy_longdouble#\n  * #c = f,,l#\n  */\n-static void\n+static NPY_INLINE int\n @name@_ctype_absolute(@type@ a, @rtype@ *out)\n {\n     *out = npy_cabs@c@(a);\n+    return 0;\n }\n /**end repeat**/\n \n@@ -558,196 +630,400 @@ static void\n  *         ulong, longlong, ulonglong#\n  */\n \n-#define @name@_ctype_invert(a, out) *(out) = ~a;\n+static NPY_INLINE int\n+@name@_ctype_invert(npy_@name@ a, npy_@name@ *out)\n+{\n+    *out = ~a;\n+    return 0;\n+}\n \n /**end repeat**/\n \n /*** END OF BASIC CODE **/\n \n \n-/* The general strategy for commutative binary operators is to\n+/*\n+ * How binary operators work\n+ * -------------------------\n+ *\n+ * All binary (numeric) operators use the larger of the two types, with the\n+ * exception of unsigned int and signed int mixed cases which must promote\n+ * to a larger type.\n+ *\n+ * The strategy employed for all binary operation is that we coerce the other\n+ * scalar if it is safe to do.  E.g. `float64 + float32` the `float64` can\n+ * convert `float32` and do the operation as `float64 + float64`.\n+ * OTOH, for `float32 + float64` it is safe, and we should defer to `float64`.\n+ *\n+ * So we have multiple possible paths:\n+ * - The other scalar is a subclass.  In principle *both* inputs could be\n+ *   different subclasses.  In this case it would make sense to defer, but\n+ *   Python's `int` does not try this as well, so we do not here:\n+ *\n+ *      class A(int): pass\n+ *      class B(int):\n+ *          def __add__(self, other): return \"b\"\n+ *          __radd__ = __add__\n+ *\n+ *      A(1) + B(1)  # return 2\n+ *      B(1) + A(1)  # return \"b\"\n+ *\n+ * - The other scalar can be converted:  All is good, we do the operation\n+ * - The other scalar cannot be converted, there are two possibilities:\n+ *   - The reverse should work, so we return NotImplemented to defer.\n+ *     (If self is a subclass, this will end up in the \"unknown\" path.)\n+ *   - Neither works (e.g. `uint8 + int8`):  We currently use the array path.\n+ * - The other object is a unknown.  It could be either a scalar, an array,\n+ *   or an array-like (including a list!).  Because NumPy scalars pretend to be\n+ *   arrays we fall into the array fallback path here _normally_ (through\n+ *   the generic scalar path).\n+ *   First we check if we should defer, though.\n+ *\n+ * The last possibility is awkward and leads to very confusing situations.\n+ * The problem is that usually we should defer (return NotImplemented)\n+ * in that path.\n+ * If the other object is a NumPy array (or array-like) it will know what to\n+ * do.  If NumPy knows that it is a scalar (not generic `object`), then it\n+ * would make sense to try and use the \"array path\" (i.e. deal with it\n+ * using the ufunc machinery).\n+ *\n+ * But this overlooks two things that currently work:\n+ *\n+ * 1. `np.float64(3) * [1, 2, 3]`  happily returns an array result.\n+ * 2. `np.int32(3) * decimal.Decimal(3)` works!  (see below)\n+ *\n+ * The first must work, because scalars pretend to be arrays.  Which means\n+ * they inherit the greedy \"convert the other object to an array\" logic.\n+ * This may be a questionable choice, but is fine.\n+ * (As of now, it is not negotiable, since NumPy often converts 0-D arrays\n+ * to scalars.)\n+ *\n+ * The second one is more confusing.  This works also by using the ufunc\n+ * machinery (array path), but it works because:\n+ *\n+ *     np.add(np.int32(3), decimal.Decimal(3))\n+ *\n+ * Will convert the `int32` to an int32 array, and the decimal to an object\n+ * array.  It then *casts* the `int32` array to an object array.\n+ * The casting step CONVERTS the integer to a Python integer.  The ufunc object\n+ * loop will then call back into Python scalar logic.\n  *\n- * 1) Convert the types to the common type if both are scalars (0 return)\n- * 2) If both are not scalars use ufunc machinery (-2 return)\n- * 3) If both are scalars but cannot be cast to the right type\n- * return NotImplemented (-1 return)\n+ * The above would be recursive, if it was not for the conversion of the int32\n+ * to a Python integer!\n+ * This leads us to the EXCEEDINGLY IMPORTANT special case:\n  *\n- * 4) Perform the function on the C-type.\n- * 5) If an error condition occurred, check to see\n- * what the current error-handling is and handle the error.\n+ * WARNING: longdouble and clongdouble do NOT convert to a Python scalar\n+ *          when cast to object.  Thus they MUST NEVER take the array-path.\n+ *          However, they STILL should defer at least for\n+ *          `np.longdouble(3) + array`.\n  *\n- * 6) Construct and return the output scalar.\n+ *\n+ * As a general note, in the above we defer exactly when we know that deferring\n+ * will work.  `longdouble` uses the \"simple\" logic of generally deferring\n+ * though, because it would otherwise easily run into an infinite recursion.\n+ *\n+ *\n+ * The future?!\n+ * ------------\n+ *\n+ * This is very tricky and it would be nice to formalize away that \"recursive\"\n+ * path we currently use.  I (seberg) have currently no great idea on this,\n+ * this is more brainstorming!\n+ *\n+ * If both are scalars (known to NumPy), they have a DType and we may be able\n+ * to do the ufunc promotion to make sure there is no risk of recursion.\n+ *\n+ * In principle always deferring would probably be clean.  But we likely cannot\n+ * do that?  There is also an issue that it is nice that we allow adding a\n+ * DType for an existing Python scalar (which will not know about NumPy\n+ * scalars).\n+ * The DType/ufunc machinery teaches NumPy how arrays will work with that\n+ * Python scalar, but the DType may need to help us decide whether we should\n+ * defer (return NotImplemented) or try using the ufunc machinery (or a\n+ * simplified ufunc-like machinery limited to scalars).\n  */\n \n+\n+/*\n+ * Enum used to describe the space of possibilities when converting the second\n+ * argument to a binary operation.\n+ */\n+typedef enum {\n+    /* An error occurred (should not really happen/be possible) */\n+    CONVERSION_ERROR = -1,\n+    /* A known NumPy scalar, but of higher precision: we defer */\n+    DEFER_TO_OTHER_KNOWN_SCALAR,\n+    /* Conversion was successful (known scalar of less precision) */\n+    CONVERSION_SUCCESS,\n+    /*\n+     * Other object is an unkown scalar or array-like, we (typically) use\n+     * the generic path, which normally ends up in the ufunc machinery.\n+     */\n+    OTHER_IS_UNKNOWN_OBJECT,\n+    /*\n+     * Promotion necessary\n+     */\n+    PROMOTION_REQUIRED,\n+    /*\n+     * The other object may be a subclass, conversion is successful.  We do\n+     * not special case this as Python's `int` does not either\n+     */\n+    OTHER_IS_SUBCLASS,\n+} conversion_result;\n+\n /**begin repeat\n  * #name = byte, ubyte, short, ushort, int, uint,\n  *         long, ulong, longlong, ulonglong,\n- *         half, float, longdouble,\n+ *         half, float, double, longdouble,\n  *         cfloat, cdouble, clongdouble#\n- * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n- *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n- *         npy_half, npy_float, npy_longdouble,\n- *         npy_cfloat, npy_cdouble, npy_clongdouble#\n  * #Name = Byte, UByte, Short, UShort, Int, UInt,\n  *         Long, ULong, LongLong, ULongLong,\n- *         Half, Float, LongDouble,\n+ *         Half, Float, Double, LongDouble,\n  *         CFloat, CDouble, CLongDouble#\n- * #TYPE = NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT,\n- *         NPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG,\n- *         NPY_HALF, NPY_FLOAT, NPY_LONGDOUBLE,\n- *         NPY_CFLOAT, NPY_CDOUBLE, NPY_CLONGDOUBLE#\n+ * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n+ *         LONG, ULONG, LONGLONG, ULONGLONG,\n+ *         HALF, FLOAT, DOUBLE, LONGDOUBLE,\n+ *         CFLOAT, CDOUBLE, CLONGDOUBLE#\n+ * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n+ *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n+ *         npy_half, npy_float, npy_double, npy_longdouble,\n+ *         npy_cfloat, npy_cdouble, npy_clongdouble#\n  */\n \n-static int\n-_@name@_convert_to_ctype(PyObject *a, @type@ *arg1)\n-{\n-    PyObject *temp;\n-\n-    if (PyArray_IsScalar(a, @Name@)) {\n-        *arg1 = PyArrayScalar_VAL(a, @Name@);\n-        return 0;\n-    }\n-    else if (PyArray_IsScalar(a, Generic)) {\n-        PyArray_Descr *descr1;\n+#define IS_@TYPE@ 1\n \n-        if (!PyArray_IsScalar(a, Number)) {\n-            return -1;\n-        }\n-        descr1 = PyArray_DescrFromTypeObject((PyObject *)Py_TYPE(a));\n-        if (PyArray_CanCastSafely(descr1->type_num, @TYPE@)) {\n-            PyArray_CastScalarDirect(a, descr1, arg1, @TYPE@);\n-            Py_DECREF(descr1);\n-            return 0;\n-        }\n-        else {\n-            Py_DECREF(descr1);\n-            return -1;\n-        }\n-    }\n-    else if (PyArray_GetPriority(a, NPY_PRIORITY) > NPY_PRIORITY) {\n-        return -2;\n-    }\n-    else if ((temp = PyArray_ScalarFromObject(a)) != NULL) {\n-        int retval = _@name@_convert_to_ctype(temp, arg1);\n+#define IS_SAFE(FROM, TO) _npy_can_cast_safely_table[FROM][TO]\n \n-        Py_DECREF(temp);\n-        return retval;\n-    }\n-    return -2;\n-}\n+/*\n+ * TODO: This whole thing is awkward, and we should create a helper header to\n+ *       define inline functions that convert single elements for all numeric\n+ *       types.  That could then also be used to define all cast loops.\n+ *       (Even if that may get more complex for SIMD at some point.)\n+ *       For now, half casts could be optimized because of that.\n+ */\n \n-/**end repeat**/\n+#if defined(IS_HALF)\n+    #define CONVERT_TO_RESULT(value)  \\\n+        *result = npy_float_to_half((float)(value))\n+#elif defined(IS_CFLOAT) || defined(IS_CDOUBLE) || defined(IS_CLONGDOUBLE)\n+    #define CONVERT_TO_RESULT(value)  \\\n+        result->real = value;  \\\n+        result->imag = 0\n+#else\n+    #define CONVERT_TO_RESULT(value) *result = value\n+#endif\n \n \n-/* Same as above but added exact checks against known python types for speed */\n+#define GET_VALUE_OR_DEFER(OTHER, Other, value)  \\\n+    case NPY_##OTHER:  \\\n+        if (IS_SAFE(NPY_##OTHER, NPY_@TYPE@)) {  \\\n+            assert(Py_TYPE(value) == &Py##Other##ArrType_Type);  \\\n+            CONVERT_TO_RESULT(PyArrayScalar_VAL(value, Other));  \\\n+            ret = CONVERSION_SUCCESS;  \\\n+        }  \\\n+        else if (IS_SAFE(NPY_@TYPE@, NPY_##OTHER)) {  \\\n+            /*\n+             * If self can cast safely to other, this is clear:\n+             * we should definitely defer.\n+             */  \\\n+             ret = DEFER_TO_OTHER_KNOWN_SCALAR;  \\\n+        }  \\\n+        else {  \\\n+            /* Otherwise, we must promote */  \\\n+            ret = PROMOTION_REQUIRED;  \\\n+        }  \\\n+        break;\n \n-/**begin repeat\n- * #name = double#\n- * #type = npy_double#\n- * #Name = Double#\n- * #TYPE = NPY_DOUBLE#\n- * #PYCHECKEXACT = PyFloat_CheckExact#\n- * #PYEXTRACTCTYPE = PyFloat_AS_DOUBLE#\n+/*\n+ * Complex to complex (and rejecting complex to real) is a bit different:\n  */\n \n-static int\n-_@name@_convert_to_ctype(PyObject *a, @type@ *arg1)\n-{\n-    PyObject *temp;\n-\n-    if (@PYCHECKEXACT@(a)){\n-        *arg1 = @PYEXTRACTCTYPE@(a);\n-        return 0;\n-    }\n-\n-    if (PyArray_IsScalar(a, @Name@)) {\n-        *arg1 = PyArrayScalar_VAL(a, @Name@);\n-        return 0;\n-    }\n-    else if (PyArray_IsScalar(a, Generic)) {\n-        PyArray_Descr *descr1;\n+#if defined(IS_CFLOAT) || defined(IS_CDOUBLE) || defined(IS_CLONGDOUBLE)\n+\n+#define GET_CVALUE_OR_DEFER(OTHER, Other, value)  \\\n+    case NPY_##OTHER:  \\\n+        if (IS_SAFE(NPY_##OTHER, NPY_@TYPE@)) {  \\\n+            assert(Py_TYPE(value) == &Py##Other##ArrType_Type);  \\\n+            result->real = PyArrayScalar_VAL(value, Other).real;  \\\n+            result->imag = PyArrayScalar_VAL(value, Other).imag;  \\\n+            ret = 1;  \\\n+        }  \\\n+        else if (IS_SAFE(NPY_@TYPE@, NPY_##OTHER)) {  \\\n+             ret = DEFER_TO_OTHER_KNOWN_SCALAR;  \\\n+        }  \\\n+        else {  \\\n+            ret = PROMOTION_REQUIRED;  \\\n+        }  \\\n+        break;\n \n-        if (!PyArray_IsScalar(a, Number)) {\n-            return -1;\n-        }\n-        descr1 = PyArray_DescrFromTypeObject((PyObject *)Py_TYPE(a));\n-        if (PyArray_CanCastSafely(descr1->type_num, @TYPE@)) {\n-            PyArray_CastScalarDirect(a, descr1, arg1, @TYPE@);\n-            Py_DECREF(descr1);\n-            return 0;\n-        }\n-        else {\n-            Py_DECREF(descr1);\n-            return -1;\n-        }\n-    }\n-    else if (PyArray_GetPriority(a, NPY_PRIORITY) > NPY_PRIORITY) {\n-        return -2;\n-    }\n-    else if ((temp = PyArray_ScalarFromObject(a)) != NULL) {\n-        int retval = _@name@_convert_to_ctype(temp, arg1);\n+#else\n \n-        Py_DECREF(temp);\n-        return retval;\n-    }\n-    return -2;\n-}\n+/* Getting a complex value to real is never safe: */\n+#define GET_CVALUE_OR_DEFER(OTHER, Other, value)  \\\n+    case NPY_##OTHER:  \\\n+        if (IS_SAFE(NPY_@TYPE@, NPY_##OTHER)) {  \\\n+            ret = DEFER_TO_OTHER_KNOWN_SCALAR;  \\\n+        }  \\\n+        else {  \\\n+            ret = PROMOTION_REQUIRED;  \\\n+        }  \\\n+        break;\n \n-/**end repeat**/\n+#endif\n \n \n-/**begin repeat\n- * #name = byte, ubyte, short, ushort, int, uint,\n- *         long, ulong, longlong, ulonglong,\n- *         half, float, double, cfloat, cdouble#\n- * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n- *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n- *         npy_half, npy_float, npy_double, npy_cfloat, npy_cdouble#\n+/**\n+ * Convert the value to the own type and and store the result.\n+ *\n+ * @param value The value to convert (if compatible)\n+ * @param result The result value (output)\n+ * @result The result value indicating what we did with `value` or what type\n+ *         of object it is (see `conversion_result`).\n  */\n-static int\n-_@name@_convert2_to_ctypes(PyObject *a, @type@ *arg1,\n-                           PyObject *b, @type@ *arg2)\n+static NPY_INLINE conversion_result\n+convert_to_@name@(PyObject *value, @type@ *result)\n {\n-    int ret;\n-    ret = _@name@_convert_to_ctype(a, arg1);\n-    if (ret < 0) {\n-        return ret;\n+    if (Py_TYPE(value) == &Py@Name@ArrType_Type) {\n+        *result = PyArrayScalar_VAL(value, @Name@);\n+        return CONVERSION_SUCCESS;\n     }\n-    ret = _@name@_convert_to_ctype(b, arg2);\n-    if (ret < 0) {\n-        return ret;\n+    /* Optimize the identical scalar specifically. */\n+    if (PyArray_IsScalar(value, @Name@)) {\n+        *result = PyArrayScalar_VAL(value, @Name@);\n+        /*\n+         * In principle special, assyemetric, handling could be possible for\n+         * subclasses.  But in practice even we do not bother later.\n+         */\n+        return OTHER_IS_SUBCLASS;\n     }\n-    return 0;\n-}\n-/**end repeat**/\n \n-/**begin repeat\n- * #name = longdouble, clongdouble#\n- * #type = npy_longdouble, npy_clongdouble#\n- */\n+    /*\n+     * Then we check for the basic Python types float, int, and complex.\n+     * (this is a bit tedious to do right for complex).\n+     */\n+    if (PyBool_Check(value)) {\n+        CONVERT_TO_RESULT(value == Py_True);\n+        return CONVERSION_SUCCESS;\n+    }\n \n-static int\n-_@name@_convert2_to_ctypes(PyObject *a, @type@ *arg1,\n-                           PyObject *b, @type@ *arg2)\n-{\n-    int ret;\n-    ret = _@name@_convert_to_ctype(a, arg1);\n-    if (ret == -2) {\n-        ret = -3;\n+    if (IS_SAFE(NPY_DOUBLE, NPY_@TYPE@) && PyFloat_CheckExact(value)) {\n+        CONVERT_TO_RESULT(PyFloat_AS_DOUBLE(value));\n+        return CONVERSION_SUCCESS;\n     }\n-    if (ret < 0) {\n-        return ret;\n+\n+    if (IS_SAFE(NPY_LONG, NPY_@TYPE@) && PyLong_CheckExact(value)) {\n+        int overflow;\n+        long val = PyLong_AsLongAndOverflow(value, &overflow);\n+        if (overflow) {\n+            return OTHER_IS_UNKNOWN_OBJECT;  /* handle as if arbitrary object */\n+        }\n+        if (error_converting(val)) {\n+            return CONVERSION_ERROR;  /* should not be possible */\n+        }\n+        CONVERT_TO_RESULT(val);\n+        return CONVERSION_SUCCESS;\n     }\n-    ret = _@name@_convert_to_ctype(b, arg2);\n-    if (ret == -2) {\n-        ret = -3;\n+\n+#if defined(IS_CFLOAT) || defined(IS_CDOUBLE) || defined(IS_CLONGDOUBLE)\n+    if (IS_SAFE(NPY_CDOUBLE, NPY_@TYPE@) && PyComplex_CheckExact(value)) {\n+        Py_complex val = PyComplex_AsCComplex(value);\n+        if (error_converting(val.real)) {\n+            return CONVERSION_ERROR;  /* should not be possible */\n+        }\n+        result->real = val.real;\n+        result->imag = val.imag;\n+        return CONVERSION_SUCCESS;\n     }\n-    if (ret < 0) {\n-        return ret;\n+#endif  /* defined(IS_CFLOAT) || ... */\n+\n+    PyObject *dtype = PyArray_DiscoverDTypeFromScalarType(Py_TYPE(value));\n+    if (dtype == Py_None) {\n+        Py_DECREF(dtype);\n+        /* Signal that this is an array or array-like: Defer to array logic */\n+        return OTHER_IS_UNKNOWN_OBJECT;\n     }\n-    return 0;\n+    else if (dtype == NULL) {\n+        /*\n+         * The input is an unknown python object.  This should probably defer\n+         * but only does so for float128.\n+         * For all other cases, we defer to the array logic.  If the object\n+         * is indeed not an array-like, this will end up converting the NumPy\n+         * scalar to a Python scalar and then try again.\n+         * The logic is that the ufunc casts the input to object, which does\n+         * the conversion.\n+         */\n+        return OTHER_IS_UNKNOWN_OBJECT;\n+    }\n+    /*\n+     * Otherwise, we have a clear NumPy scalar, find if it is a compatible\n+     * builtin scalar.\n+     * Each `GET_VALUE_OR_DEFER` represents a case clause for its type number,\n+     * extracting the value if it is safe and otherwise deferring.\n+     * (Safety is known at compile time, so the switch statement should be\n+     * simplified by the compiler accordingly.)\n+     * If we have a scalar that is not listed or not safe, we defer to it.\n+     *\n+     * We should probably defer more aggressively, but that is too big a change,\n+     * since it would disable `np.float64(1.) * [1, 2, 3, 4]`.\n+     */\n+    int ret;  /* set by the GET_VALUE_OR_DEFER macro */\n+    switch (((PyArray_DTypeMeta *)dtype)->type_num) {\n+        GET_VALUE_OR_DEFER(BOOL, Bool, value);\n+        /* UInts */\n+        GET_VALUE_OR_DEFER(UBYTE, UByte, value);\n+        GET_VALUE_OR_DEFER(USHORT, UShort, value);\n+        GET_VALUE_OR_DEFER(UINT, UInt, value);\n+        GET_VALUE_OR_DEFER(ULONG, ULong, value);\n+        GET_VALUE_OR_DEFER(ULONGLONG, ULongLong, value);\n+        /* Ints */\n+        GET_VALUE_OR_DEFER(BYTE, Byte, value);\n+        GET_VALUE_OR_DEFER(SHORT, Short, value);\n+        GET_VALUE_OR_DEFER(INT, Int, value);\n+        GET_VALUE_OR_DEFER(LONG, Long, value);\n+        GET_VALUE_OR_DEFER(LONGLONG, LongLong, value);\n+        /* Floats */\n+        case NPY_HALF:\n+            if (IS_SAFE(NPY_HALF, NPY_@TYPE@)) {\n+                assert(Py_TYPE(value) == &PyHalfArrType_Type);\n+                CONVERT_TO_RESULT(npy_half_to_float(PyArrayScalar_VAL(value, Half)));\n+                ret = 1;\n+            }\n+            else if (IS_SAFE(NPY_@TYPE@, NPY_HALF)) {\n+                ret = DEFER_TO_OTHER_KNOWN_SCALAR;\n+            }\n+            else {\n+                ret = PROMOTION_REQUIRED;\n+            }\n+            break;\n+        GET_VALUE_OR_DEFER(FLOAT, Float, value);\n+        GET_VALUE_OR_DEFER(DOUBLE, Double, value);\n+        GET_VALUE_OR_DEFER(LONGDOUBLE, LongDouble, value);\n+        /* Complex: We should still defer, but the code won't work... */\n+        GET_CVALUE_OR_DEFER(CFLOAT, CFloat, value);\n+        GET_CVALUE_OR_DEFER(CDOUBLE, CDouble, value);\n+        GET_CVALUE_OR_DEFER(CLONGDOUBLE, CLongDouble, value);\n+        default:\n+            /*\n+             * If there is no match, this is an unknown scalar object.  It\n+             * would make sense to defer generously here, but it should also\n+             * always be safe to use the array path.\n+             * The issue is, that the other scalar may or may not be designed\n+             * to deal with NumPy scalars.  Without knowing that, we cannot\n+             * defer (which would be much faster potentially).\n+             * TODO: We could add a DType flag to allow opting in to deferring!\n+             */\n+            ret = OTHER_IS_UNKNOWN_OBJECT;\n+    }\n+    Py_DECREF(dtype);\n+    return ret;\n }\n \n+#undef IS_SAFE\n+#undef CONVERT_TO_RESULT\n+#undef GET_VALUE_OR_DEFER\n+#undef GET_CVALUE_OR_DEFER\n+#undef IS_@TYPE@\n+\n /**end repeat**/\n \n \n@@ -756,102 +1032,145 @@ _@name@_convert2_to_ctypes(PyObject *a, @type@ *arg1,\n  * #name = (byte, ubyte, short, ushort, int, uint,\n  *             long, ulong, longlong, ulonglong)*12,\n  *         (half, float, double, longdouble,\n- *             cfloat, cdouble, clongdouble)*5,\n- *         (half, float, double, longdouble)*2#\n+ *             cfloat, cdouble, clongdouble)*4,\n+ *         (half, float, double, longdouble)*3#\n  * #Name = (Byte, UByte, Short, UShort, Int, UInt,\n  *             Long, ULong,LongLong,ULongLong)*12,\n  *         (Half, Float, Double, LongDouble,\n- *             CFloat, CDouble, CLongDouble)*5,\n- *         (Half, Float, Double, LongDouble)*2#\n+ *             CFloat, CDouble, CLongDouble)*4,\n+ *         (Half, Float, Double, LongDouble)*3#\n  * #type = (npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n  *             npy_long, npy_ulong, npy_longlong, npy_ulonglong)*12,\n  *         (npy_half, npy_float, npy_double, npy_longdouble,\n- *             npy_cfloat, npy_cdouble, npy_clongdouble)*5,\n- *         (npy_half, npy_float, npy_double, npy_longdouble)*2#\n+ *             npy_cfloat, npy_cdouble, npy_clongdouble)*4,\n+ *         (npy_half, npy_float, npy_double, npy_longdouble)*3#\n  *\n  * #oper = add*10, subtract*10, multiply*10, remainder*10,\n  *         divmod*10, floor_divide*10, lshift*10, rshift*10, and*10,\n  *         or*10, xor*10, true_divide*10,\n- *         add*7, subtract*7, multiply*7, floor_divide*7, true_divide*7,\n- *         divmod*4, remainder*4#\n+ *         add*7, subtract*7, multiply*7, true_divide*7,\n+ *         floor_divide*4, divmod*4, remainder*4#\n  *\n- * #fperr = 1*60,0*50,1*10,\n- *          1*35,\n- *          1*8#\n+ * #fperr = 0*110, 1*10,\n+ *          1*28, 1*12#\n  * #twoout = 0*40,1*10,0*70,\n- *           0*35,\n- *           1*4,0*4#\n+ *           0*28,\n+ *           0*4,1*4,0*4#\n  * #otype = (npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n  *             npy_long, npy_ulong, npy_longlong, npy_ulonglong)*11,\n- *         npy_float*4, npy_double*6,\n+ *         npy_double*10,\n  *         (npy_half, npy_float, npy_double, npy_longdouble,\n- *             npy_cfloat, npy_cdouble, npy_clongdouble)*5,\n- *         (npy_half, npy_float, npy_double, npy_longdouble)*2#\n+ *             npy_cfloat, npy_cdouble, npy_clongdouble)*4,\n+ *         (npy_half, npy_float, npy_double, npy_longdouble)*3#\n  * #OName = (Byte, UByte, Short, UShort, Int, UInt,\n  *              Long, ULong, LongLong, ULongLong)*11,\n- *          Float*4, Double*6,\n+ *          Double*10,\n  *          (Half, Float, Double, LongDouble,\n- *              CFloat, CDouble, CLongDouble)*5,\n- *          (Half, Float, Double, LongDouble)*2#\n+ *              CFloat, CDouble, CLongDouble)*4,\n+ *          (Half, Float, Double, LongDouble)*3#\n  */\n+#define IS_@name@\n \n static PyObject *\n @name@_@oper@(PyObject *a, PyObject *b)\n {\n     PyObject *ret;\n-    @type@ arg1, arg2;\n-    @otype@ out;\n+    @type@ arg1, arg2, other_val;\n \n-#if @twoout@\n-    @otype@ out2;\n-    PyObject *obj;\n-#endif\n-\n-#if @fperr@\n-    int retstatus;\n-    int first;\n-#endif\n+    /*\n+     * Check if this operation may be considered forward.  Note `is_forward`\n+     * does not imply that we can defer to a subclass `b`, we need to check\n+     * `BINOP_IS_FORWARD` for that (it takes into account that both may be\n+     * identicalclass).\n+     */\n+    int is_forward = (Py_TYPE(a)->tp_as_number != NULL\n+            && (void *)(Py_TYPE(a)->tp_as_number->nb_@oper@) == (void*)(@name@_@oper@));\n \n-    BINOP_GIVE_UP_IF_NEEDED(a, b, nb_@oper@, @name@_@oper@);\n+    /*\n+     * Extract the other value (if it is compatible).  Otherwise, decide\n+     * how to deal with it.  This is somewhat complicated.\n+     *\n+     * Note: This pattern is used multiple times below.\n+     */\n+    PyObject *other = is_forward ? b : a;\n \n-    switch(_@name@_convert2_to_ctypes(a, &arg1, b, &arg2)) {\n-        case 0:\n-            break;\n-        case -1:\n-            /* one of them can't be cast safely must be mixed-types*/\n-            return PyArray_Type.tp_as_number->nb_@oper@(a,b);\n-        case -2:\n-            /* use default handling */\n-            if (PyErr_Occurred()) {\n-                return NULL;\n-            }\n+    conversion_result res = convert_to_@name@(other, &other_val);\n+    switch (res) {\n+        case CONVERSION_ERROR:\n+            return NULL;  /* an error occurred (should never happen) */\n+        case DEFER_TO_OTHER_KNOWN_SCALAR:\n+            /*\n+             * defer to other;  This is normally a forward operation.  However,\n+             * it could be backward if an operation is undefined forward.\n+             * An example is the complex remainder `complex % bool` will defer\n+             * even though it would normally handle the operation.\n+             */\n+            Py_RETURN_NOTIMPLEMENTED;\n+        case CONVERSION_SUCCESS:\n+            break;  /* successfully extracted value we can proceed */\n+        case OTHER_IS_UNKNOWN_OBJECT:\n+#if defined(IS_longdouble) || defined(IS_clongdouble)\n+            Py_RETURN_NOTIMPLEMENTED;\n+#endif\n+            BINOP_GIVE_UP_IF_NEEDED(a, b, nb_@oper@, @name@_@oper@);\n+        case PROMOTION_REQUIRED:\n+            /*\n+             * Either an array-like, unknown scalar or we need to promote.\n+             *\n+             * TODO: We could special case the promotion case here for much\n+             *       better speed and to deal with integer overflow warnings\n+             *       correctly.  (e.g. `uint8 * int8` cannot warn).\n+             */\n             return PyGenericArrType_Type.tp_as_number->nb_@oper@(a,b);\n-        case -3:\n+        case OTHER_IS_SUBCLASS:\n             /*\n-             * special case for longdouble and clongdouble\n-             * because they have a recursive getitem in their dtype\n+             * Success converting.  We _could_ in principle defer in cases\n+             * were the other subclass does not inherit the behavior.  In\n+             * practice not even Python's `int` attempt this, so we also punt.\n              */\n-            Py_INCREF(Py_NotImplemented);\n-            return Py_NotImplemented;\n+            break;\n     }\n \n #if @fperr@\n-    npy_clear_floatstatus_barrier((char*)&out);\n+    npy_clear_floatstatus_barrier((char*)&arg1);\n+#endif\n+    if (is_forward) {\n+        arg1 = PyArrayScalar_VAL(a, @Name@);\n+        arg2 = other_val;\n+    }\n+    else {\n+        arg1 = other_val;\n+        arg2 = PyArrayScalar_VAL(b, @Name@);\n+    }\n+\n+    /*\n+     * Prepare the actual calculation.\n+     */\n+    @otype@ out;\n+#if @twoout@\n+    @otype@ out2;\n+    PyObject *obj;\n #endif\n \n+\n+\n     /*\n      * here we do the actual calculation with arg1 and arg2\n      * as a function call.\n+     * Note that `retstatus` is the \"floating point error\" value for integer\n+     * functions.  Float functions should always return 0, and then use\n+     * the following `npy_get_floatstatus_barrier`.\n      */\n #if @twoout@\n-    @name@_ctype_@oper@(arg1, arg2, (@otype@ *)&out, &out2);\n+    int retstatus = @name@_ctype_@oper@(arg1, arg2, &out, &out2);\n #else\n-    @name@_ctype_@oper@(arg1, arg2, (@otype@ *)&out);\n+    int retstatus = @name@_ctype_@oper@(arg1, arg2, &out);\n #endif\n \n #if @fperr@\n     /* Check status flag.  If it is set, then look up what to do */\n-    retstatus = npy_get_floatstatus_barrier((char*)&out);\n+    retstatus |= npy_get_floatstatus_barrier((char*)&out);\n+#endif\n     if (retstatus) {\n         int bufsize, errmask;\n         PyObject *errobj;\n@@ -860,14 +1179,13 @@ static PyObject *\n                                 &errobj) < 0) {\n             return NULL;\n         }\n-        first = 1;\n+        int first = 1;\n         if (PyUFunc_handlefperr(errmask, errobj, retstatus, &first)) {\n             Py_XDECREF(errobj);\n             return NULL;\n         }\n         Py_XDECREF(errobj);\n     }\n-#endif\n \n \n #if @twoout@\n@@ -899,6 +1217,9 @@ static PyObject *\n     return ret;\n }\n \n+\n+#undef IS_@name@\n+\n /**end repeat**/\n \n #define _IS_ZERO(x) (x == 0)\n@@ -920,64 +1241,78 @@ static PyObject *\n  *         Half, Float, Double, LongDouble,\n  *         CFloat, CDouble, CLongDouble#\n  *\n- * #oname = float*4, double*6, half, float, double, longdouble,\n- *          cfloat, cdouble, clongdouble#\n- *\n- * #otype = npy_float*4, npy_double*6, npy_half, npy_float,\n- *          npy_double, npy_longdouble,\n- *          npy_cfloat, npy_cdouble, npy_clongdouble#\n- *\n- * #OName = Float*4, Double*6, Half, Float,\n- *          Double, LongDouble,\n- *          CFloat, CDouble, CLongDouble#\n- *\n  * #isint = 1*10,0*7#\n  * #isuint = (0,1)*5,0*7#\n  * #cmplx = 0*14,1*3#\n  * #iszero = _IS_ZERO*10, npy_half_iszero, _IS_ZERO*6#\n  * #zero = 0*10, NPY_HALF_ZERO, 0*6#\n  * #one = 1*10, NPY_HALF_ONE, 1*6#\n  */\n+#define IS_@name@\n \n static PyObject *\n @name@_power(PyObject *a, PyObject *b, PyObject *modulo)\n {\n+    if (modulo != Py_None) {\n+        /* modular exponentiation is not implemented (gh-8804) */\n+        Py_INCREF(Py_NotImplemented);\n+        return Py_NotImplemented;\n+    }\n+\n     PyObject *ret;\n-    @type@ arg1, arg2, out;\n+    @type@ arg1, arg2, other_val;\n \n-    BINOP_GIVE_UP_IF_NEEDED(a, b, nb_power, @name@_power);\n+    int is_forward = (Py_TYPE(a)->tp_as_number != NULL\n+            && (void *)(Py_TYPE(a)->tp_as_number->nb_power) == (void*)(@name@_power));\n \n-    switch(_@name@_convert2_to_ctypes(a, &arg1, b, &arg2)) {\n-        case 0:\n-            break;\n-        case -1:\n-            /* can't cast both safely mixed-types? */\n-            return PyArray_Type.tp_as_number->nb_power(a,b,modulo);\n-        case -2:\n-            /* use default handling */\n-            if (PyErr_Occurred()) {\n-                return NULL;\n-            }\n-            return PyGenericArrType_Type.tp_as_number->nb_power(a,b,modulo);\n-        case -3:\n-        default:\n+    /*\n+     * Extract the other value (if it is compatible). See the generic\n+     * (non power) version above for detailed notes.\n+     */\n+    PyObject *other = is_forward ? b : a;\n+\n+    int res = convert_to_@name@(other, &other_val);\n+    switch (res) {\n+        case CONVERSION_ERROR:\n+            return NULL;  /* an error occurred (should never happen) */\n+        case DEFER_TO_OTHER_KNOWN_SCALAR:\n+            Py_RETURN_NOTIMPLEMENTED;\n+        case CONVERSION_SUCCESS:\n+            break;  /* successfully extracted value we can proceed */\n+        case OTHER_IS_UNKNOWN_OBJECT:\n+#if defined(IS_longdouble) || defined(IS_clongdouble)\n+            Py_RETURN_NOTIMPLEMENTED;\n+#endif\n+            BINOP_GIVE_UP_IF_NEEDED(a, b, nb_power, @name@_power);\n+        case PROMOTION_REQUIRED:\n+            return PyGenericArrType_Type.tp_as_number->nb_power(a, b, modulo);\n+        case OTHER_IS_SUBCLASS:\n             /*\n-             * special case for longdouble and clongdouble\n-             * because they have a recursive getitem in their dtype\n+             * Success converting.  We _could_ in principle defer in cases\n+             * were the other subclass does not inherit the behavior.  In\n+             * practice not even Python's `int` attempt this, so we also punt.\n              */\n-            Py_INCREF(Py_NotImplemented);\n-            return Py_NotImplemented;\n-    }\n-\n-    if (modulo != Py_None) {\n-        /* modular exponentiation is not implemented (gh-8804) */\n-        Py_INCREF(Py_NotImplemented);\n-        return Py_NotImplemented;\n+            break;\n     }\n \n #if !@isint@\n-    npy_clear_floatstatus_barrier((char*)&out);\n+    npy_clear_floatstatus_barrier((char*)&arg1);\n #endif\n+\n+    if (is_forward) {\n+        arg1 = PyArrayScalar_VAL(a, @Name@);\n+        arg2 = other_val;\n+    }\n+    else {\n+        arg1 = other_val;\n+        arg2 = PyArrayScalar_VAL(b, @Name@);\n+    }\n+\n+    /*\n+     * Prepare the actual calculation:\n+     */\n+    @type@ out;\n+\n     /*\n      * here we do the actual calculation with arg1 and arg2\n      * as a function call.\n@@ -989,11 +1324,12 @@ static PyObject *\n         return NULL;\n     }\n #endif\n-    @name@_ctype_power(arg1, arg2, &out);\n+    int retstatus = @name@_ctype_power(arg1, arg2, &out);\n \n #if !@isint@\n     /* Check status flag.  If it is set, then look up what to do */\n-    int retstatus = npy_get_floatstatus_barrier((char*)&out);\n+    retstatus |= npy_get_floatstatus_barrier((char*)&out);\n+#endif\n     if (retstatus) {\n         int bufsize, errmask;\n         PyObject *errobj;\n@@ -1009,7 +1345,6 @@ static PyObject *\n         }\n         Py_XDECREF(errobj);\n     }\n-#endif\n \n     ret = PyArrayScalar_New(@Name@);\n     if (ret == NULL) {\n@@ -1021,78 +1356,28 @@ static PyObject *\n }\n \n \n+#undef IS_@name@\n /**end repeat**/\n #undef _IS_ZERO\n \n \n /**begin repeat\n  *\n- * #name = cfloat, cdouble#\n- *\n- */\n-\n-/**begin repeat1\n- *\n- * #oper = divmod, remainder#\n- *\n- */\n-\n-#define @name@_@oper@ NULL\n-\n-/**end repeat1**/\n-\n-/**end repeat**/\n-\n-/**begin repeat\n- *\n- * #oper = divmod, remainder#\n+ * #name = (cfloat, cdouble, clongdouble)*3#\n+ * #oper = floor_divide*3, divmod*3, remainder*3#\n  *\n  */\n \n /* \n-Complex numbers do not support remainder operations. Unfortunately, \n-the type inference for long doubles is complicated, and if a remainder \n-operation is not defined - if the relevant field is left NULL - then \n-operations between long doubles and objects lead to an infinite recursion \n-instead of a TypeError. This should ensure that once everything gets\n-converted to complex long doubles you correctly get a reasonably\n-informative TypeError. This fixes the last part of bug gh-18548.\n-*/\n-\n+ * Complex numbers do not support remainder so we manually make sure that the\n+ * operation is not defined.  This is/was especially important for longdoubles\n+ * due to their tendency to recurse for some operations, see gh-18548.\n+ * (We need to define the slots to avoid inheriting it.)\n+ */\n static PyObject *\n-clongdouble_@oper@(PyObject *a, PyObject *b)\n+@name@_@oper@(PyObject *NPY_UNUSED(a), PyObject *NPY_UNUSED(b))\n {\n-    npy_clongdouble arg1, arg2;\n-\n-    BINOP_GIVE_UP_IF_NEEDED(a, b, nb_@oper@, clongdouble_@oper@);\n-\n-    switch(_clongdouble_convert2_to_ctypes(a, &arg1, b, &arg2)) {\n-        case 0:\n-            break;\n-        case -1:\n-            /* one of them can't be cast safely must be mixed-types*/\n-            return PyArray_Type.tp_as_number->nb_@oper@(a,b);\n-        case -2:\n-            /* use default handling */\n-            if (PyErr_Occurred()) {\n-                return NULL;\n-            }\n-            return PyGenericArrType_Type.tp_as_number->nb_@oper@(a,b);\n-        case -3:\n-            /*\n-             * special case for longdouble and clongdouble\n-             * because they have a recursive getitem in their dtype\n-             */\n-            Py_INCREF(Py_NotImplemented);\n-            return Py_NotImplemented;\n-    }\n-\n-    /*\n-     * here we do the actual calculation with arg1 and arg2\n-     * as a function call.\n-     */\n-    PyErr_SetString(PyExc_TypeError, \"complex long doubles do not support remainder\");\n-    return NULL;\n+    Py_RETURN_NOTIMPLEMENTED;\n }\n \n /**end repeat**/\n@@ -1124,6 +1409,14 @@ clongdouble_@oper@(PyObject *a, PyObject *b)\n  *         byte, ubyte, short, ushort, int, uint,\n  *         long, ulong, longlong, ulonglong#\n  *\n+ * #Name = (Byte, UByte, Short, UShort, Int, UInt,\n+ *             Long, ULong, LongLong, ULongLong,\n+ *             Half, Float, Double, LongDouble,\n+ *             CFloat, CDouble, CLongDouble)*3,\n+ *\n+ *         Byte, UByte, Short, UShort, Int, UInt,\n+ *         Long, ULong, LongLong, ULongLong#\n+ *\n  * #type = (npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n  *             npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n  *             npy_half, npy_float, npy_double, npy_longdouble,\n@@ -1161,32 +1454,19 @@ clongdouble_@oper@(PyObject *a, PyObject *b)\n static PyObject *\n @name@_@oper@(PyObject *a)\n {\n-    @type@ arg1;\n+    @type@ val;\n     @otype@ out;\n     PyObject *ret;\n \n-    switch(_@name@_convert_to_ctype(a, &arg1)) {\n-    case 0:\n-        break;\n-    case -1:\n-        /* can't cast both safely use different add function */\n-        Py_INCREF(Py_NotImplemented);\n-        return Py_NotImplemented;\n-    case -2:\n-        /* use default handling */\n-        if (PyErr_Occurred()) {\n-            return NULL;\n-        }\n-        return PyGenericArrType_Type.tp_as_number->nb_@oper@(a);\n-    }\n+\n+    val = PyArrayScalar_VAL(a, @Name@);\n+\n+    @name@_ctype_@oper@(val, &out);\n \n     /*\n-     * here we do the actual calculation with arg1 and arg2\n-     * make it a function call.\n+     * TODO: Complex absolute should check floating point flags.\n      */\n \n-    @name@_ctype_@oper@(arg1, &out);\n-\n     ret = PyArrayScalar_New(@OName@);\n     PyArrayScalar_ASSIGN(ret, @OName@, out);\n \n@@ -1210,6 +1490,10 @@ static PyObject *\n  *         uint, long, ulong, longlong, ulonglong,\n  *         half, float, double, longdouble,\n  *         cfloat, cdouble, clongdouble#\n+ * #Name = Byte, UByte, Short, UShort, Int, UInt,\n+ *         Long, ULong, LongLong, ULongLong,\n+ *         Half, Float, Double, LongDouble,\n+ *         CFloat, CDouble, CLongDouble#\n  * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int,\n  *         npy_uint, npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n  *         npy_half, npy_float, npy_double, npy_longdouble,\n@@ -1221,24 +1505,14 @@ static int\n @name@_bool(PyObject *a)\n {\n     int ret;\n-    @type@ arg1;\n+    @type@ val;\n \n-    if (_@name@_convert_to_ctype(a, &arg1) < 0) {\n-        if (PyErr_Occurred()) {\n-            return -1;\n-        }\n-        return PyGenericArrType_Type.tp_as_number->nb_bool(a);\n-    }\n-\n-    /*\n-     * here we do the actual calculation with arg1 and arg2\n-     * make it a function call.\n-     */\n+    val = PyArrayScalar_VAL(a, @Name@);\n \n #if @simp@\n-    ret = @nonzero@(arg1);\n+    ret = @nonzero@(val);\n #else\n-    ret = (@nonzero@(arg1.real) || @nonzero@(arg1.imag));\n+    ret = (@nonzero@(val.real) || @nonzero@(val.imag));\n #endif\n \n     return ret;\n@@ -1321,7 +1595,7 @@ static PyObject *\n  * #to_ctype = , , , , , , , , , , npy_half_to_double, , , , , , #\n  * #func = PyFloat_FromDouble*17#\n  */\n-static NPY_INLINE PyObject *\n+static PyObject *\n @name@_float(PyObject *obj)\n {\n #if @cmplx@\n@@ -1353,36 +1627,50 @@ static NPY_INLINE PyObject *\n  *         long, ulong, longlong, ulonglong,\n  *         half, float, double, longdouble,\n  *         cfloat, cdouble, clongdouble#\n+ * #Name = Byte, UByte, Short, UShort, Int, UInt,\n+ *         Long, ULong, LongLong, ULongLong,\n+ *         Half, Float, Double, LongDouble,\n+ *         CFloat, CDouble, CLongDouble#\n  * #simp = def*10, def_half, def*3, cmplx*3#\n  */\n+#define IS_@name@\n+\n static PyObject*\n @name@_richcompare(PyObject *self, PyObject *other, int cmp_op)\n {\n     npy_@name@ arg1, arg2;\n-    int out=0;\n-\n-    RICHCMP_GIVE_UP_IF_NEEDED(self, other);\n+    int out = 0;\n \n-    switch(_@name@_convert2_to_ctypes(self, &arg1, other, &arg2)) {\n-    case 0:\n-        break;\n-    case -1:\n-        /* can't cast both safely use different add function */\n-    case -2:\n-        /* use ufunc */\n-        if (PyErr_Occurred()) {\n-            return NULL;\n-        }\n-        return PyGenericArrType_Type.tp_richcompare(self, other, cmp_op);\n-    case -3:\n-        /*\n-         * special case for longdouble and clongdouble\n-         * because they have a recursive getitem in their dtype\n-         */\n-        Py_INCREF(Py_NotImplemented);\n-        return Py_NotImplemented;\n+    /*\n+     * Extract the other value (if it is compatible).\n+     */\n+    int res = convert_to_@name@(other, &arg2);\n+    switch (res) {\n+        case CONVERSION_ERROR:\n+            return NULL;  /* an error occurred (should never happen) */\n+        case DEFER_TO_OTHER_KNOWN_SCALAR:\n+            Py_RETURN_NOTIMPLEMENTED;\n+        case CONVERSION_SUCCESS:\n+            break;  /* successfully extracted value we can proceed */\n+        case OTHER_IS_UNKNOWN_OBJECT:\n+#if defined(IS_longdouble) || defined(IS_clongdouble)\n+            Py_RETURN_NOTIMPLEMENTED;\n+#endif\n+            RICHCMP_GIVE_UP_IF_NEEDED(self, other);\n+        case PROMOTION_REQUIRED:\n+            return PyGenericArrType_Type.tp_richcompare(self, other, cmp_op);\n+        case OTHER_IS_SUBCLASS:\n+            /*\n+             * Success converting.  We _could_ in principle defer in cases\n+             * were the other subclass does not inherit the behavior.  In\n+             * practice not even Python's `int` attempt this, so we also punt.\n+             * (This is also even trickier for richcompare, though.)\n+             */\n+            break;\n     }\n \n+    arg1 = PyArrayScalar_VAL(self, @Name@);\n+\n     /* here we do the actual calculation with arg1 and arg2 */\n     switch (cmp_op) {\n     case Py_EQ:\n@@ -1412,6 +1700,8 @@ static PyObject*\n         PyArrayScalar_RETURN_FALSE;\n     }\n }\n+\n+#undef IS_@name@\n /**end repeat**/\n \n /**begin repeat"
            },
            {
                "filename": "numpy/core/tests/test_scalarmath.py",
                "patch": "@@ -8,6 +8,7 @@\n import pytest\n from hypothesis import given, settings, Verbosity\n from hypothesis.strategies import sampled_from\n+from hypothesis.extra import numpy as hynp\n \n import numpy as np\n from numpy.testing import (\n@@ -24,6 +25,14 @@\n floating_types = np.floating.__subclasses__()\n complex_floating_types = np.complexfloating.__subclasses__()\n \n+objecty_things = [object(), None]\n+\n+reasonable_operators_for_scalars = [\n+    operator.lt, operator.le, operator.eq, operator.ne, operator.ge,\n+    operator.gt, operator.add, operator.floordiv, operator.mod,\n+    operator.mul, operator.pow, operator.sub, operator.truediv,\n+]\n+\n \n # This compares scalarmath against ufuncs.\n \n@@ -66,6 +75,41 @@ def test_leak(self):\n             np.add(1, 1)\n \n \n+@pytest.mark.slow\n+@settings(max_examples=10000, deadline=2000)\n+@given(sampled_from(reasonable_operators_for_scalars),\n+       hynp.arrays(dtype=hynp.scalar_dtypes(), shape=()),\n+       hynp.arrays(dtype=hynp.scalar_dtypes(), shape=()))\n+def test_array_scalar_ufunc_equivalence(op, arr1, arr2):\n+    \"\"\"\n+    This is a thorough test attempting to cover important promotion paths\n+    and ensuring that arrays and scalars stay as aligned as possible.\n+    However, if it creates troubles, it should maybe just be removed.\n+    \"\"\"\n+    scalar1 = arr1[()]\n+    scalar2 = arr2[()]\n+    assert isinstance(scalar1, np.generic)\n+    assert isinstance(scalar2, np.generic)\n+\n+    if arr1.dtype.kind == \"c\" or arr2.dtype.kind == \"c\":\n+        comp_ops = {operator.ge, operator.gt, operator.le, operator.lt}\n+        if op in comp_ops and (np.isnan(scalar1) or np.isnan(scalar2)):\n+            pytest.xfail(\"complex comp ufuncs use sort-order, scalars do not.\")\n+\n+    # ignore fpe's since they may just mismatch for integers anyway.\n+    with warnings.catch_warnings(), np.errstate(all=\"ignore\"):\n+        # Comparisons DeprecationWarnings replacing errors (2022-03):\n+        warnings.simplefilter(\"error\", DeprecationWarning)\n+        try:\n+            res = op(arr1, arr2)\n+        except Exception as e:\n+            with pytest.raises(type(e)):\n+                op(scalar1, scalar2)\n+        else:\n+            scalar_res = op(scalar1, scalar2)\n+            assert_array_equal(scalar_res, res)\n+\n+\n class TestBaseMath:\n     def test_blocked(self):\n         # test alignments offsets for simd instructions\n@@ -753,7 +797,6 @@ def test_float_and_complex_hashes(self, type_code):\n             else:\n                 val = float(numpy_val)\n             assert val == numpy_val\n-            print(repr(numpy_val), repr(val))\n             assert hash(val) == hash(numpy_val)\n \n         if hash(float(np.nan)) != hash(float(np.nan)):\n@@ -779,19 +822,9 @@ def recursionlimit(n):\n         sys.setrecursionlimit(o)\n \n \n-objecty_things = [object(), None]\n-reasonable_operators_for_scalars = [\n-    operator.lt, operator.le, operator.eq, operator.ne, operator.ge,\n-    operator.gt, operator.add, operator.floordiv, operator.mod,\n-    operator.mul, operator.matmul, operator.pow, operator.sub,\n-    operator.truediv,\n-]\n-\n-\n @given(sampled_from(objecty_things),\n        sampled_from(reasonable_operators_for_scalars),\n        sampled_from(types))\n-@settings(verbosity=Verbosity.verbose)\n def test_operator_object_left(o, op, type_):\n     try:\n         with recursionlimit(200):\n@@ -845,3 +878,129 @@ def test_clongdouble_inf_loop(op):\n         op(None, np.longdouble(3))\n     except TypeError:\n         pass\n+\n+\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n+@pytest.mark.parametrize(\"operation\", [\n+        lambda min, max: max + max,\n+        lambda min, max: min - max,\n+        lambda min, max: max * max], ids=[\"+\", \"-\", \"*\"])\n+def test_scalar_integer_operation_overflow(dtype, operation):\n+    st = np.dtype(dtype).type\n+    min = st(np.iinfo(dtype).min)\n+    max = st(np.iinfo(dtype).max)\n+\n+    with pytest.warns(RuntimeWarning, match=\"overflow encountered\"):\n+        operation(min, max)\n+\n+\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"Integer\"])\n+@pytest.mark.parametrize(\"operation\", [\n+        lambda min, neg_1: abs(min),\n+        lambda min, neg_1: min * neg_1,\n+        lambda min, neg_1: min // neg_1], ids=[\"abs\", \"*\", \"//\"])\n+def test_scalar_signed_integer_overflow(dtype, operation):\n+    # The minimum signed integer can \"overflow\" for some additional operations\n+    st = np.dtype(dtype).type\n+    min = st(np.iinfo(dtype).min)\n+    neg_1 = st(-1)\n+\n+    with pytest.warns(RuntimeWarning, match=\"overflow encountered\"):\n+        operation(min, neg_1)\n+\n+\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"UnsignedInteger\"])\n+@pytest.mark.xfail  # TODO: the check is quite simply missing!\n+def test_scalar_signed_integer_overflow(dtype):\n+    val = np.dtype(dtype).type(8)\n+    with pytest.warns(RuntimeWarning, match=\"overflow encountered\"):\n+        -val\n+\n+\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n+@pytest.mark.parametrize(\"operation\", [\n+        lambda val, zero: val // zero,\n+        lambda val, zero: val % zero, ], ids=[\"//\", \"%\"])\n+def test_scalar_integer_operation_divbyzero(dtype, operation):\n+    st = np.dtype(dtype).type\n+    val = st(100)\n+    zero = st(0)\n+\n+    with pytest.warns(RuntimeWarning, match=\"divide by zero\"):\n+        operation(val, zero)\n+\n+\n+ops_with_names = [\n+    (\"__lt__\", \"__gt__\", operator.lt, True),\n+    (\"__le__\", \"__ge__\", operator.le, True),\n+    (\"__eq__\", \"__eq__\", operator.eq, True),\n+    # Note __op__ and __rop__ may be identical here:\n+    (\"__ne__\", \"__ne__\", operator.ne, True),\n+    (\"__gt__\", \"__lt__\", operator.gt, True),\n+    (\"__ge__\", \"__le__\", operator.ge, True),\n+    (\"__floordiv__\", \"__rfloordiv__\", operator.floordiv, False),\n+    (\"__truediv__\", \"__rtruediv__\", operator.truediv, False),\n+    (\"__add__\", \"__radd__\", operator.add, False),\n+    (\"__mod__\", \"__rmod__\", operator.mod, False),\n+    (\"__mul__\", \"__rmul__\", operator.mul, False),\n+    (\"__pow__\", \"__rpow__\", operator.pow, False),\n+    (\"__sub__\", \"__rsub__\", operator.sub, False),\n+]\n+\n+\n+@pytest.mark.parametrize([\"__op__\", \"__rop__\", \"op\", \"cmp\"], ops_with_names)\n+@pytest.mark.parametrize(\"sctype\", [np.float32, np.float64, np.longdouble])\n+def test_subclass_deferral(sctype, __op__, __rop__, op, cmp):\n+    \"\"\"\n+    This test covers scalar subclass deferral.  Note that this is exceedingly\n+    complicated, especially since it tends to fall back to the array paths and\n+    these additionally add the \"array priority\" mechanism.\n+\n+    The behaviour was modified subtly in 1.22 (to make it closer to how Python\n+    scalars work).  Due to its complexity and the fact that subclassing NumPy\n+    scalars is probably a bad idea to begin with.  There is probably room\n+    for adjustments here.\n+    \"\"\"\n+    class myf_simple1(sctype):\n+        pass\n+\n+    class myf_simple2(sctype):\n+        pass\n+\n+    def defer(self, other):\n+        return NotImplemented\n+\n+    def op_func(self, other):\n+        return __op__\n+\n+    def rop_func(self, other):\n+        return __rop__\n+\n+    myf_op = type(\"myf_op\", (sctype,), {__op__: op_func, __rop__: rop_func})\n+\n+    # inheritance has to override, or this is correctly lost:\n+    res = op(myf_simple1(1), myf_simple2(2))\n+    assert type(res) == sctype or type(res) == np.bool_\n+    assert op(myf_simple1(1), myf_simple2(2)) == op(1, 2)  # inherited\n+\n+    # Two independent subclasses do not really define an order.  This could\n+    # be attempted, but we do not since Python's `int` does neither:\n+    assert op(myf_op(1), myf_simple1(2)) == __op__\n+    assert op(myf_simple1(1), myf_op(2)) == op(1, 2)  # inherited\n+\n+\n+@pytest.mark.parametrize([\"__op__\", \"__rop__\", \"op\", \"cmp\"], ops_with_names)\n+@pytest.mark.parametrize(\"pytype\", [float, int, complex])\n+def test_pyscalar_subclasses(pytype, __op__, __rop__, op, cmp):\n+    def op_func(self, other):\n+        return __op__\n+\n+    def rop_func(self, other):\n+        return __rop__\n+\n+    myf = type(\"myf\", (pytype,),\n+            {__op__: op_func, __rop__: rop_func, \"__array_ufunc__\": None})\n+\n+    # Just like normally, we should never presume we can modify the float.\n+    assert op(myf(1), np.float64(2)) == __op__\n+    assert op(np.float64(1), myf(2)) == __rop__"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21258,
        "body": "This PR optimizes the following operations for Power/VSX:\r\n- equal, not_equal, less, less_equal, greater and greater_equal\r\n- logical_and, logical_not and logical_or\r\n- absolute\r\n\r\nSee below the average speedup with N=1000000:\r\n\r\n- Comparisson functions:\r\n    - arry OP arr:     bool (32.77x)  float(3.36x)  double(1.43x)\r\n    - arry OP scalar:  bool (33.47x)  float(6.34x)  double(2.25x)\r\n\r\n- Logical functions:\r\n    - logical_and:  bool (118.12x)\r\n    - logical_not:  bool ( 21.41x)\r\n    - logical_or:   bool ( 24.40x)\r\n    \r\n- Miscellaneous:\r\n    - absolute:     bool (23.12x)\r\n    \r\nNote: This PR enables the optimizations above for Power8, Power9 and Power10.",
        "changed_files": [
            {
                "filename": "numpy/core/code_generators/generate_umath.py",
                "patch": "@@ -409,7 +409,8 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.absolute'),\n           'PyUFunc_AbsoluteTypeResolver',\n-          TD(bints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd')]),\n+          TD(ints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd')]),\n+          TD('?', out='?', simd=[('vsx', '?')]),\n           TD(cmplx, simd=[('avx512f', cmplxvec)], out=('f', 'd', 'g')),\n           TD(O, f='PyNumber_Absolute'),\n           ),\n@@ -445,69 +446,69 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.greater'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', simd=[('avx2', ints), ('vsx', 'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'greater_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.greater_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', simd=[('avx2', ints), ('vsx', 'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'less':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.less'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', simd=[('avx2', ints), ('vsx', 'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'less_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.less_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', simd=[('avx2', ints), ('vsx', 'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', simd=[('avx2', ints), ('vsx', 'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'not_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.not_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', simd=[('avx2', ints)]),\n+          TD(all, out='?', simd=[('avx2', ints), ('vsx', 'fd')]),\n           [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           TD('O', out='?'),\n           ),\n 'logical_and':\n     Ufunc(2, 1, True_,\n           docstrings.get('numpy.core.umath.logical_and'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),\n+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints), ('vsx', '?fd')]),\n           TD(O, f='npy_ObjectLogicalAnd'),\n           ),\n 'logical_not':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.logical_not'),\n           None,\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),\n+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints), ('vsx', '?')]),\n           TD(O, f='npy_ObjectLogicalNot'),\n           ),\n 'logical_or':\n     Ufunc(2, 1, False_,\n           docstrings.get('numpy.core.umath.logical_or'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),\n+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints), ('vsx', '?fd')]),\n           TD(O, f='npy_ObjectLogicalOr'),\n           ),\n 'logical_xor':"
            },
            {
                "filename": "numpy/core/include/numpy/npy_common.h",
                "patch": "@@ -73,6 +73,14 @@\n #else\n #define NPY_GCC_TARGET_AVX512_SKX\n #endif\n+\n+#if defined HAVE_ATTRIBUTE_TARGET_VSX && defined HAVE_LINK_VSX\n+#define NPY_GCC_TARGET_VSX __attribute__((target(\"cpu=power8\")))\n+#elif defined HAVE_ATTRIBUTE_TARGET_VSX_WITH_INTRINSICS\n+#define NPY_GCC_TARGET_VSX __attribute__((target(\"cpu=power8\")))\n+#else\n+#define NPY_GCC_TARGET_VSX\n+#endif\n /*\n  * mark an argument (starting from 1) that must not be NULL and is not checked\n  * DO NOT USE IF FUNCTION CHECKS FOR NULL!! the compiler will remove the check\n@@ -98,6 +106,10 @@\n #if defined HAVE_IMMINTRIN_H && defined HAVE_LINK_AVX512F\n #define NPY_HAVE_AVX512F_INTRINSICS\n #endif\n+\n+#if defined HAVE_ALTIVEC_H && defined HAVE_LINK_VSX\n+#define NPY_HAVE_VSX_INTRINSICS\n+#endif\n /*\n  * give a hint to the compiler which branch is more likely or unlikely\n  * to occur, e.g. rare error cases:"
            },
            {
                "filename": "numpy/core/setup_common.py",
                "patch": "@@ -110,6 +110,7 @@ def check_api_version(apiversion, codegen_dir):\n                 \"xmmintrin.h\",  # SSE\n                 \"emmintrin.h\",  # SSE2\n                 \"immintrin.h\",  # AVX\n+                \"altivec.h\",  # VSX\n                 \"features.h\",  # for glibc version linux\n                 \"xlocale.h\",  # see GH#8367\n                 \"dlfcn.h\", # dladdr\n@@ -145,6 +146,9 @@ def check_api_version(apiversion, codegen_dir):\n                                              \"vpbroadcastmb2q %k0, %xmm0\\\\n\"',\n                         \"stdio.h\", \"LINK_AVX512_SKX\"),\n                        (\"__asm__ volatile\", '\"xgetbv\"', \"stdio.h\", \"XGETBV\"),\n+                       # check that the linker can handle vsx\n+                       (\"__asm__ volatile\", '\"vaddubm 0, 1, 2\"', \"stdio.h\",\n+                        \"LINK_VSX\"),\n                        ]\n \n # function attributes\n@@ -164,6 +168,8 @@ def check_api_version(apiversion, codegen_dir):\n                                  'attribute_target_avx512f'),\n                                 ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n                                  'attribute_target_avx512_skx'),\n+                                ('__attribute__((target (\"cpu=power8\")))',\n+                                 'attribute_target_vsx'),\n                                 ]\n \n # function attributes with intrinsics\n@@ -190,6 +196,11 @@ def check_api_version(apiversion, codegen_dir):\n                                     _mm512_castps_si512(_mm512_set1_ps(1.0));\\\n                                 _mm_mask_storeu_epi8(NULL, 0xFF, _mm_broadcastmb_epi64(temp))',\n                                 'immintrin.h'),\n+                                ('__attribute__((target (\"cpu=power8\")))',\n+                                'attribute_target_vsx_with_intrinsics',\n+                                '__vector int v1, v2, v3;\\\n+                                v3 = vec_add(v1, v2);',\n+                                'altivec.h'),\n                                 ]\n \n # variable attributes tested via \"int %s a\" % attribute"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -408,10 +408,15 @@ PyUFunc_On_Om(char **args, npy_intp const *dimensions, npy_intp const *steps, vo\n NPY_NO_EXPORT void\n BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    BINARY_LOOP {\n-        npy_bool in1 = *((npy_bool *)ip1) != 0;\n-        npy_bool in2 = *((npy_bool *)ip2) != 0;\n-        *((npy_bool *)op1)= in1 @OP@ in2;\n+    if (run_binary_simd_@kind@_BOOL(args, dimensions, steps)) {\n+        return;\n+    }\n+    else {\n+        BINARY_LOOP {\n+            npy_bool in1 = *((npy_bool *)ip1) != 0;\n+            npy_bool in2 = *((npy_bool *)ip2) != 0;\n+            *((npy_bool *)op1)= in1 @OP@ in2;\n+        }\n     }\n }\n /**end repeat**/\n@@ -423,9 +428,14 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n  * #SC =  ==, !=#\n  * #and = 1, 0#\n  **/\n-\n+/**begin repeat1\n+ * #ISA = , _vsx#\n+ * #isa = simd, vsx#\n+ * #CHK = 1, defined(HAVE_ATTRIBUTE_TARGET_VSX)#\n+ **/\n+#if @CHK@\n NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+BOOL_@kind@@ISA@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n     if(IS_BINARY_REDUCE) {\n #ifdef NPY_HAVE_SSE2_INTRINSICS\n@@ -476,7 +486,7 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n         }\n     }\n     else {\n-        if (run_binary_simd_@kind@_BOOL(args, dimensions, steps)) {\n+        if (run_binary_@isa@_@kind@_BOOL(args, dimensions, steps)) {\n             return;\n         }\n         else {\n@@ -488,16 +498,24 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n         }\n     }\n }\n+#endif\n+/**end repeat1**/\n /**end repeat**/\n \n /**begin repeat\n  * #kind = absolute, logical_not#\n  * #OP =  !=, ==#\n  **/\n+/**begin repeat1\n+ * #ISA = , _vsx#\n+ * #isa = simd, vsx#\n+ * #CHK = 1, defined(HAVE_ATTRIBUTE_TARGET_VSX)#\n+ **/\n+#if @CHK@\n NPY_NO_EXPORT void\n-BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+BOOL_@kind@@ISA@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    if (run_unary_simd_@kind@_BOOL(args, dimensions, steps)) {\n+    if (run_unary_@isa@_@kind@_BOOL(args, dimensions, steps)) {\n         return;\n     }\n     else {\n@@ -507,6 +525,8 @@ BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void\n         }\n     }\n }\n+#endif\n+/**end repeat1**/\n /**end repeat**/\n \n NPY_NO_EXPORT void\n@@ -1517,10 +1537,16 @@ TIMEDELTA_mm_qm_divmod(char **args, npy_intp const *dimensions, npy_intp const *\n  *        logical_and, logical_or#\n  * #OP = ==, !=, <, <=, >, >=, &&, ||#\n  */\n+/**begin repeat2\n+ * #ISA = , _vsx#\n+ * #isa = simd, vsx#\n+ * #CHK = 1, defined(HAVE_ATTRIBUTE_TARGET_VSX)#\n+ */\n+#if @CHK@\n NPY_NO_EXPORT void\n-@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n+@TYPE@_@kind@@ISA@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n-    if (!run_binary_simd_@kind@_@TYPE@(args, dimensions, steps)) {\n+    if (!run_binary_@isa@_@kind@_@TYPE@(args, dimensions, steps)) {\n         BINARY_LOOP {\n             const @type@ in1 = *(@type@ *)ip1;\n             const @type@ in2 = *(@type@ *)ip2;\n@@ -1529,6 +1555,8 @@ NPY_NO_EXPORT void\n     }\n     npy_clear_floatstatus_barrier((char*)dimensions);\n }\n+#endif\n+/**end repeat2**/\n /**end repeat1**/\n \n NPY_NO_EXPORT void"
            },
            {
                "filename": "numpy/core/src/umath/loops.h.src",
                "patch": "@@ -29,13 +29,23 @@\n  */\n \n /**begin repeat\n- * #kind = equal, not_equal, greater, greater_equal, less, less_equal,\n- *         logical_and, logical_or, absolute, logical_not#\n+ * #kind = equal, not_equal, greater, greater_equal, less, less_equal#\n  **/\n NPY_NO_EXPORT void\n BOOL_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n /**end repeat**/\n \n+/**begin repeat\n+ * #kind = logical_and, logical_or, absolute, logical_not#\n+ **/\n+/**begin repeat1\n+ * #isa = , _vsx#\n+ */\n+NPY_NO_EXPORT void\n+BOOL_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+/**end repeat1**/\n+/**end repeat**/\n+\n NPY_NO_EXPORT void\n BOOL__ones_like(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(data));\n \n@@ -313,8 +323,12 @@ NPY_NO_EXPORT void\n  *        logical_and, logical_or#\n  * #OP = ==, !=, <, <=, >, >=, &&, ||#\n  */\n+/**begin repeat2\n+ * #isa = , _vsx#\n+ */\n NPY_NO_EXPORT void\n-@TYPE@_@kind@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+@TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func));\n+/**end repeat2**/\n /**end repeat1**/\n \n NPY_NO_EXPORT void"
            },
            {
                "filename": "numpy/core/src/umath/simd.inc.src",
                "patch": "@@ -28,6 +28,9 @@\n #undef __AVX512F__\n #endif\n #endif\n+#ifdef NPY_HAVE_VSX_INTRINSICS\n+#include <altivec.h>\n+#endif\n #include \"loops_utils.h\" // nomemoverlap\n #include <assert.h>\n #include <stdlib.h>\n@@ -163,48 +166,54 @@ run_@name@_simd_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp\n  *         logical_and, logical_or#\n  * #simd = 1, 1, 1, 1, 1, 1, 0, 0#\n  */\n+/**begin repeat2\n+ * #ISA   = SSE2, VSX#\n+ * #isa   = sse2, vsx#\n+ * #fisa  = simd, vsx#\n+ */\n \n-#if @vector@ && @simd@ && defined NPY_HAVE_SSE2_INTRINSICS\n+#if @vector@ && @simd@ && defined NPY_HAVE_@ISA@_INTRINSICS\n \n /* prototypes */\n static void\n-sse2_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n-                          npy_intp n);\n+@isa@_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n+                           npy_intp n);\n static void\n-sse2_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n-                                  npy_intp n);\n+@isa@_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n+                                   npy_intp n);\n static void\n-sse2_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n-                                  npy_intp n);\n+@isa@_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n+                                   npy_intp n);\n \n #endif\n \n static NPY_INLINE int\n-run_binary_simd_@kind@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+run_binary_@fisa@_@kind@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)\n {\n-#if @vector@ && @simd@ && defined NPY_HAVE_SSE2_INTRINSICS\n+#if @vector@ && @simd@ && defined NPY_HAVE_@ISA@_INTRINSICS\n     @type@ * ip1 = (@type@ *)args[0];\n     @type@ * ip2 = (@type@ *)args[1];\n     npy_bool * op = (npy_bool *)args[2];\n     npy_intp n = dimensions[0];\n     /* argument one scalar */\n     if (IS_BLOCKABLE_BINARY_SCALAR1_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_scalar1_@kind@_@TYPE@(op, ip1, ip2, n);\n+        @isa@_binary_scalar1_@kind@_@TYPE@(op, ip1, ip2, n);\n         return 1;\n     }\n     /* argument two scalar */\n     else if (IS_BLOCKABLE_BINARY_SCALAR2_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_scalar2_@kind@_@TYPE@(op, ip1, ip2, n);\n+        @isa@_binary_scalar2_@kind@_@TYPE@(op, ip1, ip2, n);\n         return 1;\n     }\n     else if (IS_BLOCKABLE_BINARY_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_@kind@_@TYPE@(op, ip1, ip2, n);\n+        @isa@_binary_@kind@_@TYPE@(op, ip1, ip2, n);\n         return 1;\n     }\n #endif\n     return 0;\n }\n \n+/**end repeat2**/\n /**end repeat1**/\n \n /**begin repeat1\n@@ -244,6 +253,11 @@ run_@kind@_simd_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *\n /**begin repeat\n  * # kind = logical_or, logical_and#\n  */\n+/**begin repeat1\n+ * #ISA   = SSE2, VSX#\n+ * #isa   = sse2, vsx#\n+ * #fisa  = simd, vsx#\n+ */\n \n #if defined NPY_HAVE_SSE2_INTRINSICS\n static void\n@@ -252,21 +266,28 @@ sse2_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n \n static void\n sse2_reduce_@kind@_BOOL(npy_bool * op, npy_bool * ip, npy_intp n);\n+\n+#elif defined NPY_HAVE_VSX_INTRINSICS\n+\n+static void\n+vsx_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n+                       npy_intp n);\n #endif\n \n static NPY_INLINE int\n-run_binary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+run_binary_@fisa@_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n {\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n+#if defined NPY_HAVE_@ISA@_INTRINSICS\n     if (sizeof(npy_bool) == 1 &&\n             IS_BLOCKABLE_BINARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {\n-        sse2_binary_@kind@_BOOL((npy_bool*)args[2], (npy_bool*)args[0],\n-                               (npy_bool*)args[1], dimensions[0]);\n+        @isa@_binary_@kind@_BOOL((npy_bool*)args[2], (npy_bool*)args[0],\n+                                 (npy_bool*)args[1], dimensions[0]);\n         return 1;\n     }\n #endif\n     return 0;\n }\n+/**end repeat1**/\n \n \n static NPY_INLINE int\n@@ -288,19 +309,24 @@ run_reduce_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp co\n /**begin repeat\n  * # kind = absolute, logical_not#\n  */\n+/**begin repeat1\n+ * #ISA   = SSE2, VSX#\n+ * #isa   = sse2, vsx#\n+ * #fisa  = simd, vsx#\n+ */\n \n-#if defined NPY_HAVE_SSE2_INTRINSICS\n+#if defined NPY_HAVE_@ISA@_INTRINSICS\n static void\n-sse2_@kind@_BOOL(npy_bool *, npy_bool *, const npy_intp n);\n+@isa@_@kind@_BOOL(npy_bool *, npy_bool *, const npy_intp n);\n #endif\n \n static NPY_INLINE int\n-run_unary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n+run_unary_@fisa@_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)\n {\n-#if defined NPY_HAVE_SSE2_INTRINSICS\n+#if defined NPY_HAVE_@ISA@_INTRINSICS\n     if (sizeof(npy_bool) == 1 &&\n             IS_BLOCKABLE_UNARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {\n-        sse2_@kind@_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);\n+        @isa@_@kind@_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);\n         return 1;\n     }\n #endif\n@@ -309,6 +335,54 @@ run_unary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp con\n \n /**end repeat**/\n \n+/**begin repeat\n+ * #kind = equal, not_equal, greater, greater_equal, less, less_equal#\n+ **/\n+\n+#if defined NPY_HAVE_VSX_INTRINSICS\n+static void\n+vsx_binary_@kind@_BOOL(npy_bool *, npy_bool *, npy_bool *, const npy_intp n);\n+\n+static void\n+vsx_binary_scalar1_@kind@_BOOL(npy_bool *, npy_bool *, npy_bool *,\n+                               const npy_intp n);\n+\n+static void\n+vsx_binary_scalar2_@kind@_BOOL(npy_bool *, npy_bool *, npy_bool *,\n+                               const npy_intp n);\n+#endif\n+\n+static NPY_INLINE int\n+run_binary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions,\n+                            npy_intp const *steps)\n+{\n+#if defined NPY_HAVE_VSX_INTRINSICS\n+    npy_bool * ip1 = (npy_bool *)args[0];\n+    npy_bool * ip2 = (npy_bool *)args[1];\n+    npy_bool * op = (npy_bool *)args[2];\n+    npy_intp n = dimensions[0];\n+    /* argument one scalar */\n+    if (IS_BLOCKABLE_BINARY_SCALAR1_BOOL(sizeof(npy_bool),\n+                                         VECTOR_SIZE_BYTES)) {\n+        vsx_binary_scalar1_@kind@_BOOL(op, ip1, ip2, n);\n+        return 1;\n+    }\n+    /* argument two scalar */\n+    else if (IS_BLOCKABLE_BINARY_SCALAR2_BOOL(sizeof(npy_bool),\n+                                              VECTOR_SIZE_BYTES)) {\n+        vsx_binary_scalar2_@kind@_BOOL(op, ip1, ip2, n);\n+        return 1;\n+    }\n+    /* both arguments are arrays */\n+    if (IS_BLOCKABLE_BINARY_BOOL(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {\n+        vsx_binary_@kind@_BOOL(op, ip1, ip2, n);\n+        return 1;\n+    }\n+#endif\n+    return 0;\n+}\n+/**end repeat**/\n+\n #ifdef NPY_HAVE_SSE2_INTRINSICS\n \n /*\n@@ -1354,6 +1428,334 @@ sse2_@kind@_BOOL(@type@ * op, @type@ * ip, const npy_intp n)\n /**end repeat**/\n \n #undef VECTOR_SIZE_BYTES\n-#endif  /* NPY_HAVE_SSE2_INTRINSICS */\n+\n+#elif defined NPY_HAVE_VSX_INTRINSICS\n+\n+/*\n+ *****************************************************************************\n+ **                           FLOAT LOOPS\n+ *****************************************************************************\n+ */\n+\n+// Pack 4x 32-bit vectors into 1x 8-bit vector\n+NPY_FINLINE NPY_GCC_TARGET_VSX __vector unsigned char\n+pack_vf32_vu8(__vector unsigned int v1, __vector unsigned int v2,\n+              __vector unsigned int v3, __vector unsigned int v4)\n+{\n+    __vector unsigned char mask = vec_splats((unsigned char)0x1);\n+    __vector unsigned short p1 = vec_pack(v1, v2);\n+    __vector unsigned short p2 = vec_pack(v3, v4);\n+    __vector unsigned char r = vec_pack(p1, p2);\n+                           r = vec_and(r, mask);\n+    return r;\n+}\n+\n+// Pack 8x 64-bit vectors into 1x 8-bit vector\n+NPY_FINLINE NPY_GCC_TARGET_VSX __vector unsigned char\n+pack_vf64_vu8(__vector unsigned long long v1, __vector unsigned long long v2,\n+              __vector unsigned long long v3, __vector unsigned long long v4,\n+              __vector unsigned long long v5, __vector unsigned long long v6,\n+              __vector unsigned long long v7, __vector unsigned long long v8)\n+{\n+    __vector unsigned int p1 = vec_pack(v1, v2);\n+    __vector unsigned int p2 = vec_pack(v3, v4);\n+    __vector unsigned int p3 = vec_pack(v5, v6);\n+    __vector unsigned int p4 = vec_pack(v7, v8);\n+    return pack_vf32_vu8(p1, p2, p3, p4);\n+}\n+\n+/**begin repeat\n+ * #TYPE   = FLOAT, DOUBLE#\n+ * #type   = npy_float, npy_double#\n+ * #steps  = 4, 2#\n+ * #double = 0, 1#\n+ * #ivtype = __vector float, __vector double#\n+ * #ovtype = __vector unsigned int, __vector unsigned long long#\n+ * #otype  = unsigned int, unsigned long long#\n+ */\n+/**begin repeat1\n+ * #kind = equal, not_equal, greater, greater_equal, less, less_equal#\n+ * #simd = cmpeq, cmpne, cmpgt, cmpge, cmplt, cmple #\n+ * #OP   = ==, !=, >, >=, <, <=#\n+ */\n+\n+/*\n+ * Each iteration of the main loop computes 16 elements of the output, which\n+ * are packed into an 8-bit vector. In order to generate the 16 elements of the\n+ * output, it is necessary to load/compute: 1) 4x 32-bit vectors, for FLOAT; or\n+ * 2) 8x 64-bit vectors, for DOUBLE.\n+ */\n+\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n+                                 npy_intp n)\n+{\n+    const @ivtype@ a = vec_splats((@type@)ip1[0]);\n+    LOOP_BLOCK_ALIGN_VAR(ip2, @type@, VECTOR_SIZE_BYTES) {\n+        op[i] = ip1[0] @OP@ ip2[i];\n+    }\n+    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {\n+        @ivtype@ b1 = vec_ld(0, &ip2[i + (0 * @steps@)]);\n+        @ivtype@ b2 = vec_ld(0, &ip2[i + (1 * @steps@)]);\n+        @ivtype@ b3 = vec_ld(0, &ip2[i + (2 * @steps@)]);\n+        @ivtype@ b4 = vec_ld(0, &ip2[i + (3 * @steps@)]);\n+        @ovtype@ r1 = (@ovtype@)vec_@simd@(a, b1);\n+        @ovtype@ r2 = (@ovtype@)vec_@simd@(a, b2);\n+        @ovtype@ r3 = (@ovtype@)vec_@simd@(a, b3);\n+        @ovtype@ r4 = (@ovtype@)vec_@simd@(a, b4);\n+#if @double@\n+        @ivtype@ b5 = vec_ld(0, &ip2[i + (4 * @steps@)]);\n+        @ivtype@ b6 = vec_ld(0, &ip2[i + (5 * @steps@)]);\n+        @ivtype@ b7 = vec_ld(0, &ip2[i + (6 * @steps@)]);\n+        @ivtype@ b8 = vec_ld(0, &ip2[i + (7 * @steps@)]);\n+        @ovtype@ r5 = (@ovtype@)vec_@simd@(a, b5);\n+        @ovtype@ r6 = (@ovtype@)vec_@simd@(a, b6);\n+        @ovtype@ r7 = (@ovtype@)vec_@simd@(a, b7);\n+        @ovtype@ r8 = (@ovtype@)vec_@simd@(a, b8);\n+        __vector unsigned char r =\n+            pack_vf64_vu8(r1, r2, r3, r4, r5, r6, r7, r8);\n+#else\n+        __vector unsigned char r = pack_vf32_vu8(r1, r2, r3, r4);\n #endif\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        op[i] = ip1[0] @OP@ ip2[i];\n+    }\n+}\n \n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,\n+                                 npy_intp n)\n+{\n+    const @ivtype@ b = vec_splats((@type@)ip2[0]);\n+    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {\n+        op[i] = ip1[i] @OP@ ip2[0];\n+    }\n+    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {\n+        @ivtype@ a1 = vec_ld(0, &ip1[i + (0 * @steps@)]);\n+        @ivtype@ a2 = vec_ld(0, &ip1[i + (1 * @steps@)]);\n+        @ivtype@ a3 = vec_ld(0, &ip1[i + (2 * @steps@)]);\n+        @ivtype@ a4 = vec_ld(0, &ip1[i + (3 * @steps@)]);\n+        @ovtype@ r1 = (@ovtype@)vec_@simd@(a1, b);\n+        @ovtype@ r2 = (@ovtype@)vec_@simd@(a2, b);\n+        @ovtype@ r3 = (@ovtype@)vec_@simd@(a3, b);\n+        @ovtype@ r4 = (@ovtype@)vec_@simd@(a4, b);\n+#if @double@\n+        @ivtype@ a5 = vec_ld(0, &ip1[i + (4 * @steps@)]);\n+        @ivtype@ a6 = vec_ld(0, &ip1[i + (5 * @steps@)]);\n+        @ivtype@ a7 = vec_ld(0, &ip1[i + (6 * @steps@)]);\n+        @ivtype@ a8 = vec_ld(0, &ip1[i + (7 * @steps@)]);\n+        @ovtype@ r5 = (@ovtype@)vec_@simd@(a5, b);\n+        @ovtype@ r6 = (@ovtype@)vec_@simd@(a6, b);\n+        @ovtype@ r7 = (@ovtype@)vec_@simd@(a7, b);\n+        @ovtype@ r8 = (@ovtype@)vec_@simd@(a8, b);\n+        __vector unsigned char r =\n+            pack_vf64_vu8(r1, r2, r3, r4, r5, r6, r7, r8);\n+#else\n+        __vector unsigned char r = pack_vf32_vu8(r1, r2, r3, r4);\n+#endif\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        op[i] = ip1[i] @OP@ ip2[0];\n+    }\n+}\n+\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)\n+{\n+    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {\n+        op[i] = ip1[i] @OP@ ip2[i];\n+    }\n+    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {\n+        @ivtype@ a1 = vec_ld(0, &ip1[i + (0 * @steps@)]);\n+        @ivtype@ a2 = vec_ld(0, &ip1[i + (1 * @steps@)]);\n+        @ivtype@ a3 = vec_ld(0, &ip1[i + (2 * @steps@)]);\n+        @ivtype@ a4 = vec_ld(0, &ip1[i + (3 * @steps@)]);\n+        @ivtype@ b1 = vec_xl(0, &ip2[i + (0 * @steps@)]);\n+        @ivtype@ b2 = vec_xl(0, &ip2[i + (1 * @steps@)]);\n+        @ivtype@ b3 = vec_xl(0, &ip2[i + (2 * @steps@)]);\n+        @ivtype@ b4 = vec_xl(0, &ip2[i + (3 * @steps@)]);\n+        @ovtype@ r1 = (@ovtype@)vec_@simd@(a1, b1);\n+        @ovtype@ r2 = (@ovtype@)vec_@simd@(a2, b2);\n+        @ovtype@ r3 = (@ovtype@)vec_@simd@(a3, b3);\n+        @ovtype@ r4 = (@ovtype@)vec_@simd@(a4, b4);\n+#if @double@\n+        @ivtype@ a5 = vec_ld(0, &ip1[i + (4 * @steps@)]);\n+        @ivtype@ a6 = vec_ld(0, &ip1[i + (5 * @steps@)]);\n+        @ivtype@ a7 = vec_ld(0, &ip1[i + (6 * @steps@)]);\n+        @ivtype@ a8 = vec_ld(0, &ip1[i + (7 * @steps@)]);\n+        @ivtype@ b5 = vec_xl(0, &ip2[i + (4 * @steps@)]);\n+        @ivtype@ b6 = vec_xl(0, &ip2[i + (5 * @steps@)]);\n+        @ivtype@ b7 = vec_xl(0, &ip2[i + (6 * @steps@)]);\n+        @ivtype@ b8 = vec_xl(0, &ip2[i + (7 * @steps@)]);\n+        @ovtype@ r5 = (@ovtype@)vec_@simd@(a5, b5);\n+        @ovtype@ r6 = (@ovtype@)vec_@simd@(a6, b6);\n+        @ovtype@ r7 = (@ovtype@)vec_@simd@(a7, b7);\n+        @ovtype@ r8 = (@ovtype@)vec_@simd@(a8, b8);\n+        __vector unsigned char r =\n+            pack_vf64_vu8(r1, r2, r3, r4, r5, r6, r7, r8);\n+#else\n+        __vector unsigned char r = pack_vf32_vu8(r1, r2, r3, r4);\n+#endif\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        op[i] = ip1[i] @OP@ ip2[i];\n+    }\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n+/*\n+ *****************************************************************************\n+ **                           BOOL LOOPS\n+ *****************************************************************************\n+ */\n+\n+/**begin repeat\n+ * #kind  = equal, not_equal, greater, greater_equal, less, less_equal#\n+ * #simd  = cmpeq, cmpne, cmpgt, cmpge, cmplt, cmple #\n+ * #OP    = ==, !=, >, >=, <, <=#\n+ * #vtype = __vector unsigned char*6#\n+ * #stype = npy_bool*6#\n+ */\n+\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_scalar1_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n+                               npy_intp n)\n+{\n+    const @stype@ in1 = ip1[0] != 0;\n+    const @vtype@ a = vec_splats((@stype@)ip1[0]);\n+    const @vtype@ truemask = vec_splats((@stype@)0x1);\n+    LOOP_BLOCK_ALIGN_VAR(ip2, @stype@, VECTOR_SIZE_BYTES) {\n+        @stype@ in2 = ip2[i] != 0;\n+        op[i] = in1 @OP@ in2;\n+    }\n+    LOOP_BLOCKED(@stype@, VECTOR_SIZE_BYTES) {\n+        @vtype@ b = vec_xl(0, &ip2[i]);\n+        @vtype@ r = (@vtype@)vec_@simd@(a, b);\n+                r = vec_and(r, truemask);\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        @stype@ in2 = ip2[i] != 0;\n+        op[i] = in1 @OP@ in2;\n+    }\n+}\n+\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_scalar2_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n+                               npy_intp n)\n+{\n+    const @stype@ in2 = ip2[0] != 0;\n+    const @vtype@ b = vec_splats((@stype@)ip2[0]);\n+    const @vtype@ truemask = vec_splats((@stype@)0x1);\n+    LOOP_BLOCK_ALIGN_VAR(ip1, @stype@, VECTOR_SIZE_BYTES) {\n+        @stype@ in1 = ip1[i] != 0;\n+        op[i] = in1 @OP@ in2;\n+    }\n+    LOOP_BLOCKED(@stype@, VECTOR_SIZE_BYTES) {\n+        @vtype@ a = vec_xl(0, &ip1[i]);\n+        @vtype@ r = (@vtype@)vec_@simd@(a, b);\n+                r = vec_and(r, truemask);\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        @stype@ in1 = ip1[i] != 0;\n+        op[i] = in1 @OP@ in2;\n+    }\n+}\n+\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n+                       npy_intp n)\n+{\n+    const @vtype@ truemask = vec_splats((@stype@)0x1);\n+    LOOP_BLOCK_ALIGN_VAR(ip1, @stype@, VECTOR_SIZE_BYTES) {\n+        @stype@ in1 = ip1[i] != 0;\n+        @stype@ in2 = ip2[i] != 0;\n+        op[i] = in1 @OP@ in2;\n+    }\n+    LOOP_BLOCKED(@stype@, VECTOR_SIZE_BYTES) {\n+        @vtype@ a = vec_xl(0, &ip1[i]);\n+        @vtype@ b = vec_xl(0, &ip2[i]);\n+        @vtype@ r = (@vtype@)vec_@simd@(a, b);\n+                r = vec_and(r, truemask);\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        @stype@ in1 = ip1[i] != 0;\n+        @stype@ in2 = ip2[i] != 0;\n+        op[i] = in1 @OP@ in2;\n+    }\n+}\n+/**end repeat**/\n+\n+/**begin repeat\n+ * # kind  = logical_or, logical_and#\n+ * # OP    = ||, &&#\n+ * # simd  = or, and#\n+ * # vtype = __vector unsigned char*2#\n+ * # stype = npy_bool*2#\n+ */\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,\n+                       npy_intp n)\n+{\n+    const @vtype@ vzero = vec_splats((@stype@)0x0);\n+    const @vtype@ truemask = vec_splats((@stype@)0x1);\n+    LOOP_BLOCK_ALIGN_VAR(ip1, @stype@, VECTOR_SIZE_BYTES) {\n+        op[i] = ip1[i] @OP@ ip2[i];\n+    }\n+    LOOP_BLOCKED(@stype@, VECTOR_SIZE_BYTES) {\n+        @vtype@ a = vec_xl(0, &ip1[i]);\n+        @vtype@ b = vec_xl(0, &ip2[i]);\n+        /* equivalent to byte_to_true */\n+        @vtype@ tmp = (@vtype@)vec_@simd@(a, b);\n+                tmp = (@vtype@)vec_cmpeq(tmp, vzero);\n+        @vtype@ r = vec_andc(truemask, tmp);\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        op[i] = ip1[i] @OP@ ip2[i];\n+    }\n+}\n+/**end repeat**/\n+\n+/**begin repeat\n+ * # kind  = absolute, logical_not#\n+ * # OP    = !=, ==#\n+ * # not   = 0, 1#\n+ * # vtype = __vector unsigned char*2#\n+ * # stype = npy_bool*2#\n+ */\n+static void NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_VSX\n+vsx_@kind@_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)\n+{\n+    const @vtype@ vzero = vec_splats((@stype@)0x0);\n+    const @vtype@ truemask = vec_splats((@stype@)0x1);\n+    LOOP_BLOCK_ALIGN_VAR(ip, @stype@, VECTOR_SIZE_BYTES) {\n+        op[i] = ip[i] @OP@ 0;\n+    }\n+    LOOP_BLOCKED(@stype@, VECTOR_SIZE_BYTES) {\n+        @vtype@ a = vec_xl(0, &ip[i]);\n+        /* equivalent to byte_to_true */\n+        @vtype@ r = (@vtype@)vec_cmpeq(a, vzero);\n+#if @not@\n+        r = vec_and(r, truemask);\n+#else\n+        r = vec_andc(truemask, r);\n+#endif\n+        vec_xst(r, 0, &op[i]);\n+    }\n+    LOOP_BLOCKED_END {\n+        op[i] = ip[i] @OP@ 0;\n+    }\n+}\n+/**end repeat**/\n+\n+#undef VECTOR_SIZE_BYTES\n+#endif /* NPY_HAVE_VSX_INTRINSICS */\n+#endif"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20175,
        "body": "This PR includes the modifications required to fix #19010, plus several additions discussed in the issue, like the usage of `PySequence_Fast` and the corresponding family of functions.",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/_multiarray_tests.c.src",
                "patch": "@@ -2351,6 +2351,32 @@ npy_ensurenocopy(PyObject* NPY_UNUSED(self), PyObject* args)\n     Py_RETURN_NONE;\n }\n \n+static PyObject *\n+run_scalar_intp_converter(PyObject *NPY_UNUSED(self), PyObject *obj)\n+{\n+    PyArray_Dims dims;\n+    if (!PyArray_IntpConverter(obj, &dims)) {\n+        return NULL;\n+    }\n+    else {\n+        PyObject *result = PyArray_IntTupleFromIntp(dims.len, dims.ptr);\n+        PyDimMem_FREE(dims.ptr);\n+        return result;\n+    }\n+}\n+\n+static PyObject *\n+run_scalar_intp_from_sequence(PyObject *NPY_UNUSED(self), PyObject *obj)\n+{\n+    npy_intp vals[1];\n+\n+    int output = PyArray_IntpFromSequence(obj, vals, 1);\n+    if (output == -1) {\n+        return NULL;\n+    }\n+    return PyArray_IntTupleFromIntp(1, vals);\n+}\n+\n static PyMethodDef Multiarray_TestsMethods[] = {\n     {\"argparse_example_function\",\n          (PyCFunction)argparse_example_function,\n@@ -2541,6 +2567,12 @@ static PyMethodDef Multiarray_TestsMethods[] = {\n     {\"run_casting_converter\",\n         run_casting_converter,\n         METH_VARARGS, NULL},\n+    {\"run_scalar_intp_converter\",\n+        run_scalar_intp_converter,\n+        METH_O, NULL},\n+    {\"run_scalar_intp_from_sequence\",\n+        run_scalar_intp_from_sequence,\n+        METH_O, NULL},\n     {\"run_intp_converter\",\n         run_intp_converter,\n         METH_VARARGS, NULL},"
            },
            {
                "filename": "numpy/core/src/multiarray/conversion_utils.c",
                "patch": "@@ -78,6 +78,27 @@ PyArray_OutputConverter(PyObject *object, PyArrayObject **address)\n     }\n }\n \n+\n+/*\n+ * Convert the given value to an integer. Replaces the error when compared\n+ * to `PyArray_PyIntAsIntp`.  Exists mainly to retain old behaviour of\n+ * `PyArray_IntpConverter` and `PyArray_IntpFromSequence`\n+ */\n+static NPY_INLINE npy_intp\n+dimension_from_scalar(PyObject *ob)\n+{\n+    npy_intp value = PyArray_PyIntAsIntp(ob);\n+\n+    if (error_converting(value)) {\n+        if (PyErr_ExceptionMatches(PyExc_OverflowError)) {\n+            PyErr_SetString(PyExc_ValueError,\n+                    \"Maximum allowed dimension exceeded\");\n+        }\n+        return -1;\n+    }\n+    return value;\n+}\n+\n /*NUMPY_API\n  * Get intp chunk from sequence\n  *\n@@ -90,9 +111,6 @@ PyArray_OutputConverter(PyObject *object, PyArrayObject **address)\n NPY_NO_EXPORT int\n PyArray_IntpConverter(PyObject *obj, PyArray_Dims *seq)\n {\n-    Py_ssize_t len;\n-    int nd;\n-\n     seq->ptr = NULL;\n     seq->len = 0;\n \n@@ -110,42 +128,85 @@ PyArray_IntpConverter(PyObject *obj, PyArray_Dims *seq)\n         return NPY_SUCCEED;\n     }\n \n-    len = PySequence_Size(obj);\n-    if (len == -1) {\n-        /* Check to see if it is an integer number */\n-        if (PyNumber_Check(obj)) {\n-            /*\n-             * After the deprecation the PyNumber_Check could be replaced\n-             * by PyIndex_Check.\n-             * FIXME 1.9 ?\n-             */\n-            len = 1;\n+    PyObject *seq_obj = NULL;\n+\n+    /*\n+     * If obj is a scalar we skip all the useless computations and jump to\n+     * dimension_from_scalar as soon as possible.\n+     */\n+    if (!PyLong_CheckExact(obj) && PySequence_Check(obj)) {\n+        seq_obj = PySequence_Fast(obj,\n+               \"expected a sequence of integers or a single integer.\");\n+        if (seq_obj == NULL) {\n+            /* continue attempting to parse as a single integer. */\n+            PyErr_Clear();\n         }\n     }\n-    if (len < 0) {\n-        PyErr_SetString(PyExc_TypeError,\n-                \"expected sequence object with len >= 0 or a single integer\");\n-        return NPY_FAIL;\n-    }\n-    if (len > NPY_MAXDIMS) {\n-        PyErr_Format(PyExc_ValueError, \"maximum supported dimension for an ndarray is %d\"\n-                     \", found %d\", NPY_MAXDIMS, len);\n-        return NPY_FAIL;\n-    }\n-    if (len > 0) {\n-        seq->ptr = npy_alloc_cache_dim(len);\n+\n+    if (seq_obj == NULL) {\n+        /*\n+         * obj *might* be a scalar (if dimension_from_scalar does not fail, at\n+         * the moment no check have been performed to verify this hypothesis).\n+         */\n+        seq->ptr = npy_alloc_cache_dim(1);\n         if (seq->ptr == NULL) {\n             PyErr_NoMemory();\n             return NPY_FAIL;\n         }\n+        else {\n+            seq->len = 1;\n+\n+            seq->ptr[0] = dimension_from_scalar(obj);\n+            if (error_converting(seq->ptr[0])) {\n+                /*\n+                 * If the error occurred is a type error (cannot convert the\n+                 * value to an integer) communicate that we expected a sequence\n+                 * or an integer from the user.\n+                 */\n+                if (PyErr_ExceptionMatches(PyExc_TypeError)) {\n+                    PyErr_Format(PyExc_TypeError,\n+                            \"expected a sequence of integers or a single \"\n+                            \"integer, got '%.100R'\", obj);\n+                }\n+                npy_free_cache_dim_obj(*seq);\n+                seq->ptr = NULL;\n+                return NPY_FAIL;\n+            }\n+        }\n     }\n-    seq->len = len;\n-    nd = PyArray_IntpFromIndexSequence(obj, (npy_intp *)seq->ptr, len);\n-    if (nd == -1 || nd != len) {\n-        npy_free_cache_dim_obj(*seq);\n-        seq->ptr = NULL;\n-        return NPY_FAIL;\n+    else {\n+        /*\n+         * `obj` is a sequence converted to the `PySequence_Fast` in `seq_obj`\n+         */\n+        Py_ssize_t len = PySequence_Fast_GET_SIZE(seq_obj);\n+        if (len > NPY_MAXDIMS) {\n+            PyErr_Format(PyExc_ValueError,\n+                    \"maximum supported dimension for an ndarray \"\n+                    \"is %d, found %d\", NPY_MAXDIMS, len);\n+            Py_DECREF(seq_obj);\n+            return NPY_FAIL;\n+        }\n+        if (len > 0) {\n+            seq->ptr = npy_alloc_cache_dim(len);\n+            if (seq->ptr == NULL) {\n+                PyErr_NoMemory();\n+                Py_DECREF(seq_obj);\n+                return NPY_FAIL;\n+            }\n+        }\n+\n+        seq->len = len;\n+        int nd = PyArray_IntpFromIndexSequence(seq_obj,\n+                (npy_intp *)seq->ptr, len);\n+        Py_DECREF(seq_obj);\n+\n+        if (nd == -1 || nd != len) {\n+            npy_free_cache_dim_obj(*seq);\n+            seq->ptr = NULL;\n+            return NPY_FAIL;\n+        }\n     }\n+\n     return NPY_SUCCEED;\n }\n \n@@ -1004,64 +1065,35 @@ PyArray_IntpFromPyIntConverter(PyObject *o, npy_intp *val)\n }\n \n \n-/*\n- * PyArray_IntpFromIndexSequence\n- * Returns the number of dimensions or -1 if an error occurred.\n- * vals must be large enough to hold maxvals.\n- * Opposed to PyArray_IntpFromSequence it uses and returns npy_intp\n- * for the number of values.\n+/**\n+ * Reads values from a sequence of integers and stores them into an array.\n+ *\n+ * @param  seq      A sequence created using `PySequence_Fast`.\n+ * @param  vals     Array used to store dimensions (must be large enough to\n+ *                      hold `maxvals` values).\n+ * @param  max_vals Maximum number of dimensions that can be written into `vals`.\n+ * @return          Number of dimensions or -1 if an error occurred.\n+ *\n+ * .. note::\n+ *\n+ *   Opposed to PyArray_IntpFromSequence it uses and returns `npy_intp`\n+ *      for the number of values.\n  */\n NPY_NO_EXPORT npy_intp\n PyArray_IntpFromIndexSequence(PyObject *seq, npy_intp *vals, npy_intp maxvals)\n {\n-    Py_ssize_t nd;\n-    npy_intp i;\n-    PyObject *op, *err;\n-\n     /*\n-     * Check to see if sequence is a single integer first.\n-     * or, can be made into one\n+     * First of all, check if sequence is a scalar integer or if it can be\n+     * \"casted\" into a scalar.\n      */\n-    nd = PySequence_Length(seq);\n-    if (nd == -1) {\n-        if (PyErr_Occurred()) {\n-            PyErr_Clear();\n-        }\n+    Py_ssize_t nd = PySequence_Fast_GET_SIZE(seq);\n+    PyObject *op;\n+    for (Py_ssize_t i = 0; i < PyArray_MIN(nd, maxvals); i++) {\n+        op = PySequence_Fast_GET_ITEM(seq, i);\n \n-        vals[0] = PyArray_PyIntAsIntp(seq);\n-        if(vals[0] == -1) {\n-            err = PyErr_Occurred();\n-            if (err &&\n-                    PyErr_GivenExceptionMatches(err, PyExc_OverflowError)) {\n-                PyErr_SetString(PyExc_ValueError,\n-                        \"Maximum allowed dimension exceeded\");\n-            }\n-            if(err != NULL) {\n-                return -1;\n-            }\n-        }\n-        nd = 1;\n-    }\n-    else {\n-        for (i = 0; i < PyArray_MIN(nd,maxvals); i++) {\n-            op = PySequence_GetItem(seq, i);\n-            if (op == NULL) {\n-                return -1;\n-            }\n-\n-            vals[i] = PyArray_PyIntAsIntp(op);\n-            Py_DECREF(op);\n-            if(vals[i] == -1) {\n-                err = PyErr_Occurred();\n-                if (err &&\n-                        PyErr_GivenExceptionMatches(err, PyExc_OverflowError)) {\n-                    PyErr_SetString(PyExc_ValueError,\n-                            \"Maximum allowed dimension exceeded\");\n-                }\n-                if(err != NULL) {\n-                    return -1;\n-                }\n-            }\n+        vals[i] = dimension_from_scalar(op);\n+        if (error_converting(vals[i])) {\n+            return -1;\n         }\n     }\n     return nd;\n@@ -1075,7 +1107,34 @@ PyArray_IntpFromIndexSequence(PyObject *seq, npy_intp *vals, npy_intp maxvals)\n NPY_NO_EXPORT int\n PyArray_IntpFromSequence(PyObject *seq, npy_intp *vals, int maxvals)\n {\n-    return PyArray_IntpFromIndexSequence(seq, vals, (npy_intp)maxvals);\n+    PyObject *seq_obj = NULL;\n+    if (!PyLong_CheckExact(seq) && PySequence_Check(seq)) {\n+        seq_obj = PySequence_Fast(seq,\n+            \"expected a sequence of integers or a single integer\");\n+        if (seq_obj == NULL) {\n+            /* continue attempting to parse as a single integer. */\n+            PyErr_Clear();\n+        }\n+    }\n+\n+    if (seq_obj == NULL) {\n+        vals[0] = dimension_from_scalar(seq);\n+        if (error_converting(vals[0])) {\n+            if (PyErr_ExceptionMatches(PyExc_TypeError)) {\n+                PyErr_Format(PyExc_TypeError,\n+                        \"expected a sequence of integers or a single \"\n+                        \"integer, got '%.100R'\", seq);\n+            }\n+            return -1;\n+        }\n+        return 1;\n+    }\n+    else {\n+        int res;\n+        res = PyArray_IntpFromIndexSequence(seq_obj, vals, (npy_intp)maxvals);\n+        Py_DECREF(seq_obj);\n+        return res;\n+    }\n }\n \n "
            },
            {
                "filename": "numpy/core/tests/test_conversion_utils.py",
                "patch": "@@ -2,12 +2,13 @@\n Tests for numpy/core/src/multiarray/conversion_utils.c\n \"\"\"\n import re\n+import sys\n \n import pytest\n \n import numpy as np\n import numpy.core._multiarray_tests as mt\n-from numpy.testing import assert_warns\n+from numpy.testing import assert_warns, IS_PYPY\n \n \n class StringConverterTestCase:\n@@ -189,6 +190,8 @@ def test_none(self):\n         with pytest.warns(DeprecationWarning):\n             assert self.conv(None) == ()\n \n+    @pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+            reason=\"PyPy bug in error formatting\")\n     def test_float(self):\n         with pytest.raises(TypeError):\n             self.conv(1.0)"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -444,6 +444,9 @@ def test_array(self):\n     def test_array_empty(self):\n         assert_raises(TypeError, np.array)\n \n+    def test_0d_array_shape(self):\n+        assert np.ones(np.array(3)).shape == (3,)\n+\n     def test_array_copy_false(self):\n         d = np.array([1, 2, 3])\n         e = np.array(d, copy=False)\n@@ -506,6 +509,7 @@ def test_array_as_keyword(self, func):\n         else:\n             func(a=3)\n \n+\n class TestAssignment:\n     def test_assignment_broadcasting(self):\n         a = np.arange(6).reshape(2, 3)\n@@ -3954,6 +3958,41 @@ def test_IsPythonScalar(self):\n         assert_(IsPythonScalar(2.))\n         assert_(IsPythonScalar(\"a\"))\n \n+    @pytest.mark.parametrize(\"converter\",\n+             [_multiarray_tests.run_scalar_intp_converter,\n+              _multiarray_tests.run_scalar_intp_from_sequence])\n+    def test_intp_sequence_converters(self, converter):\n+        # Test simple values (-1 is special for error return paths)\n+        assert converter(10) == (10,)\n+        assert converter(-1) == (-1,)\n+        # A 0-D array looks a bit like a sequence but must take the integer\n+        # path:\n+        assert converter(np.array(123)) == (123,)\n+        # Test simple sequences (intp_from_sequence only supports length 1):\n+        assert converter((10,)) == (10,)\n+        assert converter(np.array([11])) == (11,)\n+\n+    @pytest.mark.parametrize(\"converter\",\n+             [_multiarray_tests.run_scalar_intp_converter,\n+              _multiarray_tests.run_scalar_intp_from_sequence])\n+    @pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+            reason=\"PyPy bug in error formatting\")\n+    def test_intp_sequence_converters_errors(self, converter):\n+        with pytest.raises(TypeError,\n+                match=\"expected a sequence of integers or a single integer, \"):\n+            converter(object())\n+        with pytest.raises(TypeError,\n+                match=\"expected a sequence of integers or a single integer, \"\n+                      \"got '32.0'\"):\n+            converter(32.)\n+        with pytest.raises(TypeError,\n+                match=\"'float' object cannot be interpreted as an integer\"):\n+            converter([32.])\n+        with pytest.raises(ValueError,\n+                match=\"Maximum allowed dimension\"):\n+            # These converters currently convert overflows to a ValueError\n+            converter(2**64)\n+\n \n class TestSubscripting:\n     def test_test_zero_rank(self):"
            },
            {
                "filename": "numpy/core/tests/test_regression.py",
                "patch": "@@ -2310,6 +2310,8 @@ def test_reshape_size_overflow(self):\n             new_shape = (2, 7, 7, 43826197)\n         assert_raises(ValueError, a.reshape, new_shape)\n \n+    @pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+            reason=\"PyPy bug in error formatting\")\n     def test_invalid_structured_dtypes(self):\n         # gh-2865\n         # mapping python objects to other dtypes"
            },
            {
                "filename": "numpy/lib/tests/test_format.py",
                "patch": "@@ -283,7 +283,7 @@\n import numpy as np\n from numpy.testing import (\n     assert_, assert_array_equal, assert_raises, assert_raises_regex,\n-    assert_warns,\n+    assert_warns, IS_PYPY,\n     )\n from numpy.lib import format\n \n@@ -940,6 +940,8 @@ def test_unicode_field_names(tmpdir):\n         float, np.dtype({'names': ['c'], 'formats': [np.dtype(int, metadata={})]})\n     ]}), False)\n     ])\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+        reason=\"PyPy bug in error formatting\")\n def test_metadata_dtype(dt, fail):\n     # gh-14142\n     arr = np.ones(10, dtype=dt)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21117,
        "body": "Fixes #21116\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/common.h",
                "patch": "@@ -269,17 +269,31 @@ npy_memchr(char * haystack, char needle,\n     else {\n         /* usually find elements to skip path */\n         if (!NPY_ALIGNMENT_REQUIRED && needle == 0 && stride == 1) {\n-            /* iterate until last multiple of 4 */\n-            char * block_end = haystack + size - (size % sizeof(unsigned int));\n-            while (p < block_end) {\n-                unsigned int  v = *(unsigned int*)p;\n-                if (v != 0) {\n-                    break;\n-                }\n-                p += sizeof(unsigned int);\n+            /* find the first sizeof(unsigned int)-aligned block */\n+            npy_intp first_block_offset = Py_MIN(\n+                sizeof(unsigned int) - (npy_uintp)p % sizeof(unsigned int),\n+                size);\n+            char * first_block = p + first_block_offset;\n+            while (p < first_block && *p == 0) {\n+                p++;\n             }\n-            /* handle rest */\n             subloopsize = (p - haystack);\n+\n+            if (subloopsize < size && *p == 0) {\n+                /* iterate until last multiple of sizeof(unsigned int) */\n+                npy_intp adjusted_size = size - first_block_offset;\n+                char * block_end = p +\n+                    adjusted_size - (adjusted_size % sizeof(unsigned int));\n+                while (p < block_end) {\n+                    unsigned int  v = *(unsigned int*)p;\n+                    if (v != 0) {\n+                        break;\n+                    }\n+                    p += sizeof(unsigned int);\n+                }\n+                /* handle rest */\n+                subloopsize = (p - haystack);\n+            }\n         }\n         while (subloopsize < size && *p == needle) {\n             subloopsize++;"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21381,
        "body": "This PR improves the documentation for\r\n\r\n* `np.correlate`: Use math environment instead of formula shown as code\r\n* `np.convolve`: Use more natural subscripts instead of brackets to denote item in sequence\r\n\r\nI've checked that it displays correctly in the html.",
        "changed_files": [
            {
                "filename": "numpy/core/numeric.py",
                "patch": "@@ -675,16 +675,16 @@ def _correlate_dispatcher(a, v, mode=None):\n \n @array_function_dispatch(_correlate_dispatcher)\n def correlate(a, v, mode='valid'):\n-    \"\"\"\n+    r\"\"\"\n     Cross-correlation of two 1-dimensional sequences.\n \n     This function computes the correlation as generally defined in signal\n-    processing texts::\n+    processing texts:\n \n-        c_{av}[k] = sum_n a[n+k] * conj(v[n])\n+    .. math:: c_k = \\sum_n a_{n+k} \\cdot \\overline{v_n}\n \n-    with a and v sequences being zero-padded where necessary and conj being\n-    the conjugate.\n+    with a and v sequences being zero-padded where necessary and\n+    :math:`\\overline x` denoting complex conjugation.\n \n     Parameters\n     ----------\n@@ -711,11 +711,11 @@ def correlate(a, v, mode='valid'):\n     Notes\n     -----\n     The definition of correlation above is not unique and sometimes correlation\n-    may be defined differently. Another common definition is::\n+    may be defined differently. Another common definition is:\n \n-        c'_{av}[k] = sum_n a[n] conj(v[n+k])\n+    .. math:: c'_k = \\sum_n a_{n} \\cdot \\overline{v_{n+k}}\n \n-    which is related to ``c_{av}[k]`` by ``c'_{av}[k] = c_{av}[-k]``.\n+    which is related to :math:`c_k` by :math:`c'_k = c_{-k}`.\n \n     `numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does\n     not use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might\n@@ -737,8 +737,8 @@ def correlate(a, v, mode='valid'):\n     array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])\n \n     Note that you get the time reversed, complex conjugated result\n-    when the two input sequences change places, i.e.,\n-    ``c_{va}[k] = c^{*}_{av}[-k]``:\n+    (:math:`\\overline{c_{-k}}`) when the two input sequences a and v change \n+    places:\n \n     >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')\n     array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])\n@@ -804,7 +804,7 @@ def convolve(a, v, mode='full'):\n     -----\n     The discrete convolution operation is defined as\n \n-    .. math:: (a * v)[n] = \\\\sum_{m = -\\\\infty}^{\\\\infty} a[m] v[n - m]\n+    .. math:: (a * v)_n = \\\\sum_{m = -\\\\infty}^{\\\\infty} a_m v_{n - m}\n \n     It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n     is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21394,
        "body": "The `numpy.linalg.norm` method has some overhead for small and medium sized arrays. This PR reduces the overhead by making two small adjustments to the implementation. Details are discussed below.\r\n\r\nBenchmark:\r\n```\r\nimport numpy as np\r\nx=np.random.rand(100,)\r\n%timeit np.linalg.norm(x) \r\n%timeit np.sqrt(x.dot(x)) # for comparison\r\n\r\nx=x+1j*np.random.rand(100,)\r\n%timeit np.linalg.norm(x)\r\n```\r\nResults for main\r\n```\r\n3.63 \u00b5s \u00b1 16.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n1.91 \u00b5s \u00b1 7.91 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n5.33 \u00b5s \u00b1 13.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n```\r\nThis PR\r\n```\r\n2.27 \u00b5s \u00b1 10.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n1.92 \u00b5s \u00b1 4.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n3.35 \u00b5s \u00b1 10.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)```\r\n```\r\n\r\n* The call `dot(x,x)` is replaced with `x.dot(x)`. Since `x` has been cast with `asarray` and `ravel`, we know that `x` is a 1D numpy array. An alternative would be `np.inner(x,x)`. The call with `inner` is faster than `dot(x,x)` but slower than `x.dot(x)`\r\n \r\n* The call to `np.sqrt` is replaced with a specialized call for float numbers (the common case). Ideally, the call would be replaced with a call to a numpy version of `sqrt` that only handles the scalar case. I could not find such a method exposed in the numpy codebase. Would it be possible to use a create such a method?\r\n\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/linalg/linalg.py",
                "patch": "@@ -2520,9 +2520,11 @@ def norm(x, ord=None, axis=None, keepdims=False):\n \n             x = x.ravel(order='K')\n             if isComplexType(x.dtype.type):\n-                sqnorm = dot(x.real, x.real) + dot(x.imag, x.imag)\n+                x_real = x.real\n+                x_imag = x.imag\n+                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n             else:\n-                sqnorm = dot(x, x)\n+                sqnorm = x.dot(x)\n             ret = sqrt(sqnorm)\n             if keepdims:\n                 ret = ret.reshape(ndim*[1])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21423,
        "body": "This PR reduces the overhead of `ufunc_generic_fastcall`, which is used by many numpy methods. For many input arguments a check is performed on the existence of the `__array_ufunc__` attribute. The check involves a call to `tp_getattro` from `PyTypeObject` which requires a `PyUniCode` object.\r\n\r\nThis PR avoids construction of the `PyUniCode` for each invocation of the attribute lookup.\r\n\r\nBenchmark\r\n```\r\nimport pyperf\r\nrunner = pyperf.Runner()\r\n\r\nrunner.timeit(name=f\"np.sqrt\", stmt=f\"np.sqrt(v)\", setup='import numpy as np; v=np.float64(1.1)')\r\nrunner.timeit(name=f\"np.cos\", stmt=f\"np.cos(v)\", setup='import numpy as np; v=np.float64(1.1)')\r\n```\r\nResults\r\n```\r\nnp.sqrt: Mean +- std dev: [base] 887 ns +- 37 ns -> [patch] 807 ns +- 49 ns: 1.10x faster\r\nnp.cos: Mean +- std dev: [base] 914 ns +- 46 ns -> [patch] 829 ns +- 61 ns: 1.10x faster\r\n\r\nGeometric mean: 1.10x faster\r\n```\r\n\r\n\r\n<details>\r\n<summary>Results of numpy benchmarks </summary>\r\nBenchmark:\r\n\r\n```\r\npython runtests.py --bench-compare main bench_ufunc\r\n```\r\nThe results show some tests with improved and some with decreased performance. It looks like some instability on my system.\r\nMost changes are in `bench_ufunc_strides.BinaryInt`, on a re-run that values changed and the increases or decreases do not look systematic. \r\n\r\nFull results\r\n```\r\nendebakpt@woelmuis:~/numpy$ python runtests.py --bench-compare main \"bench_ufunc\"\r\n\u00b7 Creating environments\r\n\u00b7 Discovering benchmarks\r\n\u00b7\u00b7 Uninstalling from virtualenv-py3.8-Cython\r\n\u00b7\u00b7 Building ee8c683a <performance_cache_unicode_array_ufunc> for virtualenv-py3.8-Cython..................................................\r\n\u00b7\u00b7 Installing ee8c683a <performance_cache_unicode_array_ufunc> into virtualenv-py3.8-Cython.\r\n\u00b7 Running 68 total benchmarks (2 commits * 1 environments * 34 benchmarks)\r\n[  0.00%] \u00b7 For numpy commit fd646bd6 <main> (round 1/2):\r\n[  0.00%] \u00b7\u00b7 Building for virtualenv-py3.8-Cython.....................................................\r\n[  0.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython\r\n[  0.00%] \u00b7\u00b7\u00b7 Importing benchmark suite produced output:\r\n[  0.00%] \u00b7\u00b7\u00b7\u00b7 NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?\r\n[  0.74%] \u00b7\u00b7\u00b7 Running (bench_ufunc.ArgParsing.time_add_arg_parsing--).........................\r\n[ 19.12%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.AVX_UFunc_log.time_log--).....\r\n[ 22.79%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.BinaryInt.time_ufunc--).\r\n[ 23.53%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.LogisticRegression.time_train--)..\r\n[ 25.00%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.Unary.time_ufunc--).\r\n[ 25.00%] \u00b7 For numpy commit ee8c683a <performance_cache_unicode_array_ufunc> (round 1/2):\r\n[ 25.00%] \u00b7\u00b7 Building for virtualenv-py3.8-Cython..\r\n[ 25.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython\r\n[ 25.74%] \u00b7\u00b7\u00b7 Running (bench_ufunc.ArgParsing.time_add_arg_parsing--).........................\r\n[ 44.12%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.AVX_UFunc_log.time_log--).....\r\n[ 47.79%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.BinaryInt.time_ufunc--).\r\n[ 48.53%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.LogisticRegression.time_train--)..\r\n[ 50.00%] \u00b7\u00b7\u00b7 Running (bench_ufunc_strides.Unary.time_ufunc--).\r\n[ 50.00%] \u00b7 For numpy commit ee8c683a <performance_cache_unicode_array_ufunc> (round 2/2):\r\n[ 50.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython\r\n[ 50.74%] \u00b7\u00b7\u00b7 bench_ufunc.ArgParsing.time_add_arg_parsing                                                                                                                                  ok\r\n[ 50.74%] \u00b7\u00b7\u00b7 =============================================================== ==========\r\n                                         arg_kwarg                                      \r\n              --------------------------------------------------------------- ----------\r\n                                   (array(1.), array(2.))                      710\u00b160ns \r\n                             (array(1.), array(2.), array(3.))                 598\u00b110ns \r\n                           (array(1.), array(2.), out=array(3.))               681\u00b120ns \r\n                          (array(1.), array(2.), out=(array(3.),))             660\u00b130ns \r\n               (array(1.), array(2.), out=array(3.), subok=True, where=True)   692\u00b120ns \r\n                             (array(1.), array(2.), subok=True)                788\u00b160ns \r\n                       (array(1.), array(2.), subok=True, where=True)          823\u00b160ns \r\n                 (array(1.), array(2.), array(3.), subok=True, where=True)     695\u00b130ns \r\n              =============================================================== ==========\r\n\r\n[ 51.47%] \u00b7\u00b7\u00b7 bench_ufunc.ArgParsingReduce.time_add_reduce_arg_parsing                                                                                                                     ok\r\n[ 51.47%] \u00b7\u00b7\u00b7 ====================================================== =============\r\n                                    arg_kwarg                                     \r\n              ------------------------------------------------------ -------------\r\n                                (array([0., 1.]))                     1.34\u00b10.01\u03bcs \r\n                               (array([0., 1.]), 0)                   1.36\u00b10.02\u03bcs \r\n                            (array([0., 1.]), axis=0)                 1.42\u00b10.02\u03bcs \r\n                            (array([0., 1.]), 0, None)                1.36\u00b10.02\u03bcs \r\n                      (array([0., 1.]), axis=0, dtype=None)           1.44\u00b10.01\u03bcs \r\n                      (array([0., 1.]), 0, None, array(0.))             1.23\u00b10\u03bcs  \r\n               (array([0., 1.]), axis=0, dtype=None, out=array(0.))   1.29\u00b10.01\u03bcs \r\n                         (array([0., 1.]), out=array(0.))             1.28\u00b10.03\u03bcs \r\n              ====================================================== =============\r\n\r\n[ 52.21%] \u00b7\u00b7\u00b7 bench_ufunc.Broadcast.time_broadcast                                                                                                                                10.5\u00b10.04ms\r\n[ 52.94%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_and_bool                                                                                                                                    1.68\u00b10.02\u03bcs\r\n[ 53.68%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_nonzero                                                                                                                                      13.1\u00b10.1\u03bcs\r\n[ 54.41%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_not_bool                                                                                                                                    1.46\u00b10.03\u03bcs\r\n[ 55.15%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_or_bool                                                                                                                                     1.61\u00b10.02\u03bcs\r\n[ 55.88%] \u00b7\u00b7\u00b7 bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int                                                                                                                  ok\r\n[ 55.88%] \u00b7\u00b7\u00b7 ============== ============= ============= =============\r\n              --                                size                  \r\n              -------------- -----------------------------------------\r\n                  dtype           100          10000        1000000   \r\n              ============== ============= ============= =============\r\n                numpy.int8    1.11\u00b10.01\u03bcs    73.7\u00b10.3\u03bcs   7.87\u00b10.03ms \r\n               numpy.int16    1.11\u00b10.02\u03bcs    75.1\u00b10.3\u03bcs   8.29\u00b10.05ms \r\n               numpy.int32    1.07\u00b10.03\u03bcs    94.0\u00b10.6\u03bcs   10.3\u00b10.05ms \r\n               numpy.int64    1.64\u00b10.02\u03bcs     161\u00b11\u03bcs      17.6\u00b10.2ms \r\n               numpy.uint8    1.00\u00b10.03\u03bcs    38.7\u00b10.1\u03bcs   3.76\u00b10.04ms \r\n               numpy.uint16   1.02\u00b10.03\u03bcs   38.8\u00b10.06\u03bcs   3.79\u00b10.01ms \r\n               numpy.uint32   1.03\u00b10.03\u03bcs    38.7\u00b10.1\u03bcs   3.83\u00b10.04ms \r\n               numpy.uint64   1.44\u00b10.03\u03bcs    82.3\u00b10.3\u03bcs   8.27\u00b10.06ms \r\n              ============== ============= ============= =============\r\n\r\n[ 56.62%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_char_or                                                                                                                               46.4\u00b10.2\u03bcs\r\n[ 57.35%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_char_or_temp                                                                                                                          60.0\u00b10.5\u03bcs\r\n[ 58.09%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_double_add                                                                                                                            56.6\u00b10.1\u03bcs\r\n[ 58.82%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_double_add_temp                                                                                                                       75.1\u00b10.1\u03bcs\r\n[ 59.56%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_float_add                                                                                                                             57.5\u00b10.3\u03bcs\r\n[ 60.29%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_float_add_temp                                                                                                                        76.1\u00b10.3\u03bcs\r\n[ 61.03%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_int_or                                                                                                                                56.5\u00b10.2\u03bcs\r\n[ 61.76%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_int_or_temp                                                                                                                           71.6\u00b10.6\u03bcs\r\n[ 62.50%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_add_scalar2                                                                                                                                    ok\r\n[ 62.50%] \u00b7\u00b7\u00b7 =============== ============\r\n                   dtype                  \r\n              --------------- ------------\r\n               numpy.float32   6.17\u00b10.2\u03bcs \r\n               numpy.float64   12.8\u00b10.1\u03bcs \r\n              =============== ============\r\n\r\n[ 63.24%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_divide_scalar2                                                                                                                                 ok\r\n[ 63.24%] \u00b7\u00b7\u00b7 =============== =============\r\n                   dtype                   \r\n              --------------- -------------\r\n               numpy.float32   12.4\u00b10.06\u03bcs \r\n               numpy.float64    26.4\u00b10.2\u03bcs \r\n              =============== =============\r\n\r\n[ 63.97%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_divide_scalar2_inplace                                                                                                                         ok\r\n[ 63.97%] \u00b7\u00b7\u00b7 =============== ============\r\n                   dtype                  \r\n              --------------- ------------\r\n               numpy.float32   12.7\u00b10.2\u03bcs \r\n               numpy.float64   26.4\u00b10.3\u03bcs \r\n              =============== ============\r\n\r\n[ 64.71%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_less_than_scalar2                                                                                                                              ok\r\n[ 64.71%] \u00b7\u00b7\u00b7 =============== =============\r\n                   dtype                   \r\n              --------------- -------------\r\n               numpy.float32   4.05\u00b10.08\u03bcs \r\n               numpy.float64    7.19\u00b10.1\u03bcs \r\n              =============== =============\r\n\r\n[ 65.44%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int                                                                                                                 ok\r\n[ 65.44%] \u00b7\u00b7\u00b7 ============== ============= ============= ============= =============\r\n              --                                     divisors                       \r\n              -------------- -------------------------------------------------------\r\n                  dtype            8             -8            43           -43     \r\n              ============== ============= ============= ============= =============\r\n                numpy.int8    3.03\u00b10.08\u03bcs    3.16\u00b10.2\u03bcs   3.01\u00b10.09\u03bcs    3.08\u00b10.2\u03bcs \r\n               numpy.int16    3.32\u00b10.07\u03bcs   3.23\u00b10.08\u03bcs    3.36\u00b10.2\u03bcs    3.34\u00b10.1\u03bcs \r\n               numpy.int32    5.60\u00b10.02\u03bcs    5.51\u00b10.1\u03bcs   5.61\u00b10.05\u03bcs   5.44\u00b10.06\u03bcs \r\n               numpy.int64    14.9\u00b10.03\u03bcs    15.1\u00b10.1\u03bcs    14.9\u00b10.1\u03bcs   14.8\u00b10.06\u03bcs \r\n               numpy.uint8     3.24\u00b10.1\u03bcs       n/a       3.24\u00b10.07\u03bcs       n/a     \r\n               numpy.uint16    3.63\u00b10.1\u03bcs       n/a       3.70\u00b10.08\u03bcs       n/a     \r\n               numpy.uint32    5.40\u00b10.2\u03bcs       n/a        5.43\u00b10.1\u03bcs       n/a     \r\n               numpy.uint64    12.1\u00b10.1\u03bcs       n/a        12.1\u00b10.2\u03bcs       n/a     \r\n              ============== ============= ============= ============= =============\r\n\r\n[ 65.44%] \u00b7\u00b7\u00b7\u00b7 For parameters: <class 'numpy.uint8'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint8'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint16'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint16'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint32'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint32'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint64'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint64'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n\r\n[ 66.18%] \u00b7\u00b7\u00b7 bench_ufunc.Scalar.time_add_scalar                                                                                                                                     575\u00b150ns\r\n[ 66.91%] \u00b7\u00b7\u00b7 bench_ufunc.Scalar.time_add_scalar_conv                                                                                                                                803\u00b120ns\r\n[ 67.65%] \u00b7\u00b7\u00b7 bench_ufunc.Scalar.time_add_scalar_conv_complex                                                                                                                        825\u00b130ns\r\n[ 68.38%] \u00b7\u00b7\u00b7 bench_ufunc.UFunc.time_ufunc_types                                                                                                                                           ok\r\n[ 68.38%] \u00b7\u00b7\u00b7 =============== =============\r\n                   ufunc                   \r\n              --------------- -------------\r\n                    abs          808\u00b14\u03bcs   \r\n                  absolute       813\u00b14\u03bcs   \r\n                    add          385\u00b11\u03bcs   \r\n                   arccos      6.26\u00b10.04ms \r\n                  arccosh      5.93\u00b10.02ms \r\n                   arcsin      6.32\u00b10.06ms \r\n                  arcsinh      5.82\u00b10.05ms \r\n                   arctan      3.47\u00b10.01ms \r\n                  arctan2        1.84\u00b10ms  \r\n                  arctanh      3.73\u00b10.02ms \r\n                bitwise_and     32.8\u00b10.3\u03bcs \r\n                bitwise_not    21.2\u00b10.04\u03bcs \r\n                 bitwise_or     32.6\u00b10.1\u03bcs \r\n                bitwise_xor     32.6\u00b10.1\u03bcs \r\n                    cbrt       1.85\u00b10.01ms \r\n                    ceil         208\u00b11\u03bcs   \r\n                    conj        189\u00b10.6\u03bcs  \r\n                 conjugate       190\u00b11\u03bcs   \r\n                  copysign      177\u00b10.7\u03bcs  \r\n                    cos        6.95\u00b10.03ms \r\n                    cosh       6.12\u00b10.05ms \r\n                  deg2rad        240\u00b11\u03bcs   \r\n                  degrees       240\u00b10.9\u03bcs  \r\n                   divide        681\u00b14\u03bcs   \r\n                   divmod        1.01\u00b10ms  \r\n                   equal         301\u00b13\u03bcs   \r\n                    exp        4.90\u00b10.05ms \r\n                    exp2       4.70\u00b10.03ms \r\n                   expm1       9.31\u00b10.04ms \r\n                    fabs        237\u00b10.8\u03bcs  \r\n                float_power    10.3\u00b10.04ms \r\n                   floor        209\u00b10.4\u03bcs  \r\n                floor_divide     887\u00b12\u03bcs   \r\n                    fmax        440\u00b10.8\u03bcs  \r\n                    fmin        434\u00b10.5\u03bcs  \r\n                    fmod         591\u00b12\u03bcs   \r\n                   frexp         342\u00b12\u03bcs   \r\n                    gcd         215\u00b10.5\u03bcs  \r\n                  greater        307\u00b11\u03bcs   \r\n               greater_equal     301\u00b11\u03bcs   \r\n                 heaviside       387\u00b12\u03bcs   \r\n                   hypot       1.04\u00b10.01ms \r\n                   invert       21.5\u00b10.3\u03bcs \r\n                  isfinite      169\u00b10.7\u03bcs  \r\n                   isinf        170\u00b10.5\u03bcs  \r\n                   isnan        146\u00b10.3\u03bcs  \r\n                   isnat         336\u00b12ns   \r\n                    lcm         336\u00b10.9\u03bcs  \r\n                   ldexp         213\u00b11\u03bcs   \r\n                 left_shift     98.8\u00b10.3\u03bcs \r\n                    less        296\u00b10.7\u03bcs  \r\n                 less_equal      292\u00b12\u03bcs   \r\n                    log        3.12\u00b10.01ms \r\n                   log10       3.35\u00b10.02ms \r\n                   log1p       3.32\u00b10.02ms \r\n                    log2       3.20\u00b10.03ms \r\n                 logaddexp       348\u00b110\u03bcs  \r\n                 logaddexp2      330\u00b11\u03bcs   \r\n                logical_and      317\u00b15\u03bcs   \r\n                logical_not      200\u00b12\u03bcs   \r\n                 logical_or     262\u00b10.8\u03bcs  \r\n                logical_xor      379\u00b16\u03bcs   \r\n                   matmul       22.7\u00b10.3ms \r\n                  maximum        416\u00b12\u03bcs   \r\n                  minimum        397\u00b11\u03bcs   \r\n                    mod          697\u00b12\u03bcs   \r\n                    modf         435\u00b12\u03bcs   \r\n                  multiply       397\u00b11\u03bcs   \r\n                  negative       224\u00b11\u03bcs   \r\n                 nextafter       399\u00b12\u03bcs   \r\n                 not_equal       311\u00b12\u03bcs   \r\n                  positive       231\u00b11\u03bcs   \r\n                   power       10.8\u00b10.02ms \r\n                  rad2deg        240\u00b11\u03bcs   \r\n                  radians       240\u00b10.7\u03bcs  \r\n                 reciprocal      724\u00b11\u03bcs   \r\n                 remainder       697\u00b13\u03bcs   \r\n                right_shift     102\u00b10.4\u03bcs  \r\n                    rint         417\u00b14\u03bcs   \r\n                    sign         249\u00b13\u03bcs   \r\n                  signbit       92.1\u00b10.4\u03bcs \r\n                    sin        6.70\u00b10.05ms \r\n                    sinh       6.64\u00b10.04ms \r\n                  spacing        432\u00b13\u03bcs   \r\n                    sqrt       1.52\u00b10.01ms \r\n                   square        244\u00b13\u03bcs   \r\n                  subtract       379\u00b12\u03bcs   \r\n                    tan        8.30\u00b10.04ms \r\n                    tanh       5.79\u00b10.02ms \r\n                true_divide      702\u00b140\u03bcs  \r\n                   trunc        204\u00b10.7\u03bcs  \r\n              =============== =============\r\n\r\n[ 69.12%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_UFunc_log.time_log                                                                                                                                   ok\r\n[ 69.12%] \u00b7\u00b7\u00b7 ======== ============= ============\r\n              --                 dtype           \r\n              -------- --------------------------\r\n               stride        f            d      \r\n              ======== ============= ============\r\n                 1       22.1\u00b10.8\u03bcs   59.9\u00b10.4\u03bcs \r\n                 2      28.8\u00b10.09\u03bcs   60.4\u00b10.4\u03bcs \r\n                 4       29.9\u00b10.2\u03bcs   60.9\u00b10.2\u03bcs \r\n              ======== ============= ============\r\n\r\n[ 69.85%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc                                                                                                                          ok\r\n[ 69.85%] \u00b7\u00b7\u00b7 ========== ============= ============= ============= ============= ============= ============\r\n              --                                           stride / dtype                                  \r\n              ---------- ----------------------------------------------------------------------------------\r\n                bfunc        1 / F         1 / D         2 / F         2 / D         4 / F        4 / D    \r\n              ========== ============= ============= ============= ============= ============= ============\r\n                 add      10.8\u00b10.05\u03bcs    16.4\u00b10.3\u03bcs   13.4\u00b10.08\u03bcs   23.8\u00b10.02\u03bcs   20.6\u00b10.04\u03bcs   38.2\u00b10.1\u03bcs \r\n               subtract   10.8\u00b10.05\u03bcs   16.4\u00b10.04\u03bcs   13.4\u00b10.06\u03bcs   23.9\u00b10.08\u03bcs   20.7\u00b10.04\u03bcs   38.5\u00b10.2\u03bcs \r\n               multiply   13.2\u00b10.06\u03bcs    16.6\u00b10.1\u03bcs    14.7\u00b10.1\u03bcs   23.9\u00b10.04\u03bcs    20.8\u00b10.1\u03bcs   38.7\u00b10.1\u03bcs \r\n                divide     42.9\u00b10.2\u03bcs    48.4\u00b10.1\u03bcs    43.1\u00b10.2\u03bcs    48.5\u00b10.2\u03bcs    44.1\u00b10.7\u03bcs   49.4\u00b10.2\u03bcs \r\n              ========== ============= ============= ============= ============= ============= ============\r\n\r\n[ 70.59%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_cmplx_funcs.time_ufunc                                                                                                                               ok\r\n[ 70.59%] \u00b7\u00b7\u00b7 ============ ============= ============ ============= ============= ============= ============\r\n              --                                             stride / dtype                                 \r\n              ------------ ---------------------------------------------------------------------------------\r\n                 bfunc         1 / F        1 / D         2 / F         2 / D         4 / F        4 / D    \r\n              ============ ============= ============ ============= ============= ============= ============\r\n               reciprocal    62.8\u00b10.3\u03bcs   72.1\u00b10.4\u03bcs    63.3\u00b10.3\u03bcs    72.1\u00b10.2\u03bcs    62.9\u00b10.4\u03bcs   72.7\u00b10.3\u03bcs \r\n                absolute     50.6\u00b10.2\u03bcs   50.7\u00b10.1\u03bcs    50.9\u00b10.1\u03bcs    52.1\u00b10.9\u03bcs    51.2\u00b10.3\u03bcs    54.2\u00b11\u03bcs  \r\n                 square     12.9\u00b10.03\u03bcs   13.9\u00b10.6\u03bcs   13.3\u00b10.09\u03bcs    16.8\u00b10.3\u03bcs   15.2\u00b10.09\u03bcs   22.6\u00b10.1\u03bcs \r\n               conjugate    8.29\u00b10.03\u03bcs   11.2\u00b10.2\u03bcs    9.52\u00b10.1\u03bcs   14.7\u00b10.05\u03bcs   12.2\u00b10.07\u03bcs   21.9\u00b10.2\u03bcs \r\n              ============ ============= ============ ============= ============= ============= ============\r\n\r\n[ 71.32%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_ldexp.time_ufunc                                                                                                                                     ok\r\n[ 71.32%] \u00b7\u00b7\u00b7 ======= ============ ============ ============\r\n              --                      stride                \r\n              ------- --------------------------------------\r\n               dtype       1            2            4      \r\n              ======= ============ ============ ============\r\n                 f     57.8\u00b10.7\u03bcs   58.1\u00b10.1\u03bcs   58.2\u00b10.3\u03bcs \r\n                 d     59.6\u00b10.2\u03bcs   59.8\u00b10.2\u03bcs   60.3\u00b10.5\u03bcs \r\n              ======= ============ ============ ============\r\n\r\n[ 72.06%] \u00b7\u00b7\u00b7 bench_ufunc_strides.Binary.time_ufunc                                                                                                                                        ok\r\n[ 72.06%] \u00b7\u00b7\u00b7 ========= ============ ============ ============= ============ ============ =========== =========== ==========\r\n              --                                                              stride_out / dtype                            \r\n              ----------------------------------- --------------------------------------------------------------------------\r\n                ufunc    stride_in0   stride_in1      1 / f        1 / d        2 / f        2 / d       4 / f      4 / d   \r\n              ========= ============ ============ ============= ============ ============ =========== =========== ==========\r\n               maximum       1            1         38.0\u00b10.3\u03bcs   74.5\u00b10.8\u03bcs    85.8\u00b15\u03bcs    111\u00b10.9\u03bcs    126\u00b13\u03bcs    225\u00b18\u03bcs  \r\n               maximum       1            2        75.6\u00b10.04\u03bcs    95.3\u00b11\u03bcs     111\u00b18\u03bcs      138\u00b11\u03bcs     129\u00b14\u03bcs    277\u00b110\u03bcs \r\n               maximum       1            4         79.4\u00b10.4\u03bcs    162\u00b15\u03bcs     115\u00b10.3\u03bcs     220\u00b18\u03bcs     146\u00b12\u03bcs    356\u00b120\u03bcs \r\n               maximum       2            1         75.9\u00b10.3\u03bcs   95.4\u00b10.9\u03bcs    105\u00b18\u03bcs      141\u00b11\u03bcs    130\u00b10.9\u03bcs   267\u00b110\u03bcs \r\n               maximum       2            2         129\u00b10.08\u03bcs    125\u00b14\u03bcs     146\u00b10.3\u03bcs     174\u00b12\u03bcs    149\u00b10.9\u03bcs   296\u00b19\u03bcs  \r\n               maximum       2            4         130\u00b10.4\u03bcs     214\u00b13\u03bcs     150\u00b10.7\u03bcs     263\u00b12\u03bcs     163\u00b12\u03bcs    412\u00b110\u03bcs \r\n               maximum       4            1         79.5\u00b10.3\u03bcs    158\u00b12\u03bcs      113\u00b11\u03bcs      215\u00b18\u03bcs     144\u00b12\u03bcs    389\u00b120\u03bcs \r\n               maximum       4            2         130\u00b10.4\u03bcs     201\u00b17\u03bcs      151\u00b12\u03bcs      260\u00b110\u03bcs    171\u00b12\u03bcs    419\u00b16\u03bcs  \r\n               maximum       4            4          137\u00b12\u03bcs      307\u00b17\u03bcs      163\u00b12\u03bcs      385\u00b14\u03bcs     199\u00b13\u03bcs    595\u00b130\u03bcs \r\n               minimum       1            1         37.8\u00b10.1\u03bcs   73.9\u00b10.3\u03bcs    86.2\u00b15\u03bcs    110\u00b10.7\u03bcs    126\u00b13\u03bcs    217\u00b15\u03bcs  \r\n               minimum       1            2         75.7\u00b10.1\u03bcs   95.9\u00b10.9\u03bcs    103\u00b110\u03bcs     139\u00b11\u03bcs     128\u00b14\u03bcs    260\u00b18\u03bcs  \r\n               minimum       1            4         79.3\u00b10.3\u03bcs    160\u00b12\u03bcs     112\u00b10.5\u03bcs     214\u00b11\u03bcs     144\u00b12\u03bcs    354\u00b16\u03bcs  \r\n               minimum       2            1         72.7\u00b10.6\u03bcs    95.9\u00b11\u03bcs    98.9\u00b110\u03bcs     141\u00b11\u03bcs     131\u00b12\u03bcs    272\u00b120\u03bcs \r\n               minimum       2            2         113\u00b10.6\u03bcs     124\u00b11\u03bcs      150\u00b12\u03bcs      173\u00b17\u03bcs    154\u00b10.9\u03bcs   291\u00b16\u03bcs  \r\n               minimum       2            4          117\u00b12\u03bcs      208\u00b13\u03bcs      157\u00b14\u03bcs      264\u00b14\u03bcs     175\u00b11\u03bcs    419\u00b17\u03bcs  \r\n               minimum       4            1         79.0\u00b10.5\u03bcs    158\u00b12\u03bcs      112\u00b11\u03bcs      215\u00b12\u03bcs     149\u00b14\u03bcs    386\u00b120\u03bcs \r\n               minimum       4            2         116\u00b10.3\u03bcs     204\u00b14\u03bcs     153\u00b10.6\u03bcs     262\u00b13\u03bcs     166\u00b12\u03bcs    425\u00b130\u03bcs \r\n               minimum       4            4          138\u00b16\u03bcs      304\u00b13\u03bcs      170\u00b12\u03bcs      386\u00b12\u03bcs     199\u00b17\u03bcs    566\u00b110\u03bcs \r\n                 fmax        1            1         38.0\u00b10.2\u03bcs   74.5\u00b10.1\u03bcs   60.2\u00b10.2\u03bcs    110\u00b12\u03bcs     94.2\u00b11\u03bcs   215\u00b16\u03bcs  \r\n                 fmax        1            2         75.9\u00b10.4\u03bcs    95.5\u00b11\u03bcs     98.4\u00b11\u03bcs     136\u00b12\u03bcs     112\u00b14\u03bcs    261\u00b17\u03bcs  \r\n                 fmax        1            4         79.5\u00b10.5\u03bcs    160\u00b13\u03bcs      111\u00b12\u03bcs      222\u00b14\u03bcs     133\u00b12\u03bcs    349\u00b14\u03bcs  \r\n                 fmax        2            1        75.6\u00b10.04\u03bcs    95.5\u00b12\u03bcs    93.8\u00b10.2\u03bcs    138\u00b11\u03bcs     112\u00b14\u03bcs    271\u00b110\u03bcs \r\n                 fmax        2            2         120\u00b10.1\u03bcs     124\u00b12\u03bcs     142\u00b10.5\u03bcs     171\u00b12\u03bcs    144\u00b10.6\u03bcs   316\u00b120\u03bcs \r\n                 fmax        2            4          127\u00b13\u03bcs      205\u00b15\u03bcs      155\u00b13\u03bcs      265\u00b15\u03bcs    160\u00b10.8\u03bcs   416\u00b15\u03bcs  \r\n                 fmax        4            1         79.2\u00b10.3\u03bcs    158\u00b13\u03bcs     107\u00b10.7\u03bcs     216\u00b17\u03bcs     139\u00b12\u03bcs    382\u00b120\u03bcs \r\n                 fmax        4            2         123\u00b10.9\u03bcs     204\u00b16\u03bcs     146\u00b10.7\u03bcs     260\u00b12\u03bcs     156\u00b12\u03bcs    420\u00b14\u03bcs  \r\n                 fmax        4            4          129\u00b12\u03bcs      303\u00b13\u03bcs      163\u00b12\u03bcs      384\u00b14\u03bcs     194\u00b13\u03bcs    558\u00b110\u03bcs \r\n                 fmin        1            1         37.8\u00b10.2\u03bcs    74.0\u00b11\u03bcs     62.5\u00b12\u03bcs    110\u00b10.6\u03bcs    93.3\u00b12\u03bcs   212\u00b18\u03bcs  \r\n                 fmin        1            2         75.5\u00b10.1\u03bcs    96.1\u00b11\u03bcs    93.9\u00b10.3\u03bcs    139\u00b12\u03bcs    108\u00b10.6\u03bcs   265\u00b15\u03bcs  \r\n                 fmin        1            4         79.6\u00b10.3\u03bcs    163\u00b15\u03bcs     110\u00b10.6\u03bcs     226\u00b17\u03bcs     137\u00b13\u03bcs    354\u00b14\u03bcs  \r\n                 fmin        2            1         72.1\u00b10.2\u03bcs    97.5\u00b12\u03bcs     96.0\u00b12\u03bcs     139\u00b11\u03bcs    108\u00b10.4\u03bcs   268\u00b120\u03bcs \r\n                 fmin        2            2         128\u00b10.2\u03bcs     122\u00b11\u03bcs      138\u00b11\u03bcs      171\u00b12\u03bcs     144\u00b11\u03bcs    299\u00b110\u03bcs \r\n                 fmin        2            4          133\u00b14\u03bcs      198\u00b110\u03bcs     151\u00b13\u03bcs      269\u00b110\u03bcs    164\u00b12\u03bcs    422\u00b14\u03bcs  \r\n                 fmin        4            1         77.9\u00b10.3\u03bcs    161\u00b12\u03bcs      110\u00b11\u03bcs      223\u00b18\u03bcs     131\u00b12\u03bcs    380\u00b17\u03bcs  \r\n                 fmin        4            2         130\u00b10.3\u03bcs     206\u00b16\u03bcs      144\u00b11\u03bcs      261\u00b15\u03bcs     166\u00b14\u03bcs    417\u00b12\u03bcs  \r\n                 fmin        4            4          148\u00b15\u03bcs      301\u00b13\u03bcs      157\u00b12\u03bcs      389\u00b18\u03bcs     199\u00b16\u03bcs    549\u00b19\u03bcs  \r\n              ========= ============ ============ ============= ============ ============ =========== =========== ==========\r\n\r\n[ 72.79%] \u00b7\u00b7\u00b7 bench_ufunc_strides.BinaryInt.time_ufunc                                                                                                                                     ok\r\n[ 72.79%] \u00b7\u00b7\u00b7 ========= ============ ============ ============ ======= =============\r\n                ufunc    stride_in0   stride_in1   stride_out   dtype               \r\n              --------- ------------ ------------ ------------ ------- -------------\r\n               maximum       1            1            1          b      8.65\u00b10.2\u03bcs \r\n               maximum       1            1            1          B      8.42\u00b10.2\u03bcs \r\n               maximum       1            1            1          h     17.6\u00b10.07\u03bcs \r\n               maximum       1            1            1          H     17.8\u00b10.09\u03bcs \r\n               maximum       1            1            1          i      33.8\u00b10.2\u03bcs \r\n               maximum       1            1            1          I      33.7\u00b10.1\u03bcs \r\n               maximum       1            1            1          l      65.9\u00b10.2\u03bcs \r\n               maximum       1            1            1          L      66.9\u00b10.5\u03bcs \r\n               maximum       1            1            1          q      66.5\u00b10.4\u03bcs \r\n               maximum       1            1            1          Q      67.2\u00b10.5\u03bcs \r\n               maximum       1            1            2          b       72.1\u00b11\u03bcs  \r\n               maximum       1            1            2          B      73.3\u00b10.9\u03bcs \r\n               maximum       1            1            2          h      71.6\u00b10.1\u03bcs \r\n               maximum       1            1            2          H       73.4\u00b12\u03bcs  \r\n               maximum       1            1            2          i      72.1\u00b10.7\u03bcs \r\n               maximum       1            1            2          I      72.0\u00b10.3\u03bcs \r\n               maximum       1            1            2          l       108\u00b15\u03bcs   \r\n               maximum       1            1            2          L       105\u00b11\u03bcs   \r\n               maximum       1            1            2          q       103\u00b12\u03bcs   \r\n               maximum       1            1            2          Q       110\u00b16\u03bcs   \r\n               maximum       1            1            4          b      75.6\u00b10.7\u03bcs \r\n               maximum       1            1            4          B      74.8\u00b10.3\u03bcs \r\n               maximum       1            1            4          h      71.9\u00b10.2\u03bcs \r\n               maximum       1            1            4          H      72.7\u00b10.6\u03bcs \r\n               maximum       1            1            4          i       87.4\u00b11\u03bcs  \r\n               maximum       1            1            4          I       87.6\u00b11\u03bcs  \r\n               maximum       1            1            4          l       202\u00b14\u03bcs   \r\n               maximum       1            1            4          L       198\u00b17\u03bcs   \r\n               maximum       1            1            4          q       194\u00b15\u03bcs   \r\n               maximum       1            1            4          Q       200\u00b15\u03bcs   \r\n               maximum       1            2            1          b      71.0\u00b10.6\u03bcs \r\n               maximum       1            2            1          B       72.4\u00b12\u03bcs  \r\n               maximum       1            2            1          h      71.7\u00b10.4\u03bcs \r\n               maximum       1            2            1          H      71.6\u00b10.3\u03bcs \r\n               maximum       1            2            1          i      72.4\u00b10.3\u03bcs \r\n               maximum       1            2            1          I      72.0\u00b10.5\u03bcs \r\n               maximum       1            2            1          l      92.9\u00b10.7\u03bcs \r\n               maximum       1            2            1          L       93.2\u00b11\u03bcs  \r\n               maximum       1            2            1          q       94.2\u00b12\u03bcs  \r\n               maximum       1            2            1          Q       95.4\u00b11\u03bcs  \r\n               maximum       1            2            2          b      71.0\u00b10.4\u03bcs \r\n               maximum       1            2            2          B       74.4\u00b11\u03bcs  \r\n               maximum       1            2            2          h       74.5\u00b13\u03bcs  \r\n               maximum       1            2            2          H      71.8\u00b10.4\u03bcs \r\n               maximum       1            2            2          i      74.1\u00b10.2\u03bcs \r\n               maximum       1            2            2          I       80.4\u00b16\u03bcs  \r\n               maximum       1            2            2          l       128\u00b13\u03bcs   \r\n               maximum       1            2            2          L       132\u00b13\u03bcs   \r\n               maximum       1            2            2          q      130\u00b10.6\u03bcs  \r\n               maximum       1            2            2          Q       132\u00b13\u03bcs   \r\n               maximum       1            2            4          b      75.1\u00b10.6\u03bcs \r\n               maximum       1            2            4          B      75.4\u00b10.4\u03bcs \r\n               maximum       1            2            4          h      72.3\u00b10.3\u03bcs \r\n               maximum       1            2            4          H      71.9\u00b10.2\u03bcs \r\n               maximum       1            2            4          i       93.2\u00b11\u03bcs  \r\n               maximum       1            2            4          I      92.5\u00b10.5\u03bcs \r\n               maximum       1            2            4          l       240\u00b16\u03bcs   \r\n               maximum       1            2            4          L       240\u00b18\u03bcs   \r\n               maximum       1            2            4          q       243\u00b14\u03bcs   \r\n               maximum       1            2            4          Q       251\u00b110\u03bcs  \r\n               maximum       1            4            1          b      70.1\u00b10.4\u03bcs \r\n               maximum       1            4            1          B       73.0\u00b13\u03bcs  \r\n               maximum       1            4            1          h      72.0\u00b10.3\u03bcs \r\n               maximum       1            4            1          H      71.9\u00b10.3\u03bcs \r\n               maximum       1            4            1          i      83.0\u00b10.2\u03bcs \r\n               maximum       1            4            1          I       83.1\u00b13\u03bcs  \r\n               maximum       1            4            1          l       155\u00b15\u03bcs   \r\n               maximum       1            4            1          L       155\u00b16\u03bcs   \r\n               maximum       1            4            1          q       155\u00b12\u03bcs   \r\n               maximum       1            4            1          Q       154\u00b15\u03bcs   \r\n               maximum       1            4            2          b      70.9\u00b10.4\u03bcs \r\n               maximum       1            4            2          B      70.9\u00b10.3\u03bcs \r\n               maximum       1            4            2          h      72.0\u00b10.3\u03bcs \r\n               maximum       1            4            2          H      72.1\u00b10.2\u03bcs \r\n               maximum       1            4            2          i      91.5\u00b10.9\u03bcs \r\n               maximum       1            4            2          I       94.8\u00b14\u03bcs  \r\n               maximum       1            4            2          l       209\u00b12\u03bcs   \r\n               maximum       1            4            2          L       219\u00b15\u03bcs   \r\n               maximum       1            4            2          q       205\u00b14\u03bcs   \r\n               maximum       1            4            2          Q       207\u00b19\u03bcs   \r\n               maximum       1            4            4          b       76.5\u00b11\u03bcs  \r\n               maximum       1            4            4          B      77.4\u00b10.8\u03bcs \r\n               maximum       1            4            4          h      73.0\u00b10.3\u03bcs \r\n               maximum       1            4            4          H       77.2\u00b15\u03bcs  \r\n               maximum       1            4            4          i       114\u00b12\u03bcs   \r\n               maximum       1            4            4          I       115\u00b13\u03bcs   \r\n               maximum       1            4            4          l       331\u00b120\u03bcs  \r\n               maximum       1            4            4          L       374\u00b130\u03bcs  \r\n               maximum       1            4            4          q       329\u00b13\u03bcs   \r\n               maximum       1            4            4          Q       332\u00b110\u03bcs  \r\n               maximum       2            1            1          b      69.9\u00b10.3\u03bcs \r\n               maximum       2            1            1          B      75.8\u00b10.4\u03bcs \r\n               maximum       2            1            1          h       77.7\u00b12\u03bcs  \r\n               maximum       2            1            1          H      71.9\u00b10.5\u03bcs \r\n               maximum       2            1            1          i       79.1\u00b17\u03bcs  \r\n               maximum       2            1            1          I       78.4\u00b17\u03bcs  \r\n               maximum       2            1            1          l       94.8\u00b11\u03bcs  \r\n               maximum       2            1            1          L       99.6\u00b14\u03bcs  \r\n               maximum       2            1            1          q      93.4\u00b10.6\u03bcs \r\n               maximum       2            1            1          Q       95.6\u00b11\u03bcs  \r\n               maximum       2            1            2          b       73.2\u00b12\u03bcs  \r\n               maximum       2            1            2          B       73.1\u00b12\u03bcs  \r\n               maximum       2            1            2          h      71.4\u00b10.2\u03bcs \r\n               maximum       2            1            2          H      71.4\u00b10.1\u03bcs \r\n               maximum       2            1            2          i      72.8\u00b10.2\u03bcs \r\n               maximum       2            1            2          I      73.0\u00b10.5\u03bcs \r\n               maximum       2            1            2          l       126\u00b12\u03bcs   \r\n               maximum       2            1            2          L       128\u00b12\u03bcs   \r\n               maximum       2            1            2          q       132\u00b14\u03bcs   \r\n               maximum       2            1            2          Q       127\u00b12\u03bcs   \r\n               maximum       2            1            4          b      75.2\u00b10.3\u03bcs \r\n               maximum       2            1            4          B      74.9\u00b10.3\u03bcs \r\n               maximum       2            1            4          h      72.7\u00b10.8\u03bcs \r\n               maximum       2            1            4          H      72.1\u00b10.6\u03bcs \r\n               maximum       2            1            4          i      93.9\u00b10.8\u03bcs \r\n               maximum       2            1            4          I       102\u00b16\u03bcs   \r\n               maximum       2            1            4          l       238\u00b110\u03bcs  \r\n               maximum       2            1            4          L       259\u00b16\u03bcs   \r\n               maximum       2            1            4          q       244\u00b120\u03bcs  \r\n               maximum       2            1            4          Q       250\u00b120\u03bcs  \r\n               maximum       2            2            1          b      70.0\u00b10.3\u03bcs \r\n               maximum       2            2            1          B       73.8\u00b13\u03bcs  \r\n               maximum       2            2            1          h      71.9\u00b10.3\u03bcs \r\n               maximum       2            2            1          H       75.6\u00b14\u03bcs  \r\n               maximum       2            2            1          i       77.1\u00b16\u03bcs  \r\n               maximum       2            2            1          I       81.2\u00b18\u03bcs  \r\n               maximum       2            2            1          l       120\u00b14\u03bcs   \r\n               maximum       2            2            1          L       122\u00b13\u03bcs   \r\n               maximum       2            2            1          q       125\u00b15\u03bcs   \r\n               maximum       2            2            1          Q      120\u00b10.7\u03bcs  \r\n               maximum       2            2            2          b      71.2\u00b10.6\u03bcs \r\n               maximum       2            2            2          B      71.2\u00b10.4\u03bcs \r\n               maximum       2            2            2          h      71.6\u00b10.2\u03bcs \r\n               maximum       2            2            2          H      71.3\u00b10.2\u03bcs \r\n               maximum       2            2            2          i       77.9\u00b18\u03bcs  \r\n               maximum       2            2            2          I       77.8\u00b11\u03bcs  \r\n               maximum       2            2            2          l       161\u00b14\u03bcs   \r\n               maximum       2            2            2          L       157\u00b12\u03bcs   \r\n               maximum       2            2            2          q       156\u00b12\u03bcs   \r\n               maximum       2            2            2          Q       159\u00b14\u03bcs   \r\n               maximum       2            2            4          b      74.7\u00b10.4\u03bcs \r\n               maximum       2            2            4          B      75.5\u00b10.4\u03bcs \r\n               maximum       2            2            4          h      72.1\u00b10.3\u03bcs \r\n               maximum       2            2            4          H      72.5\u00b10.5\u03bcs \r\n               maximum       2            2            4          i      101\u00b10.6\u03bcs  \r\n               maximum       2            2            4          I      103\u00b10.8\u03bcs  \r\n               maximum       2            2            4          l       303\u00b110\u03bcs  \r\n               maximum       2            2            4          L       304\u00b130\u03bcs  \r\n               maximum       2            2            4          q       299\u00b110\u03bcs  \r\n               maximum       2            2            4          Q       277\u00b18\u03bcs   \r\n               maximum       2            4            1          b      70.9\u00b10.4\u03bcs \r\n               maximum       2            4            1          B       72.9\u00b14\u03bcs  \r\n               maximum       2            4            1          h      72.1\u00b10.3\u03bcs \r\n               maximum       2            4            1          H      72.2\u00b10.3\u03bcs \r\n               maximum       2            4            1          i       88.8\u00b12\u03bcs  \r\n               maximum       2            4            1          I       92.5\u00b16\u03bcs  \r\n               maximum       2            4            1          l       191\u00b12\u03bcs   \r\n               maximum       2            4            1          L       200\u00b18\u03bcs   \r\n               maximum       2            4            1          q       194\u00b15\u03bcs   \r\n               maximum       2            4            1          Q       195\u00b13\u03bcs   \r\n               maximum       2            4            2          b      71.1\u00b10.3\u03bcs \r\n               maximum       2            4            2          B       74.2\u00b13\u03bcs  \r\n               maximum       2            4            2          h      73.1\u00b10.4\u03bcs \r\n               maximum       2            4            2          H      72.7\u00b10.4\u03bcs \r\n               maximum       2            4            2          i       103\u00b13\u03bcs   \r\n               maximum       2            4            2          I       107\u00b11\u03bcs   \r\n               maximum       2            4            2          l       252\u00b14\u03bcs   \r\n               maximum       2            4            2          L       258\u00b110\u03bcs  \r\n               maximum       2            4            2          q       251\u00b13\u03bcs   \r\n               maximum       2            4            2          Q       252\u00b16\u03bcs   \r\n               maximum       2            4            4          b      76.0\u00b10.4\u03bcs \r\n               maximum       2            4            4          B       77.6\u00b11\u03bcs  \r\n               maximum       2            4            4          h      73.8\u00b10.5\u03bcs \r\n               maximum       2            4            4          H      73.9\u00b10.5\u03bcs \r\n               maximum       2            4            4          i       130\u00b12\u03bcs   \r\n               maximum       2            4            4          I       132\u00b13\u03bcs   \r\n               maximum       2            4            4          l       398\u00b14\u03bcs   \r\n               maximum       2            4            4          L       405\u00b15\u03bcs   \r\n               maximum       2            4            4          q       406\u00b120\u03bcs  \r\n               maximum       2            4            4          Q       403\u00b15\u03bcs   \r\n               maximum       4            1            1          b      70.4\u00b10.3\u03bcs \r\n               maximum       4            1            1          B      70.9\u00b10.4\u03bcs \r\n               maximum       4            1            1          h      71.8\u00b10.3\u03bcs \r\n               maximum       4            1            1          H      72.2\u00b10.2\u03bcs \r\n               maximum       4            1            1          i      84.8\u00b10.5\u03bcs \r\n               maximum       4            1            1          I      85.5\u00b10.9\u03bcs \r\n               maximum       4            1            1          l       156\u00b14\u03bcs   \r\n               maximum       4            1            1          L       152\u00b15\u03bcs   \r\n               maximum       4            1            1          q       152\u00b14\u03bcs   \r\n               maximum       4            1            1          Q       154\u00b14\u03bcs   \r\n               maximum       4            1            2          b      71.5\u00b10.3\u03bcs \r\n               maximum       4            1            2          B      70.8\u00b10.3\u03bcs \r\n               maximum       4            1            2          h      72.1\u00b10.2\u03bcs \r\n               maximum       4            1            2          H      72.1\u00b10.3\u03bcs \r\n               maximum       4            1            2          i       96.3\u00b17\u03bcs  \r\n               maximum       4            1            2          I       94.6\u00b12\u03bcs  \r\n               maximum       4            1            2          l       213\u00b18\u03bcs   \r\n               maximum       4            1            2          L       220\u00b110\u03bcs  \r\n               maximum       4            1            2          q       222\u00b120\u03bcs  \r\n               maximum       4            1            2          Q       213\u00b16\u03bcs   \r\n               maximum       4            1            4          b       76.4\u00b11\u03bcs  \r\n               maximum       4            1            4          B       77.2\u00b12\u03bcs  \r\n               maximum       4            1            4          h       75.9\u00b13\u03bcs  \r\n               maximum       4            1            4          H       75.9\u00b13\u03bcs  \r\n               maximum       4            1            4          i       122\u00b12\u03bcs   \r\n               maximum       4            1            4          I       119\u00b14\u03bcs   \r\n               maximum       4            1            4          l       371\u00b120\u03bcs  \r\n               maximum       4            1            4          L       365\u00b110\u03bcs  \r\n               maximum       4            1            4          q       384\u00b130\u03bcs  \r\n               maximum       4            1            4          Q       380\u00b120\u03bcs  \r\n               maximum       4            2            1          b      72.1\u00b10.9\u03bcs \r\n               maximum       4            2            1          B       71.8\u00b12\u03bcs  \r\n               maximum       4            2            1          h       79.9\u00b14\u03bcs  \r\n               maximum       4            2            1          H       78.9\u00b15\u03bcs  \r\n               maximum       4            2            1          i       97.7\u00b14\u03bcs  \r\n               maximum       4            2            1          I       92.2\u00b13\u03bcs  \r\n               maximum       4            2            1          l       204\u00b15\u03bcs   \r\n               maximum       4            2            1          L       190\u00b13\u03bcs   \r\n               maximum       4            2            1          q       206\u00b13\u03bcs   \r\n               maximum       4            2            1          Q       204\u00b110\u03bcs  \r\n               maximum       4            2            2          b       74.3\u00b13\u03bcs  \r\n               maximum       4            2            2          B       72.9\u00b12\u03bcs  \r\n               maximum       4            2            2          h       80.4\u00b14\u03bcs  \r\n               maximum       4            2            2          H       75.1\u00b11\u03bcs  \r\n               maximum       4            2            2          i       99.3\u00b13\u03bcs  \r\n               maximum       4            2            2          I       102\u00b14\u03bcs   \r\n               maximum       4            2            2          l       255\u00b110\u03bcs  \r\n               maximum       4            2            2          L       262\u00b18\u03bcs   \r\n               maximum       4            2            2          q       277\u00b110\u03bcs  \r\n               maximum       4            2            2          Q       263\u00b16\u03bcs   \r\n               maximum       4            2            4          b       78.7\u00b13\u03bcs  \r\n               maximum       4            2            4          B       78.6\u00b13\u03bcs  \r\n               maximum       4            2            4          h       75.4\u00b12\u03bcs  \r\n               maximum       4            2            4          H       78.8\u00b15\u03bcs  \r\n               maximum       4            2            4          i       140\u00b13\u03bcs   \r\n               maximum       4            2            4          I       129\u00b12\u03bcs   \r\n               maximum       4            2            4          l       423\u00b120\u03bcs  \r\n               maximum       4            2            4          L       442\u00b120\u03bcs  \r\n               maximum       4            2            4          q       439\u00b110\u03bcs  \r\n               maximum       4            2            4          Q       457\u00b120\u03bcs  \r\n               maximum       4            4            1          b       72.9\u00b13\u03bcs  \r\n               maximum       4            4            1          B       79.1\u00b12\u03bcs  \r\n               maximum       4            4            1          h      73.6\u00b10.6\u03bcs \r\n               maximum       4            4            1          H       77.6\u00b14\u03bcs  \r\n               maximum       4            4            1          i       111\u00b17\u03bcs   \r\n               maximum       4            4            1          I       108\u00b14\u03bcs   \r\n               maximum       4            4            1          l       310\u00b110\u03bcs  \r\n               maximum       4            4            1          L       295\u00b19\u03bcs   \r\n               maximum       4            4            1          q       297\u00b13\u03bcs   \r\n               maximum       4            4            1          Q       302\u00b19\u03bcs   \r\n               maximum       4            4            2          b      71.0\u00b10.3\u03bcs \r\n               maximum       4            4            2          B       77.2\u00b16\u03bcs  \r\n               maximum       4            4            2          h       80.7\u00b17\u03bcs  \r\n               maximum       4            4            2          H       87.4\u00b17\u03bcs  \r\n               maximum       4            4            2          i       118\u00b14\u03bcs   \r\n               maximum       4            4            2          I       116\u00b12\u03bcs   \r\n               maximum       4            4            2          l       374\u00b110\u03bcs  \r\n               maximum       4            4            2          L       384\u00b120\u03bcs  \r\n               maximum       4            4            2          q       389\u00b120\u03bcs  \r\n               maximum       4            4            2          Q       389\u00b120\u03bcs  \r\n               maximum       4            4            4          b       79.4\u00b13\u03bcs  \r\n               maximum       4            4            4          B       77.2\u00b12\u03bcs  \r\n               maximum       4            4            4          h      76.8\u00b10.8\u03bcs \r\n               maximum       4            4            4          H      76.2\u00b10.7\u03bcs \r\n               maximum       4            4            4          i       161\u00b14\u03bcs   \r\n               maximum       4            4            4          I       161\u00b12\u03bcs   \r\n               maximum       4            4            4          l       564\u00b110\u03bcs  \r\n               maximum       4            4            4          L       627\u00b140\u03bcs  \r\n               maximum       4            4            4          q       616\u00b150\u03bcs  \r\n               maximum       4            4            4          Q       565\u00b150\u03bcs  \r\n               minimum       1            1            1          b     8.43\u00b10.08\u03bcs \r\n               minimum       1            1            1          B      8.45\u00b10.2\u03bcs \r\n               minimum       1            1            1          h     17.7\u00b10.06\u03bcs \r\n               minimum       1            1            1          H      17.7\u00b10.1\u03bcs \r\n               minimum       1            1            1          i      33.7\u00b10.2\u03bcs \r\n               minimum       1            1            1          I     33.7\u00b10.08\u03bcs \r\n               minimum       1            1            1          l      66.1\u00b10.5\u03bcs \r\n               minimum       1            1            1          L      66.4\u00b10.6\u03bcs \r\n               minimum       1            1            1          q      66.3\u00b10.4\u03bcs \r\n               minimum       1            1            1          Q      66.4\u00b10.3\u03bcs \r\n               minimum       1            1            2          b      71.0\u00b10.3\u03bcs \r\n               minimum       1            1            2          B       79.8\u00b13\u03bcs  \r\n               minimum       1            1            2          h      71.3\u00b10.3\u03bcs \r\n               minimum       1            1            2          H       81.6\u00b14\u03bcs  \r\n               minimum       1            1            2          i      72.3\u00b10.6\u03bcs \r\n               minimum       1            1            2          I       84.5\u00b17\u03bcs  \r\n               minimum       1            1            2          l      102\u00b10.2\u03bcs  \r\n               minimum       1            1            2          L      102\u00b10.9\u03bcs  \r\n               minimum       1            1            2          q       110\u00b12\u03bcs   \r\n               minimum       1            1            2          Q      102\u00b10.6\u03bcs  \r\n               minimum       1            1            4          b      75.0\u00b10.5\u03bcs \r\n               minimum       1            1            4          B      79.3\u00b10.3\u03bcs \r\n               minimum       1            1            4          h       74.2\u00b13\u03bcs  \r\n               minimum       1            1            4          H      78.0\u00b10.5\u03bcs \r\n               minimum       1            1            4          i       86.8\u00b12\u03bcs  \r\n               minimum       1            1            4          I       88.3\u00b11\u03bcs  \r\n               minimum       1            1            4          l       194\u00b16\u03bcs   \r\n               minimum       1            1            4          L       195\u00b14\u03bcs   \r\n               minimum       1            1            4          q       197\u00b15\u03bcs   \r\n               minimum       1            1            4          Q       199\u00b14\u03bcs   \r\n               minimum       1            2            1          b       73.3\u00b12\u03bcs  \r\n               minimum       1            2            1          B      77.4\u00b10.5\u03bcs \r\n               minimum       1            2            1          h       74.4\u00b13\u03bcs  \r\n               minimum       1            2            1          H      77.5\u00b10.2\u03bcs \r\n               minimum       1            2            1          i      83.0\u00b10.7\u03bcs \r\n               minimum       1            2            1          I       78.9\u00b16\u03bcs  \r\n               minimum       1            2            1          l       95.3\u00b12\u03bcs  \r\n               minimum       1            2            1          L      99.8\u00b10.9\u03bcs \r\n               minimum       1            2            1          q       93.0\u00b15\u03bcs  \r\n               minimum       1            2            1          Q       106\u00b16\u03bcs   \r\n               minimum       1            2            2          b       72.8\u00b11\u03bcs  \r\n               minimum       1            2            2          B      79.4\u00b10.3\u03bcs \r\n               minimum       1            2            2          h      72.3\u00b10.6\u03bcs \r\n               minimum       1            2            2          H       82.0\u00b15\u03bcs  \r\n               minimum       1            2            2          i       79.7\u00b16\u03bcs  \r\n               minimum       1            2            2          I      79.6\u00b10.3\u03bcs \r\n               minimum       1            2            2          l       128\u00b15\u03bcs   \r\n               minimum       1            2            2          L       129\u00b15\u03bcs   \r\n               minimum       1            2            2          q       126\u00b12\u03bcs   \r\n               minimum       1            2            2          Q       127\u00b13\u03bcs   \r\n               minimum       1            2            4          b       77.8\u00b12\u03bcs  \r\n               minimum       1            2            4          B      79.2\u00b10.2\u03bcs \r\n               minimum       1            2            4          h      71.9\u00b10.4\u03bcs \r\n               minimum       1            2            4          H      78.2\u00b10.3\u03bcs \r\n               minimum       1            2            4          i      92.7\u00b10.3\u03bcs \r\n               minimum       1            2            4          I      94.2\u00b10.7\u03bcs \r\n               minimum       1            2            4          l       244\u00b16\u03bcs   \r\n               minimum       1            2            4          L       243\u00b17\u03bcs   \r\n               minimum       1            2            4          q       245\u00b110\u03bcs  \r\n               minimum       1            2            4          Q       238\u00b16\u03bcs   \r\n               minimum       1            4            1          b      70.7\u00b10.3\u03bcs \r\n               minimum       1            4            1          B      77.9\u00b10.2\u03bcs \r\n               minimum       1            4            1          h       72.3\u00b12\u03bcs  \r\n               minimum       1            4            1          H      78.3\u00b10.4\u03bcs \r\n               minimum       1            4            1          i      83.4\u00b10.2\u03bcs \r\n               minimum       1            4            1          I      92.1\u00b10.6\u03bcs \r\n               minimum       1            4            1          l       155\u00b13\u03bcs   \r\n               minimum       1            4            1          L       149\u00b13\u03bcs   \r\n               minimum       1            4            1          q       154\u00b16\u03bcs   \r\n               minimum       1            4            1          Q       151\u00b15\u03bcs   \r\n               minimum       1            4            2          b       75.9\u00b13\u03bcs  \r\n               minimum       1            4            2          B      85.6\u00b10.3\u03bcs \r\n               minimum       1            4            2          h      72.2\u00b10.3\u03bcs \r\n               minimum       1            4            2          H      78.1\u00b10.4\u03bcs \r\n               minimum       1            4            2          i      89.7\u00b10.3\u03bcs \r\n               minimum       1            4            2          I       97.0\u00b11\u03bcs  \r\n               minimum       1            4            2          l       206\u00b17\u03bcs   \r\n               minimum       1            4            2          L       208\u00b12\u03bcs   \r\n               minimum       1            4            2          q       211\u00b14\u03bcs   \r\n               minimum       1            4            2          Q       212\u00b110\u03bcs  \r\n               minimum       1            4            4          b       75.4\u00b12\u03bcs  \r\n               minimum       1            4            4          B       80.2\u00b12\u03bcs  \r\n               minimum       1            4            4          h      72.5\u00b10.5\u03bcs \r\n               minimum       1            4            4          H      78.5\u00b10.2\u03bcs \r\n               minimum       1            4            4          i       113\u00b12\u03bcs   \r\n               minimum       1            4            4          I       118\u00b12\u03bcs   \r\n               minimum       1            4            4          l       350\u00b130\u03bcs  \r\n               minimum       1            4            4          L       336\u00b17\u03bcs   \r\n               minimum       1            4            4          q       335\u00b130\u03bcs  \r\n               minimum       1            4            4          Q       370\u00b130\u03bcs  \r\n               minimum       2            1            1          b      70.8\u00b10.3\u03bcs \r\n               minimum       2            1            1          B      78.0\u00b10.4\u03bcs \r\n               minimum       2            1            1          h      71.1\u00b10.1\u03bcs \r\n               minimum       2            1            1          H      77.5\u00b10.4\u03bcs \r\n               minimum       2            1            1          i      72.4\u00b10.3\u03bcs \r\n               minimum       2            1            1          I      78.7\u00b10.4\u03bcs \r\n               minimum       2            1            1          l       95.1\u00b11\u03bcs  \r\n               minimum       2            1            1          L       102\u00b12\u03bcs   \r\n               minimum       2            1            1          q       94.1\u00b11\u03bcs  \r\n               minimum       2            1            1          Q      101\u00b10.8\u03bcs  \r\n               minimum       2            1            2          b      70.7\u00b10.1\u03bcs \r\n               minimum       2            1            2          B      78.4\u00b10.5\u03bcs \r\n               minimum       2            1            2          h      71.8\u00b10.5\u03bcs \r\n               minimum       2            1            2          H      77.6\u00b10.2\u03bcs \r\n               minimum       2            1            2          i       81.0\u00b17\u03bcs  \r\n               minimum       2            1            2          I       78.8\u00b11\u03bcs  \r\n               minimum       2            1            2          l       130\u00b15\u03bcs   \r\n               minimum       2            1            2          L       138\u00b13\u03bcs   \r\n               minimum       2            1            2          q       132\u00b14\u03bcs   \r\n               minimum       2            1            2          Q       127\u00b12\u03bcs   \r\n               minimum       2            1            4          b      75.0\u00b10.3\u03bcs \r\n               minimum       2            1            4          B       79.7\u00b11\u03bcs  \r\n               minimum       2            1            4          h      72.3\u00b10.2\u03bcs \r\n               minimum       2            1            4          H      78.6\u00b10.5\u03bcs \r\n               minimum       2            1            4          i      94.2\u00b10.7\u03bcs \r\n               minimum       2            1            4          I       104\u00b18\u03bcs   \r\n               minimum       2            1            4          l       246\u00b110\u03bcs  \r\n               minimum       2            1            4          L       255\u00b16\u03bcs   \r\n               minimum       2            1            4          q       249\u00b17\u03bcs   \r\n               minimum       2            1            4          Q       241\u00b18\u03bcs   \r\n               minimum       2            2            1          b       71.2\u00b12\u03bcs  \r\n               minimum       2            2            1          B      78.5\u00b10.3\u03bcs \r\n               minimum       2            2            1          h       75.8\u00b14\u03bcs  \r\n               minimum       2            2            1          H       83.0\u00b16\u03bcs  \r\n               minimum       2            2            1          i       80.6\u00b17\u03bcs  \r\n               minimum       2            2            1          I       98.1\u00b12\u03bcs  \r\n               minimum       2            2            1          l       125\u00b19\u03bcs   \r\n               minimum       2            2            1          L       127\u00b15\u03bcs   \r\n               minimum       2            2            1          q       115\u00b12\u03bcs   \r\n               minimum       2            2            1          Q       128\u00b15\u03bcs   \r\n               minimum       2            2            2          b      71.4\u00b10.3\u03bcs \r\n               minimum       2            2            2          B      79.2\u00b10.4\u03bcs \r\n               minimum       2            2            2          h      71.5\u00b10.2\u03bcs \r\n               minimum       2            2            2          H      77.8\u00b10.4\u03bcs \r\n               minimum       2            2            2          i       75.7\u00b11\u03bcs  \r\n               minimum       2            2            2          I       80.3\u00b11\u03bcs  \r\n               minimum       2            2            2          l       163\u00b14\u03bcs   \r\n               minimum       2            2            2          L       158\u00b12\u03bcs   \r\n               minimum       2            2            2          q      159\u00b10.9\u03bcs  \r\n               minimum       2            2            2          Q       162\u00b14\u03bcs   \r\n               minimum       2            2            4          b       76.0\u00b11\u03bcs  \r\n               minimum       2            2            4          B      79.6\u00b10.2\u03bcs \r\n               minimum       2            2            4          h       78.2\u00b15\u03bcs  \r\n               minimum       2            2            4          H      79.6\u00b10.4\u03bcs \r\n               minimum       2            2            4          i       102\u00b11\u03bcs   \r\n               minimum       2            2            4          I       103\u00b12\u03bcs   \r\n               minimum       2            2            4          l       302\u00b110\u03bcs  \r\n               minimum       2            2            4          L       298\u00b110\u03bcs  \r\n               minimum       2            2            4          q       295\u00b110\u03bcs  \r\n               minimum       2            2            4          Q       303\u00b13\u03bcs   \r\n               minimum       2            4            1          b      70.7\u00b10.6\u03bcs \r\n               minimum       2            4            1          B      78.8\u00b10.3\u03bcs \r\n               minimum       2            4            1          h      72.7\u00b10.2\u03bcs \r\n               minimum       2            4            1          H      78.6\u00b10.8\u03bcs \r\n               minimum       2            4            1          i       87.6\u00b14\u03bcs  \r\n               minimum       2            4            1          I       102\u00b16\u03bcs   \r\n               minimum       2            4            1          l       196\u00b13\u03bcs   \r\n               minimum       2            4            1          L       194\u00b15\u03bcs   \r\n               minimum       2            4            1          q       197\u00b19\u03bcs   \r\n               minimum       2            4            1          Q       194\u00b14\u03bcs   \r\n               minimum       2            4            2          b      71.4\u00b10.4\u03bcs \r\n               minimum       2            4            2          B       80.2\u00b15\u03bcs  \r\n               minimum       2            4            2          h       77.7\u00b15\u03bcs  \r\n               minimum       2            4            2          H       85.7\u00b17\u03bcs  \r\n               minimum       2            4            2          i      94.0\u00b10.8\u03bcs \r\n               minimum       2            4            2          I       102\u00b12\u03bcs   \r\n               minimum       2            4            2          l       250\u00b12\u03bcs   \r\n               minimum       2            4            2          L       247\u00b15\u03bcs   \r\n               minimum       2            4            2          q       253\u00b16\u03bcs   \r\n               minimum       2            4            2          Q       256\u00b110\u03bcs  \r\n               minimum       2            4            4          b       76.0\u00b11\u03bcs  \r\n               minimum       2            4            4          B       83.7\u00b14\u03bcs  \r\n               minimum       2            4            4          h       74.1\u00b11\u03bcs  \r\n               minimum       2            4            4          H       79.7\u00b11\u03bcs  \r\n               minimum       2            4            4          i       129\u00b13\u03bcs   \r\n               minimum       2            4            4          I       131\u00b13\u03bcs   \r\n               minimum       2            4            4          l       405\u00b15\u03bcs   \r\n               minimum       2            4            4          L       400\u00b18\u03bcs   \r\n               minimum       2            4            4          q       408\u00b110\u03bcs  \r\n               minimum       2            4            4          Q       411\u00b17\u03bcs   \r\n               minimum       4            1            1          b      70.8\u00b10.6\u03bcs \r\n               minimum       4            1            1          B      79.0\u00b10.5\u03bcs \r\n               minimum       4            1            1          h      72.0\u00b10.3\u03bcs \r\n               minimum       4            1            1          H       84.8\u00b17\u03bcs  \r\n               minimum       4            1            1          i       89.0\u00b14\u03bcs  \r\n               minimum       4            1            1          I       97.0\u00b15\u03bcs  \r\n               minimum       4            1            1          l       160\u00b19\u03bcs   \r\n               minimum       4            1            1          L       164\u00b15\u03bcs   \r\n               minimum       4            1            1          q       153\u00b13\u03bcs   \r\n               minimum       4            1            1          Q       160\u00b12\u03bcs   \r\n               minimum       4            1            2          b      71.5\u00b10.5\u03bcs \r\n               minimum       4            1            2          B       84.4\u00b15\u03bcs  \r\n               minimum       4            1            2          h      72.2\u00b10.3\u03bcs \r\n               minimum       4            1            2          H      78.3\u00b10.3\u03bcs \r\n               minimum       4            1            2          i       92.3\u00b13\u03bcs  \r\n               minimum       4            1            2          I       98.9\u00b11\u03bcs  \r\n               minimum       4            1            2          l       207\u00b14\u03bcs   \r\n               minimum       4            1            2          L       209\u00b17\u03bcs   \r\n               minimum       4            1            2          q       217\u00b17\u03bcs   \r\n               minimum       4            1            2          Q       224\u00b18\u03bcs   \r\n               minimum       4            1            4          b       78.2\u00b13\u03bcs  \r\n               minimum       4            1            4          B       79.5\u00b14\u03bcs  \r\n               minimum       4            1            4          h       80.8\u00b16\u03bcs  \r\n               minimum       4            1            4          H      79.2\u00b10.8\u03bcs \r\n               minimum       4            1            4          i       117\u00b13\u03bcs   \r\n               minimum       4            1            4          I       121\u00b13\u03bcs   \r\n               minimum       4            1            4          l       405\u00b130\u03bcs  \r\n               minimum       4            1            4          L       368\u00b120\u03bcs  \r\n               minimum       4            1            4          q       413\u00b120\u03bcs  \r\n               minimum       4            1            4          Q       365\u00b110\u03bcs  \r\n               minimum       4            2            1          b       76.1\u00b14\u03bcs  \r\n               minimum       4            2            1          B       84.6\u00b15\u03bcs  \r\n               minimum       4            2            1          h       79.5\u00b16\u03bcs  \r\n               minimum       4            2            1          H       84.9\u00b17\u03bcs  \r\n               minimum       4            2            1          i      89.0\u00b10.4\u03bcs \r\n               minimum       4            2            1          I       98.6\u00b12\u03bcs  \r\n               minimum       4            2            1          l       196\u00b110\u03bcs  \r\n               minimum       4            2            1          L       198\u00b13\u03bcs   \r\n               minimum       4            2            1          q       195\u00b18\u03bcs   \r\n               minimum       4            2            1          Q       207\u00b19\u03bcs   \r\n               minimum       4            2            2          b      72.0\u00b10.5\u03bcs \r\n               minimum       4            2            2          B      79.6\u00b10.4\u03bcs \r\n               minimum       4            2            2          h      73.4\u00b10.5\u03bcs \r\n               minimum       4            2            2          H       88.6\u00b17\u03bcs  \r\n               minimum       4            2            2          i      97.7\u00b10.7\u03bcs \r\n               minimum       4            2            2          I       103\u00b11\u03bcs   \r\n               minimum       4            2            2          l       261\u00b110\u03bcs  \r\n               minimum       4            2            2          L       265\u00b110\u03bcs  \r\n               minimum       4            2            2          q       258\u00b16\u03bcs   \r\n               minimum       4            2            2          Q       249\u00b19\u03bcs   \r\n               minimum       4            2            4          b       79.2\u00b12\u03bcs  \r\n               minimum       4            2            4          B      79.7\u00b10.6\u03bcs \r\n               minimum       4            2            4          h       80.9\u00b17\u03bcs  \r\n               minimum       4            2            4          H      79.3\u00b10.4\u03bcs \r\n               minimum       4            2            4          i       127\u00b12\u03bcs   \r\n               minimum       4            2            4          I       131\u00b12\u03bcs   \r\n               minimum       4            2            4          l       424\u00b130\u03bcs  \r\n               minimum       4            2            4          L       404\u00b130\u03bcs  \r\n               minimum       4            2            4          q       400\u00b16\u03bcs   \r\n               minimum       4            2            4          Q       406\u00b13\u03bcs   \r\n               minimum       4            4            1          b      70.3\u00b10.3\u03bcs \r\n               minimum       4            4            1          B      78.1\u00b10.2\u03bcs \r\n               minimum       4            4            1          h      72.2\u00b10.1\u03bcs \r\n               minimum       4            4            1          H      78.4\u00b10.5\u03bcs \r\n               minimum       4            4            1          i       104\u00b11\u03bcs   \r\n               minimum       4            4            1          I       110\u00b13\u03bcs   \r\n               minimum       4            4            1          l       313\u00b110\u03bcs  \r\n               minimum       4            4            1          L       298\u00b19\u03bcs   \r\n               minimum       4            4            1          q       308\u00b110\u03bcs  \r\n               minimum       4            4            1          Q       289\u00b110\u03bcs  \r\n               minimum       4            4            2          b       76.5\u00b15\u03bcs  \r\n               minimum       4            4            2          B       85.5\u00b16\u03bcs  \r\n               minimum       4            4            2          h      73.6\u00b10.2\u03bcs \r\n               minimum       4            4            2          H      78.9\u00b10.3\u03bcs \r\n               minimum       4            4            2          i       118\u00b14\u03bcs   \r\n               minimum       4            4            2          I       133\u00b19\u03bcs   \r\n               minimum       4            4            2          l       367\u00b110\u03bcs  \r\n               minimum       4            4            2          L       369\u00b120\u03bcs  \r\n               minimum       4            4            2          q       368\u00b110\u03bcs  \r\n               minimum       4            4            2          Q       388\u00b120\u03bcs  \r\n               minimum       4            4            4          b      76.5\u00b10.4\u03bcs \r\n               minimum       4            4            4          B      78.5\u00b10.2\u03bcs \r\n               minimum       4            4            4          h      75.8\u00b10.4\u03bcs \r\n               minimum       4            4            4          H      80.6\u00b10.5\u03bcs \r\n               minimum       4            4            4          i       166\u00b13\u03bcs   \r\n               minimum       4            4            4          I       167\u00b15\u03bcs   \r\n               minimum       4            4            4          l       546\u00b120\u03bcs  \r\n               minimum       4            4            4          L       577\u00b130\u03bcs  \r\n               minimum       4            4            4          q       554\u00b140\u03bcs  \r\n               minimum       4            4            4          Q       564\u00b130\u03bcs  \r\n              ========= ============ ============ ============ ======= =============\r\n\r\n[ 73.53%] \u00b7\u00b7\u00b7 bench_ufunc_strides.LogisticRegression.time_train                                                                                                                            ok\r\n[ 73.53%] \u00b7\u00b7\u00b7 =============== ============\r\n                   dtype                  \r\n              --------------- ------------\r\n               numpy.float32   2.67\u00b10.01s \r\n               numpy.float64   4.42\u00b10.02s \r\n              =============== ============\r\n\r\n[ 74.26%] \u00b7\u00b7\u00b7 bench_ufunc_strides.Mandelbrot.time_mandel                                                                                                                           12.7\u00b10.02s\r\n[ 75.00%] \u00b7\u00b7\u00b7 bench_ufunc_strides.Unary.time_ufunc                                                                                                                                         ok\r\n[ 75.00%] \u00b7\u00b7\u00b7 ========================= =========== ============= ============= ============= ============= ============= =============\r\n              --                                                                     stride_out / dtype                                \r\n              ------------------------------------- -----------------------------------------------------------------------------------\r\n                        ufunc            stride_in      1 / f         1 / d         2 / f         2 / d         4 / f         4 / d    \r\n              ========================= =========== ============= ============= ============= ============= ============= =============\r\n                  <ufunc 'absolute'>         1        25.4\u00b10.1\u03bcs    49.1\u00b10.4\u03bcs    44.5\u00b10.3\u03bcs     81.5\u00b11\u03bcs     74.4\u00b10.9\u03bcs     164\u00b14\u03bcs   \r\n                  <ufunc 'absolute'>         2        37.0\u00b10.2\u03bcs    68.0\u00b10.9\u03bcs     58.7\u00b11\u03bcs     102\u00b10.7\u03bcs     83.1\u00b10.6\u03bcs     202\u00b14\u03bcs   \r\n                  <ufunc 'absolute'>         4        54.2\u00b10.7\u03bcs     114\u00b11\u03bcs      71.6\u00b10.6\u03bcs     165\u00b16\u03bcs       103\u00b12\u03bcs       284\u00b16\u03bcs   \r\n                   <ufunc 'arccos'>          1         989\u00b17\u03bcs     1.53\u00b10.06ms     990\u00b18\u03bcs     1.53\u00b10.03ms   1.02\u00b10.01ms   1.65\u00b10.06ms \r\n                   <ufunc 'arccos'>          2         985\u00b15\u03bcs     1.55\u00b10.02ms     995\u00b110\u03bcs    1.53\u00b10.01ms     999\u00b120\u03bcs    1.57\u00b10.07ms \r\n                   <ufunc 'arccos'>          4         981\u00b15\u03bcs     1.58\u00b10.02ms     989\u00b14\u03bcs     1.58\u00b10.03ms   1.04\u00b10.03ms   1.59\u00b10.01ms \r\n                  <ufunc 'arccosh'>          1       2.05\u00b10.01ms   2.33\u00b10.02ms   2.06\u00b10.02ms   2.37\u00b10.03ms   2.08\u00b10.02ms   2.43\u00b10.06ms \r\n                  <ufunc 'arccosh'>          2       2.07\u00b10.01ms   2.35\u00b10.02ms   2.07\u00b10.02ms   2.42\u00b10.03ms   2.09\u00b10.05ms   2.40\u00b10.08ms \r\n                  <ufunc 'arccosh'>          4       2.09\u00b10.02ms   2.37\u00b10.02ms   2.08\u00b10.03ms   2.42\u00b10.02ms   2.09\u00b10.03ms    2.36\u00b10.1ms \r\n                   <ufunc 'arcsin'>          1         840\u00b17\u03bcs     1.48\u00b10.01ms     839\u00b110\u03bcs    1.50\u00b10.04ms     860\u00b110\u03bcs    1.65\u00b10.08ms \r\n                   <ufunc 'arcsin'>          2         846\u00b16\u03bcs     1.49\u00b10.01ms     843\u00b110\u03bcs    1.50\u00b10.01ms     847\u00b110\u03bcs    1.55\u00b10.06ms \r\n                   <ufunc 'arcsin'>          4         853\u00b120\u03bcs    1.54\u00b10.03ms     842\u00b14\u03bcs     1.54\u00b10.01ms     847\u00b110\u03bcs    1.55\u00b10.06ms \r\n                  <ufunc 'arcsinh'>          1       2.31\u00b10.01ms   2.82\u00b10.03ms   2.30\u00b10.02ms   2.85\u00b10.04ms   2.38\u00b10.04ms    3.02\u00b10.1ms \r\n                  <ufunc 'arcsinh'>          2       2.30\u00b10.01ms   2.83\u00b10.02ms   2.32\u00b10.02ms   2.84\u00b10.02ms   2.35\u00b10.03ms    2.86\u00b10.1ms \r\n                  <ufunc 'arcsinh'>          4       2.29\u00b10.02ms   2.82\u00b10.02ms   2.31\u00b10.02ms   2.84\u00b10.02ms   2.34\u00b10.03ms   2.84\u00b10.02ms \r\n                   <ufunc 'arctan'>          1         1.09\u00b10ms    1.98\u00b10.01ms   1.09\u00b10.01ms   2.01\u00b10.03ms   1.13\u00b10.02ms   2.13\u00b10.07ms \r\n                   <ufunc 'arctan'>          2       1.09\u00b10.01ms   1.98\u00b10.01ms   1.10\u00b10.01ms   1.99\u00b10.01ms   1.10\u00b10.01ms   2.02\u00b10.07ms \r\n                   <ufunc 'arctan'>          4       1.09\u00b10.01ms   2.01\u00b10.02ms     1.12\u00b10ms    2.02\u00b10.01ms   1.10\u00b10.02ms   2.05\u00b10.01ms \r\n                  <ufunc 'arctanh'>          1       2.30\u00b10.01ms   2.52\u00b10.02ms   2.29\u00b10.02ms   2.56\u00b10.05ms   2.37\u00b10.05ms    2.74\u00b10.1ms \r\n                  <ufunc 'arctanh'>          2       2.31\u00b10.06ms   2.56\u00b10.02ms   2.32\u00b10.01ms   2.54\u00b10.03ms   2.31\u00b10.02ms    2.57\u00b10.1ms \r\n                  <ufunc 'arctanh'>          4       2.30\u00b10.01ms   2.57\u00b10.03ms   2.31\u00b10.02ms   2.61\u00b10.02ms   2.33\u00b10.04ms   2.56\u00b10.04ms \r\n                    <ufunc 'cbrt'>           1       1.97\u00b10.01ms   2.19\u00b10.01ms   1.99\u00b10.01ms   2.25\u00b10.05ms   2.04\u00b10.04ms   2.38\u00b10.08ms \r\n                    <ufunc 'cbrt'>           2       1.97\u00b10.01ms   2.23\u00b10.03ms   1.98\u00b10.01ms   2.22\u00b10.02ms   1.98\u00b10.01ms   2.27\u00b10.09ms \r\n                    <ufunc 'cbrt'>           4       1.97\u00b10.01ms   2.24\u00b10.02ms   1.98\u00b10.01ms   2.25\u00b10.02ms   2.01\u00b10.04ms   2.28\u00b10.03ms \r\n                    <ufunc 'ceil'>           1        25.4\u00b10.2\u03bcs    49.4\u00b10.2\u03bcs    45.1\u00b10.3\u03bcs     81.0\u00b12\u03bcs      75.0\u00b11\u03bcs      163\u00b12\u03bcs   \r\n                    <ufunc 'ceil'>           2        37.2\u00b10.9\u03bcs    68.4\u00b10.3\u03bcs     57.1\u00b11\u03bcs      101\u00b13\u03bcs       82.8\u00b11\u03bcs      193\u00b16\u03bcs   \r\n                    <ufunc 'ceil'>           4        55.1\u00b10.7\u03bcs     113\u00b13\u03bcs      70.9\u00b10.7\u03bcs     161\u00b14\u03bcs       104\u00b13\u03bcs       285\u00b110\u03bcs  \r\n               <ufunc 'conjugate'> (0)       1        45.6\u00b10.3\u03bcs    53.9\u00b10.4\u03bcs     50.1\u00b12\u03bcs      83.2\u00b11\u03bcs      75.4\u00b11\u03bcs      166\u00b14\u03bcs   \r\n               <ufunc 'conjugate'> (0)       2         48.8\u00b11\u03bcs     69.4\u00b10.6\u03bcs    53.9\u00b10.7\u03bcs     101\u00b11\u03bcs       83.2\u00b11\u03bcs      193\u00b18\u03bcs   \r\n               <ufunc 'conjugate'> (0)       4        56.9\u00b10.3\u03bcs     114\u00b15\u03bcs      70.0\u00b10.9\u03bcs     163\u00b13\u03bcs       103\u00b12\u03bcs       284\u00b12\u03bcs   \r\n                    <ufunc 'cos'>            1         147\u00b11\u03bcs       832\u00b18\u03bcs       210\u00b15\u03bcs       846\u00b120\u03bcs      209\u00b15\u03bcs       911\u00b140\u03bcs  \r\n                    <ufunc 'cos'>            2         217\u00b13\u03bcs       838\u00b14\u03bcs       276\u00b12\u03bcs       840\u00b110\u03bcs     274\u00b10.7\u03bcs      873\u00b130\u03bcs  \r\n                    <ufunc 'cos'>            4         227\u00b13\u03bcs       852\u00b16\u03bcs       286\u00b11\u03bcs       875\u00b110\u03bcs      286\u00b15\u03bcs       872\u00b17\u03bcs   \r\n                    <ufunc 'cosh'>           1       1.31\u00b10.01ms   1.33\u00b10.01ms     1.31\u00b10ms    1.34\u00b10.03ms   1.36\u00b10.03ms   1.45\u00b10.05ms \r\n                    <ufunc 'cosh'>           2         1.31\u00b10ms    1.34\u00b10.01ms   1.32\u00b10.01ms   1.34\u00b10.02ms   1.31\u00b10.01ms   1.37\u00b10.05ms \r\n                    <ufunc 'cosh'>           4       1.31\u00b10.01ms   1.36\u00b10.01ms   1.31\u00b10.01ms     1.36\u00b10ms    1.31\u00b10.03ms   1.37\u00b10.01ms \r\n                  <ufunc 'deg2rad'>          1         176\u00b11\u03bcs       176\u00b11\u03bcs      176\u00b10.9\u03bcs      201\u00b13\u03bcs       205\u00b14\u03bcs       342\u00b110\u03bcs  \r\n                  <ufunc 'deg2rad'>          2        176\u00b10.9\u03bcs      177\u00b12\u03bcs       176\u00b12\u03bcs      203\u00b10.9\u03bcs      201\u00b12\u03bcs       345\u00b18\u03bcs   \r\n                  <ufunc 'deg2rad'>          4        177\u00b10.3\u03bcs      189\u00b12\u03bcs       176\u00b11\u03bcs       241\u00b13\u03bcs       208\u00b14\u03bcs       404\u00b110\u03bcs  \r\n                  <ufunc 'degrees'>          1         176\u00b11\u03bcs       176\u00b12\u03bcs       176\u00b12\u03bcs       201\u00b13\u03bcs       205\u00b13\u03bcs       340\u00b18\u03bcs   \r\n                  <ufunc 'degrees'>          2         176\u00b11\u03bcs      179\u00b10.6\u03bcs      178\u00b12\u03bcs       204\u00b11\u03bcs       201\u00b12\u03bcs       345\u00b16\u03bcs   \r\n                  <ufunc 'degrees'>          4        177\u00b10.6\u03bcs      188\u00b11\u03bcs       176\u00b12\u03bcs       236\u00b15\u03bcs       209\u00b15\u03bcs       395\u00b110\u03bcs  \r\n                    <ufunc 'exp'>            1         154\u00b13\u03bcs       608\u00b15\u03bcs       4.45\u00b10ms      614\u00b110\u03bcs    4.64\u00b10.09ms     660\u00b120\u03bcs  \r\n                    <ufunc 'exp'>            2         222\u00b16\u03bcs       605\u00b15\u03bcs     5.03\u00b10.05ms     616\u00b16\u03bcs     5.03\u00b10.01ms     648\u00b120\u03bcs  \r\n                    <ufunc 'exp'>            4         222\u00b11\u03bcs       621\u00b16\u03bcs       5.01\u00b10ms      622\u00b13\u03bcs      5.06\u00b10.1ms     642\u00b110\u03bcs  \r\n                    <ufunc 'exp2'>           1         329\u00b13\u03bcs       450\u00b13\u03bcs       331\u00b12\u03bcs       456\u00b19\u03bcs       344\u00b16\u03bcs       491\u00b120\u03bcs  \r\n                    <ufunc 'exp2'>           2         333\u00b11\u03bcs       455\u00b16\u03bcs       337\u00b13\u03bcs       461\u00b15\u03bcs       335\u00b14\u03bcs       477\u00b120\u03bcs  \r\n                    <ufunc 'exp2'>           4        342\u00b10.9\u03bcs      483\u00b15\u03bcs       348\u00b12\u03bcs       496\u00b13\u03bcs       348\u00b18\u03bcs       528\u00b12\u03bcs   \r\n                   <ufunc 'expm1'>           1       1.10\u00b10.01ms   1.06\u00b10.01ms   1.09\u00b10.01ms   1.07\u00b10.02ms   1.13\u00b10.02ms   1.16\u00b10.05ms \r\n                   <ufunc 'expm1'>           2       1.09\u00b10.01ms   1.07\u00b10.01ms   1.10\u00b10.01ms   1.07\u00b10.01ms   1.09\u00b10.01ms   1.09\u00b10.04ms \r\n                   <ufunc 'expm1'>           4       1.12\u00b10.01ms   1.08\u00b10.01ms   1.10\u00b10.05ms   1.09\u00b10.02ms   1.10\u00b10.02ms   1.10\u00b10.01ms \r\n                    <ufunc 'fabs'>           1         177\u00b12\u03bcs       176\u00b12\u03bcs       178\u00b11\u03bcs       200\u00b13\u03bcs       205\u00b13\u03bcs       342\u00b110\u03bcs  \r\n                    <ufunc 'fabs'>           2         181\u00b14\u03bcs      177\u00b10.8\u03bcs      176\u00b12\u03bcs       201\u00b11\u03bcs       200\u00b12\u03bcs       345\u00b19\u03bcs   \r\n                    <ufunc 'fabs'>           4         178\u00b11\u03bcs       189\u00b11\u03bcs       177\u00b11\u03bcs       237\u00b13\u03bcs       210\u00b14\u03bcs       408\u00b16\u03bcs   \r\n                   <ufunc 'floor'>           1        25.5\u00b10.1\u03bcs    49.4\u00b10.2\u03bcs    45.4\u00b10.5\u03bcs    82.6\u00b10.8\u03bcs    75.2\u00b10.7\u03bcs     162\u00b16\u03bcs   \r\n                   <ufunc 'floor'>           2        36.7\u00b10.1\u03bcs    68.0\u00b10.4\u03bcs     57.8\u00b11\u03bcs     101\u00b10.9\u03bcs     84.1\u00b10.8\u03bcs     199\u00b15\u03bcs   \r\n                   <ufunc 'floor'>           4        54.2\u00b10.3\u03bcs     112\u00b15\u03bcs       71.6\u00b11\u03bcs      165\u00b16\u03bcs       102\u00b12\u03bcs       288\u00b120\u03bcs  \r\n                    <ufunc 'log'>            1         210\u00b17\u03bcs       590\u00b14\u03bcs     4.51\u00b10.01ms     601\u00b19\u03bcs     4.71\u00b10.08ms     642\u00b130\u03bcs  \r\n                    <ufunc 'log'>            2         279\u00b15\u03bcs       595\u00b16\u03bcs     5.10\u00b10.06ms     593\u00b14\u03bcs     5.09\u00b10.02ms     606\u00b130\u03bcs  \r\n                    <ufunc 'log'>            4         291\u00b14\u03bcs       603\u00b16\u03bcs     5.11\u00b10.01ms     607\u00b14\u03bcs      5.14\u00b10.1ms     626\u00b19\u03bcs   \r\n                   <ufunc 'log10'>           1         786\u00b14\u03bcs       1.02\u00b10ms      795\u00b17\u03bcs     1.03\u00b10.02ms     820\u00b120\u03bcs    1.11\u00b10.04ms \r\n                   <ufunc 'log10'>           2         787\u00b13\u03bcs     1.04\u00b10.01ms     801\u00b15\u03bcs     1.03\u00b10.02ms     789\u00b15\u03bcs     1.05\u00b10.04ms \r\n                   <ufunc 'log10'>           4         787\u00b15\u03bcs     1.08\u00b10.03ms     795\u00b16\u03bcs     1.04\u00b10.01ms     797\u00b120\u03bcs    1.05\u00b10.02ms \r\n                   <ufunc 'log1p'>           1       1.16\u00b10.01ms   1.16\u00b10.01ms   1.15\u00b10.01ms   1.18\u00b10.02ms   1.20\u00b10.03ms   1.27\u00b10.05ms \r\n                   <ufunc 'log1p'>           2       1.17\u00b10.01ms   1.18\u00b10.01ms   1.16\u00b10.01ms   1.18\u00b10.01ms   1.16\u00b10.01ms   1.20\u00b10.05ms \r\n                   <ufunc 'log1p'>           4       1.17\u00b10.01ms   1.19\u00b10.01ms   1.18\u00b10.01ms   1.24\u00b10.02ms   1.19\u00b10.03ms   1.22\u00b10.02ms \r\n                    <ufunc 'log2'>           1         380\u00b12\u03bcs       816\u00b13\u03bcs       377\u00b11\u03bcs       848\u00b120\u03bcs      392\u00b18\u03bcs       885\u00b130\u03bcs  \r\n                    <ufunc 'log2'>           2         377\u00b12\u03bcs       816\u00b17\u03bcs       380\u00b14\u03bcs       819\u00b16\u03bcs       383\u00b11\u03bcs       895\u00b120\u03bcs  \r\n                    <ufunc 'log2'>           4         385\u00b17\u03bcs       834\u00b15\u03bcs       378\u00b11\u03bcs       835\u00b16\u03bcs       393\u00b19\u03bcs       897\u00b170\u03bcs  \r\n                <ufunc 'logical_not'>        1        102\u00b10.7\u03bcs      128\u00b17\u03bcs      142\u00b10.8\u03bcs      146\u00b13\u03bcs       150\u00b12\u03bcs       225\u00b16\u03bcs   \r\n                <ufunc 'logical_not'>        2        102\u00b10.6\u03bcs      132\u00b11\u03bcs      143\u00b10.9\u03bcs      158\u00b11\u03bcs       146\u00b12\u03bcs       252\u00b17\u03bcs   \r\n                <ufunc 'logical_not'>        4        109\u00b10.6\u03bcs      174\u00b16\u03bcs      152\u00b10.9\u03bcs      227\u00b110\u03bcs      161\u00b11\u03bcs       353\u00b110\u03bcs  \r\n                  <ufunc 'negative'>         1        26.1\u00b10.8\u03bcs    49.4\u00b10.5\u03bcs    53.0\u00b10.4\u03bcs     83.9\u00b12\u03bcs      76.1\u00b11\u03bcs      166\u00b15\u03bcs   \r\n                  <ufunc 'negative'>         2         54.1\u00b11\u03bcs      72.0\u00b11\u03bcs     57.6\u00b10.7\u03bcs     101\u00b12\u03bcs      83.7\u00b10.7\u03bcs     200\u00b12\u03bcs   \r\n                  <ufunc 'negative'>         4        61.6\u00b10.5\u03bcs     116\u00b12\u03bcs      73.1\u00b10.3\u03bcs     161\u00b14\u03bcs       102\u00b13\u03bcs       313\u00b120\u03bcs  \r\n                  <ufunc 'positive'>         1        46.9\u00b10.8\u03bcs    54.2\u00b10.6\u03bcs    48.3\u00b10.5\u03bcs     82.5\u00b11\u03bcs     75.9\u00b10.9\u03bcs     167\u00b14\u03bcs   \r\n                  <ufunc 'positive'>         2        48.7\u00b10.2\u03bcs    70.1\u00b10.7\u03bcs    54.8\u00b10.9\u03bcs     101\u00b11\u03bcs       83.6\u00b11\u03bcs      199\u00b17\u03bcs   \r\n                  <ufunc 'positive'>         4        57.0\u00b10.5\u03bcs     115\u00b13\u03bcs      69.9\u00b10.6\u03bcs     165\u00b15\u03bcs       103\u00b12\u03bcs       308\u00b110\u03bcs  \r\n                  <ufunc 'rad2deg'>          1        176\u00b10.8\u03bcs      177\u00b11\u03bcs       182\u00b15\u03bcs       202\u00b13\u03bcs       205\u00b14\u03bcs       341\u00b110\u03bcs  \r\n                  <ufunc 'rad2deg'>          2        177\u00b10.7\u03bcs      176\u00b11\u03bcs       178\u00b12\u03bcs       204\u00b14\u03bcs       201\u00b13\u03bcs       343\u00b18\u03bcs   \r\n                  <ufunc 'rad2deg'>          4        178\u00b10.9\u03bcs      197\u00b16\u03bcs       178\u00b11\u03bcs       236\u00b14\u03bcs       207\u00b15\u03bcs       411\u00b17\u03bcs   \r\n                  <ufunc 'radians'>          1         177\u00b11\u03bcs       177\u00b12\u03bcs      176\u00b10.4\u03bcs      202\u00b13\u03bcs       204\u00b13\u03bcs       341\u00b110\u03bcs  \r\n                  <ufunc 'radians'>          2         176\u00b11\u03bcs       179\u00b11\u03bcs       177\u00b12\u03bcs       203\u00b13\u03bcs       200\u00b12\u03bcs       344\u00b110\u03bcs  \r\n                  <ufunc 'radians'>          4         177\u00b11\u03bcs       189\u00b12\u03bcs       177\u00b11\u03bcs       236\u00b14\u03bcs       206\u00b15\u03bcs       409\u00b110\u03bcs  \r\n                 <ufunc 'reciprocal'>        1        53.7\u00b10.3\u03bcs     206\u00b12\u03bcs       53.4\u00b11\u03bcs      209\u00b14\u03bcs      75.8\u00b10.9\u03bcs     225\u00b19\u03bcs   \r\n                 <ufunc 'reciprocal'>        2        53.6\u00b10.2\u03bcs     206\u00b12\u03bcs       64.0\u00b12\u03bcs      207\u00b11\u03bcs      83.4\u00b10.7\u03bcs     225\u00b19\u03bcs   \r\n                 <ufunc 'reciprocal'>        4        54.9\u00b10.3\u03bcs     212\u00b12\u03bcs      72.6\u00b10.3\u03bcs     218\u00b16\u03bcs       103\u00b12\u03bcs       301\u00b110\u03bcs  \r\n                    <ufunc 'rint'>           1        25.7\u00b10.2\u03bcs    50.2\u00b10.4\u03bcs    45.6\u00b10.4\u03bcs     81.9\u00b11\u03bcs      75.1\u00b12\u03bcs      164\u00b14\u03bcs   \r\n                    <ufunc 'rint'>           2        36.8\u00b10.4\u03bcs    68.8\u00b10.8\u03bcs     58.2\u00b11\u03bcs      101\u00b12\u03bcs       82.7\u00b11\u03bcs      195\u00b15\u03bcs   \r\n                    <ufunc 'rint'>           4        54.3\u00b10.5\u03bcs     113\u00b12\u03bcs       71.4\u00b11\u03bcs      161\u00b17\u03bcs       102\u00b11\u03bcs       292\u00b19\u03bcs   \r\n                    <ufunc 'sign'>           1        69.7\u00b10.3\u03bcs    68.8\u00b10.7\u03bcs    69.5\u00b10.5\u03bcs     84.8\u00b12\u03bcs      79.9\u00b12\u03bcs      168\u00b13\u03bcs   \r\n                    <ufunc 'sign'>           2         71.7\u00b13\u03bcs     78.4\u00b10.5\u03bcs     70.7\u00b11\u03bcs      108\u00b12\u03bcs      86.6\u00b10.9\u03bcs     200\u00b16\u03bcs   \r\n                    <ufunc 'sign'>           4        73.3\u00b10.2\u03bcs     124\u00b14\u03bcs       80.7\u00b12\u03bcs      170\u00b13\u03bcs       103\u00b12\u03bcs       299\u00b110\u03bcs  \r\n                    <ufunc 'sin'>            1         142\u00b11\u03bcs       973\u00b19\u03bcs       210\u00b15\u03bcs       991\u00b120\u03bcs      212\u00b17\u03bcs     1.06\u00b10.04ms \r\n                    <ufunc 'sin'>            2         218\u00b19\u03bcs       970\u00b16\u03bcs       268\u00b12\u03bcs       982\u00b16\u03bcs      265\u00b10.6\u03bcs      993\u00b140\u03bcs  \r\n                    <ufunc 'sin'>            4         227\u00b17\u03bcs       982\u00b19\u03bcs       291\u00b18\u03bcs     1.03\u00b10.03ms     284\u00b110\u03bcs      1.02\u00b10ms  \r\n                    <ufunc 'sinh'>           1       1.97\u00b10.02ms   2.00\u00b10.02ms   1.96\u00b10.01ms   2.05\u00b10.03ms   2.04\u00b10.03ms   2.16\u00b10.07ms \r\n                    <ufunc 'sinh'>           2       1.96\u00b10.01ms   2.01\u00b10.02ms   1.97\u00b10.02ms   2.01\u00b10.01ms    2.13\u00b10.3ms   2.05\u00b10.08ms \r\n                    <ufunc 'sinh'>           4       1.97\u00b10.01ms   2.02\u00b10.01ms   1.97\u00b10.02ms   2.00\u00b10.01ms   2.00\u00b10.03ms   2.03\u00b10.01ms \r\n                    <ufunc 'sqrt'>           1        53.1\u00b10.1\u03bcs    205\u00b10.7\u03bcs      54.2\u00b11\u03bcs      207\u00b15\u03bcs       75.8\u00b11\u03bcs      224\u00b17\u03bcs   \r\n                    <ufunc 'sqrt'>           2        52.4\u00b10.2\u03bcs    205\u00b10.9\u03bcs      60.7\u00b12\u03bcs      207\u00b14\u03bcs      83.2\u00b10.3\u03bcs     221\u00b16\u03bcs   \r\n                    <ufunc 'sqrt'>           4        54.4\u00b10.1\u03bcs     209\u00b11\u03bcs       71.8\u00b11\u03bcs      217\u00b12\u03bcs       102\u00b12\u03bcs       281\u00b16\u03bcs   \r\n                   <ufunc 'square'>          1         25.4\u00b19\u03bcs     49.2\u00b10.3\u03bcs    45.0\u00b10.3\u03bcs     81.8\u00b11\u03bcs      74.3\u00b11\u03bcs      164\u00b13\u03bcs   \r\n                   <ufunc 'square'>          2        36.6\u00b10.2\u03bcs    67.1\u00b10.3\u03bcs    56.0\u00b10.9\u03bcs    102\u00b10.8\u03bcs     82.8\u00b10.9\u03bcs     195\u00b18\u03bcs   \r\n                   <ufunc 'square'>          4        54.2\u00b10.4\u03bcs     113\u00b14\u03bcs      70.9\u00b10.8\u03bcs     167\u00b14\u03bcs       100\u00b12\u03bcs       308\u00b110\u03bcs  \r\n                    <ufunc 'tan'>            1       1.46\u00b10.01ms   2.31\u00b10.01ms   1.45\u00b10.02ms   2.31\u00b10.04ms   1.50\u00b10.03ms   2.47\u00b10.09ms \r\n                    <ufunc 'tan'>            2       1.45\u00b10.01ms   2.33\u00b10.02ms   1.45\u00b10.02ms   2.31\u00b10.02ms   1.46\u00b10.01ms   2.36\u00b10.09ms \r\n                    <ufunc 'tan'>            4       1.48\u00b10.01ms   2.34\u00b10.03ms   1.50\u00b10.02ms   2.34\u00b10.02ms   1.47\u00b10.03ms   2.38\u00b10.04ms \r\n                    <ufunc 'tanh'>           1         453\u00b12\u03bcs     1.64\u00b10.01ms     488\u00b13\u03bcs     1.67\u00b10.03ms     507\u00b110\u03bcs    1.80\u00b10.07ms \r\n                    <ufunc 'tanh'>           2         509\u00b11\u03bcs     1.72\u00b10.01ms     558\u00b14\u03bcs       1.73\u00b10ms      555\u00b13\u03bcs     1.77\u00b10.08ms \r\n                    <ufunc 'tanh'>           4         526\u00b13\u03bcs     1.75\u00b10.01ms     557\u00b16\u03bcs     1.77\u00b10.02ms     559\u00b110\u03bcs    1.80\u00b10.03ms \r\n                   <ufunc 'trunc'>           1       25.5\u00b10.05\u03bcs    49.6\u00b10.1\u03bcs    45.5\u00b10.7\u03bcs     81.7\u00b11\u03bcs      74.8\u00b11\u03bcs      162\u00b13\u03bcs   \r\n                   <ufunc 'trunc'>           2        36.8\u00b10.2\u03bcs    69.4\u00b10.5\u03bcs     61.5\u00b12\u03bcs      103\u00b12\u03bcs       83.8\u00b11\u03bcs      199\u00b12\u03bcs   \r\n                   <ufunc 'trunc'>           4        55.2\u00b10.8\u03bcs     118\u00b15\u03bcs       72.9\u00b12\u03bcs      165\u00b13\u03bcs       104\u00b12\u03bcs       312\u00b130\u03bcs  \r\n               <ufunc 'conjugate'> (1)       1        45.5\u00b10.2\u03bcs    54.1\u00b10.5\u03bcs     50.1\u00b12\u03bcs      82.6\u00b11\u03bcs     75.9\u00b10.4\u03bcs     163\u00b13\u03bcs   \r\n               <ufunc 'conjugate'> (1)       2        48.5\u00b10.2\u03bcs    69.3\u00b10.4\u03bcs     53.9\u00b11\u03bcs      100\u00b11\u03bcs      82.3\u00b10.2\u03bcs     198\u00b18\u03bcs   \r\n               <ufunc 'conjugate'> (1)       4        57.5\u00b10.9\u03bcs     119\u00b15\u03bcs      69.6\u00b10.3\u03bcs     171\u00b14\u03bcs       101\u00b12\u03bcs       304\u00b110\u03bcs  \r\n                 <ufunc '_ones_like'>        1        35.2\u00b10.2\u03bcs    38.4\u00b10.3\u03bcs    38.7\u00b10.3\u03bcs     65.2\u00b11\u03bcs      66.1\u00b11\u03bcs      138\u00b15\u03bcs   \r\n                 <ufunc '_ones_like'>        2        36.1\u00b10.3\u03bcs    38.0\u00b10.2\u03bcs    38.7\u00b10.3\u03bcs    63.7\u00b10.4\u03bcs    63.8\u00b10.3\u03bcs     135\u00b13\u03bcs   \r\n                 <ufunc '_ones_like'>        4        35.4\u00b10.3\u03bcs    38.2\u00b10.2\u03bcs    38.9\u00b10.4\u03bcs    64.6\u00b10.7\u03bcs     64.8\u00b11\u03bcs      134\u00b13\u03bcs   \r\n              ========================= =========== ============= ============= ============= ============= ============= =============\r\n\r\n[ 75.00%] \u00b7 For numpy commit fd646bd6 <main> (round 2/2):\r\n[ 75.00%] \u00b7\u00b7 Building for virtualenv-py3.8-Cython..\r\n[ 75.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.8-Cython\r\n[ 75.74%] \u00b7\u00b7\u00b7 bench_ufunc.ArgParsing.time_add_arg_parsing                                                                                                                                  ok\r\n[ 75.74%] \u00b7\u00b7\u00b7 =============================================================== =========\r\n                                         arg_kwarg                                     \r\n              --------------------------------------------------------------- ---------\r\n                                   (array(1.), array(2.))                      680\u00b14ns \r\n                             (array(1.), array(2.), array(3.))                 591\u00b18ns \r\n                           (array(1.), array(2.), out=array(3.))               670\u00b12ns \r\n                          (array(1.), array(2.), out=(array(3.),))             657\u00b14ns \r\n               (array(1.), array(2.), out=array(3.), subok=True, where=True)   685\u00b15ns \r\n                             (array(1.), array(2.), subok=True)                746\u00b16ns \r\n                       (array(1.), array(2.), subok=True, where=True)          758\u00b14ns \r\n                 (array(1.), array(2.), array(3.), subok=True, where=True)     679\u00b19ns \r\n              =============================================================== =========\r\n\r\n[ 76.47%] \u00b7\u00b7\u00b7 bench_ufunc.ArgParsingReduce.time_add_reduce_arg_parsing                                                                                                                     ok\r\n[ 76.47%] \u00b7\u00b7\u00b7 ====================================================== =============\r\n                                    arg_kwarg                                     \r\n              ------------------------------------------------------ -------------\r\n                                (array([0., 1.]))                     1.37\u00b10.02\u03bcs \r\n                               (array([0., 1.]), 0)                   1.39\u00b10.03\u03bcs \r\n                            (array([0., 1.]), axis=0)                 1.45\u00b10.03\u03bcs \r\n                            (array([0., 1.]), 0, None)                1.43\u00b10.06\u03bcs \r\n                      (array([0., 1.]), axis=0, dtype=None)           1.51\u00b10.08\u03bcs \r\n                      (array([0., 1.]), 0, None, array(0.))           1.22\u00b10.02\u03bcs \r\n               (array([0., 1.]), axis=0, dtype=None, out=array(0.))   1.32\u00b10.04\u03bcs \r\n                         (array([0., 1.]), out=array(0.))             1.27\u00b10.04\u03bcs \r\n              ====================================================== =============\r\n\r\n[ 77.21%] \u00b7\u00b7\u00b7 bench_ufunc.Broadcast.time_broadcast                                                                                                                                10.5\u00b10.07ms\r\n[ 77.94%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_and_bool                                                                                                                                    1.70\u00b10.02\u03bcs\r\n[ 78.68%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_nonzero                                                                                                                                     13.1\u00b10.06\u03bcs\r\n[ 79.41%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_not_bool                                                                                                                                    1.49\u00b10.03\u03bcs\r\n[ 80.15%] \u00b7\u00b7\u00b7 bench_ufunc.Custom.time_or_bool                                                                                                                                     1.64\u00b10.03\u03bcs\r\n[ 80.88%] \u00b7\u00b7\u00b7 bench_ufunc.CustomArrayFloorDivideInt.time_floor_divide_int                                                                                                                  ok\r\n[ 80.88%] \u00b7\u00b7\u00b7 ============== ============= ============ =============\r\n              --                               size                  \r\n              -------------- ----------------------------------------\r\n                  dtype           100         10000        1000000   \r\n              ============== ============= ============ =============\r\n                numpy.int8    1.07\u00b10.01\u03bcs   73.9\u00b10.5\u03bcs   7.82\u00b10.04ms \r\n               numpy.int16    1.11\u00b10.01\u03bcs   76.1\u00b10.3\u03bcs    8.24\u00b10.1ms \r\n               numpy.int32    1.07\u00b10.01\u03bcs   93.8\u00b10.4\u03bcs    10.4\u00b10.1ms \r\n               numpy.int64    1.63\u00b10.02\u03bcs   161\u00b10.8\u03bcs    17.4\u00b10.04ms \r\n               numpy.uint8    1.00\u00b10.01\u03bcs   38.7\u00b10.1\u03bcs   3.79\u00b10.03ms \r\n               numpy.uint16   1.02\u00b10.01\u03bcs   38.9\u00b10.2\u03bcs   3.84\u00b10.04ms \r\n               numpy.uint32   1.03\u00b10.03\u03bcs   38.9\u00b10.2\u03bcs   3.80\u00b10.03ms \r\n               numpy.uint64   1.44\u00b10.01\u03bcs   82.4\u00b10.3\u03bcs    8.20\u00b10.2ms \r\n              ============== ============= ============ =============\r\n\r\n[ 81.62%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_char_or                                                                                                                               46.2\u00b10.2\u03bcs\r\n[ 82.35%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_char_or_temp                                                                                                                          59.7\u00b10.5\u03bcs\r\n[ 83.09%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_double_add                                                                                                                            56.7\u00b10.1\u03bcs\r\n[ 83.82%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_double_add_temp                                                                                                                       75.7\u00b10.1\u03bcs\r\n[ 84.56%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_float_add                                                                                                                             57.7\u00b10.1\u03bcs\r\n[ 85.29%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_float_add_temp                                                                                                                        76.6\u00b10.1\u03bcs\r\n[ 86.03%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_int_or                                                                                                                                56.0\u00b10.1\u03bcs\r\n[ 86.76%] \u00b7\u00b7\u00b7 bench_ufunc.CustomInplace.time_int_or_temp                                                                                                                           71.6\u00b10.4\u03bcs\r\n[ 87.50%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_add_scalar2                                                                                                                                    ok\r\n[ 87.50%] \u00b7\u00b7\u00b7 =============== =============\r\n                   dtype                   \r\n              --------------- -------------\r\n               numpy.float32   6.15\u00b10.09\u03bcs \r\n               numpy.float64   12.5\u00b10.06\u03bcs \r\n              =============== =============\r\n\r\n[ 88.24%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_divide_scalar2                                                                                                                                 ok\r\n[ 88.24%] \u00b7\u00b7\u00b7 =============== =============\r\n                   dtype                   \r\n              --------------- -------------\r\n               numpy.float32   12.5\u00b10.06\u03bcs \r\n               numpy.float64    26.3\u00b10.5\u03bcs \r\n              =============== =============\r\n\r\n[ 88.97%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_divide_scalar2_inplace                                                                                                                         ok\r\n[ 88.97%] \u00b7\u00b7\u00b7 =============== =============\r\n                   dtype                   \r\n              --------------- -------------\r\n               numpy.float32   12.7\u00b10.09\u03bcs \r\n               numpy.float64    26.1\u00b10.1\u03bcs \r\n              =============== =============\r\n\r\n[ 89.71%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalar.time_less_than_scalar2                                                                                                                              ok\r\n[ 89.71%] \u00b7\u00b7\u00b7 =============== =============\r\n                   dtype                   \r\n              --------------- -------------\r\n               numpy.float32   4.08\u00b10.03\u03bcs \r\n               numpy.float64    7.06\u00b10.1\u03bcs \r\n              =============== =============\r\n\r\n[ 90.44%] \u00b7\u00b7\u00b7 bench_ufunc.CustomScalarFloorDivideInt.time_floor_divide_int                                                                                                                 ok\r\n[ 90.44%] \u00b7\u00b7\u00b7 ============== ============= ============= ============= =============\r\n              --                                     divisors                       \r\n              -------------- -------------------------------------------------------\r\n                  dtype            8             -8            43           -43     \r\n              ============== ============= ============= ============= =============\r\n                numpy.int8    3.03\u00b10.07\u03bcs   3.02\u00b10.07\u03bcs   3.09\u00b10.02\u03bcs   3.02\u00b10.06\u03bcs \r\n               numpy.int16    3.34\u00b10.09\u03bcs   3.32\u00b10.09\u03bcs   3.34\u00b10.03\u03bcs   3.31\u00b10.09\u03bcs \r\n               numpy.int32     5.58\u00b10.1\u03bcs   5.58\u00b10.07\u03bcs    5.81\u00b10.2\u03bcs    5.60\u00b10.1\u03bcs \r\n               numpy.int64     14.9\u00b10.1\u03bcs    15.0\u00b10.1\u03bcs   14.9\u00b10.07\u03bcs   14.9\u00b10.08\u03bcs \r\n               numpy.uint8    3.34\u00b10.02\u03bcs       n/a        3.32\u00b10.2\u03bcs       n/a     \r\n               numpy.uint16    3.79\u00b10.2\u03bcs       n/a        3.77\u00b10.1\u03bcs       n/a     \r\n               numpy.uint32    5.49\u00b10.1\u03bcs       n/a       5.53\u00b10.05\u03bcs       n/a     \r\n               numpy.uint64    12.0\u00b10.1\u03bcs       n/a        12.1\u00b10.2\u03bcs       n/a     \r\n              ============== ============= ============= ============= =============\r\n\r\n[ 90.44%] \u00b7\u00b7\u00b7\u00b7 For parameters: <class 'numpy.uint8'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint8'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint16'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint16'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint32'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint32'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint64'>, -8\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n               \r\n               For parameters: <class 'numpy.uint64'>, -43\r\n               asv: skipped: NotImplementedError('Skipping test for negative divisor with unsigned type')\r\n\r\n[ 91.18%] \u00b7\u00b7\u00b7 bench_ufunc.Scalar.time_add_scalar                                                                                                                                     547\u00b110ns\r\n[ 91.91%] \u00b7\u00b7\u00b7 bench_ufunc.Scalar.time_add_scalar_conv                                                                                                                                807\u00b130ns\r\n[ 92.65%] \u00b7\u00b7\u00b7 bench_ufunc.Scalar.time_add_scalar_conv_complex                                                                                                                        831\u00b140ns\r\n[ 93.38%] \u00b7\u00b7\u00b7 bench_ufunc.UFunc.time_ufunc_types                                                                                                                                           ok\r\n[ 93.38%] \u00b7\u00b7\u00b7 =============== =============\r\n                   ufunc                   \r\n              --------------- -------------\r\n                    abs          806\u00b14\u03bcs   \r\n                  absolute       809\u00b12\u03bcs   \r\n                    add         386\u00b10.5\u03bcs  \r\n                   arccos      6.23\u00b10.02ms \r\n                  arccosh      5.94\u00b10.02ms \r\n                   arcsin      6.31\u00b10.01ms \r\n                  arcsinh      5.78\u00b10.07ms \r\n                   arctan      3.46\u00b10.01ms \r\n                  arctan2      1.84\u00b10.01ms \r\n                  arctanh      3.71\u00b10.01ms \r\n                bitwise_and     32.7\u00b10.2\u03bcs \r\n                bitwise_not    21.2\u00b10.04\u03bcs \r\n                 bitwise_or     32.7\u00b10.1\u03bcs \r\n                bitwise_xor     32.5\u00b10.2\u03bcs \r\n                    cbrt       1.90\u00b10.04ms \r\n                    ceil         213\u00b15\u03bcs   \r\n                    conj         191\u00b13\u03bcs   \r\n                 conjugate      190\u00b10.9\u03bcs  \r\n                  copysign      179\u00b10.8\u03bcs  \r\n                    cos        6.89\u00b10.04ms \r\n                    cosh       6.03\u00b10.03ms \r\n                  deg2rad       239\u00b10.3\u03bcs  \r\n                  degrees       240\u00b10.4\u03bcs  \r\n                   divide        679\u00b14\u03bcs   \r\n                   divmod        1.01\u00b10ms  \r\n                   equal         309\u00b13\u03bcs   \r\n                    exp        4.90\u00b10.01ms \r\n                    exp2       4.71\u00b10.02ms \r\n                   expm1       9.26\u00b10.04ms \r\n                    fabs         239\u00b12\u03bcs   \r\n                float_power    10.3\u00b10.07ms \r\n                   floor        210\u00b10.9\u03bcs  \r\n                floor_divide     890\u00b13\u03bcs   \r\n                    fmax         439\u00b11\u03bcs   \r\n                    fmin         430\u00b11\u03bcs   \r\n                    fmod         590\u00b12\u03bcs   \r\n                   frexp         341\u00b11\u03bcs   \r\n                    gcd         215\u00b10.6\u03bcs  \r\n                  greater       302\u00b10.9\u03bcs  \r\n               greater_equal     301\u00b12\u03bcs   \r\n                 heaviside      394\u00b10.9\u03bcs  \r\n                   hypot         1.04\u00b10ms  \r\n                   invert       21.3\u00b10.1\u03bcs \r\n                  isfinite       171\u00b11\u03bcs   \r\n                   isinf         171\u00b11\u03bcs   \r\n                   isnan        146\u00b10.7\u03bcs  \r\n                   isnat         352\u00b15ns   \r\n                    lcm          338\u00b12\u03bcs   \r\n                   ldexp        213\u00b10.5\u03bcs  \r\n                 left_shift     98.7\u00b10.6\u03bcs \r\n                    less        295\u00b10.9\u03bcs  \r\n                 less_equal      294\u00b12\u03bcs   \r\n                    log        3.13\u00b10.01ms \r\n                   log10       3.40\u00b10.04ms \r\n                   log1p       3.34\u00b10.01ms \r\n                    log2       3.16\u00b10.01ms \r\n                 logaddexp       350\u00b11\u03bcs   \r\n                 logaddexp2      338\u00b12\u03bcs   \r\n                logical_and      312\u00b12\u03bcs   \r\n                logical_not     192\u00b10.8\u03bcs  \r\n                 logical_or      261\u00b11\u03bcs   \r\n                logical_xor      371\u00b11\u03bcs   \r\n                   matmul      22.6\u00b10.05ms \r\n                  maximum        415\u00b12\u03bcs   \r\n                  minimum       396\u00b10.3\u03bcs  \r\n                    mod          693\u00b12\u03bcs   \r\n                    modf         433\u00b12\u03bcs   \r\n                  multiply      394\u00b10.4\u03bcs  \r\n                  negative       222\u00b11\u03bcs   \r\n                 nextafter       399\u00b12\u03bcs   \r\n                 not_equal       311\u00b12\u03bcs   \r\n                  positive       230\u00b13\u03bcs   \r\n                   power       10.7\u00b10.03ms \r\n                  rad2deg       240\u00b10.7\u03bcs  \r\n                  radians        240\u00b11\u03bcs   \r\n                 reciprocal     724\u00b10.8\u03bcs  \r\n                 remainder       694\u00b14\u03bcs   \r\n                right_shift     101\u00b10.2\u03bcs  \r\n                    rint         414\u00b12\u03bcs   \r\n                    sign        246\u00b10.9\u03bcs  \r\n                  signbit        91.7\u00b12\u03bcs  \r\n                    sin        6.69\u00b10.03ms \r\n                    sinh       6.59\u00b10.02ms \r\n                  spacing        428\u00b12\u03bcs   \r\n                    sqrt       1.52\u00b10.01ms \r\n                   square        244\u00b12\u03bcs   \r\n                  subtract      380\u00b10.7\u03bcs  \r\n                    tan        8.32\u00b10.09ms \r\n                    tanh       5.77\u00b10.01ms \r\n                true_divide      679\u00b12\u03bcs   \r\n                   trunc         207\u00b11\u03bcs   \r\n              =============== =============\r\n\r\n[ 94.12%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_UFunc_log.time_log                                                                                                                                   ok\r\n[ 94.12%] \u00b7\u00b7\u00b7 ======== ============ ============\r\n              --                 dtype          \r\n              -------- -------------------------\r\n               stride       f            d      \r\n              ======== ============ ============\r\n                 1      22.1\u00b10.8\u03bcs   60.1\u00b10.6\u03bcs \r\n                 2      29.3\u00b10.4\u03bcs   60.5\u00b10.2\u03bcs \r\n                 4      30.5\u00b10.5\u03bcs   60.6\u00b10.3\u03bcs \r\n              ======== ============ ============\r\n\r\n[ 94.85%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_cmplx_arithmetic.time_ufunc                                                                                                                          ok\r\n[ 94.85%] \u00b7\u00b7\u00b7 ========== ============= ============= ============= ============= ============= =============\r\n              --                                            stride / dtype                                  \r\n              ---------- -----------------------------------------------------------------------------------\r\n                bfunc        1 / F         1 / D         2 / F         2 / D         4 / F         4 / D    \r\n              ========== ============= ============= ============= ============= ============= =============\r\n                 add      10.8\u00b10.07\u03bcs    16.3\u00b10.1\u03bcs   13.5\u00b10.09\u03bcs   23.9\u00b10.09\u03bcs   20.7\u00b10.08\u03bcs   38.0\u00b10.06\u03bcs \r\n               subtract   10.9\u00b10.02\u03bcs   16.2\u00b10.07\u03bcs   13.6\u00b10.08\u03bcs   23.8\u00b10.06\u03bcs   20.6\u00b10.05\u03bcs    38.2\u00b10.1\u03bcs \r\n               multiply   13.3\u00b10.06\u03bcs    16.6\u00b10.1\u03bcs    14.6\u00b10.2\u03bcs    24.1\u00b10.1\u03bcs   20.8\u00b10.08\u03bcs    39.0\u00b10.2\u03bcs \r\n                divide    42.5\u00b10.07\u03bcs    48.5\u00b10.2\u03bcs    43.6\u00b10.4\u03bcs    48.4\u00b10.3\u03bcs    42.9\u00b10.2\u03bcs    49.0\u00b10.2\u03bcs \r\n              ========== ============= ============= ============= ============= ============= =============\r\n\r\n[ 95.59%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_cmplx_funcs.time_ufunc                                                                                                                               ok\r\n[ 95.59%] \u00b7\u00b7\u00b7 ============ ============= ============ ============= ============ ============ =============\r\n              --                                            stride / dtype                                 \r\n              ------------ --------------------------------------------------------------------------------\r\n                 bfunc         1 / F        1 / D         2 / F        2 / D        4 / F         4 / D    \r\n              ============ ============= ============ ============= ============ ============ =============\r\n               reciprocal    62.9\u00b10.4\u03bcs   72.0\u00b10.2\u03bcs    63.3\u00b10.2\u03bcs   72.1\u00b10.5\u03bcs   63.3\u00b10.3\u03bcs    72.6\u00b10.3\u03bcs \r\n                absolute     50.8\u00b10.2\u03bcs   51.4\u00b10.3\u03bcs    50.9\u00b10.1\u03bcs   51.1\u00b10.2\u03bcs   51.2\u00b10.2\u03bcs    52.0\u00b10.1\u03bcs \r\n                 square      13.0\u00b10.1\u03bcs   14.6\u00b10.9\u03bcs   13.3\u00b10.09\u03bcs   16.9\u00b10.4\u03bcs   15.5\u00b10.3\u03bcs   22.6\u00b10.08\u03bcs \r\n               conjugate    8.34\u00b10.04\u03bcs   11.1\u00b10.2\u03bcs    9.49\u00b10.2\u03bcs   14.7\u00b10.1\u03bcs   12.2\u00b10.1\u03bcs   21.6\u00b10.08\u03bcs \r\n              ============ ============= ============ ============= ============ ============ =============\r\n\r\n[ 96.32%] \u00b7\u00b7\u00b7 bench_ufunc_strides.AVX_ldexp.time_ufunc                                                                                                                                     ok\r\n[ 96.32%] \u00b7\u00b7\u00b7 ======= ============ ============ ============\r\n              --                      stride                \r\n              ------- --------------------------------------\r\n               dtype       1            2            4      \r\n              ======= ============ ============ ============\r\n                 f     57.8\u00b10.8\u03bcs   57.8\u00b10.5\u03bcs   57.7\u00b10.3\u03bcs \r\n                 d     59.8\u00b10.1\u03bcs   60.2\u00b10.2\u03bcs   60.0\u00b10.2\u03bcs \r\n              ======= ============ ============ ============\r\n\r\n[ 97.06%] \u00b7\u00b7\u00b7 bench_ufunc_strides.Binary.time_ufunc                                                                                                                                        ok\r\n[ 97.06%] \u00b7\u00b7\u00b7 ========= ============ ============ ============= ============ ============ =========== =========== ==========\r\n              --                                                              stride_out / dtype                            \r\n              ----------------------------------- --------------------------------------------------------------------------\r\n                ufunc    stride_in0   stride_in1      1 / f        1 / d        2 / f        2 / d       4 / f      4 / d   \r\n              ========= ============ ============ ============= ============ ============ =========== =========== ==========\r\n               maximum       1            1         37.9\u00b10.2\u03bcs   74.3\u00b10.4\u03bcs    85.5\u00b15\u03bcs    110\u00b10.8\u03bcs    126\u00b13\u03bcs    220\u00b13\u03bcs  \r\n               maximum       1            2         75.8\u00b10.4\u03bcs    97.0\u00b11\u03bcs     110\u00b18\u03bcs      141\u00b11\u03bcs     129\u00b14\u03bcs    260\u00b110\u03bcs \r\n               maximum       1            4          80.3\u00b13\u03bcs     162\u00b13\u03bcs      120\u00b13\u03bcs     216\u00b10.6\u03bcs    145\u00b13\u03bcs    353\u00b13\u03bcs  \r\n               maximum       2            1         75.9\u00b10.2\u03bcs   95.5\u00b10.6\u03bcs    105\u00b19\u03bcs     140\u00b10.9\u03bcs    130\u00b11\u03bcs    267\u00b120\u03bcs \r\n               maximum       2            2         129\u00b10.2\u03bcs     124\u00b12\u03bcs     146\u00b10.3\u03bcs     178\u00b12\u03bcs     149\u00b11\u03bcs    312\u00b110\u03bcs \r\n               maximum       2            4          131\u00b12\u03bcs      201\u00b14\u03bcs      151\u00b13\u03bcs      276\u00b16\u03bcs     166\u00b13\u03bcs    434\u00b17\u03bcs  \r\n               maximum       4            1         80.3\u00b10.4\u03bcs    165\u00b17\u03bcs     114\u00b10.8\u03bcs     216\u00b15\u03bcs     144\u00b14\u03bcs    400\u00b140\u03bcs \r\n               maximum       4            2         131\u00b10.7\u03bcs     205\u00b12\u03bcs      156\u00b14\u03bcs      269\u00b110\u03bcs    171\u00b16\u03bcs    421\u00b13\u03bcs  \r\n               maximum       4            4          139\u00b14\u03bcs      309\u00b120\u03bcs     166\u00b14\u03bcs      388\u00b19\u03bcs     198\u00b14\u03bcs    594\u00b150\u03bcs \r\n               minimum       1            1         38.1\u00b10.2\u03bcs   74.3\u00b10.3\u03bcs    85.5\u00b15\u03bcs     114\u00b12\u03bcs     127\u00b14\u03bcs    220\u00b17\u03bcs  \r\n               minimum       1            2        75.7\u00b10.09\u03bcs    97.3\u00b11\u03bcs     102\u00b110\u03bcs     142\u00b14\u03bcs     132\u00b15\u03bcs    266\u00b16\u03bcs  \r\n               minimum       1            4         79.6\u00b10.3\u03bcs    159\u00b14\u03bcs     112\u00b10.6\u03bcs     220\u00b14\u03bcs     148\u00b14\u03bcs    389\u00b130\u03bcs \r\n               minimum       2            1         74.0\u00b10.2\u03bcs   95.9\u00b10.6\u03bcs    105\u00b19\u03bcs      141\u00b12\u03bcs    130\u00b10.3\u03bcs   270\u00b110\u03bcs \r\n               minimum       2            2         113\u00b10.1\u03bcs     126\u00b12\u03bcs     150\u00b10.5\u03bcs     173\u00b12\u03bcs    152\u00b10.4\u03bcs   292\u00b120\u03bcs \r\n               minimum       2            4          118\u00b13\u03bcs      207\u00b13\u03bcs      158\u00b13\u03bcs      276\u00b110\u03bcs    170\u00b14\u03bcs    453\u00b140\u03bcs \r\n               minimum       4            1         79.4\u00b10.8\u03bcs    162\u00b13\u03bcs     113\u00b10.9\u03bcs     215\u00b12\u03bcs     144\u00b12\u03bcs    384\u00b110\u03bcs \r\n               minimum       4            2          116\u00b14\u03bcs      208\u00b13\u03bcs      155\u00b11\u03bcs      277\u00b19\u03bcs     170\u00b15\u03bcs    421\u00b14\u03bcs  \r\n               minimum       4            4          127\u00b12\u03bcs      300\u00b13\u03bcs      165\u00b12\u03bcs      391\u00b14\u03bcs     203\u00b15\u03bcs    569\u00b120\u03bcs \r\n                 fmax        1            1         38.0\u00b10.2\u03bcs   74.5\u00b10.8\u03bcs    62.8\u00b12\u03bcs     112\u00b12\u03bcs     94.5\u00b12\u03bcs   212\u00b17\u03bcs  \r\n                 fmax        1            2         75.7\u00b10.1\u03bcs   95.9\u00b10.5\u03bcs   97.4\u00b10.2\u03bcs    138\u00b11\u03bcs    109\u00b10.5\u03bcs   266\u00b18\u03bcs  \r\n                 fmax        1            4         79.8\u00b10.3\u03bcs    164\u00b13\u03bcs      113\u00b12\u03bcs      225\u00b13\u03bcs     138\u00b14\u03bcs    349\u00b120\u03bcs \r\n                 fmax        2            1         75.7\u00b10.2\u03bcs    95.6\u00b11\u03bcs    93.7\u00b10.3\u03bcs    138\u00b12\u03bcs    108\u00b10.3\u03bcs   261\u00b110\u03bcs \r\n                 fmax        2            2         120\u00b10.04\u03bcs    123\u00b13\u03bcs      144\u00b11\u03bcs      173\u00b14\u03bcs     145\u00b14\u03bcs    317\u00b120\u03bcs \r\n                 fmax        2            4          123\u00b12\u03bcs      204\u00b17\u03bcs     146\u00b10.5\u03bcs     272\u00b19\u03bcs     163\u00b15\u03bcs    474\u00b140\u03bcs \r\n                 fmax        4            1         79.4\u00b10.3\u03bcs    161\u00b13\u03bcs      111\u00b12\u03bcs      216\u00b14\u03bcs     132\u00b11\u03bcs    419\u00b120\u03bcs \r\n                 fmax        4            2         123\u00b10.2\u03bcs     205\u00b17\u03bcs      147\u00b12\u03bcs      262\u00b15\u03bcs     159\u00b13\u03bcs    448\u00b130\u03bcs \r\n                 fmax        4            4          130\u00b11\u03bcs      307\u00b120\u03bcs     160\u00b13\u03bcs      387\u00b16\u03bcs     195\u00b15\u03bcs    551\u00b110\u03bcs \r\n                 fmin        1            1         37.9\u00b10.2\u03bcs   73.5\u00b10.5\u03bcs    62.5\u00b12\u03bcs    111\u00b10.8\u03bcs    93.3\u00b12\u03bcs   214\u00b14\u03bcs  \r\n                 fmin        1            2        75.7\u00b10.03\u03bcs    97.3\u00b12\u03bcs    94.2\u00b10.3\u03bcs    141\u00b12\u03bcs    110\u00b10.8\u03bcs   263\u00b16\u03bcs  \r\n                 fmin        1            4         80.5\u00b10.5\u03bcs    162\u00b12\u03bcs     110\u00b10.3\u03bcs     216\u00b14\u03bcs     132\u00b13\u03bcs    351\u00b19\u03bcs  \r\n                 fmin        2            1          73.8\u00b11\u03bcs    95.8\u00b10.8\u03bcs   93.7\u00b10.2\u03bcs   140\u00b10.8\u03bcs   109\u00b10.9\u03bcs   264\u00b110\u03bcs \r\n                 fmin        2            2         128\u00b10.2\u03bcs     124\u00b12\u03bcs     139\u00b10.7\u03bcs     174\u00b12\u03bcs     142\u00b12\u03bcs    288\u00b15\u03bcs  \r\n                 fmin        2            4          134\u00b14\u03bcs      203\u00b15\u03bcs      145\u00b11\u03bcs      268\u00b16\u03bcs    166\u00b10.7\u03bcs   459\u00b130\u03bcs \r\n                 fmin        4            1          78.1\u00b11\u03bcs     162\u00b12\u03bcs      113\u00b14\u03bcs      215\u00b15\u03bcs     131\u00b12\u03bcs    383\u00b120\u03bcs \r\n                 fmin        4            2          131\u00b12\u03bcs      204\u00b17\u03bcs      146\u00b13\u03bcs      287\u00b120\u03bcs    172\u00b110\u03bcs   474\u00b140\u03bcs \r\n                 fmin        4            4          139\u00b13\u03bcs      329\u00b110\u03bcs     164\u00b19\u03bcs      409\u00b120\u03bcs    212\u00b120\u03bcs   607\u00b140\u03bcs \r\n              ========= ============ ============ ============= ============ ============ =========== =========== ==========\r\n\r\n[ 97.79%] \u00b7\u00b7\u00b7 bench_ufunc_strides.BinaryInt.time_ufunc                                                                                                                                     ok\r\n[ 97.79%] \u00b7\u00b7\u00b7 ========= ============ ============ ============ ======= =============\r\n                ufunc    stride_in0   stride_in1   stride_out   dtype               \r\n              --------- ------------ ------------ ------------ ------- -------------\r\n               maximum       1            1            1          b     8.70\u00b10.04\u03bcs \r\n               maximum       1            1            1          B      9.16\u00b10.3\u03bcs \r\n               maximum       1            1            1          h     17.7\u00b10.08\u03bcs \r\n               maximum       1            1            1          H     17.9\u00b10.07\u03bcs \r\n               maximum       1            1            1          i      33.9\u00b10.2\u03bcs \r\n               maximum       1            1            1          I      33.8\u00b10.1\u03bcs \r\n               maximum       1            1            1          l      65.9\u00b10.5\u03bcs \r\n               maximum       1            1            1          L      66.4\u00b10.3\u03bcs \r\n               maximum       1            1            1          q      66.6\u00b10.2\u03bcs \r\n               maximum       1            1            1          Q      66.7\u00b10.5\u03bcs \r\n               maximum       1            1            2          b       72.7\u00b12\u03bcs  \r\n               maximum       1            1            2          B       71.4\u00b11\u03bcs  \r\n               maximum       1            1            2          h      71.8\u00b10.6\u03bcs \r\n               maximum       1            1            2          H      71.8\u00b10.2\u03bcs \r\n               maximum       1            1            2          i      72.8\u00b10.9\u03bcs \r\n               maximum       1            1            2          I      72.3\u00b10.2\u03bcs \r\n               maximum       1            1            2          l       111\u00b11\u03bcs   \r\n               maximum       1            1            2          L       108\u00b13\u03bcs   \r\n               maximum       1            1            2          q       109\u00b15\u03bcs   \r\n               maximum       1            1            2          Q      102\u00b10.9\u03bcs  \r\n               maximum       1            1            4          b      75.8\u00b10.3\u03bcs \r\n               maximum       1            1            4          B      74.8\u00b10.3\u03bcs \r\n               maximum       1            1            4          h      71.8\u00b10.3\u03bcs \r\n               maximum       1            1            4          H       75.3\u00b12\u03bcs  \r\n               maximum       1            1            4          i       93.5\u00b16\u03bcs  \r\n               maximum       1            1            4          I      88.3\u00b10.9\u03bcs \r\n               maximum       1            1            4          l       204\u00b11\u03bcs   \r\n               maximum       1            1            4          L       202\u00b12\u03bcs   \r\n               maximum       1            1            4          q       200\u00b13\u03bcs   \r\n               maximum       1            1            4          Q       198\u00b14\u03bcs   \r\n               maximum       1            2            1          b      71.0\u00b10.9\u03bcs \r\n               maximum       1            2            1          B      70.4\u00b10.3\u03bcs \r\n               maximum       1            2            1          h      72.1\u00b10.3\u03bcs \r\n               maximum       1            2            1          H      72.2\u00b10.4\u03bcs \r\n               maximum       1            2            1          i      72.8\u00b10.3\u03bcs \r\n               maximum       1            2            1          I      72.8\u00b10.4\u03bcs \r\n               maximum       1            2            1          l      96.9\u00b10.9\u03bcs \r\n               maximum       1            2            1          L      94.0\u00b10.6\u03bcs \r\n               maximum       1            2            1          q      93.9\u00b10.2\u03bcs \r\n               maximum       1            2            1          Q      95.3\u00b10.7\u03bcs \r\n               maximum       1            2            2          b      71.5\u00b10.3\u03bcs \r\n               maximum       1            2            2          B      71.1\u00b10.2\u03bcs \r\n               maximum       1            2            2          h      71.7\u00b10.3\u03bcs \r\n               maximum       1            2            2          H      71.6\u00b10.5\u03bcs \r\n               maximum       1            2            2          i      73.5\u00b10.3\u03bcs \r\n               maximum       1            2            2          I      73.3\u00b10.3\u03bcs \r\n               maximum       1            2            2          l      127\u00b10.4\u03bcs  \r\n               maximum       1            2            2          L       134\u00b16\u03bcs   \r\n               maximum       1            2            2          q       126\u00b12\u03bcs   \r\n               maximum       1            2            2          Q      126\u00b10.7\u03bcs  \r\n               maximum       1            2            4          b      75.6\u00b10.3\u03bcs \r\n               maximum       1            2            4          B      75.2\u00b10.3\u03bcs \r\n               maximum       1            2            4          h      72.1\u00b10.2\u03bcs \r\n               maximum       1            2            4          H      72.3\u00b10.3\u03bcs \r\n               maximum       1            2            4          i       97.2\u00b15\u03bcs  \r\n               maximum       1            2            4          I      92.6\u00b10.6\u03bcs \r\n               maximum       1            2            4          l       249\u00b17\u03bcs   \r\n               maximum       1            2            4          L       252\u00b15\u03bcs   \r\n               maximum       1            2            4          q       247\u00b17\u03bcs   \r\n               maximum       1            2            4          Q       248\u00b17\u03bcs   \r\n               maximum       1            4            1          b      70.4\u00b10.5\u03bcs \r\n               maximum       1            4            1          B      70.4\u00b10.2\u03bcs \r\n               maximum       1            4            1          h      72.3\u00b10.8\u03bcs \r\n               maximum       1            4            1          H       75.7\u00b14\u03bcs  \r\n               maximum       1            4            1          i       89.2\u00b14\u03bcs  \r\n               maximum       1            4            1          I       84.1\u00b12\u03bcs  \r\n               maximum       1            4            1          l       154\u00b13\u03bcs   \r\n               maximum       1            4            1          L       152\u00b13\u03bcs   \r\n               maximum       1            4            1          q       158\u00b110\u03bcs  \r\n               maximum       1            4            1          Q       156\u00b14\u03bcs   \r\n               maximum       1            4            2          b      71.6\u00b10.7\u03bcs \r\n               maximum       1            4            2          B       73.2\u00b14\u03bcs  \r\n               maximum       1            4            2          h       72.6\u00b14\u03bcs  \r\n               maximum       1            4            2          H      72.2\u00b10.6\u03bcs \r\n               maximum       1            4            2          i      90.7\u00b10.7\u03bcs \r\n               maximum       1            4            2          I      90.2\u00b10.9\u03bcs \r\n               maximum       1            4            2          l       206\u00b11\u03bcs   \r\n               maximum       1            4            2          L       207\u00b110\u03bcs  \r\n               maximum       1            4            2          q       209\u00b16\u03bcs   \r\n               maximum       1            4            2          Q       207\u00b110\u03bcs  \r\n               maximum       1            4            4          b      75.4\u00b10.4\u03bcs \r\n               maximum       1            4            4          B      75.7\u00b10.5\u03bcs \r\n               maximum       1            4            4          h       77.4\u00b15\u03bcs  \r\n               maximum       1            4            4          H      73.3\u00b10.4\u03bcs \r\n               maximum       1            4            4          i       114\u00b12\u03bcs   \r\n               maximum       1            4            4          I       119\u00b11\u03bcs   \r\n               maximum       1            4            4          l       335\u00b120\u03bcs  \r\n               maximum       1            4            4          L       333\u00b120\u03bcs  \r\n               maximum       1            4            4          q       359\u00b120\u03bcs  \r\n               maximum       1            4            4          Q       385\u00b19\u03bcs   \r\n               maximum       2            1            1          b       73.3\u00b13\u03bcs  \r\n               maximum       2            1            1          B       73.4\u00b13\u03bcs  \r\n               maximum       2            1            1          h      71.5\u00b10.3\u03bcs \r\n               maximum       2            1            1          H      71.4\u00b10.1\u03bcs \r\n               maximum       2            1            1          i      72.4\u00b10.3\u03bcs \r\n               maximum       2            1            1          I      72.2\u00b10.3\u03bcs \r\n               maximum       2            1            1          l      94.2\u00b10.5\u03bcs \r\n               maximum       2            1            1          L      95.3\u00b10.8\u03bcs \r\n               maximum       2            1            1          q       101\u00b14\u03bcs   \r\n               maximum       2            1            1          Q       96.0\u00b12\u03bcs  \r\n               maximum       2            1            2          b      70.6\u00b10.4\u03bcs \r\n               maximum       2            1            2          B      71.0\u00b10.4\u03bcs \r\n               maximum       2            1            2          h       74.9\u00b14\u03bcs  \r\n               maximum       2            1            2          H      71.6\u00b10.2\u03bcs \r\n               maximum       2            1            2          i       80.4\u00b17\u03bcs  \r\n               maximum       2            1            2          I      73.4\u00b10.7\u03bcs \r\n               maximum       2            1            2          l       131\u00b13\u03bcs   \r\n               maximum       2            1            2          L       128\u00b13\u03bcs   \r\n               maximum       2            1            2          q       127\u00b12\u03bcs   \r\n               maximum       2            1            2          Q       130\u00b12\u03bcs   \r\n               maximum       2            1            4          b      75.8\u00b10.5\u03bcs \r\n               maximum       2            1            4          B      75.7\u00b10.5\u03bcs \r\n               maximum       2            1            4          h      72.7\u00b10.4\u03bcs \r\n               maximum       2            1            4          H      72.5\u00b10.3\u03bcs \r\n               maximum       2            1            4          i       93.0\u00b11\u03bcs  \r\n               maximum       2            1            4          I      94.7\u00b10.8\u03bcs \r\n               maximum       2            1            4          l       248\u00b16\u03bcs   \r\n               maximum       2            1            4          L       240\u00b18\u03bcs   \r\n               maximum       2            1            4          q       247\u00b19\u03bcs   \r\n               maximum       2            1            4          Q       243\u00b16\u03bcs   \r\n               maximum       2            2            1          b      70.6\u00b10.4\u03bcs \r\n               maximum       2            2            1          B      70.8\u00b10.1\u03bcs \r\n               maximum       2            2            1          h      71.6\u00b10.3\u03bcs \r\n               maximum       2            2            1          H      79.7\u00b10.7\u03bcs \r\n               maximum       2            2            1          i      73.4\u00b10.3\u03bcs \r\n               maximum       2            2            1          I       81.4\u00b18\u03bcs  \r\n               maximum       2            2            1          l      118\u00b10.9\u03bcs  \r\n               maximum       2            2            1          L       126\u00b17\u03bcs   \r\n               maximum       2            2            1          q       122\u00b12\u03bcs   \r\n               maximum       2            2            1          Q       120\u00b13\u03bcs   \r\n               maximum       2            2            2          b      71.3\u00b10.5\u03bcs \r\n               maximum       2            2            2          B      71.5\u00b10.5\u03bcs \r\n               maximum       2            2            2          h       76.2\u00b14\u03bcs  \r\n               maximum       2            2            2          H       71.8\u00b14\u03bcs  \r\n               maximum       2            2            2          i      75.5\u00b10.4\u03bcs \r\n               maximum       2            2            2          I      75.9\u00b10.6\u03bcs \r\n               maximum       2            2            2          l       155\u00b12\u03bcs   \r\n               maximum       2            2            2          L       157\u00b15\u03bcs   \r\n               maximum       2            2            2          q       160\u00b16\u03bcs   \r\n               maximum       2            2            2          Q       160\u00b12\u03bcs   \r\n               maximum       2            2            4          b      74.7\u00b10.1\u03bcs \r\n               maximum       2            2            4          B       75.9\u00b11\u03bcs  \r\n               maximum       2            2            4          h      71.8\u00b10.5\u03bcs \r\n               maximum       2            2            4          H      72.8\u00b10.6\u03bcs \r\n               maximum       2            2            4          i       101\u00b11\u03bcs   \r\n               maximum       2            2            4          I       102\u00b11\u03bcs   \r\n               maximum       2            2            4          l       272\u00b12\u03bcs   \r\n               maximum       2            2            4          L       292\u00b120\u03bcs  \r\n               maximum       2            2            4          q       293\u00b120\u03bcs  \r\n               maximum       2            2            4          Q       296\u00b120\u03bcs  \r\n               maximum       2            4            1          b       75.7\u00b14\u03bcs  \r\n               maximum       2            4            1          B      71.1\u00b10.3\u03bcs \r\n               maximum       2            4            1          h      72.2\u00b10.3\u03bcs \r\n               maximum       2            4            1          H      72.4\u00b10.6\u03bcs \r\n               maximum       2            4            1          i       87.7\u00b11\u03bcs  \r\n               maximum       2            4            1          I       86.8\u00b11\u03bcs  \r\n               maximum       2            4            1          l       198\u00b110\u03bcs  \r\n               maximum       2            4            1          L       196\u00b14\u03bcs   \r\n               maximum       2            4            1          q       197\u00b13\u03bcs   \r\n               maximum       2            4            1          Q       204\u00b16\u03bcs   \r\n               maximum       2            4            2          b      71.5\u00b10.2\u03bcs \r\n               maximum       2            4            2          B      71.5\u00b10.6\u03bcs \r\n               maximum       2            4            2          h      73.0\u00b10.4\u03bcs \r\n               maximum       2            4            2          H      72.1\u00b10.3\u03bcs \r\n               maximum       2            4            2          i       95.9\u00b11\u03bcs  \r\n               maximum       2            4            2          I       95.1\u00b11\u03bcs  \r\n               maximum       2            4            2          l       259\u00b19\u03bcs   \r\n               maximum       2            4            2          L       258\u00b19\u03bcs   \r\n               maximum       2            4            2          q       256\u00b120\u03bcs  \r\n               maximum       2            4            2          Q       249\u00b14\u03bcs   \r\n               maximum       2            4            4          b      76.0\u00b10.2\u03bcs \r\n               maximum       2            4            4          B      76.3\u00b10.4\u03bcs \r\n               maximum       2            4            4          h      73.6\u00b10.5\u03bcs \r\n               maximum       2            4            4          H      73.8\u00b10.7\u03bcs \r\n               maximum       2            4            4          i       128\u00b12\u03bcs   \r\n               maximum       2            4            4          I      127\u00b10.7\u03bcs  \r\n               maximum       2            4            4          l       412\u00b130\u03bcs  \r\n               maximum       2            4            4          L       467\u00b120\u03bcs  \r\n               maximum       2            4            4          q       465\u00b14\u03bcs   \r\n               maximum       2            4            4          Q       431\u00b140\u03bcs  \r\n               maximum       4            1            1          b       74.4\u00b14\u03bcs  \r\n               maximum       4            1            1          B      71.2\u00b10.1\u03bcs \r\n               maximum       4            1            1          h      72.0\u00b10.2\u03bcs \r\n               maximum       4            1            1          H      71.9\u00b10.2\u03bcs \r\n               maximum       4            1            1          i      85.1\u00b10.4\u03bcs \r\n               maximum       4            1            1          I      85.4\u00b10.6\u03bcs \r\n               maximum       4            1            1          l       151\u00b110\u03bcs  \r\n               maximum       4            1            1          L       154\u00b12\u03bcs   \r\n               maximum       4            1            1          q       150\u00b16\u03bcs   \r\n               maximum       4            1            1          Q       151\u00b13\u03bcs   \r\n               maximum       4            1            2          b      71.7\u00b10.5\u03bcs \r\n               maximum       4            1            2          B      71.4\u00b10.2\u03bcs \r\n               maximum       4            1            2          h      72.6\u00b10.5\u03bcs \r\n               maximum       4            1            2          H      73.2\u00b10.6\u03bcs \r\n               maximum       4            1            2          i       94.5\u00b12\u03bcs  \r\n               maximum       4            1            2          I      91.7\u00b10.5\u03bcs \r\n               maximum       4            1            2          l       206\u00b17\u03bcs   \r\n               maximum       4            1            2          L       208\u00b15\u03bcs   \r\n               maximum       4            1            2          q       205\u00b19\u03bcs   \r\n               maximum       4            1            2          Q       211\u00b17\u03bcs   \r\n               maximum       4            1            4          b       78.1\u00b13\u03bcs  \r\n               maximum       4            1            4          B      76.3\u00b10.8\u03bcs \r\n               maximum       4            1            4          h       73.5\u00b11\u03bcs  \r\n               maximum       4            1            4          H      72.9\u00b10.6\u03bcs \r\n               maximum       4            1            4          i       119\u00b13\u03bcs   \r\n               maximum       4            1            4          I       117\u00b13\u03bcs   \r\n               maximum       4            1            4          l       362\u00b17\u03bcs   \r\n               maximum       4            1            4          L       356\u00b120\u03bcs  \r\n               maximum       4            1            4          q       363\u00b110\u03bcs  \r\n               maximum       4            1            4          Q       354\u00b110\u03bcs  \r\n               maximum       4            2            1          b      71.1\u00b10.3\u03bcs \r\n               maximum       4            2            1          B       71.2\u00b11\u03bcs  \r\n               maximum       4            2            1          h      72.6\u00b10.3\u03bcs \r\n               maximum       4            2            1          H      72.1\u00b10.2\u03bcs \r\n               maximum       4            2            1          i      88.8\u00b10.8\u03bcs \r\n               maximum       4            2            1          I       93.6\u00b15\u03bcs  \r\n               maximum       4            2            1          l       188\u00b14\u03bcs   \r\n               maximum       4            2            1          L       194\u00b17\u03bcs   \r\n               maximum       4            2            1          q       192\u00b14\u03bcs   \r\n               maximum       4            2            1          Q       196\u00b15\u03bcs   \r\n               maximum       4            2            2          b       76.5\u00b15\u03bcs  \r\n               maximum       4            2            2          B      71.4\u00b10.3\u03bcs \r\n               maximum       4            2            2          h      73.2\u00b10.9\u03bcs \r\n               maximum       4            2            2          H      84.6\u00b10.5\u03bcs \r\n               maximum       4            2            2          i       108\u00b14\u03bcs   \r\n               maximum       4            2            2          I       101\u00b13\u03bcs   \r\n               maximum       4            2            2          l       241\u00b14\u03bcs   \r\n               maximum       4            2            2          L       249\u00b110\u03bcs  \r\n               maximum       4            2            2          q       247\u00b19\u03bcs   \r\n               maximum       4            2            2          Q       252\u00b110\u03bcs  \r\n               maximum       4            2            4          b      76.2\u00b10.4\u03bcs \r\n               maximum       4            2            4          B      75.9\u00b10.3\u03bcs \r\n               maximum       4            2            4          h      74.0\u00b10.2\u03bcs \r\n               maximum       4            2            4          H      74.4\u00b10.3\u03bcs \r\n               maximum       4            2            4          i       126\u00b13\u03bcs   \r\n               maximum       4            2            4          I       128\u00b13\u03bcs   \r\n               maximum       4            2            4          l       402\u00b15\u03bcs   \r\n               maximum       4            2            4          L       403\u00b17\u03bcs   \r\n               maximum       4            2            4          q       405\u00b15\u03bcs   \r\n               maximum       4            2            4          Q       420\u00b130\u03bcs  \r\n               maximum       4            4            1          b       71.8\u00b11\u03bcs  \r\n               maximum       4            4            1          B      71.0\u00b10.3\u03bcs \r\n               maximum       4            4            1          h      72.9\u00b10.4\u03bcs \r\n               maximum       4            4            1          H      73.2\u00b10.4\u03bcs \r\n               maximum       4            4            1          i       110\u00b12\u03bcs   \r\n               maximum       4            4            1          I       107\u00b13\u03bcs   \r\n               maximum       4            4            1          l       288\u00b110\u03bcs  \r\n               maximum       4            4            1          L       296\u00b120\u03bcs  \r\n               maximum       4            4            1          q       296\u00b18\u03bcs   \r\n               maximum       4            4            1          Q       303\u00b110\u03bcs  \r\n               maximum       4            4            2          b      71.6\u00b10.1\u03bcs \r\n               maximum       4            4            2          B      71.5\u00b10.2\u03bcs \r\n               maximum       4            4            2          h      73.7\u00b10.7\u03bcs \r\n               maximum       4            4            2          H      73.5\u00b10.7\u03bcs \r\n               maximum       4            4            2          i       123\u00b12\u03bcs   \r\n               maximum       4            4            2          I       118\u00b12\u03bcs   \r\n               maximum       4            4            2          l       366\u00b16\u03bcs   \r\n               maximum       4            4            2          L       365\u00b17\u03bcs   \r\n               maximum       4            4            2          q       373\u00b17\u03bcs   \r\n               maximum       4            4            2          Q       380\u00b130\u03bcs  \r\n               maximum       4            4            4          b       79.7\u00b13\u03bcs  \r\n               maximum       4            4            4          B       81.8\u00b12\u03bcs  \r\n               maximum       4            4            4          h       84.9\u00b19\u03bcs  \r\n               maximum       4            4            4          H      76.1\u00b10.7\u03bcs \r\n               maximum       4            4            4          i       162\u00b14\u03bcs   \r\n               maximum       4            4            4          I      160\u00b10.7\u03bcs  \r\n               maximum       4            4            4          l       544\u00b110\u03bcs  \r\n               maximum       4            4            4          L       617\u00b110\u03bcs  \r\n               maximum       4            4            4          q       550\u00b110\u03bcs  \r\n               maximum       4            4            4          Q       617\u00b14\u03bcs   \r\n               minimum       1            1            1          b      8.68\u00b10.2\u03bcs \r\n               minimum       1            1            1          B     8.70\u00b10.09\u03bcs \r\n               minimum       1            1            1          h      17.9\u00b10.1\u03bcs \r\n               minimum       1            1            1          H      17.8\u00b10.1\u03bcs \r\n               minimum       1            1            1          i      33.8\u00b10.2\u03bcs \r\n               minimum       1            1            1          I      33.8\u00b10.2\u03bcs \r\n               minimum       1            1            1          l      66.4\u00b10.9\u03bcs \r\n               minimum       1            1            1          L      66.5\u00b10.6\u03bcs \r\n               minimum       1            1            1          q      66.6\u00b10.6\u03bcs \r\n               minimum       1            1            1          Q      66.8\u00b10.6\u03bcs \r\n               minimum       1            1            2          b       72.3\u00b12\u03bcs  \r\n               minimum       1            1            2          B       80.6\u00b12\u03bcs  \r\n               minimum       1            1            2          h      71.6\u00b10.2\u03bcs \r\n               minimum       1            1            2          H      77.7\u00b10.3\u03bcs \r\n               minimum       1            1            2          i      71.8\u00b10.3\u03bcs \r\n               minimum       1            1            2          I      78.1\u00b10.3\u03bcs \r\n               minimum       1            1            2          l      102\u00b10.4\u03bcs  \r\n               minimum       1            1            2          L       104\u00b12\u03bcs   \r\n               minimum       1            1            2          q       104\u00b11\u03bcs   \r\n               minimum       1            1            2          Q       104\u00b11\u03bcs   \r\n               minimum       1            1            4          b      74.8\u00b10.1\u03bcs \r\n               minimum       1            1            4          B      79.1\u00b10.3\u03bcs \r\n               minimum       1            1            4          h       72.2\u00b12\u03bcs  \r\n               minimum       1            1            4          H      78.0\u00b10.3\u03bcs \r\n               minimum       1            1            4          i       92.6\u00b16\u03bcs  \r\n               minimum       1            1            4          I       95.8\u00b17\u03bcs  \r\n               minimum       1            1            4          l       196\u00b15\u03bcs   \r\n               minimum       1            1            4          L       201\u00b17\u03bcs   \r\n               minimum       1            1            4          q       196\u00b15\u03bcs   \r\n               minimum       1            1            4          Q       199\u00b16\u03bcs   \r\n               minimum       1            2            1          b      70.4\u00b10.3\u03bcs \r\n               minimum       1            2            1          B       80.9\u00b13\u03bcs  \r\n               minimum       1            2            1          h       71.6\u00b13\u03bcs  \r\n               minimum       1            2            1          H      77.6\u00b10.4\u03bcs \r\n               minimum       1            2            1          i      72.2\u00b10.3\u03bcs \r\n               minimum       1            2            1          I      78.3\u00b10.4\u03bcs \r\n               minimum       1            2            1          l      93.4\u00b10.4\u03bcs \r\n               minimum       1            2            1          L      102\u00b10.8\u03bcs  \r\n               minimum       1            2            1          q       102\u00b15\u03bcs   \r\n               minimum       1            2            1          Q      100\u00b10.8\u03bcs  \r\n               minimum       1            2            2          b      71.3\u00b10.2\u03bcs \r\n               minimum       1            2            2          B      79.7\u00b10.3\u03bcs \r\n               minimum       1            2            2          h      71.7\u00b10.2\u03bcs \r\n               minimum       1            2            2          H      78.3\u00b10.4\u03bcs \r\n               minimum       1            2            2          i      73.7\u00b10.4\u03bcs \r\n               minimum       1            2            2          I      78.9\u00b10.5\u03bcs \r\n               minimum       1            2            2          l       131\u00b14\u03bcs   \r\n               minimum       1            2            2          L       127\u00b13\u03bcs   \r\n               minimum       1            2            2          q       125\u00b12\u03bcs   \r\n               minimum       1            2            2          Q       132\u00b16\u03bcs   \r\n               minimum       1            2            4          b      75.7\u00b10.3\u03bcs \r\n               minimum       1            2            4          B      79.1\u00b10.3\u03bcs \r\n               minimum       1            2            4          h      72.0\u00b10.4\u03bcs \r\n               minimum       1            2            4          H      78.0\u00b10.2\u03bcs \r\n               minimum       1            2            4          i      92.7\u00b10.3\u03bcs \r\n               minimum       1            2            4          I      94.0\u00b10.8\u03bcs \r\n               minimum       1            2            4          l       246\u00b110\u03bcs  \r\n               minimum       1            2            4          L       237\u00b19\u03bcs   \r\n               minimum       1            2            4          q       244\u00b15\u03bcs   \r\n               minimum       1            2            4          Q       242\u00b110\u03bcs  \r\n               minimum       1            4            1          b       73.7\u00b13\u03bcs  \r\n               minimum       1            4            1          B      78.4\u00b10.3\u03bcs \r\n               minimum       1            4            1          h      72.0\u00b10.4\u03bcs \r\n               minimum       1            4            1          H      78.1\u00b10.4\u03bcs \r\n               minimum       1            4            1          i       90.8\u00b11\u03bcs  \r\n               minimum       1            4            1          I       93.0\u00b14\u03bcs  \r\n               minimum       1            4            1          l       153\u00b13\u03bcs   \r\n               minimum       1            4            1          L       151\u00b13\u03bcs   \r\n               minimum       1            4            1          q       156\u00b13\u03bcs   \r\n               minimum       1            4            1          Q       154\u00b12\u03bcs   \r\n               minimum       1            4            2          b      71.7\u00b10.4\u03bcs \r\n               minimum       1            4            2          B      79.0\u00b10.4\u03bcs \r\n               minimum       1            4            2          h       81.1\u00b11\u03bcs  \r\n               minimum       1            4            2          H      79.0\u00b10.6\u03bcs \r\n               minimum       1            4            2          i      91.0\u00b10.5\u03bcs \r\n               minimum       1            4            2          I      99.2\u00b10.8\u03bcs \r\n               minimum       1            4            2          l       206\u00b12\u03bcs   \r\n               minimum       1            4            2          L       208\u00b18\u03bcs   \r\n               minimum       1            4            2          q       210\u00b17\u03bcs   \r\n               minimum       1            4            2          Q       212\u00b12\u03bcs   \r\n               minimum       1            4            4          b      75.6\u00b10.5\u03bcs \r\n               minimum       1            4            4          B       83.0\u00b14\u03bcs  \r\n               minimum       1            4            4          h      73.4\u00b10.4\u03bcs \r\n               minimum       1            4            4          H      79.2\u00b10.4\u03bcs \r\n               minimum       1            4            4          i      113\u00b10.3\u03bcs  \r\n               minimum       1            4            4          I       118\u00b11\u03bcs   \r\n               minimum       1            4            4          l       337\u00b120\u03bcs  \r\n               minimum       1            4            4          L       382\u00b120\u03bcs  \r\n               minimum       1            4            4          q       366\u00b130\u03bcs  \r\n               minimum       1            4            4          Q       382\u00b120\u03bcs  \r\n               minimum       2            1            1          b      71.1\u00b10.4\u03bcs \r\n               minimum       2            1            1          B      78.9\u00b10.6\u03bcs \r\n               minimum       2            1            1          h      71.6\u00b10.5\u03bcs \r\n               minimum       2            1            1          H      77.9\u00b10.5\u03bcs \r\n               minimum       2            1            1          i      72.5\u00b10.4\u03bcs \r\n               minimum       2            1            1          I      79.1\u00b10.2\u03bcs \r\n               minimum       2            1            1          l       99.5\u00b12\u03bcs  \r\n               minimum       2            1            1          L       102\u00b11\u03bcs   \r\n               minimum       2            1            1          q       98.1\u00b11\u03bcs  \r\n               minimum       2            1            1          Q       104\u00b12\u03bcs   \r\n               minimum       2            1            2          b       73.5\u00b12\u03bcs  \r\n               minimum       2            1            2          B       81.8\u00b13\u03bcs  \r\n               minimum       2            1            2          h       75.2\u00b13\u03bcs  \r\n               minimum       2            1            2          H       82.5\u00b15\u03bcs  \r\n               minimum       2            1            2          i      73.8\u00b10.6\u03bcs \r\n               minimum       2            1            2          I      79.0\u00b10.6\u03bcs \r\n               minimum       2            1            2          l       128\u00b11\u03bcs   \r\n               minimum       2            1            2          L       128\u00b11\u03bcs   \r\n               minimum       2            1            2          q       127\u00b12\u03bcs   \r\n               minimum       2            1            2          Q      131\u00b10.5\u03bcs  \r\n               minimum       2            1            4          b      75.9\u00b10.8\u03bcs \r\n               minimum       2            1            4          B       84.5\u00b11\u03bcs  \r\n               minimum       2            1            4          h       76.9\u00b14\u03bcs  \r\n               minimum       2            1            4          H       88.5\u00b15\u03bcs  \r\n               minimum       2            1            4          i       95.2\u00b11\u03bcs  \r\n               minimum       2            1            4          I      95.5\u00b10.6\u03bcs \r\n               minimum       2            1            4          l       249\u00b110\u03bcs  \r\n               minimum       2            1            4          L       244\u00b15\u03bcs   \r\n               minimum       2            1            4          q       248\u00b110\u03bcs  \r\n               minimum       2            1            4          Q       252\u00b14\u03bcs   \r\n               minimum       2            2            1          b      70.8\u00b10.1\u03bcs \r\n               minimum       2            2            1          B      78.1\u00b10.4\u03bcs \r\n               minimum       2            2            1          h      71.4\u00b10.3\u03bcs \r\n               minimum       2            2            1          H       83.7\u00b15\u03bcs  \r\n               minimum       2            2            1          i      73.4\u00b10.3\u03bcs \r\n               minimum       2            2            1          I      79.0\u00b10.3\u03bcs \r\n               minimum       2            2            1          l       121\u00b13\u03bcs   \r\n               minimum       2            2            1          L      119\u00b10.9\u03bcs  \r\n               minimum       2            2            1          q       120\u00b15\u03bcs   \r\n               minimum       2            2            1          Q       120\u00b12\u03bcs   \r\n               minimum       2            2            2          b       74.2\u00b13\u03bcs  \r\n               minimum       2            2            2          B      79.0\u00b10.2\u03bcs \r\n               minimum       2            2            2          h       75.5\u00b14\u03bcs  \r\n               minimum       2            2            2          H      77.6\u00b10.5\u03bcs \r\n               minimum       2            2            2          i      76.7\u00b10.8\u03bcs \r\n               minimum       2            2            2          I      80.6\u00b10.9\u03bcs \r\n               minimum       2            2            2          l       161\u00b15\u03bcs   \r\n               minimum       2            2            2          L       161\u00b15\u03bcs   \r\n               minimum       2            2            2          q       162\u00b19\u03bcs   \r\n               minimum       2            2            2          Q       162\u00b19\u03bcs   \r\n               minimum       2            2            4          b      75.0\u00b10.3\u03bcs \r\n               minimum       2            2            4          B      79.3\u00b10.6\u03bcs \r\n               minimum       2            2            4          h      72.4\u00b10.5\u03bcs \r\n               minimum       2            2            4          H       80.2\u00b13\u03bcs  \r\n               minimum       2            2            4          i       104\u00b15\u03bcs   \r\n               minimum       2            2            4          I      103\u00b10.7\u03bcs  \r\n               minimum       2            2            4          l       268\u00b16\u03bcs   \r\n               minimum       2            2            4          L       272\u00b110\u03bcs  \r\n               minimum       2            2            4          q       285\u00b110\u03bcs  \r\n               minimum       2            2            4          Q       271\u00b12\u03bcs   \r\n               minimum       2            4            1          b       74.7\u00b14\u03bcs  \r\n               minimum       2            4            1          B      78.7\u00b10.3\u03bcs \r\n               minimum       2            4            1          h      72.3\u00b10.4\u03bcs \r\n               minimum       2            4            1          H      78.3\u00b10.4\u03bcs \r\n               minimum       2            4            1          i      86.6\u00b10.4\u03bcs \r\n               minimum       2            4            1          I       96.7\u00b12\u03bcs  \r\n               minimum       2            4            1          l       194\u00b12\u03bcs   \r\n               minimum       2            4            1          L       197\u00b14\u03bcs   \r\n               minimum       2            4            1          q       199\u00b16\u03bcs   \r\n               minimum       2            4            1          Q       196\u00b13\u03bcs   \r\n               minimum       2            4            2          b       72.5\u00b13\u03bcs  \r\n               minimum       2            4            2          B      79.4\u00b10.5\u03bcs \r\n               minimum       2            4            2          h      72.4\u00b10.5\u03bcs \r\n               minimum       2            4            2          H       92.2\u00b15\u03bcs  \r\n               minimum       2            4            2          i      93.9\u00b10.6\u03bcs \r\n               minimum       2            4            2          I       107\u00b15\u03bcs   \r\n               minimum       2            4            2          l       249\u00b19\u03bcs   \r\n               minimum       2            4            2          L       268\u00b15\u03bcs   \r\n               minimum       2            4            2          q       254\u00b18\u03bcs   \r\n               minimum       2            4            2          Q       265\u00b12\u03bcs   \r\n               minimum       2            4            4          b       77.7\u00b11\u03bcs  \r\n               minimum       2            4            4          B      79.5\u00b10.6\u03bcs \r\n               minimum       2            4            4          h       80.1\u00b16\u03bcs  \r\n               minimum       2            4            4          H      79.7\u00b10.4\u03bcs \r\n               minimum       2            4            4          i       127\u00b12\u03bcs   \r\n               minimum       2            4            4          I       129\u00b14\u03bcs   \r\n               minimum       2            4            4          l       409\u00b130\u03bcs  \r\n               minimum       2            4            4          L       398\u00b14\u03bcs   \r\n               minimum       2            4            4          q       399\u00b120\u03bcs  \r\n               minimum       2            4            4          Q       410\u00b130\u03bcs  \r\n               minimum       4            1            1          b       76.8\u00b14\u03bcs  \r\n               minimum       4            1            1          B      79.1\u00b10.4\u03bcs \r\n               minimum       4            1            1          h       77.5\u00b15\u03bcs  \r\n               minimum       4            1            1          H      78.1\u00b10.3\u03bcs \r\n               minimum       4            1            1          i      85.3\u00b10.3\u03bcs \r\n               minimum       4            1            1          I       92.6\u00b11\u03bcs  \r\n               minimum       4            1            1          l       154\u00b13\u03bcs   \r\n               minimum       4            1            1          L       154\u00b13\u03bcs   \r\n               minimum       4            1            1          q       152\u00b12\u03bcs   \r\n               minimum       4            1            1          Q       153\u00b14\u03bcs   \r\n               minimum       4            1            2          b       75.9\u00b15\u03bcs  \r\n               minimum       4            1            2          B      79.6\u00b10.3\u03bcs \r\n               minimum       4            1            2          h      72.2\u00b10.2\u03bcs \r\n               minimum       4            1            2          H      78.3\u00b10.2\u03bcs \r\n               minimum       4            1            2          i      91.3\u00b10.6\u03bcs \r\n               minimum       4            1            2          I       98.3\u00b11\u03bcs  \r\n               minimum       4            1            2          l       207\u00b15\u03bcs   \r\n               minimum       4            1            2          L       206\u00b15\u03bcs   \r\n               minimum       4            1            2          q       205\u00b12\u03bcs   \r\n               minimum       4            1            2          Q       212\u00b17\u03bcs   \r\n               minimum       4            1            4          b      75.0\u00b10.3\u03bcs \r\n               minimum       4            1            4          B      79.0\u00b10.3\u03bcs \r\n               minimum       4            1            4          h      73.1\u00b10.7\u03bcs \r\n               minimum       4            1            4          H      79.1\u00b10.9\u03bcs \r\n               minimum       4            1            4          i       117\u00b13\u03bcs   \r\n               minimum       4            1            4          I       124\u00b14\u03bcs   \r\n               minimum       4            1            4          l       373\u00b120\u03bcs  \r\n               minimum       4            1            4          L       364\u00b120\u03bcs  \r\n               minimum       4            1            4          q       390\u00b110\u03bcs  \r\n               minimum       4            1            4          Q       358\u00b120\u03bcs  \r\n               minimum       4            2            1          b      71.3\u00b10.3\u03bcs \r\n               minimum       4            2            1          B      78.8\u00b10.3\u03bcs \r\n               minimum       4            2            1          h      83.7\u00b10.2\u03bcs \r\n               minimum       4            2            1          H       85.2\u00b17\u03bcs  \r\n               minimum       4            2            1          i      98.7\u00b10.7\u03bcs \r\n               minimum       4            2            1          I       98.2\u00b14\u03bcs  \r\n               minimum       4            2            1          l       191\u00b12\u03bcs   \r\n               minimum       4            2            1          L       190\u00b17\u03bcs   \r\n               minimum       4            2            1          q       195\u00b15\u03bcs   \r\n               minimum       4            2            1          Q       203\u00b18\u03bcs   \r\n               minimum       4            2            2          b      72.4\u00b10.5\u03bcs \r\n               minimum       4            2            2          B       85.5\u00b15\u03bcs  \r\n               minimum       4            2            2          h      73.4\u00b10.9\u03bcs \r\n               minimum       4            2            2          H       79.8\u00b12\u03bcs  \r\n               minimum       4            2            2          i       97.3\u00b11\u03bcs  \r\n               minimum       4            2            2          I       104\u00b12\u03bcs   \r\n               minimum       4            2            2          l       258\u00b110\u03bcs  \r\n               minimum       4            2            2          L       264\u00b18\u03bcs   \r\n               minimum       4            2            2          q       248\u00b110\u03bcs  \r\n               minimum       4            2            2          Q       265\u00b17\u03bcs   \r\n               minimum       4            2            4          b      76.1\u00b10.6\u03bcs \r\n               minimum       4            2            4          B      79.5\u00b10.6\u03bcs \r\n               minimum       4            2            4          h      73.4\u00b10.2\u03bcs \r\n               minimum       4            2            4          H      79.5\u00b10.6\u03bcs \r\n               minimum       4            2            4          i       130\u00b12\u03bcs   \r\n               minimum       4            2            4          I       129\u00b11\u03bcs   \r\n               minimum       4            2            4          l       412\u00b120\u03bcs  \r\n               minimum       4            2            4          L       434\u00b140\u03bcs  \r\n               minimum       4            2            4          q       413\u00b130\u03bcs  \r\n               minimum       4            2            4          Q       419\u00b130\u03bcs  \r\n               minimum       4            4            1          b      71.2\u00b10.4\u03bcs \r\n               minimum       4            4            1          B      78.7\u00b10.1\u03bcs \r\n               minimum       4            4            1          h      72.7\u00b10.3\u03bcs \r\n               minimum       4            4            1          H      78.4\u00b10.3\u03bcs \r\n               minimum       4            4            1          i      103\u00b10.9\u03bcs  \r\n               minimum       4            4            1          I       111\u00b13\u03bcs   \r\n               minimum       4            4            1          l       299\u00b19\u03bcs   \r\n               minimum       4            4            1          L       302\u00b110\u03bcs  \r\n               minimum       4            4            1          q       296\u00b16\u03bcs   \r\n               minimum       4            4            1          Q       292\u00b110\u03bcs  \r\n               minimum       4            4            2          b      71.4\u00b10.2\u03bcs \r\n               minimum       4            4            2          B      79.0\u00b10.3\u03bcs \r\n               minimum       4            4            2          h      72.9\u00b10.3\u03bcs \r\n               minimum       4            4            2          H      79.3\u00b10.6\u03bcs \r\n               minimum       4            4            2          i       115\u00b12\u03bcs   \r\n               minimum       4            4            2          I       120\u00b12\u03bcs   \r\n               minimum       4            4            2          l       364\u00b13\u03bcs   \r\n               minimum       4            4            2          L       371\u00b120\u03bcs  \r\n               minimum       4            4            2          q       370\u00b120\u03bcs  \r\n               minimum       4            4            2          Q       409\u00b120\u03bcs  \r\n               minimum       4            4            4          b      76.6\u00b10.5\u03bcs \r\n               minimum       4            4            4          B      79.3\u00b10.2\u03bcs \r\n               minimum       4            4            4          h      75.8\u00b10.4\u03bcs \r\n               minimum       4            4            4          H      81.4\u00b10.4\u03bcs \r\n               minimum       4            4            4          i       164\u00b13\u03bcs   \r\n               minimum       4            4            4          I       162\u00b15\u03bcs   \r\n               minimum       4            4            4          l       547\u00b110\u03bcs  \r\n               minimum       4            4            4          L       547\u00b120\u03bcs  \r\n               minimum       4            4            4          q       547\u00b110\u03bcs  \r\n               minimum       4            4            4          Q       540\u00b16\u03bcs   \r\n              ========= ============ ============ ============ ======= =============\r\n\r\n[ 98.53%] \u00b7\u00b7\u00b7 bench_ufunc_strides.LogisticRegression.time_train                                                                                                                            ok\r\n[ 98.53%] \u00b7\u00b7\u00b7 =============== ============\r\n                   dtype                  \r\n              --------------- ------------\r\n               numpy.float32   2.65\u00b10.01s \r\n               numpy.float64    4.41\u00b10s   \r\n              =============== ============\r\n\r\n[ 99.26%] \u00b7\u00b7\u00b7 bench_ufunc_strides.Mandelbrot.time_mandel                                                                                                                           12.7\u00b10.02s\r\n[100.00%] \u00b7\u00b7\u00b7 bench_ufunc_strides.Unary.time_ufunc                                                                                                                                         ok\r\n[100.00%] \u00b7\u00b7\u00b7 ========================= =========== ============= ============= ============= ============= ============= =============\r\n              --                                                                     stride_out / dtype                                \r\n              ------------------------------------- -----------------------------------------------------------------------------------\r\n                        ufunc            stride_in      1 / f         1 / d         2 / f         2 / d         4 / f         4 / d    \r\n              ========================= =========== ============= ============= ============= ============= ============= =============\r\n                  <ufunc 'absolute'>         1        25.9\u00b10.4\u03bcs    49.7\u00b10.4\u03bcs    44.8\u00b10.3\u03bcs     81.7\u00b12\u03bcs      74.9\u00b11\u03bcs      163\u00b14\u03bcs   \r\n                  <ufunc 'absolute'>         2         37.9\u00b11\u03bcs     68.0\u00b10.6\u03bcs    58.3\u00b10.8\u03bcs     100\u00b12\u03bcs      83.1\u00b10.7\u03bcs     204\u00b15\u03bcs   \r\n                  <ufunc 'absolute'>         4        54.1\u00b10.3\u03bcs     116\u00b13\u03bcs       70.8\u00b11\u03bcs      169\u00b16\u03bcs       102\u00b12\u03bcs       287\u00b110\u03bcs  \r\n                   <ufunc 'arccos'>          1         988\u00b13\u03bcs     1.52\u00b10.01ms     983\u00b15\u03bcs     1.54\u00b10.04ms   1.03\u00b10.02ms   1.66\u00b10.07ms \r\n                   <ufunc 'arccos'>          2         988\u00b18\u03bcs     1.52\u00b10.02ms     992\u00b110\u03bcs    1.53\u00b10.01ms     988\u00b110\u03bcs    1.55\u00b10.07ms \r\n                   <ufunc 'arccos'>          4       1.00\u00b10.01ms   1.57\u00b10.02ms     993\u00b18\u03bcs     1.57\u00b10.02ms     994\u00b120\u03bcs    1.59\u00b10.01ms \r\n                  <ufunc 'arccosh'>          1       2.06\u00b10.01ms   2.35\u00b10.05ms   2.06\u00b10.01ms   2.37\u00b10.03ms   2.08\u00b10.01ms   2.42\u00b10.06ms \r\n                  <ufunc 'arccosh'>          2       2.08\u00b10.01ms   2.35\u00b10.03ms   2.06\u00b10.02ms   2.36\u00b10.05ms   2.08\u00b10.01ms   2.38\u00b10.04ms \r\n                  <ufunc 'arccosh'>          4       2.07\u00b10.02ms   2.36\u00b10.02ms   2.07\u00b10.02ms   2.42\u00b10.05ms   2.07\u00b10.01ms    2.36\u00b10.1ms \r\n                   <ufunc 'arcsin'>          1         836\u00b14\u03bcs     1.48\u00b10.01ms     836\u00b15\u03bcs     1.50\u00b10.03ms     866\u00b110\u03bcs    1.62\u00b10.05ms \r\n                   <ufunc 'arcsin'>          2         841\u00b19\u03bcs     1.50\u00b10.01ms     842\u00b110\u03bcs    1.50\u00b10.01ms     840\u00b18\u03bcs     1.52\u00b10.08ms \r\n                   <ufunc 'arcsin'>          4         840\u00b12\u03bcs     1.52\u00b10.01ms     847\u00b17\u03bcs     1.59\u00b10.05ms     870\u00b120\u03bcs    1.66\u00b10.06ms \r\n                  <ufunc 'arcsinh'>          1       2.31\u00b10.01ms   2.80\u00b10.02ms   2.32\u00b10.03ms   2.81\u00b10.06ms   2.37\u00b10.04ms    3.02\u00b10.1ms \r\n                  <ufunc 'arcsinh'>          2       2.30\u00b10.01ms   2.82\u00b10.02ms   2.31\u00b10.01ms   2.82\u00b10.02ms   2.31\u00b10.01ms    2.83\u00b10.1ms \r\n                  <ufunc 'arcsinh'>          4       2.29\u00b10.01ms   2.83\u00b10.03ms   2.33\u00b10.03ms   2.84\u00b10.02ms   2.32\u00b10.05ms   2.84\u00b10.02ms \r\n                   <ufunc 'arctan'>          1         1.09\u00b10ms    1.98\u00b10.01ms   1.09\u00b10.01ms   1.98\u00b10.04ms   1.13\u00b10.02ms   2.13\u00b10.06ms \r\n                   <ufunc 'arctan'>          2         1.09\u00b10ms    1.98\u00b10.01ms   1.08\u00b10.01ms   1.99\u00b10.01ms   1.09\u00b10.01ms   2.06\u00b10.06ms \r\n                   <ufunc 'arctan'>          4       1.11\u00b10.04ms   2.00\u00b10.01ms   1.12\u00b10.02ms   2.02\u00b10.03ms   1.13\u00b10.02ms   2.05\u00b10.02ms \r\n                  <ufunc 'arctanh'>          1       2.30\u00b10.01ms   2.51\u00b10.02ms   2.30\u00b10.01ms   2.55\u00b10.05ms   2.38\u00b10.05ms    2.76\u00b10.1ms \r\n                  <ufunc 'arctanh'>          2       2.30\u00b10.01ms   2.56\u00b10.02ms   2.31\u00b10.02ms   2.53\u00b10.06ms   2.30\u00b10.01ms    2.55\u00b10.1ms \r\n                  <ufunc 'arctanh'>          4       2.30\u00b10.01ms   2.53\u00b10.04ms   2.30\u00b10.02ms   2.64\u00b10.06ms   2.32\u00b10.04ms   2.58\u00b10.02ms \r\n                    <ufunc 'cbrt'>           1       1.98\u00b10.01ms   2.21\u00b10.01ms     1.97\u00b10ms    2.22\u00b10.04ms   2.04\u00b10.05ms    2.39\u00b10.1ms \r\n                    <ufunc 'cbrt'>           2       1.97\u00b10.01ms   2.22\u00b10.02ms   1.99\u00b10.01ms   2.25\u00b10.04ms   1.97\u00b10.01ms   2.26\u00b10.09ms \r\n                    <ufunc 'cbrt'>           4       1.98\u00b10.01ms   2.22\u00b10.02ms   2.00\u00b10.02ms   2.28\u00b10.04ms   2.02\u00b10.03ms   2.25\u00b10.02ms \r\n                    <ufunc 'ceil'>           1        25.8\u00b10.2\u03bcs    49.5\u00b10.1\u03bcs    45.2\u00b10.3\u03bcs     81.8\u00b11\u03bcs      74.6\u00b11\u03bcs      161\u00b14\u03bcs   \r\n                    <ufunc 'ceil'>           2        37.0\u00b10.6\u03bcs    69.4\u00b10.8\u03bcs     57.8\u00b11\u03bcs      101\u00b13\u03bcs       84.9\u00b13\u03bcs      204\u00b16\u03bcs   \r\n                    <ufunc 'ceil'>           4        55.3\u00b10.9\u03bcs     118\u00b13\u03bcs      71.3\u00b10.8\u03bcs     168\u00b13\u03bcs       104\u00b12\u03bcs       293\u00b110\u03bcs  \r\n               <ufunc 'conjugate'> (0)       1        45.4\u00b10.4\u03bcs    54.5\u00b10.4\u03bcs    48.4\u00b10.3\u03bcs     83.6\u00b11\u03bcs     75.6\u00b10.8\u03bcs     166\u00b12\u03bcs   \r\n               <ufunc 'conjugate'> (0)       2        48.6\u00b10.2\u03bcs    69.6\u00b10.9\u03bcs    54.5\u00b10.8\u03bcs     103\u00b12\u03bcs       82.4\u00b11\u03bcs      197\u00b17\u03bcs   \r\n               <ufunc 'conjugate'> (0)       4        56.8\u00b10.3\u03bcs     118\u00b11\u03bcs      69.5\u00b10.6\u03bcs     165\u00b14\u03bcs       104\u00b11\u03bcs       317\u00b120\u03bcs  \r\n                    <ufunc 'cos'>            1        147\u00b10.6\u03bcs      841\u00b14\u03bcs       211\u00b15\u03bcs       842\u00b120\u03bcs      209\u00b15\u03bcs       915\u00b140\u03bcs  \r\n                    <ufunc 'cos'>            2         218\u00b12\u03bcs       840\u00b12\u03bcs       274\u00b12\u03bcs       841\u00b110\u03bcs      274\u00b12\u03bcs       864\u00b140\u03bcs  \r\n                    <ufunc 'cos'>            4         225\u00b12\u03bcs       858\u00b14\u03bcs       286\u00b12\u03bcs       868\u00b19\u03bcs       288\u00b15\u03bcs       874\u00b16\u03bcs   \r\n                    <ufunc 'cosh'>           1         1.31\u00b10ms    1.35\u00b10.01ms   1.31\u00b10.01ms   1.37\u00b10.02ms   1.37\u00b10.03ms   1.46\u00b10.06ms \r\n                    <ufunc 'cosh'>           2       1.31\u00b10.02ms   1.35\u00b10.01ms   1.33\u00b10.01ms   1.35\u00b10.02ms   1.33\u00b10.02ms   1.38\u00b10.05ms \r\n                    <ufunc 'cosh'>           4       1.31\u00b10.01ms   1.36\u00b10.01ms   1.31\u00b10.01ms   1.36\u00b10.01ms   1.32\u00b10.03ms     1.36\u00b10ms  \r\n                  <ufunc 'deg2rad'>          1         176\u00b11\u03bcs       176\u00b12\u03bcs      176\u00b10.3\u03bcs      202\u00b13\u03bcs       204\u00b13\u03bcs       340\u00b18\u03bcs   \r\n                  <ufunc 'deg2rad'>          2         175\u00b11\u03bcs       179\u00b12\u03bcs       178\u00b12\u03bcs       203\u00b12\u03bcs       201\u00b12\u03bcs       345\u00b18\u03bcs   \r\n                  <ufunc 'deg2rad'>          4         178\u00b12\u03bcs       190\u00b12\u03bcs       177\u00b11\u03bcs       248\u00b13\u03bcs       207\u00b14\u03bcs       408\u00b120\u03bcs  \r\n                  <ufunc 'degrees'>          1         176\u00b12\u03bcs       176\u00b11\u03bcs       175\u00b11\u03bcs       200\u00b14\u03bcs       204\u00b13\u03bcs       341\u00b110\u03bcs  \r\n                  <ufunc 'degrees'>          2        176\u00b10.8\u03bcs      177\u00b12\u03bcs       177\u00b11\u03bcs       203\u00b12\u03bcs       200\u00b11\u03bcs       344\u00b18\u03bcs   \r\n                  <ufunc 'degrees'>          4        177\u00b10.6\u03bcs      188\u00b16\u03bcs      178\u00b10.6\u03bcs      235\u00b13\u03bcs       207\u00b15\u03bcs       410\u00b18\u03bcs   \r\n                    <ufunc 'exp'>            1         154\u00b13\u03bcs       608\u00b16\u03bcs       4.49\u00b10ms      610\u00b110\u03bcs    4.67\u00b10.09ms     658\u00b120\u03bcs  \r\n                    <ufunc 'exp'>            2         214\u00b12\u03bcs       610\u00b15\u03bcs     5.05\u00b10.08ms     617\u00b18\u03bcs     5.06\u00b10.01ms     632\u00b120\u03bcs  \r\n                    <ufunc 'exp'>            4         223\u00b12\u03bcs       638\u00b120\u03bcs    5.07\u00b10.01ms     628\u00b19\u03bcs      5.08\u00b10.1ms     645\u00b120\u03bcs  \r\n                    <ufunc 'exp2'>           1         330\u00b11\u03bcs       450\u00b12\u03bcs       329\u00b12\u03bcs       455\u00b19\u03bcs       341\u00b17\u03bcs       491\u00b120\u03bcs  \r\n                    <ufunc 'exp2'>           2         335\u00b12\u03bcs       457\u00b13\u03bcs       333\u00b14\u03bcs       460\u00b17\u03bcs       337\u00b13\u03bcs       476\u00b120\u03bcs  \r\n                    <ufunc 'exp2'>           4         342\u00b12\u03bcs       482\u00b13\u03bcs       345\u00b14\u03bcs       515\u00b110\u03bcs      352\u00b16\u03bcs       535\u00b120\u03bcs  \r\n                   <ufunc 'expm1'>           1       1.09\u00b10.01ms   1.06\u00b10.01ms     1.09\u00b10ms    1.07\u00b10.02ms   1.13\u00b10.02ms   1.16\u00b10.05ms \r\n                   <ufunc 'expm1'>           2       1.09\u00b10.01ms   1.06\u00b10.01ms   1.10\u00b10.01ms   1.07\u00b10.01ms     1.10\u00b10ms    1.11\u00b10.04ms \r\n                   <ufunc 'expm1'>           4       1.09\u00b10.01ms   1.07\u00b10.01ms     1.10\u00b10ms    1.10\u00b10.02ms   1.11\u00b10.02ms   1.10\u00b10.02ms \r\n                    <ufunc 'fabs'>           1         176\u00b11\u03bcs       176\u00b11\u03bcs       177\u00b11\u03bcs       203\u00b12\u03bcs       204\u00b13\u03bcs       341\u00b110\u03bcs  \r\n                    <ufunc 'fabs'>           2        176\u00b10.8\u03bcs      178\u00b11\u03bcs       177\u00b11\u03bcs       202\u00b12\u03bcs       201\u00b12\u03bcs       341\u00b110\u03bcs  \r\n                    <ufunc 'fabs'>           4        176\u00b10.9\u03bcs      189\u00b12\u03bcs       177\u00b12\u03bcs       245\u00b18\u03bcs       212\u00b14\u03bcs       400\u00b110\u03bcs  \r\n                   <ufunc 'floor'>           1        25.8\u00b10.3\u03bcs    50.4\u00b10.6\u03bcs    45.0\u00b10.3\u03bcs     83.1\u00b11\u03bcs      74.9\u00b11\u03bcs      166\u00b16\u03bcs   \r\n                   <ufunc 'floor'>           2        36.9\u00b10.3\u03bcs     71.1\u00b12\u03bcs      58.1\u00b11\u03bcs      107\u00b12\u03bcs      83.6\u00b10.7\u03bcs     198\u00b13\u03bcs   \r\n                   <ufunc 'floor'>           4        54.3\u00b10.3\u03bcs     115\u00b12\u03bcs      72.9\u00b10.8\u03bcs     164\u00b11\u03bcs       102\u00b12\u03bcs       290\u00b120\u03bcs  \r\n                    <ufunc 'log'>            1         211\u00b17\u03bcs       592\u00b15\u03bcs     4.52\u00b10.01ms     594\u00b110\u03bcs     4.71\u00b10.1ms     642\u00b130\u03bcs  \r\n                    <ufunc 'log'>            2         274\u00b12\u03bcs       597\u00b18\u03bcs     5.09\u00b10.07ms     601\u00b120\u03bcs    5.09\u00b10.01ms     608\u00b130\u03bcs  \r\n                    <ufunc 'log'>            4         289\u00b13\u03bcs       607\u00b14\u03bcs     5.11\u00b10.02ms     639\u00b130\u03bcs     5.13\u00b10.1ms     676\u00b140\u03bcs  \r\n                   <ufunc 'log10'>           1         788\u00b12\u03bcs     1.03\u00b10.01ms     793\u00b14\u03bcs     1.03\u00b10.02ms     821\u00b18\u03bcs     1.12\u00b10.05ms \r\n                   <ufunc 'log10'>           2         792\u00b17\u03bcs     1.02\u00b10.01ms     791\u00b15\u03bcs     1.03\u00b10.01ms     788\u00b17\u03bcs     1.04\u00b10.05ms \r\n                   <ufunc 'log10'>           4         788\u00b14\u03bcs     1.06\u00b10.02ms     791\u00b16\u03bcs     1.06\u00b10.03ms     795\u00b120\u03bcs    1.05\u00b10.01ms \r\n                   <ufunc 'log1p'>           1       1.16\u00b10.01ms   1.17\u00b10.01ms   1.15\u00b10.01ms   1.19\u00b10.03ms   1.20\u00b10.03ms   1.27\u00b10.05ms \r\n                   <ufunc 'log1p'>           2       1.15\u00b10.01ms   1.17\u00b10.01ms   1.19\u00b10.09ms     1.17\u00b10ms    1.16\u00b10.01ms   1.19\u00b10.05ms \r\n                   <ufunc 'log1p'>           4       1.16\u00b10.01ms   1.18\u00b10.02ms   1.16\u00b10.01ms   1.19\u00b10.01ms   1.18\u00b10.03ms   1.21\u00b10.01ms \r\n                    <ufunc 'log2'>           1         381\u00b12\u03bcs       819\u00b14\u03bcs       383\u00b13\u03bcs       853\u00b110\u03bcs      397\u00b18\u03bcs       886\u00b130\u03bcs  \r\n                    <ufunc 'log2'>           2         379\u00b12\u03bcs       821\u00b16\u03bcs       383\u00b14\u03bcs       887\u00b120\u03bcs      381\u00b13\u03bcs       840\u00b140\u03bcs  \r\n                    <ufunc 'log2'>           4         392\u00b110\u03bcs      895\u00b140\u03bcs      382\u00b13\u03bcs       842\u00b110\u03bcs      396\u00b15\u03bcs       849\u00b110\u03bcs  \r\n                <ufunc 'logical_not'>        1        101\u00b10.3\u03bcs     125\u00b10.6\u03bcs      147\u00b15\u03bcs       159\u00b110\u03bcs      160\u00b15\u03bcs       226\u00b15\u03bcs   \r\n                <ufunc 'logical_not'>        2        102\u00b10.5\u03bcs     133\u00b10.7\u03bcs     144\u00b10.8\u03bcs      168\u00b18\u03bcs       147\u00b11\u03bcs       247\u00b110\u03bcs  \r\n                <ufunc 'logical_not'>        4        110\u00b10.7\u03bcs      172\u00b12\u03bcs       152\u00b11\u03bcs       225\u00b14\u03bcs       160\u00b12\u03bcs       358\u00b19\u03bcs   \r\n                  <ufunc 'negative'>         1        25.6\u00b10.4\u03bcs    49.6\u00b10.4\u03bcs    53.6\u00b10.3\u03bcs     85.3\u00b12\u03bcs      77.0\u00b12\u03bcs      166\u00b13\u03bcs   \r\n                  <ufunc 'negative'>         2        53.9\u00b10.3\u03bcs     72.2\u00b11\u03bcs     57.6\u00b10.9\u03bcs     103\u00b12\u03bcs      83.2\u00b10.6\u03bcs     202\u00b15\u03bcs   \r\n                  <ufunc 'negative'>         4        61.5\u00b10.2\u03bcs     116\u00b13\u03bcs       72.1\u00b11\u03bcs      167\u00b13\u03bcs       104\u00b12\u03bcs       306\u00b18\u03bcs   \r\n                  <ufunc 'positive'>         1        45.7\u00b10.4\u03bcs    54.6\u00b10.5\u03bcs    48.8\u00b10.4\u03bcs     82.8\u00b12\u03bcs      76.8\u00b12\u03bcs      163\u00b13\u03bcs   \r\n                  <ufunc 'positive'>         2        48.9\u00b10.3\u03bcs    69.7\u00b10.5\u03bcs    54.0\u00b10.8\u03bcs     100\u00b11\u03bcs       83.8\u00b11\u03bcs      195\u00b14\u03bcs   \r\n                  <ufunc 'positive'>         4        57.0\u00b10.5\u03bcs     116\u00b12\u03bcs      69.7\u00b10.3\u03bcs     161\u00b15\u03bcs       103\u00b12\u03bcs       288\u00b16\u03bcs   \r\n                  <ufunc 'rad2deg'>          1         178\u00b11\u03bcs       177\u00b11\u03bcs       176\u00b11\u03bcs       202\u00b13\u03bcs       206\u00b13\u03bcs       347\u00b120\u03bcs  \r\n                  <ufunc 'rad2deg'>          2        176\u00b10.7\u03bcs     178\u00b10.9\u03bcs      177\u00b12\u03bcs       203\u00b12\u03bcs       199\u00b11\u03bcs       344\u00b16\u03bcs   \r\n                  <ufunc 'rad2deg'>          4         177\u00b11\u03bcs       187\u00b12\u03bcs       177\u00b12\u03bcs       233\u00b16\u03bcs       211\u00b13\u03bcs       395\u00b120\u03bcs  \r\n                  <ufunc 'radians'>          1        176\u00b10.7\u03bcs     176\u00b10.6\u03bcs      177\u00b11\u03bcs       201\u00b13\u03bcs       204\u00b12\u03bcs       339\u00b17\u03bcs   \r\n                  <ufunc 'radians'>          2        176\u00b10.4\u03bcs      181\u00b13\u03bcs       181\u00b12\u03bcs       208\u00b14\u03bcs       202\u00b12\u03bcs       350\u00b110\u03bcs  \r\n                  <ufunc 'radians'>          4        181\u00b10.7\u03bcs      194\u00b11\u03bcs       180\u00b12\u03bcs       239\u00b17\u03bcs       213\u00b13\u03bcs       401\u00b14\u03bcs   \r\n                 <ufunc 'reciprocal'>        1        54.7\u00b10.5\u03bcs    210\u00b10.9\u03bcs     54.3\u00b10.6\u03bcs     210\u00b14\u03bcs       76.1\u00b11\u03bcs      225\u00b18\u03bcs   \r\n                 <ufunc 'reciprocal'>        2         54.5\u00b11\u03bcs      209\u00b13\u03bcs       63.2\u00b12\u03bcs      210\u00b12\u03bcs      85.6\u00b10.7\u03bcs     227\u00b17\u03bcs   \r\n                 <ufunc 'reciprocal'>        4        55.3\u00b10.7\u03bcs     214\u00b12\u03bcs       73.3\u00b11\u03bcs      219\u00b13\u03bcs       104\u00b12\u03bcs       307\u00b17\u03bcs   \r\n                    <ufunc 'rint'>           1        26.0\u00b10.2\u03bcs    50.4\u00b10.6\u03bcs    45.6\u00b10.3\u03bcs    83.0\u00b10.9\u03bcs     75.2\u00b11\u03bcs      168\u00b15\u03bcs   \r\n                    <ufunc 'rint'>           2        37.2\u00b10.5\u03bcs     70.6\u00b12\u03bcs      58.8\u00b12\u03bcs      105\u00b14\u03bcs      84.3\u00b10.6\u03bcs     199\u00b14\u03bcs   \r\n                    <ufunc 'rint'>           4        55.0\u00b10.5\u03bcs     116\u00b11\u03bcs      73.0\u00b10.6\u03bcs     169\u00b13\u03bcs       103\u00b13\u03bcs       298\u00b110\u03bcs  \r\n                    <ufunc 'sign'>           1        70.2\u00b10.6\u03bcs    70.6\u00b10.8\u03bcs    71.8\u00b10.7\u03bcs     87.8\u00b12\u03bcs      79.6\u00b11\u03bcs      169\u00b14\u03bcs   \r\n                    <ufunc 'sign'>           2        70.5\u00b10.4\u03bcs     79.1\u00b11\u03bcs      71.8\u00b11\u03bcs      104\u00b11\u03bcs       87.8\u00b12\u03bcs      205\u00b17\u03bcs   \r\n                    <ufunc 'sign'>           4        74.5\u00b10.9\u03bcs     124\u00b13\u03bcs       80.3\u00b11\u03bcs      177\u00b19\u03bcs       105\u00b13\u03bcs       312\u00b110\u03bcs  \r\n                    <ufunc 'sin'>            1         146\u00b17\u03bcs       987\u00b110\u03bcs      211\u00b16\u03bcs     1.01\u00b10.02ms     209\u00b15\u03bcs     1.06\u00b10.03ms \r\n                    <ufunc 'sin'>            2         211\u00b12\u03bcs       991\u00b18\u03bcs       269\u00b12\u03bcs       996\u00b110\u03bcs      265\u00b13\u03bcs     1.02\u00b10.04ms \r\n                    <ufunc 'sin'>            4         220\u00b11\u03bcs     1.01\u00b10.02ms     282\u00b12\u03bcs     1.02\u00b10.03ms     287\u00b110\u03bcs    1.05\u00b10.04ms \r\n                    <ufunc 'sinh'>           1       1.98\u00b10.03ms   2.03\u00b10.02ms   2.00\u00b10.04ms   2.06\u00b10.03ms   2.04\u00b10.03ms   2.17\u00b10.08ms \r\n                    <ufunc 'sinh'>           2       1.98\u00b10.01ms   2.02\u00b10.02ms   2.00\u00b10.03ms   2.03\u00b10.03ms   1.97\u00b10.03ms   2.08\u00b10.07ms \r\n                    <ufunc 'sinh'>           4       1.97\u00b10.01ms   2.02\u00b10.02ms   1.97\u00b10.02ms   2.04\u00b10.03ms   1.98\u00b10.04ms   2.04\u00b10.01ms \r\n                    <ufunc 'sqrt'>           1        53.2\u00b10.3\u03bcs     205\u00b11\u03bcs      52.7\u00b10.3\u03bcs     211\u00b13\u03bcs       75.7\u00b11\u03bcs      226\u00b17\u03bcs   \r\n                    <ufunc 'sqrt'>           2        52.5\u00b10.2\u03bcs     207\u00b11\u03bcs       62.5\u00b12\u03bcs      209\u00b12\u03bcs       84.4\u00b11\u03bcs      223\u00b16\u03bcs   \r\n                    <ufunc 'sqrt'>           4        54.7\u00b10.4\u03bcs     211\u00b11\u03bcs       72.2\u00b11\u03bcs      216\u00b12\u03bcs       104\u00b12\u03bcs       284\u00b15\u03bcs   \r\n                   <ufunc 'square'>          1         25.9\u00b19\u03bcs     49.2\u00b10.4\u03bcs     45.3\u00b14\u03bcs      82.0\u00b11\u03bcs      74.6\u00b11\u03bcs      165\u00b14\u03bcs   \r\n                   <ufunc 'square'>          2        36.9\u00b10.3\u03bcs    68.5\u00b10.6\u03bcs     56.5\u00b11\u03bcs      100\u00b11\u03bcs       83.3\u00b11\u03bcs      199\u00b17\u03bcs   \r\n                   <ufunc 'square'>          4        53.9\u00b10.1\u03bcs     111\u00b11\u03bcs      70.5\u00b10.8\u03bcs     165\u00b16\u03bcs       104\u00b11\u03bcs       289\u00b110\u03bcs  \r\n                    <ufunc 'tan'>            1       1.44\u00b10.01ms   2.30\u00b10.01ms   1.45\u00b10.01ms   2.30\u00b10.06ms   1.51\u00b10.04ms   2.46\u00b10.09ms \r\n                    <ufunc 'tan'>            2       1.45\u00b10.01ms   2.31\u00b10.01ms   1.45\u00b10.02ms   2.40\u00b10.06ms   1.46\u00b10.01ms   2.35\u00b10.09ms \r\n                    <ufunc 'tan'>            4       1.47\u00b10.01ms   2.32\u00b10.01ms   1.47\u00b10.01ms   2.33\u00b10.01ms   1.51\u00b10.03ms   2.35\u00b10.02ms \r\n                    <ufunc 'tanh'>           1         457\u00b18\u03bcs     1.64\u00b10.02ms     493\u00b110\u03bcs    1.68\u00b10.03ms     508\u00b110\u03bcs    1.82\u00b10.07ms \r\n                    <ufunc 'tanh'>           2         519\u00b17\u03bcs     1.72\u00b10.01ms     557\u00b16\u03bcs     1.73\u00b10.01ms     556\u00b14\u03bcs     1.78\u00b10.08ms \r\n                    <ufunc 'tanh'>           4         530\u00b16\u03bcs     1.75\u00b10.01ms     558\u00b15\u03bcs     1.77\u00b10.01ms     559\u00b110\u03bcs    1.81\u00b10.01ms \r\n                   <ufunc 'trunc'>           1        25.7\u00b10.2\u03bcs     50.6\u00b11\u03bcs     45.1\u00b10.2\u03bcs     83.3\u00b12\u03bcs      75.4\u00b11\u03bcs      161\u00b13\u03bcs   \r\n                   <ufunc 'trunc'>           2        36.8\u00b10.3\u03bcs     69.7\u00b12\u03bcs      57.4\u00b11\u03bcs      100\u00b11\u03bcs      84.5\u00b10.3\u03bcs     195\u00b16\u03bcs   \r\n                   <ufunc 'trunc'>           4        53.9\u00b10.1\u03bcs     113\u00b14\u03bcs       72.4\u00b11\u03bcs      162\u00b11\u03bcs       103\u00b12\u03bcs       290\u00b14\u03bcs   \r\n               <ufunc 'conjugate'> (1)       1        47.2\u00b10.9\u03bcs    54.3\u00b10.6\u03bcs     50.1\u00b12\u03bcs      82.5\u00b12\u03bcs      75.6\u00b11\u03bcs      162\u00b13\u03bcs   \r\n               <ufunc 'conjugate'> (1)       2        49.0\u00b10.2\u03bcs     70.3\u00b11\u03bcs      54.6\u00b11\u03bcs      102\u00b11\u03bcs       84.1\u00b11\u03bcs      198\u00b17\u03bcs   \r\n               <ufunc 'conjugate'> (1)       4        57.7\u00b10.7\u03bcs     114\u00b17\u03bcs       70.1\u00b11\u03bcs      165\u00b13\u03bcs       102\u00b13\u03bcs       296\u00b120\u03bcs  \r\n                 <ufunc '_ones_like'>        1        35.3\u00b10.2\u03bcs   37.8\u00b10.09\u03bcs    38.1\u00b10.2\u03bcs    64.8\u00b10.9\u03bcs     66.0\u00b11\u03bcs      135\u00b15\u03bcs   \r\n                 <ufunc '_ones_like'>        2        35.5\u00b10.9\u03bcs    38.2\u00b10.2\u03bcs    39.2\u00b10.6\u03bcs    64.2\u00b10.6\u03bcs    64.0\u00b10.4\u03bcs     135\u00b14\u03bcs   \r\n                 <ufunc '_ones_like'>        4        35.9\u00b10.4\u03bcs    38.2\u00b10.2\u03bcs    39.0\u00b10.3\u03bcs    64.7\u00b10.4\u03bcs    64.9\u00b10.9\u03bcs     136\u00b13\u03bcs   \r\n              ========================= =========== ============= ============= ============= ============= ============= =============\r\n\r\n       before           after         ratio\r\n     [fd646bd6]       [ee8c683a]\r\n     <main>           <performance_cache_unicode_array_ufunc>\r\n+      79.0\u00b10.3\u03bcs         98.1\u00b12\u03bcs     1.24  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 1, 'I')\r\n+      72.2\u00b10.3\u03bcs       83.0\u00b10.7\u03bcs     1.15  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 2, 1, 'i')\r\n+        95.1\u00b11\u03bcs          107\u00b11\u03bcs     1.12  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 2, 'I')\r\n+         271\u00b12\u03bcs          303\u00b13\u03bcs     1.12  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 2, 4, 'Q')\r\n+      71.0\u00b10.3\u03bcs         79.1\u00b12\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 4, 1, 'B')\r\n+         126\u00b13\u03bcs          140\u00b13\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'i')\r\n+         120\u00b12\u03bcs          133\u00b19\u03bcs     1.11  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 4, 2, 'I')\r\n+      72.6\u00b10.3\u03bcs         79.9\u00b14\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'h')\r\n+      73.2\u00b10.9\u03bcs         80.4\u00b14\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'h')\r\n+      73.3\u00b10.3\u03bcs         80.4\u00b16\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 1, 2, 2, 'I')\r\n+      72.1\u00b10.2\u03bcs         78.9\u00b15\u03bcs     1.10  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'H')\r\n+      71.5\u00b10.3\u03bcs         77.7\u00b12\u03bcs     1.09  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 1, 'h')\r\n+         128\u00b11\u03bcs          138\u00b13\u03bcs     1.09  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 2, 'L')\r\n+      79.0\u00b10.4\u03bcs       85.6\u00b10.3\u03bcs     1.08  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'B')\r\n+         405\u00b15\u03bcs         439\u00b110\u03bcs     1.08  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'q')\r\n+         192\u00b14\u03bcs          206\u00b13\u03bcs     1.07  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 1, 'q')\r\n+      71.3\u00b10.3\u03bcs         76.1\u00b14\u03bcs     1.07  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'b')\r\n+      74.4\u00b10.3\u03bcs         78.8\u00b15\u03bcs     1.06  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 4, 'H')\r\n+       146\u00b10.5\u03bcs          155\u00b13\u03bcs     1.06  bench_ufunc_strides.Binary.time_ufunc('fmax', 2, 4, 2, 'f')\r\n-     1.43\u00b10.06\u03bcs      1.36\u00b10.02\u03bcs     0.95  bench_ufunc.ArgParsingReduce.time_add_reduce_arg_parsing((array([0., 1.]), 0, None))\r\n-        74.4\u00b14\u03bcs       70.4\u00b10.3\u03bcs     0.95  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 1, 1, 'b')\r\n-         107\u00b12\u03bcs        101\u00b10.9\u03bcs     0.94  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'floor'>, 2, 2, 'd')\r\n-        887\u00b120\u03bcs          819\u00b16\u03bcs     0.92  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'log2'>, 2, 2, 'd')\r\n-        90.8\u00b11\u03bcs       83.4\u00b10.2\u03bcs     0.92  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 1, 'i')\r\n-        80.4\u00b17\u03bcs       72.8\u00b10.2\u03bcs     0.91  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 1, 2, 'i')\r\n-        607\u00b140\u03bcs          549\u00b19\u03bcs     0.90  bench_ufunc_strides.Binary.time_ufunc('fmin', 4, 4, 4, 'd')\r\n-      98.7\u00b10.7\u03bcs       89.0\u00b10.4\u03bcs     0.90  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 4, 2, 1, 'i')\r\n-        81.1\u00b11\u03bcs       72.2\u00b10.3\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 1, 4, 2, 'h')\r\n-        88.5\u00b15\u03bcs       78.6\u00b10.5\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('minimum', 2, 1, 4, 'H')\r\n-      84.6\u00b10.5\u03bcs         75.1\u00b11\u03bcs     0.89  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 4, 2, 2, 'H')\r\n-        467\u00b120\u03bcs          405\u00b15\u03bcs     0.87  bench_ufunc_strides.BinaryInt.time_ufunc('maximum', 2, 4, 4, 'L')\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE DECREASED.\r\n```\r\n</details>\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/common/binop_override.h",
                "patch": "@@ -128,7 +128,7 @@ binop_should_defer(PyObject *self, PyObject *other, int inplace)\n      * Classes with __array_ufunc__ are living in the future, and only need to\n      * check whether __array_ufunc__ equals None.\n      */\n-    attr = PyArray_LookupSpecial(other, \"__array_ufunc__\");\n+    attr = PyArray_LookupSpecial(other, npy_um_str_array_ufunc);\n     if (attr != NULL) {\n         defer = !inplace && (attr == Py_None);\n         Py_DECREF(attr);"
            },
            {
                "filename": "numpy/core/src/common/get_attr_string.h",
                "patch": "@@ -1,6 +1,9 @@\n #ifndef NUMPY_CORE_SRC_COMMON_GET_ATTR_STRING_H_\n #define NUMPY_CORE_SRC_COMMON_GET_ATTR_STRING_H_\n \n+#include <Python.h>\n+#include \"ufunc_object.h\"\n+\n static NPY_INLINE npy_bool\n _is_basic_python_type(PyTypeObject *tp)\n {\n@@ -33,43 +36,6 @@ _is_basic_python_type(PyTypeObject *tp)\n     );\n }\n \n-/*\n- * Stripped down version of PyObject_GetAttrString(obj, name) that does not\n- * raise PyExc_AttributeError.\n- *\n- * This allows it to avoid creating then discarding exception objects when\n- * performing lookups on objects without any attributes.\n- *\n- * Returns attribute value on success, NULL without an exception set if\n- * there is no such attribute, and NULL with an exception on failure.\n- */\n-static NPY_INLINE PyObject *\n-maybe_get_attr(PyObject *obj, char const *name)\n-{\n-    PyTypeObject *tp = Py_TYPE(obj);\n-    PyObject *res = (PyObject *)NULL;\n-\n-    /* Attribute referenced by (char *)name */\n-    if (tp->tp_getattr != NULL) {\n-        res = (*tp->tp_getattr)(obj, (char *)name);\n-        if (res == NULL && PyErr_ExceptionMatches(PyExc_AttributeError)) {\n-            PyErr_Clear();\n-        }\n-    }\n-    /* Attribute referenced by (PyObject *)name */\n-    else if (tp->tp_getattro != NULL) {\n-        PyObject *w = PyUnicode_InternFromString(name);\n-        if (w == NULL) {\n-            return (PyObject *)NULL;\n-        }\n-        res = (*tp->tp_getattro)(obj, w);\n-        Py_DECREF(w);\n-        if (res == NULL && PyErr_ExceptionMatches(PyExc_AttributeError)) {\n-            PyErr_Clear();\n-        }\n-    }\n-    return res;\n-}\n \n /*\n  * Lookup a special method, following the python approach of looking up\n@@ -81,17 +47,24 @@ maybe_get_attr(PyObject *obj, char const *name)\n  * In future, could be made more like _Py_LookupSpecial\n  */\n static NPY_INLINE PyObject *\n-PyArray_LookupSpecial(PyObject *obj, char const *name)\n+PyArray_LookupSpecial(PyObject *obj, PyObject *name_unicode)\n {\n     PyTypeObject *tp = Py_TYPE(obj);\n \n     /* We do not need to check for special attributes on trivial types */\n     if (_is_basic_python_type(tp)) {\n         return NULL;\n     }\n-    return maybe_get_attr((PyObject *)tp, name);\n+    PyObject *res = PyObject_GetAttr((PyObject *)tp, name_unicode);\n+\n+    if (res == NULL && PyErr_ExceptionMatches(PyExc_AttributeError)) {\n+        PyErr_Clear();\n+    }\n+\n+    return res;\n }\n \n+\n /*\n  * PyArray_LookupSpecial_OnInstance:\n  *\n@@ -101,7 +74,7 @@ PyArray_LookupSpecial(PyObject *obj, char const *name)\n  * Kept for backwards compatibility. In future, we should deprecate this.\n  */\n static NPY_INLINE PyObject *\n-PyArray_LookupSpecial_OnInstance(PyObject *obj, char const *name)\n+PyArray_LookupSpecial_OnInstance(PyObject *obj, PyObject *name_unicode)\n {\n     PyTypeObject *tp = Py_TYPE(obj);\n \n@@ -110,7 +83,13 @@ PyArray_LookupSpecial_OnInstance(PyObject *obj, char const *name)\n         return NULL;\n     }\n \n-    return maybe_get_attr(obj, name);\n+    PyObject *res = PyObject_GetAttr(obj, name_unicode);\n+\n+    if (res == NULL && PyErr_ExceptionMatches(PyExc_AttributeError)) {\n+        PyErr_Clear();\n+    }\n+\n+    return res;\n }\n \n #endif  /* NUMPY_CORE_SRC_COMMON_GET_ATTR_STRING_H_ */"
            },
            {
                "filename": "numpy/core/src/common/ufunc_override.c",
                "patch": "@@ -34,7 +34,7 @@ PyUFuncOverride_GetNonDefaultArrayUfunc(PyObject *obj)\n      * Does the class define __array_ufunc__? (Note that LookupSpecial has fast\n      * return for basic python types, so no need to worry about those here)\n      */\n-    cls_array_ufunc = PyArray_LookupSpecial(obj, \"__array_ufunc__\");\n+    cls_array_ufunc = PyArray_LookupSpecial(obj, npy_um_str_array_ufunc);\n     if (cls_array_ufunc == NULL) {\n         if (PyErr_Occurred()) {\n             PyErr_Clear(); /* TODO[gh-14801]: propagate crashes during attribute access? */"
            },
            {
                "filename": "numpy/core/src/multiarray/arrayfunction_override.c",
                "patch": "@@ -37,7 +37,7 @@ get_array_function(PyObject *obj)\n         return ndarray_array_function;\n     }\n \n-    PyObject *array_function = PyArray_LookupSpecial(obj, \"__array_function__\");\n+    PyObject *array_function = PyArray_LookupSpecial(obj, npy_ma_str_array_function);\n     if (array_function == NULL && PyErr_Occurred()) {\n         PyErr_Clear(); /* TODO[gh-14801]: propagate crashes during attribute access? */\n     }"
            },
            {
                "filename": "numpy/core/src/multiarray/ctors.c",
                "patch": "@@ -2109,7 +2109,7 @@ PyArray_FromStructInterface(PyObject *input)\n     PyObject *attr;\n     char endian = NPY_NATBYTE;\n \n-    attr = PyArray_LookupSpecial_OnInstance(input, \"__array_struct__\");\n+    attr = PyArray_LookupSpecial_OnInstance(input, npy_ma_str_array_struct);\n     if (attr == NULL) {\n         if (PyErr_Occurred()) {\n             return NULL;\n@@ -2233,7 +2233,7 @@ PyArray_FromInterface(PyObject *origin)\n     npy_intp dims[NPY_MAXDIMS], strides[NPY_MAXDIMS];\n     int dataflags = NPY_ARRAY_BEHAVED;\n \n-    iface = PyArray_LookupSpecial_OnInstance(origin, \"__array_interface__\");\n+    iface = PyArray_LookupSpecial_OnInstance(origin, npy_ma_str_array_interface);\n \n     if (iface == NULL) {\n         if (PyErr_Occurred()) {\n@@ -2514,7 +2514,7 @@ PyArray_FromArrayAttr_int(\n     PyObject *new;\n     PyObject *array_meth;\n \n-    array_meth = PyArray_LookupSpecial_OnInstance(op, \"__array__\");\n+    array_meth = PyArray_LookupSpecial_OnInstance(op, npy_ma_str_array);\n     if (array_meth == NULL) {\n         if (PyErr_Occurred()) {\n             return NULL;"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -128,7 +128,7 @@ PyArray_GetPriority(PyObject *obj, double default_)\n         return NPY_SCALAR_PRIORITY;\n     }\n \n-    ret = PyArray_LookupSpecial_OnInstance(obj, \"__array_priority__\");\n+    ret = PyArray_LookupSpecial_OnInstance(obj, npy_ma_str_array_priority);\n     if (ret == NULL) {\n         if (PyErr_Occurred()) {\n             /* TODO[gh-14801]: propagate crashes during attribute access? */\n@@ -4665,6 +4665,11 @@ set_flaginfo(PyObject *d)\n     return;\n }\n \n+NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array = NULL;\n+NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array_function = NULL;\n+NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array_struct = NULL;\n+NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array_interface = NULL;\n+NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array_priority = NULL;\n NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array_wrap = NULL;\n NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_array_finalize = NULL;\n NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_implementation = NULL;\n@@ -4676,6 +4681,26 @@ NPY_VISIBILITY_HIDDEN PyObject * npy_ma_str_numpy = NULL;\n static int\n intern_strings(void)\n {\n+    npy_ma_str_array = PyUnicode_InternFromString(\"__array__\");\n+    if (npy_ma_str_array == NULL) {\n+        return -1;\n+    }\n+    npy_ma_str_array_function = PyUnicode_InternFromString(\"__array_function__\");\n+    if (npy_ma_str_array_function == NULL) {\n+        return -1;\n+    }\n+    npy_ma_str_array_struct = PyUnicode_InternFromString(\"__array_struct__\");\n+    if (npy_ma_str_array_struct == NULL) {\n+        return -1;\n+    }\n+    npy_ma_str_array_priority = PyUnicode_InternFromString(\"__array_priority__\");\n+    if (npy_ma_str_array_priority == NULL) {\n+        return -1;\n+    }\n+    npy_ma_str_array_interface = PyUnicode_InternFromString(\"__array_interface__\");\n+    if (npy_ma_str_array_interface == NULL) {\n+        return -1;\n+    }\n     npy_ma_str_array_wrap = PyUnicode_InternFromString(\"__array_wrap__\");\n     if (npy_ma_str_array_wrap == NULL) {\n         return -1;"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.h",
                "patch": "@@ -1,6 +1,11 @@\n #ifndef NUMPY_CORE_SRC_MULTIARRAY_MULTIARRAYMODULE_H_\n #define NUMPY_CORE_SRC_MULTIARRAY_MULTIARRAYMODULE_H_\n \n+NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array;\n+NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array_function;\n+NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array_struct;\n+NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array_priority;\n+NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array_interface;\n NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array_wrap;\n NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_array_finalize;\n NPY_VISIBILITY_HIDDEN extern PyObject * npy_ma_str_implementation;"
            },
            {
                "filename": "numpy/core/src/umath/ufunc_object.h",
                "patch": "@@ -13,6 +13,7 @@ NPY_NO_EXPORT const char*\n ufunc_get_name_cstr(PyUFuncObject *ufunc);\n \n /* strings from umathmodule.c that are interned on umath import */\n+NPY_VISIBILITY_HIDDEN extern PyObject *npy_um_str_array_ufunc;\n NPY_VISIBILITY_HIDDEN extern PyObject *npy_um_str_array_prepare;\n NPY_VISIBILITY_HIDDEN extern PyObject *npy_um_str_array_wrap;\n NPY_VISIBILITY_HIDDEN extern PyObject *npy_um_str_pyvals_name;"
            },
            {
                "filename": "numpy/core/src/umath/umathmodule.c",
                "patch": "@@ -213,6 +213,7 @@ add_newdoc_ufunc(PyObject *NPY_UNUSED(dummy), PyObject *args)\n  *****************************************************************************\n  */\n \n+NPY_VISIBILITY_HIDDEN PyObject *npy_um_str_array_ufunc = NULL;\n NPY_VISIBILITY_HIDDEN PyObject *npy_um_str_array_prepare = NULL;\n NPY_VISIBILITY_HIDDEN PyObject *npy_um_str_array_wrap = NULL;\n NPY_VISIBILITY_HIDDEN PyObject *npy_um_str_pyvals_name = NULL;\n@@ -221,6 +222,10 @@ NPY_VISIBILITY_HIDDEN PyObject *npy_um_str_pyvals_name = NULL;\n static int\n intern_strings(void)\n {\n+    npy_um_str_array_ufunc = PyUnicode_InternFromString(\"__array_ufunc__\");\n+    if (npy_um_str_array_ufunc == NULL) {\n+        return -1;\n+    }\n     npy_um_str_array_prepare = PyUnicode_InternFromString(\"__array_prepare__\");\n     if (npy_um_str_array_prepare == NULL) {\n         return -1;"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21360,
        "body": "Add flag to disable the voltab section in the generated library, which\r\n(if present) appears to break mingw-w64 linking.\r\n\r\nSee:\r\nhttps://github.com/ocaml/ocaml/commit/0ac73587579bb6648dac6aee2b58fb873bd652a6\r\n\r\nand\r\n\r\nhttps://github.com/matthew-brett/dll_investigation/issues/1#issuecomment-1100468171\r\n\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->",
        "changed_files": [
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -774,24 +774,29 @@ def get_mathlib_info(*args):\n                        join('src', 'npymath', 'halffloat.c')\n                        ]\n \n-    def gl_if_msvc(build_cmd):\n-        \"\"\" Add flag if we are using MSVC compiler\n+    def opts_if_msvc(build_cmd):\n+        \"\"\" Add flags if we are using MSVC compiler\n \n-        We can't see this in our scope, because we have not initialized the\n-        distutils build command, so use this deferred calculation to run when\n-        we are building the library.\n+        We can't see `build_cmd` in our scope, because we have not initialized\n+        the distutils build command, so use this deferred calculation to run\n+        when we are building the library.\n         \"\"\"\n-        if build_cmd.compiler.compiler_type == 'msvc':\n-            # explicitly disable whole-program optimization\n-            return ['/GL-']\n-        return []\n+        if build_cmd.compiler.compiler_type != 'msvc':\n+            return []\n+        # Explicitly disable whole-program optimization.\n+        flags = ['/GL-']\n+        # Disable voltbl section for vc142 to allow link using mingw-w64; see:\n+        # https://github.com/matthew-brett/dll_investigation/issues/1#issuecomment-1100468171\n+        if build_cmd.compiler_opt.cc_test_flags(['-d2VolatileMetadata-']):\n+            flags.append('-d2VolatileMetadata-')\n+        return flags\n \n     config.add_installed_library('npymath',\n             sources=npymath_sources + [get_mathlib_info],\n             install_dir='lib',\n             build_info={\n                 'include_dirs' : [],  # empty list required for creating npy_math_internal.h\n-                'extra_compiler_args': [gl_if_msvc],\n+                'extra_compiler_args': [opts_if_msvc],\n             })\n     config.add_npy_pkg_config(\"npymath.ini.in\", \"lib/npy-pkg-config\",\n             subst_dict)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21354,
        "body": "### Improve `np.kron` performance\r\n\r\n* Use broadcasting during multiply to speed up the performance\r\n* Also removed transpose logic to reduce total operations.\r\n\r\n### Boost amount\r\n\r\n<details>\r\n\r\n<summary> Compare with bb811f45 (main) </summary>\r\n\r\n```\r\n~/os/numpy (perf_kron_21257) \u00bb python3 runtests.py --bench-compare main bench_shape_base.Kron                                                                         129 \u21b5 ganesh@ganesh-MS-7B86\r\n\u00b7 Creating environments\r\n\u00b7 Discovering benchmarks\r\n\u00b7\u00b7 Uninstalling from virtualenv-py3.9-Cython\r\n\u00b7\u00b7 Installing 06d34945 <perf_kron_21257> into virtualenv-py3.9-Cython.\r\n\u00b7 Running 6 total benchmarks (2 commits * 1 environments * 3 benchmarks)\r\n[  0.00%] \u00b7 For numpy commit bb811f45 <main> (round 1/2):\r\n[  0.00%] \u00b7\u00b7 Building for virtualenv-py3.9-Cython.\r\n[  0.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[  0.00%] \u00b7\u00b7\u00b7 Importing benchmark suite produced output:\r\n[  0.00%] \u00b7\u00b7\u00b7\u00b7 NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?\r\n[  8.33%] \u00b7\u00b7\u00b7 Running (bench_shape_base.Kron.time_arr_kron--)...\r\n[ 25.00%] \u00b7 For numpy commit 06d34945 <perf_kron_21257> (round 1/2):\r\n[ 25.00%] \u00b7\u00b7 Building for virtualenv-py3.9-Cython.\r\n[ 25.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[ 33.33%] \u00b7\u00b7\u00b7 Running (bench_shape_base.Kron.time_arr_kron--)...\r\n[ 50.00%] \u00b7 For numpy commit 06d34945 <perf_kron_21257> (round 2/2):\r\n[ 50.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[ 58.33%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_arr_kron                                                                                                                                          274\u00b12ms\r\n[ 66.67%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_mat_kron                                                                                                                                          215\u00b12ms\r\n[ 75.00%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_scalar_kron                                                                                                                                   3.96\u00b10.05\u03bcs\r\n[ 75.00%] \u00b7 For numpy commit bb811f45 <main> (round 2/2):\r\n[ 75.00%] \u00b7\u00b7 Building for virtualenv-py3.9-Cython.\r\n[ 75.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[ 83.33%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_arr_kron                                                                                                                                          438\u00b12ms\r\n[ 91.67%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_mat_kron                                                                                                                                          413\u00b17ms\r\n[100.00%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_scalar_kron                                                                                                                                   3.87\u00b10.07\u03bcs\r\n       before           after         ratio\r\n     [bb811f45]       [06d34945]\r\n     <main>           <perf_kron_21257>\r\n-         438\u00b12ms          274\u00b12ms     0.63  bench_shape_base.Kron.time_arr_kron\r\n-         413\u00b17ms          215\u00b12ms     0.52  bench_shape_base.Kron.time_mat_kron\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n<summary> Compare with latest release (v1.22.3) </summary>\r\n\r\n```\r\n~/os/numpy (perf_kron_21257) \u00bb python3 runtests.py --bench-compare v1.22.3 bench_shape_base.Kron                                                                            ganesh@ganesh-MS-7B86\r\n\u00b7 Creating environments\r\n\u00b7 Discovering benchmarks\r\n\u00b7\u00b7 Uninstalling from virtualenv-py3.9-Cython\r\n\u00b7\u00b7 Installing 06d34945 <perf_kron_21257> into virtualenv-py3.9-Cython.\r\n\u00b7 Running 6 total benchmarks (2 commits * 1 environments * 3 benchmarks)\r\n[  0.00%] \u00b7 For numpy commit 7d4349e3 <v1.22.3^0> (round 1/2):\r\n[  0.00%] \u00b7\u00b7 Building for virtualenv-py3.9-Cython.......................................\r\n[  0.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[  0.00%] \u00b7\u00b7\u00b7 Importing benchmark suite produced output:\r\n[  0.00%] \u00b7\u00b7\u00b7\u00b7 NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?\r\n[  8.33%] \u00b7\u00b7\u00b7 Running (bench_shape_base.Kron.time_arr_kron--)...\r\n[ 25.00%] \u00b7 For numpy commit 06d34945 <perf_kron_21257> (round 1/2):\r\n[ 25.00%] \u00b7\u00b7 Building for virtualenv-py3.9-Cython.\r\n[ 25.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[ 33.33%] \u00b7\u00b7\u00b7 Running (bench_shape_base.Kron.time_arr_kron--)...\r\n[ 50.00%] \u00b7 For numpy commit 06d34945 <perf_kron_21257> (round 2/2):\r\n[ 50.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[ 58.33%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_arr_kron                                                                                                                                          276\u00b13ms\r\n[ 66.67%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_mat_kron                                                                                                                                          220\u00b14ms\r\n[ 75.00%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_scalar_kron                                                                                                                                   3.94\u00b10.05\u03bcs\r\n[ 75.00%] \u00b7 For numpy commit 7d4349e3 <v1.22.3^0> (round 2/2):\r\n[ 75.00%] \u00b7\u00b7 Building for virtualenv-py3.9-Cython..\r\n[ 75.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.9-Cython\r\n[ 83.33%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_arr_kron                                                                                                                                       1.32\u00b10.02s\r\n[ 91.67%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_mat_kron                                                                                                                                         733\u00b120ms\r\n[100.00%] \u00b7\u00b7\u00b7 bench_shape_base.Kron.time_scalar_kron                                                                                                                                    3.78\u00b10.1\u03bcs\r\n       before           after         ratio\r\n     [7d4349e3]       [06d34945]\r\n     <v1.22.3^0>       <perf_kron_21257>\r\n-        733\u00b120ms          220\u00b14ms     0.30  bench_shape_base.Kron.time_mat_kron\r\n-      1.32\u00b10.02s          276\u00b13ms     0.21  bench_shape_base.Kron.time_arr_kron\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE INCREASED.\r\n```\r\n\r\n</details>\r\n\r\nTotal speedup of about 70-80% compared to the current release\r\n\r\n### Explanation\r\nOk let me try my best to explain the current flow:\r\n1. Let's take two arrays `a` and `b` such that\r\n```\r\na = np.ones((2,0,2))\r\nb = np.ones((2,2))\r\n```\r\n2. Transform the ~shape~ ndims of smaller array (`b` in this case) to make them equal, hence `a`'s shape stays `(2,0,2)` while `b` becomes `(1,2,2)`. We prepend in case you were wondering. This is arbitrary from my searching, as few people prefer to append as well.\r\n3. Now insert dimensions to both, such that we add them at odd axes for `a` and even for `b`. This is to compute the product for the required sub parts. Using broadcasting for the product of course which is helping in the performance.\r\n4. The shape of `a` is now `(2, 1, 0, 1, 2, 1)` and `b` will be `(1, 1, 1, 2, 1, 2)`\r\n5. After computing the product we reshape the result to the krons shape, `2, 0, 4`. We get this shape by multiplying shapes of `a` and `b`.\r\n\r\n### TODO\r\n- [x] One release note for performance [e18e312](https://github.com/numpy/numpy/pull/21354/commits/e18e3123d2483497d6c006d2ab251e01fb66be60)\r\n- [x] Add inline comments [8e447c8](https://github.com/numpy/numpy/pull/21354/commits/8e447c8d64fc88390a5dcc4f205fad40fe470f4e)\r\n\r\nPart of #21257 ",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/21354.performance.rst",
                "patch": "@@ -0,0 +1,4 @@\n+Faster ``np.kron``\n+------------------\n+`numpy.kron` is about 80% faster as the product is now computed\n+using broadcasting."
            },
            {
                "filename": "numpy/lib/shape_base.py",
                "patch": "@@ -1139,8 +1139,18 @@ def kron(a, b):\n     True\n \n     \"\"\"\n+    # Working:\n+    # 1. Equalise the shapes by prepending smaller array with 1s\n+    # 2. Expand shapes of both the arrays by adding new axes at\n+    #    odd positions for 1st array and even positions for 2nd\n+    # 3. Compute the product of the modified array\n+    # 4. The inner most array elements now contain the rows of\n+    #    the Kronecker product\n+    # 5. Reshape the result to kron's shape, which is same as\n+    #    product of shapes of the two arrays.\n     b = asanyarray(b)\n     a = array(a, copy=False, subok=True, ndmin=b.ndim)\n+    is_any_mat = isinstance(a, matrix) or isinstance(b, matrix)\n     ndb, nda = b.ndim, a.ndim\n     nd = max(ndb, nda)\n \n@@ -1158,17 +1168,17 @@ def kron(a, b):\n     as_ = (1,)*max(0, ndb-nda) + as_\n     bs = (1,)*max(0, nda-ndb) + bs\n \n+    # Insert empty dimensions\n+    a_arr = expand_dims(a, axis=tuple(range(ndb-nda)))\n+    b_arr = expand_dims(b, axis=tuple(range(nda-ndb)))\n+\n     # Compute the product\n-    a_arr = a.reshape(a.size, 1)\n-    b_arr = b.reshape(1, b.size)\n-    is_any_mat = isinstance(a_arr, matrix) or isinstance(b_arr, matrix)\n+    a_arr = expand_dims(a_arr, axis=tuple(range(1, nd*2, 2)))\n+    b_arr = expand_dims(b_arr, axis=tuple(range(0, nd*2, 2)))\n     # In case of `mat`, convert result to `array`\n     result = _nx.multiply(a_arr, b_arr, subok=(not is_any_mat))\n \n     # Reshape back\n-    result = result.reshape(as_+bs)\n-    transposer = _nx.arange(nd*2).reshape([2, nd]).ravel(order='f')\n-    result = result.transpose(transposer)\n     result = result.reshape(_nx.multiply(as_, bs))\n \n     return result if not is_any_mat else matrix(result, copy=False)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 6075,
        "body": "Seems I was productive during trvale.\n\nShould be good enough to try around, if someone can contribute, please do, it will probably be a while before I look at it again seriously.\n",
        "changed_files": [
            {
                "filename": "numpy/core/code_generators/numpy_api.py",
                "patch": "@@ -74,6 +74,9 @@\n     'PyHalfArrType_Type':               (217,),\n     'NpyIter_Type':                     (218,),\n     # End 1.6 API\n+    # Start 1.11 API\n+    'PyArrayAttributeIndexer_Type':     (304,),\n+    'PyArrayMultiIndex_Type':           (305,),\n }\n \n #define NPY_NUMUSERTYPES (*(int *)PyArray_API[6])"
            },
            {
                "filename": "numpy/core/include/numpy/ndarraytypes.h",
                "patch": "@@ -1315,8 +1315,9 @@ typedef struct {\n         char                  *baseoffset;\n \n         /*\n-         * after binding consec denotes at which axis the fancy axes\n-         * are inserted.\n+         * after binding consec != 0 means that a transpose is\n+         * necessary. The exact value carries (temporarely unused) info for\n+         * fancy indexing transpose. (Did not rename for API compat)\n          */\n         int                   consec;\n         char                  *dataptr;\n@@ -1355,9 +1356,15 @@ typedef struct {\n \n         /* Count for the external loop (which ever it is) for API iteration */\n         npy_intp              iter_count;\n+        /* \n+         * Specify how to transpose for MapIterSwapAxes\n+         */\n+        npy_intp              set_perm[NPY_MAXDIMS];\n+        npy_intp              get_perm[NPY_MAXDIMS];\n \n } PyArrayMapIterObject;\n \n+\n enum {\n     NPY_NEIGHBORHOOD_ITER_ZERO_PADDING,\n     NPY_NEIGHBORHOOD_ITER_ONE_PADDING,"
            },
            {
                "filename": "numpy/core/memmap.py",
                "patch": "@@ -332,7 +332,10 @@ def __array_wrap__(self, arr, context=None):\n         return arr.view(np.ndarray)\n \n     def __getitem__(self, index):\n+        # TODO: May not be necessary, but might be on python 2 to correctly\n+        # channel special single item getting and simple slice getting.\n         res = super(memmap, self).__getitem__(index)\n         if type(res) is memmap and res._mmap is None:\n             return res.view(type=ndarray)\n         return res\n+"
            },
            {
                "filename": "numpy/core/src/multiarray/common.h",
                "patch": "@@ -252,6 +252,7 @@ npy_memchr(char * haystack, char needle,\n     return p;\n }\n \n+\n /*\n  * Convert NumPy stride to BLAS stride. Returns 0 if conversion cannot be done\n  * (BLAS won't handle negative or zero strides the way we want)."
            },
            {
                "filename": "numpy/core/src/multiarray/getset.c",
                "patch": "@@ -21,6 +21,8 @@\n #include \"mem_overlap.h\"\n #include \"alloc.h\"\n \n+#include \"mapping.h\"\n+\n /*******************  array attribute get and set routines ******************/\n \n static PyObject *\n@@ -961,6 +963,24 @@ array_transpose_get(PyArrayObject *self)\n     return PyArray_Transpose(self, NULL);\n }\n \n+static PyObject *\n+array_oindex_get(PyArrayObject *self)\n+{\n+    return PyArray_AttributeIndexerNew(self, OUTER_INDEXING);\n+}\n+\n+static PyObject *\n+array_vindex_get(PyArrayObject *self)\n+{\n+    return PyArray_AttributeIndexerNew(self, VECTOR_INDEXING);\n+}\n+\n+static PyObject *\n+array_lindex_get(PyArrayObject *self)\n+{\n+    return PyArray_AttributeIndexerNew(self, FANCY_INDEXING);\n+}\n+\n /* If this is None, no function call is made\n    --- default sub-class behavior\n */\n@@ -1031,6 +1051,18 @@ NPY_NO_EXPORT PyGetSetDef array_getsetlist[] = {\n         (getter)array_transpose_get,\n         NULL,\n         NULL, NULL},\n+    {\"oindex\",\n+        (getter)array_oindex_get,\n+        NULL,\n+        NULL, NULL},\n+    {\"vindex\",\n+        (getter)array_vindex_get,\n+        NULL,\n+        NULL, NULL},\n+    {\"lindex\",\n+        (getter)array_lindex_get,\n+        NULL,\n+        NULL, NULL},\n     {\"__array_interface__\",\n         (getter)array_interface_get,\n         NULL,"
            },
            {
                "filename": "numpy/core/src/multiarray/mapping.c",
                "patch": "@@ -42,6 +42,38 @@\n static int\n _nonzero_indices(PyObject *myBool, PyArrayObject **arrays);\n \n+\n+/*\n+ * checks if a method is overriden by the subclass or not.\n+ * This Method is insane, probably does not even work (at least for\n+ * extension type subclasses).\n+ * WARNING: THIS IS JUST A TESTING VERSION.\n+ */\n+static NPY_INLINE int _check_method_is_base(PyArrayObject *self, char *method) {\n+    PyObject *methodobj = NULL, *objclass = NULL;\n+    int res;\n+\n+    methodobj = PyObject_GetAttrString((PyObject *)self, method);\n+    if (methodobj == NULL) {\n+        return 0;\n+    }\n+\n+    /* TODO: Maybe there is a nicer way to figure this out!? */\n+    objclass = PyObject_GetAttrString(methodobj, \"__objclass__\");\n+    if (objclass == NULL) {\n+        PyErr_Clear();\n+        res = PyObject_HasAttrString(methodobj, \"im_class\") == 0;\n+        Py_DECREF(methodobj);\n+        return res;\n+    }\n+\n+    res = ((PyTypeObject *)objclass == &PyArray_Type);\n+    Py_DECREF(methodobj);\n+    Py_DECREF(objclass);\n+    return res;\n+}\n+\n+\n /******************************************************************************\n  ***                    IMPLEMENT MAPPING PROTOCOL                          ***\n  *****************************************************************************/\n@@ -67,7 +99,6 @@ NPY_NO_EXPORT void\n PyArray_MapIterSwapAxes(PyArrayMapIterObject *mit, PyArrayObject **ret, int getmap)\n {\n     PyObject *new;\n-    int n1, n2, n3, val, bnd;\n     int i;\n     PyArray_Dims permute;\n     npy_intp d[NPY_MAXDIMS];\n@@ -96,44 +127,13 @@ PyArray_MapIterSwapAxes(PyArrayMapIterObject *mit, PyArrayObject **ret, int getm\n         }\n     }\n \n-    /*\n-     * Setting and getting need to have different permutations.\n-     * On the get we are permuting the returned object, but on\n-     * setting we are permuting the object-to-be-set.\n-     * The set permutation is the inverse of the get permutation.\n-     */\n-\n-    /*\n-     * For getting the array the tuple for transpose is\n-     * (n1,...,n1+n2-1,0,...,n1-1,n1+n2,...,n3-1)\n-     * n1 is the number of dimensions of the broadcast index array\n-     * n2 is the number of dimensions skipped at the start\n-     * n3 is the number of dimensions of the result\n-     */\n-\n-    /*\n-     * For setting the array the tuple for transpose is\n-     * (n2,...,n1+n2-1,0,...,n2-1,n1+n2,...n3-1)\n-     */\n-    n1 = mit->nd_fancy;\n-    n2 = mit->consec; /* axes to insert at */\n-    n3 = mit->nd;\n-\n-    /* use n1 as the boundary if getting but n2 if setting */\n-    bnd = getmap ? n1 : n2;\n-    val = bnd;\n-    i = 0;\n-    while (val < n1 + n2) {\n-        permute.ptr[i++] = val++;\n+    if (getmap) {\n+        permute.ptr = mit->get_perm;\n     }\n-    val = 0;\n-    while (val < bnd) {\n-        permute.ptr[i++] = val++;\n-    }\n-    val = n1 + n2;\n-    while (val < n3) {\n-        permute.ptr[i++] = val++;\n+    else {\n+        permute.ptr = mit->set_perm;\n     }\n+\n     new = PyArray_Transpose(*ret, &permute);\n     Py_DECREF(*ret);\n     *ret = (PyArrayObject *)new;\n@@ -205,15 +205,21 @@ unpack_scalar(PyObject *index, PyObject **result, npy_intp result_n)\n  * @param  result    An empty buffer of PyObject* to write each index component\n  *                   to. The references written are new.\n  * @param  result_n  The length of the result buffer\n- *\n+ * @param allow_heuristic\n+                     If not True, an error is always given when the\n+ *                   sequence unpacking heuristic inherited from Numeric would\n+ *                   be used. Otherwise a FutureWarning is given.\n+ *                   The special indexing attributes use this for the more\n+ *                   strict check.\n  * @returns          The number of items in `result`, or -1 if an error occurred.\n  *                   The entries in `result` at and beyond this index should be\n  *                   assumed to contain garbage, even if they were initialized\n  *                   to NULL, so are not safe to Py_XDECREF. Use multi_DECREF to\n  *                   dispose of them.\n  */\n NPY_NO_EXPORT npy_intp\n-unpack_indices(PyObject *index, PyObject **result, npy_intp result_n)\n+unpack_indices(PyObject *index, PyObject **result, npy_intp result_n,\n+               int allow_heuristic)\n {\n     npy_intp n, i;\n     npy_bool commit_to_unpack;\n@@ -313,6 +319,14 @@ unpack_indices(PyObject *index, PyObject **result, npy_intp result_n)\n                     || PySlice_Check(tmp_obj)\n                     || tmp_obj == Py_Ellipsis\n                     || tmp_obj == Py_None) {\n+\n+                if (!allow_heuristic) {\n+                    PyErr_SetString(PyExc_IndexError,\n+                        \"using a non-tuple sequence for multidimensional \"\n+                        \"indexing is not allowed for the special indexing \"\n+                        \"attributes and deprecated for normal indexing.\");\n+                    goto fail;\n+                }\n                 if (DEPRECATE_FUTUREWARNING(\n                         \"Using a non-tuple sequence for multidimensional \"\n                         \"indexing is deprecated; use `arr[tuple(seq)]` \"\n@@ -364,13 +378,16 @@ unpack_indices(PyObject *index, PyObject **result, npy_intp result_n)\n  * @param dimension of the indexing result\n  * @param dimension of the fancy/advanced indices part\n  * @param whether to allow the boolean special case\n+ * @param whether to allow the sequence as tuple handeling and non-complete\n+ *        index.\n  *\n  * @returns the index_type or -1 on failure and fills the number of indices.\n  */\n NPY_NO_EXPORT int\n prepare_index(PyArrayObject *self, PyObject *index,\n               npy_index_info *indices,\n-              int *num, int *ndim, int *out_fancy_ndim, int allow_boolean)\n+              int *num, int *ndim, int *out_fancy_ndim, int allow_boolean,\n+              int indexing_method)\n {\n     int new_ndim, fancy_ndim, used_ndim, index_ndim;\n     int curr_idx, get_idx;\n@@ -392,7 +409,8 @@ prepare_index(PyArrayObject *self, PyObject *index,\n      */\n     PyObject *raw_indices[NPY_MAXDIMS*2];\n \n-    index_ndim = unpack_indices(index, raw_indices, NPY_MAXDIMS*2);\n+    index_ndim = unpack_indices(index, raw_indices, NPY_MAXDIMS*2,\n+                                indexing_method==PLAIN_INDEXING);\n     if (index_ndim == -1) {\n         return -1;\n     }\n@@ -415,6 +433,15 @@ prepare_index(PyArrayObject *self, PyObject *index,\n \n         obj = raw_indices[get_idx++];\n \n+        /*\n+         * Boolean arrays broadcast differently for outer indexing.\n+         * This information allows to group the converted booleans,\n+         * and can be used for error reporting (not at the time of writing\n+         * this). Note: Ellipses can be added later and boolean expanded.\n+         * For those this is set explicitly later.\n+         */\n+        indices[curr_idx].orig_index = get_idx;\n+\n         /**** Try the cascade of possible indices ****/\n \n         /* Index is an ellipsis (`...`) */\n@@ -550,6 +577,16 @@ prepare_index(PyArrayObject *self, PyObject *index,\n              */\n             PyArrayObject *nonzero_result[NPY_MAXDIMS];\n \n+            if (indexing_method == VECTOR_INDEXING) {\n+                PyErr_SetString(PyExc_IndexError,\n+                                \"boolean array index found, the `vindex`, \"\n+                                \"however, indexing attribute does not support \"\n+                                \"boolean indices. `oindex` (and plain \"\n+                                \"indexing) is the natural choice for \"\n+                                \"booleans.\");\n+                goto failed_building_indices;\n+            }\n+\n             if ((index_ndim == 1) && allow_boolean) {\n                 /*\n                  * If ndim and size match, this can be optimized as a single\n@@ -567,7 +604,7 @@ prepare_index(PyArrayObject *self, PyObject *index,\n \n                     /* keep track anyway, just to be complete */\n                     used_ndim = PyArray_NDIM(self);\n-                    fancy_ndim = PyArray_NDIM(self);\n+                    fancy_ndim = 1;\n                     curr_idx += 1;\n                     break;\n                 }\n@@ -601,8 +638,11 @@ prepare_index(PyArrayObject *self, PyObject *index,\n                 }\n \n                 used_ndim += 0;\n-                if (fancy_ndim < 1) {\n-                    fancy_ndim = 1;\n+                if (indexing_method == OUTER_INDEXING) {\n+                    fancy_ndim += 1;\n+                }\n+                else {\n+                    fancy_ndim = (fancy_ndim < 1) ? 1 : fancy_ndim;\n                 }\n                 curr_idx += 1;\n                 continue;\n@@ -629,6 +669,8 @@ prepare_index(PyArrayObject *self, PyObject *index,\n             /* Add the arrays from the nonzero result to the index */\n             index_type |= HAS_FANCY;\n             for (i=0; i < n; i++) {\n+                /* Set orig_index again, we have to set it for the followups */\n+                indices[curr_idx].orig_index = get_idx;\n                 indices[curr_idx].type = HAS_FANCY;\n                 indices[curr_idx].value = PyArray_DIM(arr, i);\n                 indices[curr_idx].object = (PyObject *)nonzero_result[i];\n@@ -638,8 +680,11 @@ prepare_index(PyArrayObject *self, PyObject *index,\n             }\n \n             /* All added indices have 1 dimension */\n-            if (fancy_ndim < 1) {\n-                fancy_ndim = 1;\n+            if (indexing_method == OUTER_INDEXING) {\n+                fancy_ndim += 1;\n+            }\n+            else {\n+                fancy_ndim = (fancy_ndim < 1) ? 1 : fancy_ndim;\n             }\n             continue;\n         }\n@@ -678,8 +723,12 @@ prepare_index(PyArrayObject *self, PyObject *index,\n             indices[curr_idx].object = (PyObject *)arr;\n \n             used_ndim += 1;\n-            if (fancy_ndim < PyArray_NDIM(arr)) {\n-                fancy_ndim = PyArray_NDIM(arr);\n+            if (indexing_method == OUTER_INDEXING) {\n+                fancy_ndim += PyArray_NDIM(arr);\n+            }\n+            else {\n+                fancy_ndim = (fancy_ndim < PyArray_NDIM(arr)) ?\n+                                        PyArray_NDIM(arr) : fancy_ndim;\n             }\n             curr_idx += 1;\n             continue;\n@@ -709,26 +758,40 @@ prepare_index(PyArrayObject *self, PyObject *index,\n      * to find the ellipsis value or append an ellipsis if necessary.\n      */\n     if (used_ndim < PyArray_NDIM(self)) {\n-       if (index_type & HAS_ELLIPSIS) {\n-           indices[ellipsis_pos].value = PyArray_NDIM(self) - used_ndim;\n-           used_ndim = PyArray_NDIM(self);\n-           new_ndim += indices[ellipsis_pos].value;\n-       }\n-       else {\n-           /*\n-            * There is no ellipsis yet, but it is not a full index\n-            * so we append an ellipsis to the end.\n-            */\n-           index_type |= HAS_ELLIPSIS;\n-           indices[curr_idx].object = NULL;\n-           indices[curr_idx].type = HAS_ELLIPSIS;\n-           indices[curr_idx].value = PyArray_NDIM(self) - used_ndim;\n-           ellipsis_pos = curr_idx;\n-\n-           used_ndim = PyArray_NDIM(self);\n-           new_ndim += indices[curr_idx].value;\n-           curr_idx += 1;\n-       }\n+        if (index_type & HAS_ELLIPSIS) {\n+            indices[ellipsis_pos].value = PyArray_NDIM(self) - used_ndim;\n+            used_ndim = PyArray_NDIM(self);\n+            new_ndim += indices[ellipsis_pos].value;\n+        }\n+        else if (indexing_method == PLAIN_INDEXING) {\n+            /*\n+             * There is no ellipsis yet, but it is not a full index\n+             * so we append an ellipsis to the end. Use -1 to signal not given.\n+             */\n+            indices[curr_idx].orig_index = -1;\n+            index_type |= HAS_ELLIPSIS;\n+            indices[curr_idx].object = NULL;\n+            indices[curr_idx].type = HAS_ELLIPSIS;\n+            indices[curr_idx].value = PyArray_NDIM(self) - used_ndim;\n+            ellipsis_pos = curr_idx;\n+\n+            used_ndim = PyArray_NDIM(self);\n+            new_ndim += indices[curr_idx].value;\n+            curr_idx += 1;\n+        }\n+        else {\n+            /*\n+             * This is a non-plain indexing operation, and the dimensions\n+             * should match up exactly.\n+             */\n+            PyErr_SetString(PyExc_IndexError,\n+                            \"too few indices for array; non-plain indexing \"\n+                            \"requires indices and dimensions to match exactly. \"\n+                            \"Use slices (`:`) as appropriate or prepend an \"\n+                            \"ellipsis (`...`) to allow an arbitrary number of \"\n+                            \"additional array dimensions.\");\n+            goto failed_building_indices;\n+        }\n     }\n     else if (used_ndim > PyArray_NDIM(self)) {\n         PyErr_SetString(PyExc_IndexError,\n@@ -1384,7 +1447,7 @@ array_item(PyArrayObject *self, Py_ssize_t i)\n NPY_NO_EXPORT PyObject *\n array_subscript_asarray(PyArrayObject *self, PyObject *op)\n {\n-    return PyArray_EnsureAnyArray(array_subscript(self, op));\n+    return PyArray_EnsureAnyArray(array_subscript(self, op, PLAIN_INDEXING));\n }\n \n /*\n@@ -1605,15 +1668,17 @@ _get_field_view(PyArrayObject *arr, PyObject *ind, PyArrayObject **view,\n     return -1;\n }\n \n+\n /*\n  * General function for indexing a NumPy array with a Python object.\n  */\n NPY_NO_EXPORT PyObject *\n-array_subscript(PyArrayObject *self, PyObject *op)\n+array_subscript(PyArrayObject *self, PyObject *op, int indexing_method)\n {\n     int index_type;\n     int index_num;\n     int i, ndim, fancy_ndim;\n+\n     /*\n      * Index info array. We can have twice as many indices as dimensions\n      * (because of None). The + 1 is to not need to check as much.\n@@ -1625,8 +1690,22 @@ array_subscript(PyArrayObject *self, PyObject *op)\n \n     PyArrayMapIterObject * mit = NULL;\n \n+    if (Py_TYPE(op) == &PyArrayMultiIndex_Type) {\n+        /* We have a forwarded attribute index */\n+        if (indexing_method != PLAIN_INDEXING) {\n+            PyErr_SetString(PyExc_ValueError,\n+                            \"an attribute indexer (e.g. oindex) was \"\n+                            \"passed into a non-plain indexing method. \"\n+                            \"This is unsupported.\");\n+            return NULL;\n+        }\n+        indexing_method = ((PyArrayMultiIndex *)op)->indexing_method;\n+        op = ((PyArrayMultiIndex *)op)->index;\n+    }\n+\n     /* return fields if op is a string index */\n-    if (PyDataType_HASFIELDS(PyArray_DESCR(self))) {\n+    if (indexing_method == PLAIN_INDEXING &&\n+                    PyDataType_HASFIELDS(PyArray_DESCR(self))) {\n         PyArrayObject *view;\n         int ret = _get_field_view(self, op, &view, 0);\n         if (ret == 0){\n@@ -1639,7 +1718,7 @@ array_subscript(PyArrayObject *self, PyObject *op)\n \n     /* Prepare the indices */\n     index_type = prepare_index(self, op, indices, &index_num,\n-                               &ndim, &fancy_ndim, 1);\n+                               &ndim, &fancy_ndim, 1, indexing_method);\n \n     if (index_type < 0) {\n         return NULL;\n@@ -1751,13 +1830,10 @@ array_subscript(PyArrayObject *self, PyObject *op)\n     }\n \n     /* fancy indexing has to be used. And view is the subspace. */\n-    mit = (PyArrayMapIterObject *)PyArray_MapIterNew(indices, index_num,\n-                                                     index_type,\n-                                                     ndim, fancy_ndim,\n-                                                     self, view, 0,\n-                                                     NPY_ITER_READONLY,\n-                                                     NPY_ITER_WRITEONLY,\n-                                                     NULL, PyArray_DESCR(self));\n+    mit = (PyArrayMapIterObject *)PyArray_MapIterNew(\n+            indices, index_num, index_type, ndim, fancy_ndim,\n+            self, view, 0, NPY_ITER_READONLY, NPY_ITER_WRITEONLY,\n+            NULL, PyArray_DESCR(self), indexing_method);\n     if (mit == NULL) {\n         goto finish;\n     }\n@@ -1885,8 +1961,9 @@ array_assign_item(PyArrayObject *self, Py_ssize_t i, PyObject *op)\n /*\n  * General assignment with python indexing objects.\n  */\n-static int\n-array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op)\n+NPY_NO_EXPORT int\n+array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op,\n+                       int indexing_method, int allow_getitem_hack)\n {\n     int index_type;\n     int index_num;\n@@ -1903,12 +1980,27 @@ array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op)\n                         \"cannot delete array elements\");\n         return -1;\n     }\n+\n+    if (Py_TYPE(ind) == &PyArrayMultiIndex_Type) {\n+        /* We have a forwarded attribute index */\n+        if (indexing_method != PLAIN_INDEXING) {\n+            PyErr_SetString(PyExc_ValueError,\n+                            \"an attribute indexer (e.g. oindex) was \"\n+                            \"passed into a non-plain indexing method. \"\n+                            \"This is unsupported.\");\n+            return -1;\n+        }\n+        indexing_method = ((PyArrayMultiIndex *)ind)->indexing_method;\n+        ind = ((PyArrayMultiIndex *)ind)->index;\n+    }\n+\n     if (PyArray_FailUnlessWriteable(self, \"assignment destination\") < 0) {\n         return -1;\n     }\n \n     /* field access */\n-    if (PyDataType_HASFIELDS(PyArray_DESCR(self))){\n+    if (indexing_method == PLAIN_INDEXING &&\n+                    PyDataType_HASFIELDS(PyArray_DESCR(self))){\n         PyArrayObject *view;\n         int ret = _get_field_view(self, ind, &view, 1);\n         if (ret == 0){\n@@ -1926,7 +2018,7 @@ array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op)\n \n     /* Prepare the indices */\n     index_type = prepare_index(self, ind, indices, &index_num,\n-                               &ndim, &fancy_ndim, 1);\n+                               &ndim, &fancy_ndim, 1, indexing_method);\n \n     if (index_type < 0) {\n         return -1;\n@@ -1997,7 +2089,8 @@ array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op)\n      *          Many subclasses should probably call __setitem__\n      *          with a base class ndarray view to avoid this.\n      */\n-    else if (!(index_type & (HAS_FANCY | HAS_SCALAR_ARRAY))\n+    else if (allow_getitem_hack &&\n+                !(index_type & (HAS_FANCY | HAS_SCALAR_ARRAY))\n                 && !PyArray_CheckExact(self)) {\n         view = (PyArrayObject *)PyObject_GetItem((PyObject *)self, ind);\n         if (view == NULL) {\n@@ -2105,15 +2198,11 @@ array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op)\n      *       correctly, but such an operand always has the full\n      *       size anyway.\n      */\n-    mit = (PyArrayMapIterObject *)PyArray_MapIterNew(indices,\n-                                             index_num, index_type,\n-                                             ndim, fancy_ndim, self,\n-                                             view, 0,\n-                                             NPY_ITER_WRITEONLY,\n-                                             ((tmp_arr == NULL) ?\n-                                                  NPY_ITER_READWRITE :\n-                                                  NPY_ITER_READONLY),\n-                                             tmp_arr, descr);\n+    mit = (PyArrayMapIterObject *)PyArray_MapIterNew(\n+            indices, index_num, index_type, ndim, fancy_ndim,\n+            self, view, 0, NPY_ITER_WRITEONLY,\n+            ((tmp_arr == NULL) ? NPY_ITER_READWRITE : NPY_ITER_READONLY),\n+            tmp_arr, descr, indexing_method);\n \n     if (mit == NULL) {\n         goto fail;\n@@ -2175,10 +2264,27 @@ array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op)\n }\n \n \n+\n+/*\n+ * General subscription and assignment with python indexing objects.\n+ */\n+PyObject *\n+array_subscript_fancy(PyArrayObject *self, PyObject *ind)\n+{\n+    return array_subscript(self, ind, PLAIN_INDEXING);\n+}\n+\n+static int\n+array_assign_subscript_fancy(PyArrayObject *self, PyObject *ind, PyObject *op)\n+{\n+    return array_assign_subscript(self, ind, op, PLAIN_INDEXING, 1);\n+}\n+\n+\n NPY_NO_EXPORT PyMappingMethods array_as_mapping = {\n     (lenfunc)array_length,              /*mp_length*/\n-    (binaryfunc)array_subscript,        /*mp_subscript*/\n-    (objobjargproc)array_assign_subscript,       /*mp_ass_subscript*/\n+    (binaryfunc)array_subscript_fancy,        /*mp_subscript*/\n+    (objobjargproc)array_assign_subscript_fancy,       /*mp_ass_subscript*/\n };\n \n /****************** End of Mapping Protocol ******************************/\n@@ -2411,15 +2517,249 @@ PyArray_MapIterNext(PyArrayMapIterObject *mit)\n }\n \n \n+void setup_fancy_permutation(PyArrayMapIterObject *mit) {\n+    int n1, n2, n3, val, bnd;\n+    int i;\n+ \n+    /*\n+     * Setting and getting need to have different permutations.\n+     * On the get we are permuting the returned object, but on\n+     * setting we are permuting the object-to-be-set.\n+     * The set permutation is the inverse of the get permutation.\n+     */\n+   \n+    /*\n+     * For getting the array the tuple for transpose is\n+     * (n1,...,n1+n2-1,0,...,n1-1,n1+n2,...,n3-1)\n+     * n1 is the number of dimensions of the broadcast index array\n+     * n2 is the number of dimensions skipped at the start\n+     * n3 is the number of dimensions of the result\n+     */\n+\n+    /*\n+     * For setting the array the tuple for transpose is\n+     * (n2,...,n1+n2-1,0,...,n2-1,n1+n2,...n3-1)\n+     */\n+    n1 = mit->nd_fancy;\n+    n2 = mit->consec; /* axes to insert at */\n+    n3 = mit->nd;\n+\n+    /* use n2 as the boundary if setting (getting done later; bnd=n2 works) */\n+    bnd = n1;\n+    val = bnd;\n+    i = 0;\n+    while (val < n1 + n2) {\n+        mit->get_perm[i++] = val++;\n+    }\n+    val = 0;\n+    while (val < bnd) {\n+        mit->get_perm[i++] = val++;\n+    }\n+    val = n1 + n2;\n+    while (val < n3) {\n+        mit->get_perm[i++] = val++;\n+    }\n+}\n+\n+\n+/* Identical to setup_fancy_permutation but Deprecates/Warns */\n+int setup_plain_permutation(PyArrayMapIterObject *mit,\n+                             npy_index_info *indices, int index_num)\n+{\n+    int i;\n+    if (mit->numiter > 1) {\n+        int orig_index = -1;\n+        /* Check indices, we might have a single boolean index, which is OK */\n+        for (i = 0; i < index_num; i++) {\n+            if (indices[i].type & (HAS_FANCY|HAS_BOOL)) {\n+                if (orig_index != indices[i].orig_index && orig_index != -1) {\n+                    /*\n+                     * TODO: We probably have to remove/make this warning less\n+                     *       pronounced!\n+                     */\n+                    if (DEPRECATE(\n+                            \"more than two array indices found; \"\n+                            \"please use `arr.oindex`, `arr.vindex`, or \"\n+                            \"`arr.lindex` to clarify use case.\") < 0) {\n+                        return -1;\n+                    }\n+                }\n+                orig_index = indices[i].orig_index;\n+            }\n+        }\n+    }\n+    else if (mit->consec == 0) {\n+        /* We need to check whether or not the fancy index is \"first\" */\n+        for (i = 0; i < index_num; i++) {\n+            if (indices[i].type & (HAS_FANCY|HAS_BOOL)) {\n+                /*\n+                 * First dimension being the fancy one is correct the\n+                 * index is thus clear.\n+                 */\n+                break;\n+            }\n+            else if (indices[i].type == HAS_INTEGER) {\n+                /* Integer indices have no output dim, so ignore them */\n+                continue;\n+            }\n+            else if ((indices[i].type == HAS_ELLIPSIS) &&\n+                     (indices[i].value == 0)) {\n+                /* no-slice Ellipsis does not matter */\n+                continue  ;       \n+            }\n+            /* If we are here, the idex is not clear */\n+            if (DEPRECATE(\n+                        \"advanced index is not clear due to combination of \"\n+                        \"scalars and array-indices; \"\n+                        \"please use `arr.oindex`, `arr.vindex`, or \"\n+                        \"`arr.lindex` to clarify use case.\") < 0) {\n+                return -1;\n+            }\n+            break;\n+        }            \n+    } \n+\n+    setup_fancy_permutation(mit);\n+    return 0;\n+}\n+\n+\n+void setup_outer_permutation(PyArrayMapIterObject *mit,\n+                             npy_index_info *indices, int index_num) {\n+    /*\n+     * Go backwards through all indices (does a bit duplicate work compared\n+     * to the mapiter_fill_info), and fill in the transposes for fancy and\n+     * subspace indices. Note that for set_perm, we need to transpose from:\n+     * fancy1, fancy2, ..., subspace1, subspace2, ...\n+     * to wherever we are in the total index.\n+     */\n+    int i, j;\n+    int curr_subspace = mit->nd - 1;\n+    int curr_fancy = mit->nd_fancy - 1;\n+    int arr_ndim;\n+    int explained_fancy_ndim = 0;\n+    int working_dim = mit->nd - 1;  /* ndim being set/worked on */\n+    int fill_dim;\n+    for (i = index_num-1; i >= 0; i--) {\n+        if (indices[i].type & HAS_FANCY) {\n+            arr_ndim = PyArray_NDIM((PyArrayObject *)indices[i].object);\n+            /* Number of dimensions contributed by this array */\n+            arr_ndim -= explained_fancy_ndim;\n+            explained_fancy_ndim += arr_ndim;\n+            for (j=0; j < arr_ndim; j++) {\n+                mit->get_perm[working_dim] = curr_fancy;\n+                if (working_dim != curr_fancy) {\n+                    mit->consec = 1;\n+                }\n+                working_dim -= 1;\n+                curr_fancy -= 1;\n+            }\n+        }\n+        /* fill in transpose for non-fancy indices and advance working_ndim */\n+        else {\n+            if (indices[i].type == HAS_ELLIPSIS) {\n+                fill_dim = indices[i].value;\n+            }\n+            else if (indices[i].type == HAS_INTEGER) {\n+                fill_dim = 0;\n+            }\n+            else {\n+                fill_dim = 1;\n+            }\n+            for (j=0; j < fill_dim; j++) {\n+                mit->get_perm[working_dim] = curr_subspace;\n+                if (working_dim != curr_subspace) {\n+                    mit->consec = 1;\n+                }\n+                curr_subspace -= 1;\n+                working_dim -= 1;\n+            }\n+        }\n+    }\n+}\n+\n+\n+void setup_vector_permutation(PyArrayMapIterObject *mit,\n+                              npy_index_info *indices, int index_num) {\n+    /*\n+     * Go backwards through all indices (does a bit duplicate work compared\n+     * to the mapiter_fill_info), and fill in the transposes for fancy and\n+     * subspace indices. Note that for set_perm, we need to transpose from:\n+     * fancy1, fancy2, ..., subspace1, subspace2, ...\n+     * to wherever we are in the total index.\n+     */\n+    int i, j;\n+    int curr_subspace = mit->nd - 1;\n+    int curr_fancy = mit->nd_fancy - 1;\n+    int arr_ndim;\n+    int boolean_fancy_ndim = 0;\n+    int working_dim = mit->nd - 1;  /* ndim being set/worked on */\n+    int fill_dim;\n+\n+    for (i = index_num-1; i >= 0; i--) {\n+        /* If it is a boolean index */\n+        if (indices[i].type & HAS_FANCY) {\n+            if (indices[i].value != -1) {\n+                arr_ndim = PyArray_NDIM((PyArrayObject *)indices[i].object);\n+                /* Number of dimensions contributed by this array */\n+                arr_ndim -= boolean_fancy_ndim;\n+                boolean_fancy_ndim += arr_ndim;\n+\n+                for (j=0; j < arr_ndim; j++) {\n+                    mit->get_perm[working_dim] = curr_fancy;\n+                    if (working_dim != curr_fancy) {\n+                        mit->consec = 1;\n+                    }\n+                    working_dim -= 1;\n+                    curr_fancy -= 1;\n+                }\n+            }\n+            /* Non-boolean indices */\n+            else {\n+                /* Do nothing, we initialize them at the end. */\n+            }\n+        }\n+        /* fill in transpose for non-fancy indices and advance working_ndim */\n+        else {\n+            if (indices[i].type == HAS_ELLIPSIS) {\n+                fill_dim = indices[i].value;\n+            }\n+            else if (indices[i].type == HAS_INTEGER) {\n+                fill_dim = 0;\n+            }\n+            else {\n+                fill_dim = 1;\n+            }\n+\n+            for (j=0; j < fill_dim; j++) {\n+                mit->get_perm[working_dim] = curr_subspace;\n+                if (working_dim != curr_subspace) {\n+                    mit->consec = 1;\n+                }\n+                curr_subspace -= 1;\n+                working_dim -= 1;\n+            }\n+        }\n+    }\n+    \n+    /* Initialize the fancy ndims */\n+    for (i = 0; i < mit->nd_fancy - boolean_fancy_ndim; i++) {\n+        mit->get_perm[i] = i;\n+    }\n+}\n+\n+\n /**\n  * Fill information about the iterator. The MapIterObject does not\n  * need to have any information set for this function to work.\n  * (PyArray_MapIterSwapAxes requires also nd and nd_fancy info)\n  *\n  * Sets the following information:\n- *    * mit->consec: The axis where the fancy indices need transposing to.\n+ *    * mit->consec:\n+ *          Whether or not to transpose. For fancy indexing, the axis where\n+ *          the fancy indices need transposing to (used only internally).\n  *    * mit->iteraxes: The axis which the fancy index corresponds to.\n- *    * mit-> fancy_dims: the dimension of `arr` along the indexed dimension\n+ *    * mit->fancy_dims: the dimension of `arr` along the indexed dimension\n  *          for each fancy index.\n  *    * mit->fancy_strides: the strides for the dimension being indexed\n  *          by each fancy index.\n@@ -2567,36 +2907,197 @@ mapiter_fill_info(PyArrayMapIterObject *mit, npy_index_info *indices,\n }\n \n \n-/*\n- * Check whether the fancy indices are out of bounds.\n- * Returns 0 on success and -1 on failure.\n- * (Gets operands from the outer iterator, but iterates them independently)\n- */\n-NPY_NO_EXPORT int\n-PyArray_MapIterCheckIndices(PyArrayMapIterObject *mit)\n-{\n-    PyArrayObject *op;\n-    NpyIter *op_iter;\n-    NpyIter_IterNextFunc *op_iternext;\n-    npy_intp outer_dim, indval;\n-    int outer_axis;\n-    npy_intp itersize, *iterstride;\n-    char **iterptr;\n-    PyArray_Descr *intp_type;\n-    int i;\n-    NPY_BEGIN_THREADS_DEF;\n+int setup_outer_index(npy_index_info *indices, int num,\n+                      int *ndim, int *fancy_ndim) {\n+    int i, j, arr_ndim;\n+    int prev_indx = -1;\n+    int fancy_ndim_seen = 0;\n+    PyArray_Dims permute;\n+    npy_intp d[NPY_MAXDIMS];\n+    PyArrayObject *arr;\n \n-    if (mit->size == 0) {\n-        /* All indices got broadcast away, do *not* check as it always was */\n-        return 0;\n+    permute.ptr = d;\n+    permute.len = 0;\n+\n+    for (i=num-1; i >= 0; i--) {\n+        /* We have to expand even 0-d booleans here, so use &: */\n+        if (indices[i].type & HAS_FANCY) {\n+            if (fancy_ndim_seen == 0) {\n+                fancy_ndim_seen +=\n+                        PyArray_NDIM((PyArrayObject *)indices[i].object);\n+                prev_indx = indices[i].orig_index;\n+                continue;\n+            }\n+            \n+            /* Get all the info for reshaping */\n+            arr = (PyArrayObject *)indices[i].object;\n+            arr_ndim = PyArray_NDIM(arr);\n+            \n+            if (prev_indx != indices[i].orig_index) {\n+                /*\n+                 * Expand the output further, unless it is boolean and was\n+                 * already expanded.\n+                 */\n+                fancy_ndim_seen += arr_ndim;  /* should be always 1 */\n+                prev_indx = indices[i].orig_index;\n+            }\n+            for (j = 0; j < PyArray_NDIM(arr); j++) {\n+                permute.ptr[j] = PyArray_DIMS(arr)[j];\n+            }\n+            for (j = arr_ndim; j < fancy_ndim_seen; j++) {\n+                permute.ptr[j] = 1;\n+            }\n+            \n+            permute.len = fancy_ndim_seen;\n+            indices[i].object = (PyObject *)\n+                PyArray_Newshape(arr, &permute, NPY_ANYORDER);\n+            Py_DECREF(arr);\n+            if (indices[i].object == NULL) {\n+                return -1;\n+            }\n+        }\n     }\n \n-    intp_type = PyArray_DescrFromType(NPY_INTP);\n+    if (*ndim >= NPY_MAXDIMS) {\n+        PyErr_Format(PyExc_IndexError,\n+                 \"number of dimensions must be within [0, %d], \"\n+                 \"indexing result would have %d\",\n+                 NPY_MAXDIMS, *ndim);\n+        return -1;\n+    }\n+    return 0;\n+}\n \n-    NPY_BEGIN_THREADS;\n \n-    for (i=0; i < mit->numiter; i++) {\n-        op = NpyIter_GetOperandArray(mit->outer)[i];\n+int setup_vector_index(npy_index_info *indices, int num,\n+                      int *ndim, int *fancy_ndim) {\n+    /* TODO: Shoudl not really have to do anything! */\n+    int i, j, arr_ndim, boolean_ndim, total_boolean_ndim, expand_ndim;\n+    int prev_indx = -1;\n+    PyArray_Dims permute;\n+    npy_intp d[NPY_MAXDIMS];\n+    PyArrayObject *arr;\n+\n+    /* TODO: Remove this part completely, it is only true if bools are allowed in vindex! */\n+    /* remove the old fancy ndim, since they are wrong here */\n+    *ndim -= *fancy_ndim;\n+    *fancy_ndim = 0;\n+\n+    permute.ptr = d;\n+    permute.len = 0;\n+\n+    /*\n+     * Count the number of boolean indices the information is necessary\n+     * since we need to expand the vector indices enough to not collide\n+     * with the boolean ones.\n+     */\n+    total_boolean_ndim = 0;\n+    for (i=0; i < num; i++) {\n+        if (indices[i].type & HAS_FANCY) {\n+            /* If it was boolean and not already counted (expanded boolean) */\n+            if ((indices[i].value != -1) &&\n+                    (indices[i].orig_index != prev_indx)) {\n+                total_boolean_ndim += 1;\n+                prev_indx = indices[i].orig_index;        \n+            }\n+        }\n+    }\n+    prev_indx = -1;\n+    boolean_ndim = 0;\n+    for (i=num-1; i >= 0; i--) {\n+        /* We have to expand even 0-d booleans here, so use &: */\n+        if (indices[i].type & HAS_FANCY) {\n+            /* Get all the info for reshaping */\n+            arr = (PyArrayObject *)indices[i].object;\n+            arr_ndim = PyArray_NDIM(arr);\n+            \n+            /* If it was a boolean array */\n+            if (indices[i].value != -1) {\n+                /*\n+                 * Expand the output further, unless it is boolean and was\n+                 * already expanded.\n+                 */\n+                if (prev_indx != indices[i].orig_index) {\n+                    boolean_ndim += arr_ndim;\n+                    prev_indx = indices[i].orig_index;\n+                }\n+                expand_ndim = boolean_ndim;\n+            }\n+            else {\n+                /* We need to expand up to all boolean ndims */\n+                expand_ndim = total_boolean_ndim + arr_ndim;\n+\n+                if (*fancy_ndim < arr_ndim) {\n+                    *fancy_ndim = arr_ndim;\n+                }\n+            }\n+            \n+            if (expand_ndim == arr_ndim) {\n+                /* Nothing to do, skip \"costly\" transpose */\n+                continue;\n+            }\n+\n+            for (j = 0; j < PyArray_NDIM(arr); j++) {\n+                permute.ptr[j] = PyArray_DIMS(arr)[j];\n+            }\n+            for (j = arr_ndim; j < expand_ndim; j++) {\n+                permute.ptr[j] = 1;\n+            }\n+            \n+            permute.len = expand_ndim;\n+            indices[i].object = (PyObject *)\n+                PyArray_Newshape(arr, &permute, NPY_ANYORDER);\n+            Py_DECREF(arr);\n+            if (indices[i].object == NULL) {\n+                printf(\"poooof\\n\");\n+                return -1;\n+            }\n+        }\n+    }\n+    \n+    *fancy_ndim += total_boolean_ndim;\n+    *ndim += *fancy_ndim;\n+    if (*ndim >= NPY_MAXDIMS) {\n+        PyErr_Format(PyExc_IndexError,\n+                 \"number of dimensions must be within [0, %d], \"\n+                 \"indexing result would have %d\",\n+                 NPY_MAXDIMS, *ndim);\n+        return -1;\n+    }\n+    return 0;\n+}\n+\n+\n+/*\n+ * Check whether the fancy indices are out of bounds.\n+ * Returns 0 on success and -1 on failure.\n+ * (Gets operands from the outer iterator, but iterates them independently)\n+ */\n+NPY_NO_EXPORT int\n+PyArray_MapIterCheckIndices(PyArrayMapIterObject *mit)\n+{\n+    PyArrayObject *op;\n+    NpyIter *op_iter;\n+    NpyIter_IterNextFunc *op_iternext;\n+    npy_intp outer_dim, indval;\n+    int outer_axis;\n+    npy_intp itersize, *iterstride;\n+    char **iterptr;\n+    PyArray_Descr *intp_type;\n+    int i;\n+    NPY_BEGIN_THREADS_DEF;\n+\n+    if (mit->size == 0) {\n+        /* All indices got broadcast away, do *not* check as it always was */\n+        return 0;\n+    }\n+\n+    intp_type = PyArray_DescrFromType(NPY_INTP);\n+\n+    NPY_BEGIN_THREADS;\n+\n+    for (i=0; i < mit->numiter; i++) {\n+        op = NpyIter_GetOperandArray(mit->outer)[i];\n \n         outer_dim = mit->fancy_dims[i];\n         outer_axis = mit->iteraxes[i];\n@@ -2714,7 +3215,8 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n                    PyArrayObject *arr, PyArrayObject *subspace,\n                    npy_uint32 subspace_iter_flags, npy_uint32 subspace_flags,\n                    npy_uint32 extra_op_flags, PyArrayObject *extra_op,\n-                   PyArray_Descr *extra_op_dtype)\n+                   PyArray_Descr *extra_op_dtype,\n+                   int indexing_method)\n {\n     PyObject *errmsg, *tmp;\n     /* For shape reporting on error */\n@@ -2748,6 +3250,27 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n     Py_XINCREF(subspace);\n     mit->subspace = subspace;\n \n+    /*\n+     * outer indexing needs to flag this to the get/set swapaxis\n+     *\n+     * NOTE: fancy indexing fills the permutation *after* finding the\n+     *       consec flag. Fiding out consec could possibly be moved.\n+     */\n+    if (indexing_method == OUTER_INDEXING) {\n+        if (setup_outer_index(\n+                indices, index_num, &ndim, &fancy_ndim) < 0) {\n+            Py_DECREF(mit);\n+            return NULL;\n+        }\n+    }\n+    else if (indexing_method == VECTOR_INDEXING) {\n+        if (setup_vector_index(\n+                indices, index_num, &ndim, &fancy_ndim) < 0) {\n+            Py_DECREF(mit);\n+            return NULL;\n+        }\n+    }\n+\n     /*\n      * The subspace, the part of the array which is not indexed by\n      * arrays, needs to be iterated when the size of the subspace\n@@ -2784,13 +3307,45 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n         }\n     }\n \n-    if (mit->numiter == 0) {\n-        /*\n-         * For MapIterArray, it is possible that there is no fancy index.\n-         * to support this case, add a a dummy iterator.\n-         * Since it is 0-d its transpose, etc. does not matter.\n-         */\n \n+    /*\n+     * Fill in the set map, depending on which type of indexing we use.\n+     */\n+    if (indexing_method == FANCY_INDEXING) {\n+        setup_fancy_permutation(mit);\n+    }\n+    else if (indexing_method == PLAIN_INDEXING) {\n+        if (setup_plain_permutation(mit, indices, index_num) < 0) {\n+            Py_DECREF(mit);\n+            return NULL;\n+        }\n+    }\n+    else if (indexing_method == OUTER_INDEXING) {\n+        setup_outer_permutation(mit, indices, index_num);\n+    }\n+    else if (indexing_method == VECTOR_INDEXING) {\n+        setup_vector_permutation(mit, indices, index_num);\n+    }\n+    else {\n+        PyErr_SetString(PyExc_SystemError,\n+                        \"internal indexing error; invalid indexing type.\");\n+        Py_DECREF(mit);\n+        return NULL;\n+    }\n+    /*\n+     * Fill the get map from the setmap info for convenience,\n+     * could be done later as well, but assume that this is very fast.\n+     */\n+    for (i = 0; i < mit->nd; i++) {\n+        mit->set_perm[mit->get_perm[i]] = i;\n+    }\n+\n+    /*\n+     * For MapIterArray, it is possible that there is no fancy index.\n+     * to support this case, add a a dummy iterator.\n+     * Since it is 0-d its transpose, etc. does not matter.\n+     */\n+    if (mit->numiter == 0) {\n         /* signal necessity to decref... */\n         dummy_array = 1;\n \n@@ -3306,7 +3861,7 @@ PyArray_MapIterArrayCopyIfOverlap(PyArrayObject * a, PyObject * index,\n     PyArrayObject *a_copy = NULL;\n \n     index_type = prepare_index(a, index, indices, &index_num,\n-                               &ndim, &fancy_ndim, 0);\n+                               &ndim, &fancy_ndim, 0, PLAIN_INDEXING);\n \n     if (index_type < 0) {\n         return NULL;\n@@ -3341,12 +3896,9 @@ PyArray_MapIterArrayCopyIfOverlap(PyArrayObject * a, PyObject * index,\n         }\n     }\n \n-    mit = (PyArrayMapIterObject *)PyArray_MapIterNew(indices, index_num,\n-                                                     index_type, ndim,\n-                                                     fancy_ndim,\n-                                                     a, subspace, 0,\n-                                                     NPY_ITER_READWRITE,\n-                                                     0, NULL, NULL);\n+    mit = (PyArrayMapIterObject *)PyArray_MapIterNew(\n+        indices, index_num, index_type, ndim, fancy_ndim,\n+        a, subspace, 0, NPY_ITER_READWRITE, 0, NULL, NULL, FANCY_INDEXING);\n     if (mit == NULL) {\n         goto fail;\n     }\n@@ -3393,16 +3945,6 @@ PyArray_MapIterArray(PyArrayObject * a, PyObject * index)\n }\n \n \n-#undef HAS_INTEGER\n-#undef HAS_NEWAXIS\n-#undef HAS_SLICE\n-#undef HAS_ELLIPSIS\n-#undef HAS_FANCY\n-#undef HAS_BOOL\n-#undef HAS_SCALAR_ARRAY\n-#undef HAS_0D_BOOL\n-\n-\n static void\n arraymapiter_dealloc(PyArrayMapIterObject *mit)\n {\n@@ -3494,3 +4036,557 @@ NPY_NO_EXPORT PyTypeObject PyArrayMapIter_Type = {\n     0,                                          /* tp_del */\n     0,                                          /* tp_version_tag */\n };\n+\n+\n+/*\n+ * This is an attribute indexer, to forward things such as: \n+ * arr.oindex[indices], We need to create the arr.oindex object.\n+ */\n+\n+static void\n+arrayattributeindexer_dealloc(PyArrayAttributeIndexer *attr_indexer)\n+{\n+    Py_XDECREF(attr_indexer->array);\n+    PyArray_free(attr_indexer);\n+}\n+\n+NPY_NO_EXPORT PyObject *\n+PyArray_AttributeIndexerNew(PyArrayObject *array, int indexing_method)\n+{\n+    PyArrayAttributeIndexer *attr_indexer;\n+    /* create new AttrinbuteIndexer object */\n+    attr_indexer = (PyArrayAttributeIndexer *)\n+        PyArray_malloc(sizeof(PyArrayAttributeIndexer));\n+    if (attr_indexer == NULL) {\n+        return NULL;\n+    }\n+    /* set all attributes of mapiter to zero */\n+    Py_INCREF(array);\n+    attr_indexer->array = array;\n+    attr_indexer->indexing_method = indexing_method;\n+    PyObject_Init((PyObject *)attr_indexer, &PyArrayAttributeIndexer_Type);\n+    return (PyObject *)attr_indexer;\n+}\n+\n+\n+NPY_NO_EXPORT PyObject *\n+arrayattributeindexer_subscript(PyArrayAttributeIndexer *self,\n+                                PyObject *op)\n+{\n+    PyObject *multiindex;\n+\n+    if (PyArray_CheckExact(self->array)) {\n+        return array_subscript(self->array, op, self->indexing_method);\n+    }\n+\n+    /* TODO: This probably makes sense, but only if the test is fast \n+    if (_check_method_is_base(attr_indexer->array, \"__getitem__\") &&\n+                _check_method_is_base(attr_indexer->array, \"__setitem__\")) {\n+        return array_subscript(\n+            attr_indexer->array, op, attr_indexer->indexing_method);\n+    } OR, it is wrong, at least on newer pythons? */\n+\n+\n+    multiindex = PyArray_MultiIndexNew(op, self->array, self->indexing_method);\n+    if (multiindex == NULL) {\n+        return NULL;\n+    }\n+\n+    return PyObject_GetItem((PyObject *)self->array, multiindex);\n+}\n+\n+\n+\n+NPY_NO_EXPORT int\n+arrayattributeindexer_assign_subscript(PyArrayAttributeIndexer *self,\n+                                       PyObject *op, PyObject *vals)\n+{\n+    PyObject *multiindex;\n+\n+    if (PyArray_CheckExact(self->array)) {\n+        return array_assign_subscript(self->array, op, vals,\n+                                      self->indexing_method, 0);\n+    }\n+\n+    /* TODO: This probably makes sense, but only if the test is fast\n+    if (_check_method_is_base(attr_indexer->array, \"__getitem__\") &&\n+                _check_method_is_base(attr_indexer->array, \"__setitem__\")) {\n+        return array_assign_subscript(\n+            attr_indexer->array, op, vals, attr_indexer->indexing_method, 0);\n+    } OR, it is wrong, at least on newer pythons? */\n+\n+\n+    multiindex = PyArray_MultiIndexNew(op, self->array, self->indexing_method);\n+    if (multiindex == NULL) {\n+        return -1;\n+    }\n+\n+    return PyObject_SetItem((PyObject *)self->array, multiindex, vals);\n+}\n+\n+\n+NPY_NO_EXPORT PyMappingMethods arrayattributeindexer_as_mapping = {\n+    NULL,                                                   /*mp_length*/\n+    (binaryfunc)arrayattributeindexer_subscript,            /*mp_subscript*/\n+    (objobjargproc)arrayattributeindexer_assign_subscript,  /*mp_ass_subscript*/\n+};\n+\n+\n+/*\n+ * Attribute indexer, i.e. `arr.oindex[indices]`.\n+ */\n+NPY_NO_EXPORT PyTypeObject PyArrayAttributeIndexer_Type = {\n+#if defined(NPY_PY3K)\n+    PyVarObject_HEAD_INIT(NULL, 0)\n+#else\n+    PyObject_HEAD_INIT(NULL)\n+    0,                                          /* ob_size */\n+#endif\n+    \"numpy.attribute_indexer\",                  /* tp_name */\n+    sizeof(PyArrayAttributeIndexer),            /* tp_basicsize */\n+    0,                                          /* tp_itemsize */\n+    /* methods */\n+    (destructor)arrayattributeindexer_dealloc,  /* tp_dealloc */\n+    0,                                          /* tp_print */\n+    0,                                          /* tp_getattr */\n+    0,                                          /* tp_setattr */\n+#if defined(NPY_PY3K)\n+    0,                                          /* tp_reserved */\n+#else\n+    0,                                          /* tp_compare */\n+#endif\n+    0,                                          /* tp_repr */\n+    0,                                          /* tp_as_number */\n+    0,                                          /* tp_as_sequence */\n+    &arrayattributeindexer_as_mapping,          /* tp_as_mapping */\n+    0,                                          /* tp_hash */\n+    0,                                          /* tp_call */\n+    0,                                          /* tp_str */\n+    0,                                          /* tp_getattro */\n+    0,                                          /* tp_setattro */\n+    0,                                          /* tp_as_buffer */\n+    Py_TPFLAGS_DEFAULT,                         /* tp_flags */\n+    \"Helper to enable indexing such as arr.oindex\",  /* tp_doc */\n+    0,                                          /* tp_traverse */\n+    0,                                          /* tp_clear */\n+    0,                                          /* tp_richcompare */\n+    0,                                          /* tp_weaklistoffset */\n+    0,                                          /* tp_iter */\n+    0,                                          /* tp_iternext */\n+    0,                                          /* tp_methods */\n+    0,                                          /* tp_members */\n+    0,                                          /* tp_getset */\n+    0,                                          /* tp_base */\n+    0,                                          /* tp_dict */\n+    0,                                          /* tp_descr_get */\n+    0,                                          /* tp_descr_set */\n+    0,                                          /* tp_dictoffset */\n+    0,                                          /* tp_init */\n+    0,                                          /* tp_alloc */\n+    0,                                          /* tp_new */\n+    0,                                          /* tp_free */\n+    0,                                          /* tp_is_gc */\n+    0,                                          /* tp_bases */\n+    0,                                          /* tp_mro */\n+    0,                                          /* tp_cache */\n+    0,                                          /* tp_subclasses */\n+    0,                                          /* tp_weaklist */\n+    0,                                          /* tp_del */\n+#if PY_VERSION_HEX >= 0x02060000\n+    0,                                          /* tp_version_tag */\n+#endif\n+};\n+\n+\n+NPY_NO_EXPORT PyObject *\n+PyArray_MultiIndexNew(PyObject *index, PyArrayObject *array, int indexing_method)\n+{\n+    PyArrayMultiIndex *multiindex;\n+    /* create new AttrinbuteIndexer object */\n+    multiindex = (PyArrayMultiIndex *)PyArray_malloc(sizeof(PyArrayMultiIndex));\n+    if (multiindex == NULL) {\n+        return NULL;\n+    }\n+    /* set all attributes of mapiter to zero */\n+    Py_INCREF(index);\n+    multiindex->index = index;\n+    multiindex->indexing_method = indexing_method;\n+    PyObject_Init((PyObject *)multiindex, &PyArrayMultiIndex_Type);\n+\n+    /*\n+     * For possible future code, anticipate that array can be NULL\n+     */\n+    if (array != NULL) {\n+        multiindex->bound = 1;\n+        multiindex->orig_ndim = PyArray_NDIM(array);\n+        memcpy(multiindex->orig_shape, PyArray_DIMS(array),\n+               multiindex->orig_ndim * sizeof(npy_intp));\n+        multiindex->orig_dtype = PyArray_DESCR(array);\n+        Py_INCREF(multiindex->orig_dtype);\n+    }\n+    else {\n+        multiindex->orig_dtype = NULL;\n+        multiindex->orig_ndim = 0;\n+        multiindex->bound = 0;\n+    }\n+\n+    return (PyObject *)multiindex;\n+}\n+\n+\n+static void\n+multiindex_dealloc(PyArrayMultiIndex *multiindex)\n+{\n+    Py_XDECREF(multiindex->index);\n+    Py_XDECREF(multiindex->orig_dtype);\n+    PyArray_free(multiindex);\n+}\n+\n+\n+NPY_NO_EXPORT Py_ssize_t\n+multiindex_length(PyArrayObject *self)\n+{\n+    PyErr_SetString(PyExc_AttributeError,\n+                    \"the specialized MultiIndex object cannot be modified. \"\n+                    \"It will get more functionality but right now can only \"\n+                    \"be forwarded to numpy for indexing.\");\n+    return -1;\n+}\n+\n+\n+NPY_NO_EXPORT PyObject *\n+multiindex_subscript(PyArrayAttributeIndexer *attr_indexer,\n+                                PyObject *op)\n+{\n+    PyErr_SetString(PyExc_AttributeError,\n+                    \"the specialized MultiIndex object cannot be modified. \"\n+                    \"It will get more functionality but right now can only \"\n+                    \"be forwarded to numpy for indexing.\");\n+    return NULL;\n+}\n+\n+\n+NPY_NO_EXPORT PySequenceMethods multiindex_as_mapping = {\n+    (lenfunc)multiindex_length,                        /*mp_length*/\n+    (binaryfunc)multiindex_subscript,                  /*mp_subscript*/\n+    NULL,                                              /*mp_ass_subscript*/\n+};\n+\n+\n+static PyObject *\n+multiindex_prepared(PyArrayMultiIndex *self, PyObject *args, PyObject *kwds)\n+{\n+    /*\n+     * Exposed as a method, this allows to get some basic information about\n+     * an index (independend of the method).\n+     * Preliminary checks that are guaranteed (I guess):\n+     *   1. Number of indices\n+     *   2. Boolean array shape.\n+     *   3. Types of indices are correct.\n+     *\n+     * Errors are raised if these are incorrect. Also returns whether a copy\n+     * has to be enforced.\n+     * Shape information is needed to test the dimension of the index, the\n+     * actual shape (size of individual dimension) is only used to check\n+     * boolean arrays.\n+     */\n+    int i;\n+    Py_ssize_t tuple_index;\n+\n+    int dummy_ndim;\n+    PyObject *shape_obj = NULL;\n+    npy_intp shape[NPY_MAXDIMS];\n+    npy_intp *shape_ptr = NULL;\n+    PyObject *dtype_obj = NULL;\n+    PyArray_Descr *dtype = NULL;\n+    char *convert_booleans_str = \"not_single\";\n+    int allow_single_boolean;\n+    static char *kwlist[] = {\"shape\", \"dtype\", \"convert_booleans\", NULL};\n+\n+    PyArrayObject *dummy_array;\n+    npy_intp dummy_strides[NPY_MAXDIMS] = {0};  /* TODO: Is this right? */\n+\n+    int index_type, index_num, fancy_ndim, ndim;\n+    npy_intp ellipis_dims = -1;\n+    npy_index_info indices[NPY_MAXDIMS * 2 + 1];\n+    char *index_type_str;\n+    PyObject *new_index, *tmp;\n+\n+    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|OOs:prepared\", kwlist,\n+                                     &shape_obj, &dtype_obj,\n+                                     &convert_booleans_str)) {\n+        return NULL;\n+    }\n+\n+    if (strcmp(convert_booleans_str, \"not_single\") == 0) {\n+        allow_single_boolean = 1;\n+    }\n+    else if (strcmp(convert_booleans_str,  \"always\") == 0) {\n+        allow_single_boolean = 0;\n+    }\n+    else {\n+        PyErr_SetString(PyExc_ValueError,\n+                \"`convert_booleans` must be either 'not_single' or \"\n+                \"'always'.\");\n+        return NULL;\n+    }\n+\n+    if (shape_obj == NULL || shape_obj == Py_None) {\n+        /* Use shape of intially bound array */\n+        if (!self->bound) {\n+            PyErr_SetString(PyExc_ValueError,\n+                    \"must pass in a shape if the indexing object is not \"\n+                    \"created for a specific array.\");\n+            return NULL;\n+        }\n+        dummy_ndim = self->orig_ndim;\n+        shape_ptr = self->orig_shape;\n+    }\n+    else {\n+        dummy_ndim = PyArray_IntpFromSequence(shape_obj, shape, NPY_MAXDIMS);\n+        if (dummy_ndim < 0) {\n+            return NULL;\n+        }\n+        shape_ptr = shape;\n+    }\n+\n+    /* Get the dtype (NewFromDescr will steal the reference) */\n+    if (self->bound && (dtype_obj == NULL || dtype_obj == Py_None)) {\n+        dtype = self->orig_dtype;\n+        Py_INCREF(dtype);\n+    }\n+    else {\n+        if (PyArray_DescrConverter(dtype_obj, &dtype) < 0) {\n+            return NULL;\n+        }\n+    }\n+\n+    /*\n+     * This should likely be refactored above, but for now create a dummy arr.\n+     * While easy for prepare_index, it is a bit less so `get_field_view`,\n+     * although that is unused when writing this, because plain indexing\n+     * cannot be used to create this object.\n+     */\n+    dummy_array = (PyArrayObject *)PyArray_NewFromDescr(\n+            &PyArray_Type, dtype,\n+            dummy_ndim, shape_ptr, dummy_strides,\n+            dummy_strides,  /* data will never be read */\n+            0, NULL);\n+\n+    /* Check if we have a field access going on */\n+    if (self->indexing_method == PLAIN_INDEXING &&\n+                    PyDataType_HASFIELDS(PyArray_DESCR(dummy_array))) {\n+        /* TODO: Untested, and cannot be reached at the moment */\n+        PyArrayObject *view;\n+        int ret = _get_field_view(dummy_array, self->index, &view, 1);\n+        if (ret == 0){\n+            Py_DECREF(dummy_array);\n+            if (view == NULL) {\n+                return NULL;\n+            }\n+            Py_DECREF(view);\n+\n+            return Py_BuildValue(\"{ss, sO}\",\n+                                 \"type\", \"field-access\",\n+                                 \"orig_index\", self->index);\n+        }\n+    }\n+    /* Normal multi-dimensional indexing */\n+    index_type = prepare_index(dummy_array, self->index, indices, &index_num,\n+                               &ndim, &fancy_ndim, allow_single_boolean,\n+                               self->indexing_method);\n+    Py_DECREF(dummy_array);\n+    if (index_type < 0) {\n+        return NULL;\n+    }\n+\n+    if (index_type == HAS_BOOL) {\n+        assert(index_num == 1);\n+        new_index = indices[0].object;\n+        Py_INCREF(new_index);\n+    }\n+    else {\n+        new_index = PyTuple_New(index_num);\n+        if (new_index == NULL) {\n+            return NULL;\n+        }\n+        for (i=0; i < index_num; i++) {\n+            if (indices[i].type & HAS_INTEGER) {\n+                tmp = PyLong_FromSsize_t(indices[i].value);\n+                if (tmp == NULL) {\n+                    goto failed_packing_tuple;\n+                }\n+                PyTuple_SET_ITEM(new_index, i, tmp);\n+                tuple_index++;\n+            }\n+            else if (indices[i].type & HAS_NEWAXIS) {\n+                Py_INCREF(Py_None);\n+                PyTuple_SET_ITEM(new_index, i, Py_None);\n+                tuple_index++;\n+            }\n+            else if (indices[i].type & HAS_SLICE) {\n+                Py_INCREF(indices[i].object);\n+                PyTuple_SET_ITEM(new_index, i, indices[i].object);\n+                tuple_index++;\n+            }\n+            else if (indices[i].type & HAS_ELLIPSIS) {\n+                Py_INCREF(Py_Ellipsis);\n+                PyTuple_SET_ITEM(new_index, i, Py_Ellipsis);\n+                ellipis_dims = indices[i].value;\n+            }\n+            else if (indices[i].type & HAS_BOOL) {\n+                /* if this happens should be the only element really */\n+                Py_INCREF(indices[i].object);\n+                PyTuple_SET_ITEM(new_index, i, indices[i].object);\n+                tuple_index++;\n+            }\n+            else if (indices[i].type & HAS_0D_BOOL) {\n+                /* if this happens should be the only element really */\n+                tmp = indices[i].value ? Py_True : Py_False;\n+                Py_INCREF(tmp);\n+                PyTuple_SET_ITEM(new_index, i, tmp);\n+                tuple_index++;\n+            }\n+            else if (indices[i].type & HAS_FANCY) {\n+                Py_INCREF(indices[i].object);\n+                PyTuple_SET_ITEM(new_index, i, indices[i].object);\n+                tuple_index++;\n+            }\n+            else {\n+                PyErr_SetString(PyExc_RuntimeError,\n+                                \"internal numpy error, please contact the \"\n+                                \"numpy developers.\");\n+                goto failed_packing_tuple;\n+            }\n+        }\n+    }\n+    /* Cleanup index */\n+    for (i=0; i < index_num; i++) {\n+        Py_XDECREF(indices[i].object);\n+    }\n+\n+    if (self->indexing_method == PLAIN_INDEXING) {\n+        index_type_str = \"plain\";\n+    }\n+    else if (self->indexing_method == VECTOR_INDEXING) {\n+        index_type_str = \"vindex\";\n+    }\n+    else if (self->indexing_method == OUTER_INDEXING) {\n+        index_type_str = \"oindex\";\n+    }\n+    else {\n+        index_type_str = \"lindex\";\n+    }\n+\n+    return Py_BuildValue(\n+                \"{ss, ss, sO, sO, sN, sO, sN, si}\",\n+                \"type\", \"index\",\n+                \"method\", index_type_str,\n+                \"orig_index\", self->index,\n+                \"view\", ((index_type & (HAS_FANCY|HAS_BOOL|HAS_SCALAR_ARRAY)) ||\n+                                            (index_type == HAS_INTEGER)) ?\n+                                Py_False : Py_True,\n+                \"simplified_index\", new_index,\n+                \"scalar\", (index_type == HAS_INTEGER) ? Py_True : Py_False,\n+                \"ellipsis_dims\", (ellipis_dims < 0) ?\n+                                       (Py_INCREF(Py_None), Py_None) :\n+                                       PyLong_FromSsize_t(ellipis_dims),\n+                \"result_ndim\", ndim);\n+\n+  failed_packing_tuple:\n+    for (i=0; i < index_num; i++) {\n+        Py_XDECREF(indices[i].object);\n+    }\n+    Py_DECREF(new_index);\n+    return NULL;\n+}\n+\n+\n+NPY_NO_EXPORT PyMethodDef multiindex_methods[] = {\n+\n+    /* for subtypes */\n+    {\"prepared\",\n+        (PyCFunction)multiindex_prepared,\n+        METH_VARARGS | METH_KEYWORDS, NULL},\n+    {NULL, NULL, 0, NULL}           /* sentinel */\n+};\n+\n+/*\n+ * MultiIndex object. This object holds just a tuple to a python index\n+ * as well as the information whether it was created through oindex/vindex\n+ * or not. If passed in as a numpy index (to plain indexing), it will work\n+ * like the corresponding vindex/oindex or legacy indexing method.\n+ */\n+NPY_NO_EXPORT PyTypeObject PyArrayMultiIndex_Type = {\n+#if defined(NPY_PY3K)\n+    PyVarObject_HEAD_INIT(NULL, 0)\n+#else\n+    PyObject_HEAD_INIT(NULL)\n+    0,                                          /* ob_size */\n+#endif\n+    \"numpy.MultiIndex\",                         /* tp_name */\n+    sizeof(PyArrayMultiIndex),                  /* tp_basicsize */\n+    0,                                          /* tp_itemsize */\n+    /* methods */\n+    (destructor)multiindex_dealloc,             /* tp_dealloc */\n+    0,                                          /* tp_print */\n+    0,                                          /* tp_getattr */\n+    0,                                          /* tp_setattr */\n+#if defined(NPY_PY3K)\n+    0,                                          /* tp_reserved */\n+#else\n+    0,                                          /* tp_compare */\n+#endif\n+    0,                                          /* tp_repr */\n+    0,                                          /* tp_as_number */\n+    &multiindex_as_mapping,                     /* tp_as_sequence */\n+    0,                                          /* tp_as_mapping */\n+    0,                                          /* tp_hash */\n+    0,                                          /* tp_call */\n+    0,                                          /* tp_str */\n+    0,                                          /* tp_getattro */\n+    0,                                          /* tp_setattro */\n+    0,                                          /* tp_as_buffer */\n+    Py_TPFLAGS_DEFAULT,                         /* tp_flags */\n+    \"Multi-dimensional indexing object helper\", /* tp_doc */\n+    0,                                          /* tp_traverse */\n+    0,                                          /* tp_clear */\n+    0,                                          /* tp_richcompare */\n+    0,                                          /* tp_weaklistoffset */\n+    0,                                          /* tp_iter */\n+    0,                                          /* tp_iternext */\n+    multiindex_methods,                         /* tp_methods */\n+    0,                                          /* tp_members */\n+    0,                                          /* tp_getset */\n+    0,                                          /* tp_base */\n+    0,                                          /* tp_dict */\n+    0,                                          /* tp_descr_get */\n+    0,                                          /* tp_descr_set */\n+    0,                                          /* tp_dictoffset */\n+    0,                                          /* tp_init */\n+    0,                                          /* tp_alloc */\n+    0,                                          /* tp_new */\n+    0,                                          /* tp_free */\n+    0,                                          /* tp_is_gc */\n+    0,                                          /* tp_bases */\n+    0,                                          /* tp_mro */\n+    0,                                          /* tp_cache */\n+    0,                                          /* tp_subclasses */\n+    0,                                          /* tp_weaklist */\n+    0,                                          /* tp_del */\n+#if PY_VERSION_HEX >= 0x02060000\n+    0,                                          /* tp_version_tag */\n+#endif\n+};\n+\n+\n+#undef HAS_INTEGER\n+#undef HAS_NEWAXIS\n+#undef HAS_SLICE\n+#undef HAS_ELLIPSIS\n+#undef HAS_FANCY\n+#undef HAS_BOOL\n+#undef HAS_SCALAR_ARRAY\n+#undef HAS_0D_BOOL\n+\n+"
            },
            {
                "filename": "numpy/core/src/multiarray/mapping.h",
                "patch": "@@ -3,6 +3,15 @@\n \n extern NPY_NO_EXPORT PyMappingMethods array_as_mapping;\n \n+/*\n+ * Plain indexing is python code writing arr[...]. Fancy means explicitly\n+ * the old behavious, legacy is mostly like fancy, but does not accept\n+ * array-likes as tuples.\n+ */\n+#define PLAIN_INDEXING 1\n+#define OUTER_INDEXING 2\n+#define VECTOR_INDEXING 4\n+#define FANCY_INDEXING 8\n \n /*\n  * Struct into which indices are parsed.\n@@ -23,6 +32,8 @@ typedef struct {\n     npy_intp value;\n     /* kind of index, see constants in mapping.c */\n     int type;\n+    /* original index number, mostly for boolean. -1 if implicit Ellipsis */\n+    int orig_index;\n } npy_index_info;\n \n \n@@ -42,11 +53,15 @@ NPY_NO_EXPORT PyObject *\n array_subscript_asarray(PyArrayObject *self, PyObject *op);\n \n NPY_NO_EXPORT PyObject *\n-array_subscript(PyArrayObject *self, PyObject *op);\n+array_subscript(PyArrayObject *self, PyObject *op, int indexing_method);\n \n NPY_NO_EXPORT int\n array_assign_item(PyArrayObject *self, Py_ssize_t i, PyObject *v);\n \n+NPY_NO_EXPORT int\n+array_assign_subscript(PyArrayObject *self, PyObject *ind, PyObject *op,\n+                       int indexing_method, int allow_getitem_hack);\n+\n /*\n  * Prototypes for Mapping calls --- not part of the C-API\n  * because only useful as part of a getitem call.\n@@ -69,5 +84,57 @@ PyArray_MapIterNew(npy_index_info *indices , int index_num, int index_type,\n                    PyArrayObject *arr, PyArrayObject *subspace,\n                    npy_uint32 subspace_iter_flags, npy_uint32 subspace_flags,\n                    npy_uint32 extra_op_flags, PyArrayObject *extra_op,\n-                   PyArray_Descr *extra_op_dtype);\n+                   PyArray_Descr *extra_op_dtype, int outer_indexing);\n+\n+/*\n+ * Prototypes for attribute indexing helper (arr.oindex, etc.)\n+ */\n+typedef struct {\n+        PyObject_HEAD\n+        /*\n+         * Attribute information portion.\n+         */\n+        PyArrayObject *array;\n+        int indexing_method;  /* See mapping.h */\n+} PyArrayAttributeIndexer;\n+\n+\n+NPY_NO_EXPORT PyObject *\n+PyArray_AttributeIndexerNew(PyArrayObject *array, int indexing_method);\n+\n+\n+/*\n+ * Prototypes for multi index objects passed to subclasses by arr.oindex, etc.\n+ */\n+typedef struct {\n+        PyObject_HEAD\n+        /*\n+         * Attribute information portion.\n+         */\n+        PyObject *index;            /* The indexing object */\n+        int indexing_method;        /* See mapping.h */\n+        /* If bound is 1, the following are information about the array */\n+        int bound;\n+        npy_intp orig_shape[NPY_MAXDIMS];\n+        int orig_ndim;\n+        PyArray_Descr *orig_dtype;\n+} PyArrayMultiIndex;\n+\n+\n+NPY_NO_EXPORT PyObject *\n+PyArray_MultiIndexNew(PyObject *index, PyArrayObject *array,\n+                      int indexing_method);\n+\n+\n+NPY_NO_EXPORT PyObject *\n+arrayattributeindexer_subscript(PyArrayAttributeIndexer *attr_indexer,\n+                                 PyObject *op);\n+\n+NPY_NO_EXPORT int\n+arrayattributeindexer_assign_subscript(PyArrayAttributeIndexer *attr_indexer,\n+                                        PyObject *op, PyObject *vals);\n+\n+\n+NPY_NO_EXPORT npy_intp\n+arrayattributeindexer_length(PyArrayAttributeIndexer *attr_indexer);\n #endif"
            },
            {
                "filename": "numpy/core/src/multiarray/methods.c",
                "patch": "@@ -21,6 +21,7 @@\n #include \"conversion_utils.h\"\n #include \"shape.h\"\n #include \"strfuncs.h\"\n+#include \"mapping.h\"\n \n #include \"methods.h\"\n #include \"alloc.h\"\n@@ -2457,6 +2458,7 @@ array_setslice(PyArrayObject *self, PyObject *args)\n \n #endif\n \n+\n NPY_NO_EXPORT PyMethodDef array_methods[] = {\n \n     /* for subtypes */"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -4703,6 +4703,12 @@ PyMODINIT_FUNC initmultiarray(void) {\n     if (PyType_Ready(&PyArrayMultiIter_Type) < 0) {\n         goto err;\n     }\n+    if (PyType_Ready(&PyArrayAttributeIndexer_Type) < 0) {\n+        goto err;\n+    }\n+    if (PyType_Ready(&PyArrayMultiIndex_Type) < 0) {\n+        goto err;\n+    }\n     PyArrayNeighborhoodIter_Type.tp_new = PyType_GenericNew;\n     if (PyType_Ready(&PyArrayNeighborhoodIter_Type) < 0) {\n         goto err;\n@@ -4781,6 +4787,11 @@ PyMODINIT_FUNC initmultiarray(void) {\n \n     PyDict_SetItemString(d, \"ndarray\", (PyObject *)&PyArray_Type);\n     PyDict_SetItemString(d, \"flatiter\", (PyObject *)&PyArrayIter_Type);\n+    Py_INCREF(&PyArrayMultiIter_Type);\n+    PyDict_SetItemString(d, \"_attributeindexer\", (PyObject *)&PyArrayAttributeIndexer_Type);\n+    Py_INCREF(&PyArrayAttributeIndexer_Type);\n+    PyDict_SetItemString(d, \"_multiindex\", (PyObject *)&PyArrayMultiIndex_Type);\n+    Py_INCREF(&PyArrayMultiIndex_Type);\n     PyDict_SetItemString(d, \"nditer\", (PyObject *)&NpyIter_Type);\n     PyDict_SetItemString(d, \"broadcast\",\n                          (PyObject *)&PyArrayMultiIter_Type);"
            },
            {
                "filename": "numpy/core/src/multiarray/scalartypes.c.src",
                "patch": "@@ -2327,7 +2327,7 @@ voidtype_subscript(PyVoidScalarObject *self, PyObject *ind)\n      * other cases (field names, empty tuple) will return either\n      * scalar or non-0d array. Compute this using ndarray subscript.\n      */\n-    ret = array_subscript((PyArrayObject *)res, ind);\n+    ret = array_subscript((PyArrayObject *)res, ind, PLAIN_INDEXING);\n     Py_DECREF(res);\n     return PyArray_Return((PyArrayObject*)ret);\n }\n@@ -3563,7 +3563,7 @@ gen_arrtype_subscript(PyObject *self, PyObject *key)\n \n     res = PyArray_FromScalar(self, NULL);\n \n-    ret = array_subscript((PyArrayObject *)res, key);\n+    ret = array_subscript((PyArrayObject *)res, key, PLAIN_INDEXING);\n     Py_DECREF(res);\n     if (ret == NULL) {\n         PyErr_SetString(PyExc_IndexError,"
            },
            {
                "filename": "numpy/core/tests/test_deprecations.py",
                "patch": "@@ -14,7 +14,7 @@\n import numpy as np\n from numpy.testing import (\n     assert_raises, assert_warns, assert_no_warnings, assert_array_equal,\n-    assert_\n+    assert_, suppress_warnings\n     )\n \n try:\n@@ -137,7 +137,8 @@ class _VisibleDeprecationTestCase(_DeprecationTestCase):\n class TestNonTupleNDIndexDeprecation(object):\n     def test_basic(self):\n         a = np.zeros((5, 5))\n-        with warnings.catch_warnings():\n+        with suppress_warnings() as sup:\n+            sup.filter(DeprecationWarning, \"more than two array indices found\")\n             warnings.filterwarnings('always')\n             assert_warns(FutureWarning, a.__getitem__, [[0, 1], [0, 1]])\n             assert_warns(FutureWarning, a.__getitem__, [slice(None)])"
            },
            {
                "filename": "numpy/core/tests/test_einsum.py",
                "patch": "@@ -694,7 +694,7 @@ def test_einsum_fixed_collapsingbug(self):\n         y1 = np.zeros((5, 5))\n         np.einsum('aabb->ab', x, out=y1)\n         idx = np.arange(5)\n-        y2 = x[idx[:, None], idx[:, None], idx, idx]\n+        y2 = x.vindex[idx[:, None], idx[:, None], idx, idx]\n         assert_equal(y1, y2)\n \n     def test_einsum_all_contig_non_contig_output(self):"
            },
            {
                "filename": "numpy/core/tests/test_indexing.py",
                "patch": "@@ -117,7 +117,8 @@ def test_same_kind_index_casting(self):\n         assert_array_equal(arr, np.arange(5)[:,None].repeat(2, axis=1))\n \n         arr = np.arange(25).reshape(5, 5)\n-        assert_array_equal(arr[u_index, u_index], arr[index, index])\n+        assert_array_equal(arr.vindex[u_index, u_index],\n+                           arr.lindex[index, index])\n \n     def test_empty_fancy_index(self):\n         # Empty list index creates an empty array\n@@ -290,7 +291,7 @@ def test_uncontiguous_subspace_assignment(self):\n     def test_too_many_fancy_indices_special_case(self):\n         # Just documents behaviour, this is a small limitation.\n         a = np.ones((1,) * 32)  # 32 is NPY_MAXDIMS\n-        assert_raises(IndexError, a.__getitem__, (np.array([0]),) * 32)\n+        assert_raises(IndexError, a.lindex.__getitem__, (np.array([0]),) * 32)\n \n     def test_scalar_array_bool(self):\n         # NumPy bools can be used as boolean index (python ones as of yet not)\n@@ -466,7 +467,7 @@ class TupleSubclass(tuple):\n             pass\n         index = ([1], [1])\n         index = TupleSubclass(index)\n-        assert_(arr[index].shape == (1,))\n+        assert_(arr.lindex[index].shape == (1,))\n         # Unlike the non nd-index:\n         assert_(arr[index,].shape != (1,))\n \n@@ -507,7 +508,7 @@ def test_indexing_array_weird_strides(self):\n         assert_array_equal(x[ind], x[ind.copy()])\n         # higher dimensional advanced index\n         zind = np.zeros(4, dtype=np.intp)\n-        assert_array_equal(x2[ind, zind], x2[ind.copy(), zind])\n+        assert_array_equal(x2.lindex[ind, zind], x2.vindex[ind.copy(), zind])\n \n     def test_indexing_array_negative_strides(self):\n         # From gh-8264,\n@@ -541,7 +542,7 @@ def test_prepending_ones(self):\n         a[[0, 1, 2], :] = np.ones((1, 3, 2))\n         a[:, [0, 1]] = np.ones((1, 3, 2))\n         # Fancy without subspace (with broadcasting)\n-        a[[[0], [1], [2]], [0, 1]] = np.ones((1, 3, 2))\n+        a.vindex[[[0], [1], [2]], [0, 1]] = np.ones((1, 3, 2))\n \n     def test_prepend_not_one(self):\n         assign = self.assign\n@@ -567,7 +568,7 @@ def test_simple_broadcasting_errors(self):\n     def test_index_is_larger(self):\n         # Simple case of fancy index broadcasting of the index.\n         a = np.zeros((5, 5))\n-        a[[[0], [1], [2]], [0, 1, 2]] = [2, 3, 4]\n+        a.vindex[[[0], [1], [2]], [0, 1, 2]] = [2, 3, 4]\n \n         assert_((a[:3, :3] == [2, 3, 4]).all())\n \n@@ -673,8 +674,9 @@ def test_boolean_index_cast_assign(self):\n         assert_equal(zero_array[0, 1], 1)\n \n         # Fancy indexing works, although we get a cast warning.\n-        assert_warns(np.ComplexWarning,\n-                     zero_array.__setitem__, ([0], [1]), np.array([2 + 1j]))\n+        assert_warns(\n+            np.ComplexWarning,\n+            zero_array.lindex.__setitem__, ([0], [1]), np.array([2 + 1j]))\n         assert_equal(zero_array[0, 1], 2)  # No complex part\n \n         # Cast complex to float, throwing away the imaginary portion.\n@@ -1031,13 +1033,15 @@ def _check_multi_index(self, arr, index):\n             Index being tested.\n         \"\"\"\n         # Test item getting\n+        if all(_ is not Ellipsis for _ in index):\n+            index = index + (Ellipsis,)\n         try:\n             mimic_get, no_copy = self._get_multi_index(arr, index)\n         except Exception as e:\n             if HAS_REFCOUNT:\n                 prev_refcount = sys.getrefcount(arr)\n-            assert_raises(type(e), arr.__getitem__, index)\n-            assert_raises(type(e), arr.__setitem__, index, 0)\n+            assert_raises(type(e), arr.lindex.__getitem__, index)\n+            assert_raises(type(e), arr.lindex.__setitem__, index, 0)\n             if HAS_REFCOUNT:\n                 assert_equal(prev_refcount, sys.getrefcount(arr))\n             return\n@@ -1066,13 +1070,17 @@ def _check_single_index(self, arr, index):\n                 assert_equal(prev_refcount, sys.getrefcount(arr))\n             return\n \n-        self._compare_index_result(arr, index, mimic_get, no_copy)\n+        self._compare_index_result(arr, index, mimic_get, no_copy, lindex=False)\n \n-    def _compare_index_result(self, arr, index, mimic_get, no_copy):\n+    def _compare_index_result(self, arr, index, mimic_get, no_copy,\n+            lindex=True):\n         \"\"\"Compare mimicked result to indexing result.\n         \"\"\"\n         arr = arr.copy()\n-        indexed_arr = arr[index]\n+        if lindex:\n+            indexed_arr = arr.lindex[index]\n+        else:\n+            indexed_arr = arr[index]\n         assert_array_equal(indexed_arr, mimic_get)\n         # Check if we got a view, unless its a 0-sized or 0-d array.\n         # (then its not a view, and that does not matter)\n@@ -1088,7 +1096,11 @@ def _compare_index_result(self, arr, index, mimic_get, no_copy):\n \n         # Test non-broadcast setitem:\n         b = arr.copy()\n-        b[index] = mimic_get + 1000\n+        if lindex:\n+            b.lindex[index] = mimic_get + 1000\n+        else:\n+            b[index] = mimic_get + 1000\n+\n         if b.size == 0:\n             return  # nothing to compare here...\n         if no_copy and indexed_arr.ndim != 0:\n@@ -1223,9 +1235,9 @@ def test_bool_as_int_argument_errors(self):\n     def test_boolean_indexing_weirdness(self):\n         # Weird boolean indexing things\n         a = np.ones((2, 3, 4))\n-        a[False, True, ...].shape == (0, 2, 3, 4)\n-        a[True, [0, 1], True, True, [1], [[2]]] == (1, 2)\n-        assert_raises(IndexError, lambda: a[False, [0, 1], ...])\n+        a.lindex[False, True, ...].shape == (0, 2, 3, 4)\n+        a.lindex[True, [0, 1], True, True, [1], [[2]]] == (1, 2)\n+        assert_raises(IndexError, lambda: a.lindex[False, [0, 1], ...])\n \n \n class TestArrayToIndexDeprecation(object):"
            },
            {
                "filename": "numpy/core/tests/test_memmap.py",
                "patch": "@@ -127,7 +127,8 @@ def test_arithmetic_drops_references(self):\n     def test_indexing_drops_references(self):\n         fp = memmap(self.tmpfp, dtype=self.dtype, mode='w+',\n                     shape=self.shape)\n-        tmp = fp[(1, 2), (2, 3)]\n+\n+        tmp = fp.vindex[(1, 2), (2, 3)]\n         if isinstance(tmp, memmap):\n             assert_(tmp._mmap is not fp._mmap)\n "
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -2410,8 +2410,8 @@ def test_partition(self):\n                        msg=\"%d: %r <= %r\" % (i, p[:, i], p[:, :i].T))\n                     at((p[:, i + 1:].T > p[:, i]).all(),\n                        msg=\"%d: %r < %r\" % (i, p[:, i], p[:, i + 1:].T))\n-                    aae(p, d1[np.arange(d1.shape[0])[:, None],\n-                        np.argpartition(d1, i, axis=1, kind=k)])\n+                    aae(p, d1.vindex[np.arange(d1.shape[0])[:, None],\n+                                     np.argpartition(d1, i, axis=1, kind=k)])\n \n                     p = np.partition(d0, i, axis=0, kind=k)\n                     aae(p[i, :], np.array([i] * d1.shape[0], dtype=dt))\n@@ -2420,8 +2420,8 @@ def test_partition(self):\n                        msg=\"%d: %r <= %r\" % (i, p[i, :], p[:i, :]))\n                     at((p[i + 1:, :] > p[i, :]).all(),\n                        msg=\"%d: %r < %r\" % (i, p[i, :], p[:, i + 1:]))\n-                    aae(p, d0[np.argpartition(d0, i, axis=0, kind=k),\n-                        np.arange(d0.shape[1])[None, :]])\n+                    aae(p, d0.vindex[np.argpartition(d0, i, axis=0, kind=k),\n+                                     np.arange(d0.shape[1])[None, :]])\n \n                     # check inplace\n                     dc = d.copy()\n@@ -2498,14 +2498,14 @@ def test_partition_iterative(self):\n \n             kth = (1, 6, 7, -1)\n             p = np.partition(d1, kth, axis=1)\n-            pa = d1[np.arange(d1.shape[0])[:, None],\n-                    d1.argpartition(kth, axis=1)]\n+            pa = d1.vindex[np.arange(d1.shape[0])[:, None],\n+                           d1.argpartition(kth, axis=1)]\n             assert_array_equal(p, pa)\n             for i in range(d1.shape[0]):\n                 self.assert_partitioned(p[i,:], kth)\n             p = np.partition(d0, kth, axis=0)\n-            pa = d0[np.argpartition(d0, kth, axis=0),\n-                    np.arange(d0.shape[1])[None,:]]\n+            pa = d0.vindex[np.argpartition(d0, kth, axis=0),\n+                           np.arange(d0.shape[1])[None, :]]\n             assert_array_equal(p, pa)\n             for i in range(d0.shape[1]):\n                 self.assert_partitioned(p[:, i], kth)\n@@ -2963,9 +2963,10 @@ def test_swapaxes(self):\n                     # check array contents\n                     i0, i1, i2, i3 = [dim-1 for dim in c.shape]\n                     j0, j1, j2, j3 = [dim-1 for dim in src.shape]\n-                    assert_equal(src[idx[j0], idx[j1], idx[j2], idx[j3]],\n-                                 c[idx[i0], idx[i1], idx[i2], idx[i3]],\n-                                 str((i, j, k)))\n+                    assert_equal(\n+                        src.vindex[idx[j0], idx[j1], idx[j2], idx[j3]],\n+                        c.vindex[idx[i0], idx[i1], idx[i2], idx[i3]],\n+                        str((i, j, k)))\n                     # check a view is always returned, gh-5260\n                     assert_(not c.flags['OWNDATA'], str((i, j, k)))\n                     # check on non-contiguous input array"
            },
            {
                "filename": "numpy/core/tests/test_numeric.py",
                "patch": "@@ -1174,7 +1174,7 @@ def test_boolean(self):\n         V = rand(5, 8)\n         g1 = randint(0, 5, size=15)\n         g2 = randint(0, 8, size=15)\n-        V[g1, g2] = -V[g1, g2]\n+        V.vindex[g1, g2] = -V.vindex[g1, g2]\n         assert_((np.array([a[0][V > 0], a[1][V > 0], a[2][V > 0]]) == a[:, V > 0]).all())\n \n     def test_boolean_edgecase(self):\n@@ -2411,23 +2411,23 @@ def test_results(self):\n             # positive axis, positive start\n             res = np.rollaxis(a, axis=i, start=j)\n             i0, i1, i2, i3 = aind[np.array(res.shape) - 1]\n-            assert_(np.all(res[i0, i1, i2, i3] == a))\n+            assert_(np.all(res.vindex[i0, i1, i2, i3] == a))\n             assert_(res.shape == self.tgtshape[(i, j)], str((i,j)))\n             assert_(not res.flags['OWNDATA'])\n \n             # negative axis, positive start\n             ip = i + 1\n             res = np.rollaxis(a, axis=-ip, start=j)\n             i0, i1, i2, i3 = aind[np.array(res.shape) - 1]\n-            assert_(np.all(res[i0, i1, i2, i3] == a))\n+            assert_(np.all(res.vindex[i0, i1, i2, i3] == a))\n             assert_(res.shape == self.tgtshape[(4 - ip, j)])\n             assert_(not res.flags['OWNDATA'])\n \n             # positive axis, negative start\n             jp = j + 1 if j < 4 else j\n             res = np.rollaxis(a, axis=i, start=-jp)\n             i0, i1, i2, i3 = aind[np.array(res.shape) - 1]\n-            assert_(np.all(res[i0, i1, i2, i3] == a))\n+            assert_(np.all(res.vindex[i0, i1, i2, i3] == a))\n             assert_(res.shape == self.tgtshape[(i, 4 - jp)])\n             assert_(not res.flags['OWNDATA'])\n \n@@ -2436,7 +2436,7 @@ def test_results(self):\n             jp = j + 1 if j < 4 else j\n             res = np.rollaxis(a, axis=-ip, start=-jp)\n             i0, i1, i2, i3 = aind[np.array(res.shape) - 1]\n-            assert_(np.all(res[i0, i1, i2, i3] == a))\n+            assert_(np.all(res.vindex[i0, i1, i2, i3] == a))\n             assert_(res.shape == self.tgtshape[(4 - ip, 4 - jp)])\n             assert_(not res.flags['OWNDATA'])\n "
            },
            {
                "filename": "numpy/core/tests/test_regression.py",
                "patch": "@@ -658,7 +658,7 @@ def test_array_index(self):\n         # Make sure optimization is not called in this case.\n         a = np.array([1, 2, 3])\n         a2 = np.array([[1, 2, 3]])\n-        assert_equal(a[np.where(a == 3)], a2[np.where(a2 == 3)])\n+        assert_equal(a[np.where(a == 3)], a2.vindex[np.where(a2 == 3)])\n \n     def test_object_argmax(self):\n         a = np.array([1, 2, 3], dtype=object)\n@@ -1461,7 +1461,7 @@ def test_structured_arrays_with_objects1(self):\n         stra = 'aaaa'\n         strb = 'bbbb'\n         x = np.array([[(0, stra), (1, strb)]], 'i8,O')\n-        x[x.nonzero()] = x.ravel()[:1]\n+        x.vindex[x.nonzero()] = x.ravel()[:1]\n         assert_(x[0, 1] == x[0, 0])\n \n     @pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n@@ -1472,7 +1472,7 @@ def test_structured_arrays_with_objects2(self):\n         numb = sys.getrefcount(strb)\n         numa = sys.getrefcount(stra)\n         x = np.array([[(0, stra), (1, strb)]], 'i8,O')\n-        x[x.nonzero()] = x.ravel()[:1]\n+        x.vindex[x.nonzero()] = x.ravel()[:1]\n         assert_(sys.getrefcount(strb) == numb)\n         assert_(sys.getrefcount(stra) == numa + 2)\n "
            },
            {
                "filename": "numpy/lib/shape_base.py",
                "patch": "@@ -22,7 +22,7 @@\n \n \n def _make_along_axis_idx(arr_shape, indices, axis):\n-\t# compute dimensions to iterate over\n+    # compute dimensions to iterate over\n     if not _nx.issubdtype(indices.dtype, _nx.integer):\n         raise IndexError('`indices` must be an integer array')\n     if len(arr_shape) != indices.ndim:\n@@ -31,17 +31,17 @@ def _make_along_axis_idx(arr_shape, indices, axis):\n     shape_ones = (1,) * indices.ndim\n     dest_dims = list(range(axis)) + [None] + list(range(axis+1, indices.ndim))\n \n-    # build a fancy index, consisting of orthogonal aranges, with the\n+    # build a vector index, consisting of orthogonal aranges, with the\n     # requested index inserted at the right location\n-    fancy_index = []\n+    vector_index = []\n     for dim, n in zip(dest_dims, arr_shape):\n         if dim is None:\n-            fancy_index.append(indices)\n+            vector_index.append(indices)\n         else:\n             ind_shape = shape_ones[:dim] + (-1,) + shape_ones[dim+1:]\n-            fancy_index.append(_nx.arange(n).reshape(ind_shape))\n+            vector_index.append(_nx.arange(n).reshape(ind_shape))\n \n-    return tuple(fancy_index)\n+    return tuple(vector_index)\n \n \n def take_along_axis(arr, indices, axis):\n@@ -149,15 +149,16 @@ def take_along_axis(arr, indices, axis):\n     \"\"\"\n     # normalize inputs\n     if axis is None:\n-        arr = arr.flat\n         arr_shape = (len(arr),)  # flatiter has no .shape\n         axis = 0\n+        # TODO: Make along axis needed here for error checks probably?\n+        return arr.flat[_make_along_axis_idx(arr_shape, indices, axis)]\n     else:\n         axis = normalize_axis_index(axis, arr.ndim)\n         arr_shape = arr.shape\n \n-    # use the fancy index\n-    return arr[_make_along_axis_idx(arr_shape, indices, axis)]\n+    # use the vector index\n+    return arr.vindex[_make_along_axis_idx(arr_shape, indices, axis)]\n \n \n def put_along_axis(arr, indices, values, axis):\n@@ -234,15 +235,17 @@ def put_along_axis(arr, indices, values, axis):\n     \"\"\"\n     # normalize inputs\n     if axis is None:\n-        arr = arr.flat\n         axis = 0\n         arr_shape = (len(arr),)  # flatiter has no .shape\n+        # Use the vector index (it is just one in this case)\n+        arr.flat[_make_along_axis_idx(arr_shape, indices, axis)] = values\n+        return\n     else:\n         axis = normalize_axis_index(axis, arr.ndim)\n         arr_shape = arr.shape\n \n-    # use the fancy index\n-    arr[_make_along_axis_idx(arr_shape, indices, axis)] = values\n+    # use the vector index\n+    arr.vindex[_make_along_axis_idx(arr_shape, indices, axis)] = values\n \n \n def apply_along_axis(func1d, axis, arr, *args, **kwargs):"
            },
            {
                "filename": "numpy/lib/tests/test_arraysetops.py",
                "patch": "@@ -59,8 +59,8 @@ def test_intersect1d_indices(self):\n         ui1 = np.unravel_index(i1, a.shape)\n         ui2 = np.unravel_index(i2, b.shape)\n         ea = np.array([2, 6, 7, 8])\n-        assert_array_equal(ea, a[ui1])\n-        assert_array_equal(ea, b[ui2])\n+        assert_array_equal(ea, a.vindex[ui1])\n+        assert_array_equal(ea, b.vindex[ui2])\n     \n         # non1d, not assumed to be uniqueinputs\n         a = np.array([[2, 4, 5, 6, 6], [4, 7, 8, 7, 2]])\n@@ -69,8 +69,8 @@ def test_intersect1d_indices(self):\n         ui1 = np.unravel_index(i1, a.shape)\n         ui2 = np.unravel_index(i2, b.shape)\n         ea = np.array([2, 7, 8])\n-        assert_array_equal(ea, a[ui1])\n-        assert_array_equal(ea, b[ui2])\n+        assert_array_equal(ea, a.vindex[ui1])\n+        assert_array_equal(ea, b.vindex[ui2])\n         \n     def test_setxor1d(self):\n         a = np.array([5, 7, 1, 2])"
            },
            {
                "filename": "numpy/lib/tests/test_histograms.py",
                "patch": "@@ -569,7 +569,7 @@ def test_simple(self):\n         assert_array_equal(H, answer)\n \n         Z = np.zeros((5, 5, 5))\n-        Z[list(range(5)), list(range(5)), list(range(5))] = 1.\n+        Z.vindex[list(range(5)), list(range(5)), list(range(5))] = 1.\n         H, edges = histogramdd([np.arange(5), np.arange(5), np.arange(5)], 5)\n         assert_array_equal(H, Z)\n "
            },
            {
                "filename": "numpy/lib/tests/test_index_tricks.py",
                "patch": "@@ -326,7 +326,7 @@ def test_diag_indices():\n                   [5, 6, 7, 8],\n                   [9, 10, 11, 12],\n                   [13, 14, 15, 16]])\n-    a[di] = 100\n+    a.vindex[di] = 100\n     assert_array_equal(\n         a, np.array([[100, 2, 3, 4],\n                      [5, 100, 7, 8],\n@@ -339,7 +339,7 @@ def test_diag_indices():\n \n     # And use it to set the diagonal of a zeros array to 1:\n     a = np.zeros((2, 2, 2), int)\n-    a[d3] = 1\n+    a.vindex[d3] = 1\n     assert_array_equal(\n         a, np.array([[[1, 0],\n                       [0, 0]],"
            },
            {
                "filename": "numpy/lib/tests/test_nanfunctions.py",
                "patch": "@@ -607,7 +607,7 @@ def test_keepdims(self):\n         # Randomly set some elements to NaN:\n         w = np.random.random((4, 200)) * np.array(d.shape)[:, None]\n         w = w.astype(np.intp)\n-        d[tuple(w)] = np.nan\n+        d.vindex[tuple(w)] = np.nan\n         with suppress_warnings() as sup:\n             sup.filter(RuntimeWarning)\n             res = np.nanmedian(d, axis=None, keepdims=True)\n@@ -773,7 +773,7 @@ def test_keepdims(self):\n         # Randomly set some elements to NaN:\n         w = np.random.random((4, 200)) * np.array(d.shape)[:, None]\n         w = w.astype(np.intp)\n-        d[tuple(w)] = np.nan\n+        d.vindex[tuple(w)] = np.nan\n         with suppress_warnings() as sup:\n             sup.filter(RuntimeWarning)\n             res = np.nanpercentile(d, 90, axis=None, keepdims=True)"
            },
            {
                "filename": "numpy/lib/tests/test_regression.py",
                "patch": "@@ -145,13 +145,13 @@ def dp():\n             n = 3\n             a = np.ones((n,)*5)\n             i = np.random.randint(0, n, size=thesize)\n-            a[np.ix_(i, i, i, i, i)] = 0\n+            a.lindex[np.ix_(i, i, i, i, i)] = 0\n \n         def dp2():\n             n = 3\n             a = np.ones((n,)*5)\n             i = np.random.randint(0, n, size=thesize)\n-            a[np.ix_(i, i, i, i, i)]\n+            a.lindex[np.ix_(i, i, i, i, i)]\n \n         assert_raises(ValueError, dp)\n         assert_raises(ValueError, dp2)"
            },
            {
                "filename": "numpy/lib/tests/test_twodim_base.py",
                "patch": "@@ -359,10 +359,10 @@ def test_mask_indices():\n     # simple test without offset\n     iu = mask_indices(3, np.triu)\n     a = np.arange(9).reshape(3, 3)\n-    assert_array_equal(a[iu], array([0, 1, 2, 4, 5, 8]))\n+    assert_array_equal(a.vindex[iu], array([0, 1, 2, 4, 5, 8]))\n     # Now with an offset\n     iu1 = mask_indices(3, np.triu, 1)\n-    assert_array_equal(a[iu1], array([1, 2, 5]))\n+    assert_array_equal(a.vindex[iu1], array([1, 2, 5]))\n \n \n def test_tril_indices():\n@@ -379,32 +379,32 @@ def test_tril_indices():\n     b = np.arange(1, 21).reshape(4, 5)\n \n     # indexing:\n-    assert_array_equal(a[il1],\n+    assert_array_equal(a.vindex[il1],\n                        array([1, 5, 6, 9, 10, 11, 13, 14, 15, 16]))\n-    assert_array_equal(b[il3],\n+    assert_array_equal(b.vindex[il3],\n                        array([1, 6, 7, 11, 12, 13, 16, 17, 18, 19]))\n \n     # And for assigning values:\n-    a[il1] = -1\n+    a.vindex[il1] = -1\n     assert_array_equal(a,\n                        array([[-1, 2, 3, 4],\n                               [-1, -1, 7, 8],\n                               [-1, -1, -1, 12],\n                               [-1, -1, -1, -1]]))\n-    b[il3] = -1\n+    b.vindex[il3] = -1\n     assert_array_equal(b,\n                        array([[-1, 2, 3, 4, 5],\n                               [-1, -1, 8, 9, 10],\n                               [-1, -1, -1, 14, 15],\n                               [-1, -1, -1, -1, 20]]))\n     # These cover almost the whole array (two diagonals right of the main one):\n-    a[il2] = -10\n+    a.vindex[il2] = -10\n     assert_array_equal(a,\n                        array([[-10, -10, -10, 4],\n                               [-10, -10, -10, -10],\n                               [-10, -10, -10, -10],\n                               [-10, -10, -10, -10]]))\n-    b[il4] = -10\n+    b.vindex[il4] = -10\n     assert_array_equal(b,\n                        array([[-10, -10, -10, 4, 5],\n                               [-10, -10, -10, -10, 10],\n@@ -426,20 +426,20 @@ def test_triu_indices(self):\n         b = np.arange(1, 21).reshape(4, 5)\n \n         # Both for indexing:\n-        assert_array_equal(a[iu1],\n+        assert_array_equal(a.vindex[iu1],\n                            array([1, 2, 3, 4, 6, 7, 8, 11, 12, 16]))\n-        assert_array_equal(b[iu3],\n+        assert_array_equal(b.vindex[iu3],\n                            array([1, 2, 3, 4, 5, 7, 8, 9,\n                                   10, 13, 14, 15, 19, 20]))\n \n         # And for assigning values:\n-        a[iu1] = -1\n+        a.vindex[iu1] = -1\n         assert_array_equal(a,\n                            array([[-1, -1, -1, -1],\n                                   [5, -1, -1, -1],\n                                   [9, 10, -1, -1],\n                                   [13, 14, 15, -1]]))\n-        b[iu3] = -1\n+        b.vindex[iu3] = -1\n         assert_array_equal(b,\n                            array([[-1, -1, -1, -1, -1],\n                                   [6, -1, -1, -1, -1],\n@@ -448,13 +448,13 @@ def test_triu_indices(self):\n \n         # These cover almost the whole array (two diagonals right of the\n         # main one):\n-        a[iu2] = -10\n+        a.vindex[iu2] = -10\n         assert_array_equal(a,\n                            array([[-1, -1, -10, -10],\n                                   [5, -1, -1, -10],\n                                   [9, 10, -1, -1],\n                                   [13, 14, 15, -1]]))\n-        b[iu4] = -10\n+        b.vindex[iu4] = -10\n         assert_array_equal(b,\n                            array([[-1, -1, -10, -10, -10],\n                                   [6, -1, -1, -10, -10],"
            },
            {
                "filename": "numpy/ma/extras.py",
                "patch": "@@ -749,7 +749,6 @@ def _median(a, axis=None, out=None, overwrite_input=False):\n \n     counts = count(asorted, axis=axis, keepdims=True)\n     h = counts // 2\n-\n     # duplicate high if odd number of elements so mean does nothing\n     odd = counts % 2 == 1\n     l = np.where(odd, h, h-1)"
            },
            {
                "filename": "numpy/matrixlib/tests/test_defmatrix.py",
                "patch": "@@ -309,7 +309,7 @@ def test_instance_methods(self):\n             'partition', 'argpartition',\n             'take', 'tofile', 'tolist', 'tostring', 'tobytes', 'all', 'any',\n             'sum', 'argmax', 'argmin', 'min', 'max', 'mean', 'var', 'ptp',\n-            'prod', 'std', 'ctypes', 'itemset',\n+            'prod', 'std', 'ctypes', 'itemset', 'vindex', 'oindex', 'lindex',\n             ]\n         for attrib in dir(a):\n             if attrib.startswith('_') or attrib in excluded_methods:\n@@ -366,7 +366,7 @@ def test_fancy_indexing(self):\n         x = a[[1, 0]]\n         assert_(isinstance(x, matrix))\n         assert_equal(x, matrix([[3,  4], [1, 2]]))\n-        x = a[[[1], [0]], [[1, 0], [0, 1]]]\n+        x = a.vindex[[[1], [0]], [[1, 0], [0, 1]]]\n         assert_(isinstance(x, matrix))\n         assert_equal(x, matrix([[4,  3], [1,  2]]))\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 15777,
        "body": "<!-- Please be sure you are following the instructions in the dev guidelines\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html\r\n-->\r\n\r\n<!-- We'd appreciate it if your commit message is properly formatted\r\nhttp://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n-->\r\n\r\nClose #8720, at the cost of behavior changes in the `resids` return value. Marking as draft since I am publishing it primarily to facilitate discussion at that issue.\r\n",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/15777.compatibility.rst",
                "patch": "@@ -0,0 +1,11 @@\n+Changes to the ``residuals`` return value of `numpy.linalg.lstsq`\n+-----------------------------------------------------------------\n+\n+This no longer returns a ``(0,)``-shaped array if no residuals are\n+computed, instead returning `NaN`.\n+\n+For well-determined problems with ``rank == N == M``, this is now\n+an array of zeros.\n+\n+When ``b`` is a 1d array, this is now a 0d array rather than a\n+``(1,)``-shaped array."
            },
            {
                "filename": "numpy/linalg/linalg.py",
                "patch": "@@ -2180,11 +2180,14 @@ def lstsq(a, b, rcond=\"warn\"):\n     Euclidean 2-norm :math:`||b - ax||`. If there are multiple minimizing \n     solutions, the one with the smallest 2-norm :math:`||x||` is returned.\n \n+    .. versionchanged:: 1.19\n+       Can now operate on stacks of matrices\n+\n     Parameters\n     ----------\n-    a : (M, N) array_like\n+    a : (..., M, N) array_like\n         \"Coefficient\" matrix.\n-    b : {(M,), (M, K)} array_like\n+    b : {(M,), (..., M, K)} array_like\n         Ordinate or \"dependent variable\" values. If `b` is two-dimensional,\n         the least-squares solution is calculated for each of the `K` columns\n         of `b`.\n@@ -2203,17 +2206,27 @@ def lstsq(a, b, rcond=\"warn\"):\n \n     Returns\n     -------\n-    x : {(N,), (N, K)} ndarray\n+    x : {(N,), (..., N, K)} ndarray\n         Least-squares solution. If `b` is two-dimensional,\n         the solutions are in the `K` columns of `x`.\n-    residuals : {(1,), (K,), (0,)} ndarray\n+    residuals : {(), (..., K,)} ndarray\n         Sums of squared residuals: Squared Euclidean 2-norm for each column in\n         ``b - a @ x``.\n-        If the rank of `a` is < N or M <= N, this is an empty array.\n-        If `b` is 1-dimensional, this is a (1,) shape array.\n+        If the rank of `a` is < N (which is always true if M < N), this is\n+        filled with NaN.\n+        If `b` is 1-dimensional, this is a 0d array.\n         Otherwise the shape is (K,).\n+\n+        .. versionchanged:: 1.21.0\n+            This no longer returns a ``(0,)``-shaped array if no residuals are\n+            computed, instead returning `NaN`.\n+            For well-determined problems with ``rank == N == M``, this is now\n+            an array of zeros.\n+            When ``b`` is a 1d array, this is now a 0d array rather than a\n+            ``(1,)``-shaped array.\n+\n     rank : int\n-        Rank of matrix `a`.\n+        Rank of `a`. Note that this is at most ``min(M, N)``.\n     s : (min(M, N),) ndarray\n         Singular values of `a`.\n \n@@ -2268,7 +2281,7 @@ def lstsq(a, b, rcond=\"warn\"):\n     is_1d = b.ndim == 1\n     if is_1d:\n         b = b[:, newaxis]\n-    _assert_2d(a, b)\n+    _assert_stacked_2d(a, b)\n     m, n = a.shape[-2:]\n     m2, n_rhs = b.shape[-2:]\n     if m != m2:\n@@ -2314,12 +2327,7 @@ def lstsq(a, b, rcond=\"warn\"):\n     # remove the axis we added\n     if is_1d:\n         x = x.squeeze(axis=-1)\n-        # we probably should squeeze resids too, but we can't\n-        # without breaking compatibility.\n-\n-    # as documented\n-    if rank != n or m <= n:\n-        resids = array([], result_real_t)\n+        resids = resids.squeeze(axis=-1)\n \n     # coerce output arrays\n     s = s.astype(result_real_t, copy=False)"
            },
            {
                "filename": "numpy/linalg/tests/test_linalg.py",
                "patch": "@@ -902,31 +902,33 @@ def test_0_size(self):\n         assert_(res[1].dtype.type is np.float64)\n \n \n-class LstsqCases(LinalgSquareTestCase, LinalgNonsquareTestCase):\n+class LstsqCases(LinalgSquareTestCase,\n+                 LinalgNonsquareTestCase,\n+                 LinalgGeneralizedSquareTestCase,\n+                 LinalgGeneralizedNonsquareTestCase):\n \n     def do(self, a, b, tags):\n         arr = np.asarray(a)\n-        m, n = arr.shape\n+        m, n = arr.shape[-2:]\n         u, s, vt = linalg.svd(a, False)\n         x, residuals, rank, sv = linalg.lstsq(a, b, rcond=-1)\n         if m == 0:\n             assert_((x == 0).all())\n         if m <= n:\n-            assert_almost_equal(b, dot(a, x))\n+            assert_almost_equal(b, a @ x)\n             assert_equal(rank, m)\n         else:\n             assert_equal(rank, n)\n         assert_almost_equal(sv, sv.__array_wrap__(s))\n-        if rank == n and m > n:\n-            expect_resids = (\n-                np.asarray(abs(np.dot(a, x) - b)) ** 2).sum(axis=0)\n-            expect_resids = np.asarray(expect_resids)\n-            if np.asarray(b).ndim == 1:\n-                expect_resids.shape = (1,)\n-                assert_equal(residuals.shape, expect_resids.shape)\n-        else:\n-            expect_resids = np.array([]).view(type(x))\n+        expect_resids = np.where(\n+            rank == n,\n+            # asarray prevents `**` being interpreted as matrix_power\n+            (abs(np.asarray(a @ x - b)) ** 2).sum(axis=0),\n+            # lapack does not compute this for us, so we do not return it\n+            np.nan\n+        ).view(type(x))\n         assert_almost_equal(residuals, expect_resids)\n+        assert_equal(residuals.shape, expect_resids.shape)\n         assert_(np.issubdtype(residuals.dtype, np.floating))\n         assert_(consistent_subclass(x, b))\n         assert_(consistent_subclass(residuals, b))\n@@ -967,7 +969,7 @@ def test_empty_a_b(self, m, n, n_rhs):\n         if m == 0:\n             assert_((x == 0).all())\n         assert_equal(x.shape, (n, n_rhs))\n-        assert_equal(residuals.shape, ((n_rhs,) if m > n else (0,)))\n+        assert_equal(residuals.shape, (n_rhs,))\n         if m > n and n_rhs > 0:\n             # residuals are exactly the squared norms of b's columns\n             r = b - np.dot(a, x)"
            },
            {
                "filename": "numpy/linalg/umath_linalg.c.src",
                "patch": "@@ -3198,7 +3198,6 @@ static void\n                 *(npy_int*) args[5] = params.RANK;\n                 delinearize_@REALTYPE@_matrix(args[6], params.S, &s_out);\n \n-                /* Note that linalg.lstsq discards this when excess == 0 */\n                 if (excess >= 0 && params.RANK == n) {\n                     /* Compute the residuals as the square sum of each column */\n                     int i;\n@@ -3215,7 +3214,7 @@ static void\n                     }\n                 }\n                 else {\n-                    /* Note that this is always discarded by linalg.lstsq */\n+                    /* Not computed for free by lapack, so we don't include it. */\n                     nan_@REALTYPE@_matrix(args[4], &r_out);\n                 }\n             } else {"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 8615,
        "body": "Fixes #7552\r\n\r\n\r\nThis enables code like the following, by exposing a `keys` method:\r\n\r\n```python\r\ndef foo(a, b):\r\n    print(a, b)\r\n\r\ndata = np.empty(20, dtype=[('a', np.float32), ('b', np.float32)])\r\nfoo(**data)\r\n\r\ndata_as_dict = dict(data)  # this would previous treat the data as {a: b}, but now does the same as above\r\nfoo(**data_as_dict)\r\n```\r\n\r\nAs a side effect, this changes the error message of code like this:\r\n\r\n```python\r\nx = np.zeros((4, 2), np.void)\r\nx_as_dict = dict(x)  # TypeError: void not hashable\r\n                     # now TypeError: Only structured arrays can be used as mappings\r\nx_as_dict = dict(list(x)) # TypeError: void not hashable, before and after\r\n```",
        "changed_files": [
            {
                "filename": "numpy/core/include/numpy/npy_3kcompat.h",
                "patch": "@@ -93,6 +93,7 @@ static NPY_INLINE int PyInt_Check(PyObject *op) {\n #define PyUString_Size PyUnicode_Size\n #define PyUString_InternFromString PyUnicode_InternFromString\n #define PyUString_Format PyUnicode_Format\n+#define PyUString_AsUTF8 PyUnicode_AsUTF8\n \n #else\n \n@@ -122,6 +123,7 @@ static NPY_INLINE int PyInt_Check(PyObject *op) {\n #define PyUString_Size PyString_Size\n #define PyUString_InternFromString PyString_InternFromString\n #define PyUString_Format PyString_Format\n+#define PyUString_AsUTF8 PyString_AsString\n \n #endif /* NPY_PY3K */\n \n@@ -184,7 +186,7 @@ npy_PyFile_Dup2(PyObject *file, char *mode, npy_off_t *orig_pos)\n     if (ret == NULL) {\n         return NULL;\n     }\n-    fd2 = PyNumber_AsSsize_t(ret, NULL);\n+    fd2 = (int) PyNumber_AsSsize_t(ret, NULL);\n     Py_DECREF(ret);\n \n     /* Convert to FILE* handle */"
            },
            {
                "filename": "numpy/core/src/multiarray/arrayobject.c",
                "patch": "@@ -463,7 +463,8 @@ dump_data(char **string, Py_ssize_t *n, Py_ssize_t *max_n, char *data, int nd,\n     PyArray_Descr *descr=PyArray_DESCR(self);\n     PyObject *op = NULL, *sp = NULL;\n     char *ostring;\n-    npy_intp i, N, ret = 0;\n+    npy_intp i, N;\n+    int ret = 0;\n \n #define CHECK_MEMORY do {                           \\\n         if (extend(string, *n, max_n) == NULL) {    \\\n@@ -1201,7 +1202,7 @@ _void_compare(PyArrayObject *self, PyArrayObject *other, int cmp_op)\n         PyObject *key, *value, *temp2;\n         PyObject *op;\n         Py_ssize_t pos = 0;\n-        npy_intp result_ndim = PyArray_NDIM(self) > PyArray_NDIM(other) ?\n+        int result_ndim = PyArray_NDIM(self) > PyArray_NDIM(other) ?\n                             PyArray_NDIM(self) : PyArray_NDIM(other);\n \n         op = (cmp_op == Py_EQ ? n_ops.logical_and : n_ops.logical_or);\n@@ -1534,7 +1535,7 @@ NPY_NO_EXPORT int\n PyArray_ElementStrides(PyObject *obj)\n {\n     PyArrayObject *arr;\n-    int itemsize;\n+    npy_intp itemsize;\n     int i, ndim;\n     npy_intp *strides;\n \n@@ -1769,6 +1770,79 @@ array_free(PyObject * v)\n     PyObject_Free(v);\n }\n \n+static const char *\n+_attr_name_as_utf8(PyObject *attr_name)\n+{\n+    const char *c_attr_name;\n+    if (PyUString_Check(attr_name)) {\n+        c_attr_name = PyUString_AsUTF8(attr_name);\n+        if (c_attr_name == NULL) {\n+            return NULL;\n+        }\n+    }\n+#if !defined(NPY_PY3K)\n+    else if (PyUnicode_Check(attr_name)) {\n+        /*\n+        PyString_AsString uses the \"Default encoding\", which is probabably good\n+        enough\n+        */\n+        c_attr_name = PyString_AsString(attr_name);\n+        if (c_attr_name == NULL) {\n+            return NULL;\n+        }\n+    }\n+#endif\n+    else {\n+        PyErr_SetNone(PyExc_TypeError);\n+        return NULL;\n+    }\n+    return c_attr_name;\n+}\n+\n+static PyObject *\n+array_getattro(PyArrayObject *arr, PyObject *attr_name)\n+{\n+    PyArray_Descr *descr = PyArray_DESCR(arr);\n+    PyMethodDef *method;\n+    PyObject *result;\n+    const char *c_attr_name;\n+\n+    PyObject *etype, *evalue, *etraceback;\n+\n+    /* look up with the normal mechanism first - we don't allow overrides */\n+    result = PyObject_GenericGetAttr((PyObject *)arr, attr_name);\n+    if (result != NULL) {\n+        return result;\n+    }\n+    PyErr_Fetch(&etype, &evalue, &etraceback);\n+\n+    /* convert name to utf8 char* for strcmp */\n+    c_attr_name = _attr_name_as_utf8(attr_name);\n+    if (c_attr_name == NULL) {\n+        goto fail;\n+    }\n+\n+    /* Look up the method name in the list of extras */\n+    method = descr->typeobj->tp_methods;\n+    if (method != NULL) {\n+        for (; method->ml_name != NULL; method++) {\n+            if (strcmp(method->ml_name, c_attr_name) == 0) {\n+                goto succeed;\n+            }\n+        }\n+    }\n+\n+fail:\n+    /* Rethrow the normal error, if our custom lookup didn't help */\n+    PyErr_Restore(etype, evalue, etraceback);\n+    return NULL;\n+\n+succeed:\n+    Py_XDECREF(etype);\n+    Py_XDECREF(evalue);\n+    Py_XDECREF(etraceback);\n+    return PyCFunction_New(method, (PyObject *)arr);\n+}\n \n NPY_NO_EXPORT PyTypeObject PyArray_Type = {\n #if defined(NPY_PY3K)\n@@ -1801,7 +1875,7 @@ NPY_NO_EXPORT PyTypeObject PyArray_Type = {\n     (hashfunc)0,                                /* tp_hash */\n     (ternaryfunc)0,                             /* tp_call */\n     (reprfunc)array_str,                        /* tp_str */\n-    (getattrofunc)0,                            /* tp_getattro */\n+    (getattrofunc)array_getattro,               /* tp_getattro */\n     (setattrofunc)0,                            /* tp_setattro */\n     &array_as_buffer,                           /* tp_as_buffer */\n     (Py_TPFLAGS_DEFAULT"
            },
            {
                "filename": "numpy/core/src/multiarray/methods.c",
                "patch": "@@ -2417,6 +2417,42 @@ array_complex(PyArrayObject *self, PyObject *NPY_UNUSED(args))\n     return c;\n }\n \n+static PyObject *\n+array_dir(PyArrayObject *arr, PyObject *NPY_UNUSED(args))\n+{\n+    PyObject *super;\n+    PyObject *base_dir;\n+    PyArray_Descr *descr = PyArray_DESCR(arr);\n+    PyMethodDef *method;\n+\n+    /* invoke super().__dir__() */\n+    super = PyObject_CallFunctionObjArgs(\n+        &PySuper_Type, &PyArray_Type, arr);\n+    if (super == NULL) {\n+        return NULL;\n+    }\n+    base_dir = PyObject_CallMethod(super, \"__dir__\", \"\");\n+    if (base_dir == NULL) {\n+        return NULL;\n+    }\n+\n+    /* Append any dtype-specific methods */\n+    method = descr->typeobj->tp_methods;\n+    if (method != NULL) {\n+        for (; method->ml_name != NULL; method++) {\n+            PyObject *name = PyUString_FromString(method->ml_name);\n+            if (name == NULL) {\n+                return NULL;\n+            }\n+            if (PyList_Append(base_dir, name) < 0) {\n+                return NULL;\n+            }\n+        }\n+    }\n+\n+    return base_dir;\n+}\n+\n NPY_NO_EXPORT PyMethodDef array_methods[] = {\n \n     /* for subtypes */\n@@ -2430,6 +2466,11 @@ NPY_NO_EXPORT PyMethodDef array_methods[] = {\n         (PyCFunction)array_wraparray,\n         METH_VARARGS, NULL},\n \n+    /* for augmenting dtypes */\n+    {\"__dir__\",\n+        (PyCFunction) array_dir,\n+        METH_NOARGS, NULL},\n+\n     /* for the sys module */\n     {\"__sizeof__\",\n         (PyCFunction) array_sizeof,"
            },
            {
                "filename": "numpy/core/src/multiarray/scalartypes.c.src",
                "patch": "@@ -1887,6 +1887,18 @@ static PyObject *\n }\n /**end repeat**/\n \n+static PyObject *\n+voidtype_keys(PyObject *self, PyObject *NPY_UNUSED(args))\n+{\n+    PyArrayObject *arr = PyArray_FROM_O(self);\n+    PyArray_Descr *descr = PyArray_DESCR(arr);\n+    if (!PyDataType_HASFIELDS(descr)) {\n+        PyErr_SetString(PyExc_TypeError, \"Only structured arrays can be used as mappings\");\n+        return NULL;\n+    }\n+    return PyObject_CallMethod(descr->fields, \"keys\", \"\");\n+}\n+\n /*\n  * need to fill in doc-strings for these methods on import -- copy from\n  * array docstrings\n@@ -2122,6 +2134,11 @@ static PyMethodDef voidtype_methods[] = {\n     {\"setfield\",\n         (PyCFunction)voidtype_setfield,\n         METH_VARARGS | METH_KEYWORDS, NULL},\n+\n+    {\"keys\",\n+        (PyCFunction)voidtype_keys,\n+        METH_NOARGS, NULL},\n+\n     {NULL, NULL, 0, NULL}\n };\n "
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -1026,6 +1026,48 @@ def test_base_attr(self):\n         b = a[0]\n         assert_(b.base is a)\n \n+    def test_kwarg_splat(self):\n+        a = np.zeros(3, dtype='i4,f4')\n+        b = a[0]\n+        def f(f0, f1):\n+            return f0, f1\n+\n+        # test arrays\n+        f0, f1 = f(**a)\n+        assert_equal(f0, a['f0'])\n+        assert_equal(f1, a['f1'])\n+\n+        # test scalars\n+        f0, f1 = f(**b)\n+        assert_equal(f0, b['f0'])\n+        assert_equal(f1, b['f1'])\n+\n+        c = np.zeros((3,2))\n+        assert_raises(TypeError, lambda: f(**c))\n+\n+    def test_keys_only_on_void(self):\n+        a = np.zeros(3, dtype='i4,f4')\n+        assert_equal(sorted(a.keys()), ['f0', 'f1'])\n+        assert_('keys' in dir(a))\n+        assert_equal(sorted(a[0].keys()), ['f0', 'f1'])\n+        assert_('keys' in dir(a[0]))\n+\n+        b = np.zeros(3)\n+        assert_('keys' not in dir(b))\n+        assert_raises(AttributeError, getattr, b, 'keys')\n+        assert_('keys' not in dir(b[0]))\n+        assert_raises(AttributeError, getattr, b[0], 'keys')\n+\n+\n+    def test_dict_conversion(self):\n+        a = np.zeros(3, dtype='i4,f4')\n+        b = a[0]\n+        assert_equal(dict(a), dict(f0=a['f0'], f1=a['f1']))\n+        assert_equal(dict(b), dict(f0=b['f0'], f1=b['f1']))\n+\n+        c = np.zeros(3)\n+        assert_raises(TypeError, dict, c)\n+\n \n class TestBool(TestCase):\n     def test_test_interning(self):"
            },
            {
                "filename": "numpy/matrixlib/tests/test_defmatrix.py",
                "patch": "@@ -290,7 +290,7 @@ def test_instance_methods(self):\n             'partition', 'argpartition',\n             'take', 'tofile', 'tolist', 'tostring', 'tobytes', 'all', 'any',\n             'sum', 'argmax', 'argmin', 'min', 'max', 'mean', 'var', 'ptp',\n-            'prod', 'std', 'ctypes', 'itemset',\n+            'prod', 'std', 'ctypes', 'itemset', 'keys'\n             ]\n         for attrib in dir(a):\n             if attrib.startswith('_') or attrib in excluded_methods:"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 8910,
        "body": "Public API still has to return a full array, but we can avoid a lot of copying and memory by returning `np.broadcast_to(np.zeros((), dtype), shape)` instead of `np.zeros(dtype, shape)` when `arr.mask is nomask`.\r\n\r\nThis seems to add 2us overhead for small arrays, and starts to break even at around 10000 elements\r\n\r\nMost of the time here is lost to the `ndarray` constructor, when really all we want to do is modify `->strides` and `->shape` without checking",
        "changed_files": [
            {
                "filename": "numpy/ma/core.py",
                "patch": "@@ -1019,14 +1019,11 @@ def __call__(self, a, b, *args, **kwargs):\n             result = self.f(da, db, *args, **kwargs)\n         # Get the mask for the result\n         (ma, mb) = (getmask(a), getmask(b))\n-        if ma is nomask:\n-            if mb is nomask:\n-                m = nomask\n-            else:\n-                m = umath.logical_or(getmaskarray(a), mb)\n-        elif mb is nomask:\n-            m = umath.logical_or(ma, getmaskarray(b))\n+        if ma is nomask and mb is nomask:\n+            m = nomask\n         else:\n+            ma = _viewmaskarray(a)\n+            mb = _viewmaskarray(b)\n             m = umath.logical_or(ma, mb)\n \n         # Case 1. : scalar\n@@ -1095,8 +1092,8 @@ def outer(self, a, b):\n         if ma is nomask and mb is nomask:\n             m = nomask\n         else:\n-            ma = getmaskarray(a)\n-            mb = getmaskarray(b)\n+            ma = _viewmaskarray(a)\n+            mb = _viewmaskarray(b)\n             m = umath.logical_or.outer(ma, mb)\n         if (not m.ndim) and m:\n             return masked\n@@ -1429,8 +1426,16 @@ def getmask(a):\n \n get_mask = getmask\n \n+def _viewmaskarray(arr, allow_readonly=True):\n+    mask = getmask(arr)\n+    if mask is nomask:\n+        mask = make_mask_none(\n+            np.shape(arr),\n+            dtype=getattr(arr, 'dtype', None),\n+            writeable=not allow_readonly)\n+    return mask\n \n-def getmaskarray(arr):\n+def getmaskarray(arr, allow_readonly=False):\n     \"\"\"\n     Return the mask of a masked array, or full boolean array of False.\n \n@@ -1442,6 +1447,9 @@ def getmaskarray(arr):\n     ----------\n     arr : array_like\n         Input `MaskedArray` for which the mask is required.\n+    allow_readonly : bool, optional\n+        If True, allow this function to produce a readonly array when doing so\n+        would increase performance. The default is False.\n \n     See Also\n     --------\n@@ -1479,10 +1487,7 @@ def getmaskarray(arr):\n            [False, False]], dtype=bool)\n \n     \"\"\"\n-    mask = getmask(arr)\n-    if mask is nomask:\n-        mask = make_mask_none(np.shape(arr), getattr(arr, 'dtype', None))\n-    return mask\n+    return _viewmaskarray(arr, allow_readonly)\n \n \n def is_mask(m):\n@@ -1638,7 +1643,7 @@ def make_mask(m, copy=False, shrink=True, dtype=MaskType):\n         return result\n \n \n-def make_mask_none(newshape, dtype=None):\n+def make_mask_none(newshape, dtype=None, writeable=True):\n     \"\"\"\n     Return a boolean mask of the given shape, filled with False.\n \n@@ -1653,6 +1658,9 @@ def make_mask_none(newshape, dtype=None):\n     dtype : {None, dtype}, optional\n         If None, use a MaskType instance. Otherwise, use a new datatype with\n         the same fields as `dtype`, converted to boolean types.\n+    writeable : bool\n+        If True, return a normal array. If false, return a broadcasting array\n+        that only stores one element of memory, but as a result is readonly.\n \n     Returns\n     -------\n@@ -1682,10 +1690,29 @@ def make_mask_none(newshape, dtype=None):\n \n     \"\"\"\n     if dtype is None:\n-        result = np.zeros(newshape, dtype=MaskType)\n+        dtype = MaskType\n     else:\n-        result = np.zeros(newshape, dtype=make_mask_descr(dtype))\n-    return result\n+        dtype = make_mask_descr(dtype)\n+\n+    if writeable:\n+        return np.zeros(newshape, dtype=dtype)\n+\n+    else:\n+        # build a buffer of the single scalar mask\n+        if dtype == MaskType:\n+            # optimization - bytes->buffer is faster than np.ma.nomask->buffer\n+            buff = b'\\0'  # np.ma.nomask\n+        else:\n+            buff = np.zeros((), dtype=dtype)\n+            buff.flags.writeable = False\n+\n+        # duplicate it using zero strides\n+        # like np.lib.stride_tricks.as_strided, but faster\n+        offset = 0\n+        strides = (0,)*len(newshape)\n+\n+        # optimization - positional arguments, for speed\n+        return np.ndarray(newshape, dtype, buff, offset, strides)\n \n \n def mask_or(m1, m2, copy=False, shrink=True):\n@@ -2529,7 +2556,7 @@ def flatten_sequence(iterable):\n         out = np.array([tuple(flatten_sequence(d.item())) for d in a._data])\n         out = out.view(MaskedArray)\n         out._mask = np.array([tuple(flatten_sequence(d.item()))\n-                              for d in getmaskarray(a)])\n+                              for d in _viewmaskarray(a)])\n     else:\n         out = np.array([tuple(flatten_sequence(d.item())) for d in a])\n     if len(inishape) > 1:\n@@ -2665,7 +2692,7 @@ def __getitem__(self, indx):\n     def __setitem__(self, index, value):\n         self.dataiter[index] = getdata(value)\n         if self.maskiter is not None:\n-            self.maskiter[index] = getmaskarray(value)\n+            self.maskiter[index] = _viewmaskarray(value)\n \n     def __next__(self):\n         \"\"\"\n@@ -2817,8 +2844,9 @@ def __new__(cls, data=None, mask=nomask, dtype=None, copy=False,\n             elif isinstance(data, (tuple, list)):\n                 try:\n                     # If data is a sequence of masked array\n-                    mask = np.array([getmaskarray(m) for m in data],\n-                                    dtype=mdtype)\n+                    mask = np.array([\n+                        _viewmaskarray(m) for m in data\n+                    ], dtype=mdtype)\n                 except ValueError:\n                     # If data is nested\n                     mask = nomask\n@@ -2988,7 +3016,7 @@ def __array_finalize__(self, obj):\n             except ValueError:\n                 self._mask = nomask\n             except (TypeError, AttributeError):\n-                # When _mask.shape is not writable (because it's a void)\n+                # When _mask.shape is not writeable (because it's a void)\n                 pass\n         # Finalize the fill_value for structured arrays\n         if self.dtype.names:\n@@ -3012,7 +3040,9 @@ def __array_wrap__(self, obj, context=None):\n         if context is not None:\n             result._mask = result._mask.copy()\n             (func, args, _) = context\n-            m = reduce(mask_or, [getmaskarray(arg) for arg in args])\n+            m = reduce(mask_or, [\n+                _viewmaskarray(arg) for arg in args\n+            ])\n             # Get the domain mask\n             domain = ufunc_domain.get(func, None)\n             if domain is not None:\n@@ -3049,6 +3079,9 @@ def __array_wrap__(self, obj, context=None):\n             if result is not self and result.shape == () and m:\n                 return masked\n             else:\n+                # might be readonly still\n+                if not m.flags.writeable:\n+                    m = m.copy()\n                 result._mask = m\n                 result._sharedmask = False\n \n@@ -4575,7 +4608,7 @@ def put(self, indices, values, mode='raise'):\n         if self._mask is nomask and getmask(values) is nomask:\n             return\n \n-        m = getmaskarray(self).copy()\n+        m = _viewmaskarray(self).copy()\n \n         if getmask(values) is nomask:\n             m.put(indices, False, mode=mode)\n@@ -5881,7 +5914,8 @@ def __getstate__(self):\n         \"\"\"\n         cf = 'CF'[self.flags.fnc]\n         data_state = super(MaskedArray, self).__reduce__()[2]\n-        return data_state + (getmaskarray(self).tobytes(cf), self._fill_value)\n+        return data_state + (\n+            _viewmaskarray(self).tobytes(cf), self._fill_value)\n \n     def __setstate__(self, state):\n         \"\"\"Restore the internal state of the masked array, for\n@@ -6291,8 +6325,8 @@ def outer(self, a, b):\n         if ma is nomask and mb is nomask:\n             m = nomask\n         else:\n-            ma = getmaskarray(a)\n-            mb = getmaskarray(b)\n+            ma = _viewmaskarray(a)\n+            mb = _viewmaskarray(b)\n             m = logical_or.outer(ma, mb)\n         result = self.ufunc.outer(filled(a), filled(b))\n         if not isinstance(result, MaskedArray):\n@@ -6597,7 +6631,7 @@ def concatenate(arrays, axis=0):\n     else:\n         return data\n     # OK, so we have to concatenate the masks\n-    dm = np.concatenate([getmaskarray(a) for a in arrays], axis)\n+    dm = np.concatenate([_viewmaskarray(a) for a in arrays], axis)\n     # If we decide to keep a '_shrinkmask' option, we want to check that\n     # all of them are True, and then check for dm.any()\n     if not dm.dtype.fields and not dm.any():\n@@ -6769,7 +6803,7 @@ def putmask(a, mask, values):  # , mode='raise'):\n             a.mask |= m\n     else:\n         if valmask is nomask:\n-            valmask = getmaskarray(values)\n+            valmask = _viewmaskarray(values)\n         np.copyto(a._mask, valmask, where=mask)\n     np.copyto(a._data, valdata, where=mask)\n     return\n@@ -7003,9 +7037,9 @@ def where(condition, x=_NoValue, y=_NoValue):\n     yd = getdata(y)\n \n     # we need the full arrays here for correct final dimensions\n-    cm = getmaskarray(condition)\n-    xm = getmaskarray(x)\n-    ym = getmaskarray(y)\n+    cm = _viewmaskarray(condition)\n+    xm = _viewmaskarray(x)\n+    ym = _viewmaskarray(y)\n \n     # deal with the fact that masked.dtype == float64, but we don't actually\n     # want to treat it as that.\n@@ -7296,8 +7330,8 @@ def dot(a, b, strict=False, out=None):\n     if strict and (a.ndim == 2) and (b.ndim == 2):\n         a = mask_rowcols(a, 0)\n         b = mask_rowcols(b, 1)\n-    am = ~getmaskarray(a)\n-    bm = ~getmaskarray(b)\n+    am = ~_viewmaskarray(a)\n+    bm = ~_viewmaskarray(b)\n \n     if out is None:\n         d = np.dot(filled(a, 0), filled(b, 0))\n@@ -7349,8 +7383,8 @@ def outer(a, b):\n     mb = getmask(b)\n     if ma is nomask and mb is nomask:\n         return masked_array(d)\n-    ma = getmaskarray(a)\n-    mb = getmaskarray(b)\n+    ma = _viewmaskarray(a)\n+    mb = _viewmaskarray(b)\n     m = make_mask(1 - np.outer(1 - ma, 1 - mb), copy=0)\n     return masked_array(d, mask=m)\n outer.__doc__ = doc_note(np.outer.__doc__,\n@@ -7365,13 +7399,13 @@ def _convolve_or_correlate(f, a, v, mode, propagate_mask):\n     if propagate_mask:\n         # results which are contributed to by either item in any pair being invalid\n         mask = (\n-            f(getmaskarray(a), np.ones(np.shape(v), dtype=np.bool), mode=mode)\n-          | f(np.ones(np.shape(a), dtype=np.bool), getmaskarray(v), mode=mode)\n+            f(_viewmaskarray(a), np.ones(np.shape(v), dtype=np.bool), mode=mode)\n+          | f(np.ones(np.shape(a), dtype=np.bool), _viewmaskarray(v), mode=mode)\n         )\n         data = f(getdata(a), getdata(v), mode=mode)\n     else:\n         # results which are not contributed to by any pair of valid elements\n-        mask = ~f(~getmaskarray(a), ~getmaskarray(v))\n+        mask = ~f(~_viewmaskarray(a), ~_viewmaskarray(v))\n         data = f(filled(a, 0), filled(v, 0), mode=mode)\n \n     return masked_array(data, mask=mask)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 8514,
        "body": "Right now you can do this with primitive types:\r\n\r\n```python\r\n>>> np.zeros((2, 3), dtype=np.int).view([('a', np.int,3)])\r\narray([[([0, 0, 0],)],\r\n       [([0, 0, 0],)]], \r\n      dtype=[('a', '<i4', (3,))])\r\n```\r\n\r\nThis adds\r\n\r\n```python\r\n>>> np.zeros((2, 3), dtype=object).view([('a', object,3)])\r\narray([[([0, 0, 0],)],\r\n       [([0, 0, 0],)]], \r\n      dtype=[('a', 'O', (3,))])\r\n```\r\n\r\nWhich would previously error\r\n\r\nThis makes it possible to use `np.unique` on 2d object arrays",
        "changed_files": [
            {
                "filename": "doc/source/reference/routines.dtype.rst",
                "patch": "@@ -53,3 +53,4 @@ Miscellaneous\n    typename\n    sctype2char\n    mintypecode\n+   find_dtype_offsets"
            },
            {
                "filename": "numpy/core/_internal.py",
                "patch": "@@ -355,15 +355,38 @@ def _view_is_safe(oldtype, newtype):\n         If the new type is incompatible with the old type.\n \n     \"\"\"\n+    from numpy.lib.type_check import find_dtype_offsets\n \n     # if the types are equivalent, there is no problem.\n     # for example: dtype((np.record, 'i4,i4')) == dtype((np.void, 'i4,i4'))\n     if oldtype == newtype:\n         return\n \n-    if newtype.hasobject or oldtype.hasobject:\n-        raise TypeError(\"Cannot change data-type for object array.\")\n-    return\n+    # no object members is fine\n+    if not newtype.hasobject and not oldtype.hasobject:\n+        return\n+\n+    # if they don't both have object members, check if they line up\n+    if newtype.hasobject != oldtype.hasobject:\n+        raise TypeError(\"Cannot change data-type for object array to non object or vice versa.\")\n+\n+    # repeat the dtypes until they line up in size\n+    # recall that (dtype, shape) is itself a dtype specifier\n+    newsize = newtype.itemsize\n+    oldsize = oldtype.itemsize\n+    gcd = _gcd(newsize, oldsize)\n+    newtype = (newtype, oldsize // gcd)\n+    oldtype = (oldtype, newsize // gcd)\n+\n+    # if the locations of objects are the same, viewing is fine\n+    newoffsets = find_dtype_offsets(newtype, object_)\n+    oldoffsets = find_dtype_offsets(oldtype, object_)\n+    if len(newoffsets) == len(oldoffsets) and all(newoffsets == oldoffsets):\n+        return\n+\n+    # otherwise views are not safe\n+    raise TypeError(\"Locations of objects in new data-type must align with old data-type\")\n+\n \n # Given a string containing a PEP 3118 format specifier,\n # construct a NumPy dtype"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -6502,6 +6502,122 @@ def test_collections_hashable(self):\n         x = np.array([])\n         self.assertFalse(isinstance(x, collections.Hashable))\n \n+from numpy.core._internal import _view_is_safe\n+\n+class TestObjViewSafetyFuncs(TestCase):\n+\n+    @staticmethod\n+    def scan_view(d1, otype):\n+        \"\"\" scans through positions at which we can view a type \"\"\"\n+        goodpos = []\n+        for shift in range(d1.itemsize - np.dtype(otype).itemsize+1):\n+            d2 = np.dtype({'names': ['f0'], 'formats': [otype],\n+                        'offsets': [shift], 'itemsize': d1.itemsize})\n+            try:\n+                _view_is_safe(d1, d2)\n+            except TypeError as e:\n+                pass\n+            else:\n+                goodpos.append(shift)\n+        return goodpos\n+\n+    def test_view_safety_object(self):\n+        \"\"\" Tests the cases when changing the view does not add or hide objects \"\"\"\n+        psize = np.dtype('p').itemsize\n+\n+        # test nonequal itemsizes with objects:\n+        # these should succeed:\n+        _view_is_safe(np.dtype('O,p,O,p'), np.dtype('O,p,O,p,O,p'))\n+        _view_is_safe(np.dtype('O,O'), np.dtype('O,O,O'))\n+\n+        # these should fail:\n+        assert_raises(TypeError, _view_is_safe, np.dtype('O,O,p'), np.dtype('O,O'))\n+        assert_raises(TypeError, _view_is_safe, np.dtype('O,O,p'), np.dtype('O,p'))\n+        assert_raises(TypeError, _view_is_safe, np.dtype('O,O,p'), np.dtype('p,O'))\n+\n+        # test nested structures with objects:\n+        nestedO = np.dtype([('f0', 'p'), ('f1', 'p,O,p')])\n+        assert_array_equal(self.scan_view(nestedO, 'O'), [2*psize])\n+\n+    @dec.knownfailureif(True)\n+    def test_view_safety_partial_object(self):\n+        \"\"\" Tests the cases when changing the view adds or hides objects \"\"\"\n+        psize = np.dtype('p').itemsize\n+\n+        # test partial overlap with object field\n+        assert_array_equal(\n+            self.scan_view(np.dtype('p,O,p,p,O,O'), 'p'),\n+            [0] + list(range(2*psize, 3*psize+1))\n+        )\n+        assert_array_equal(\n+            self.scan_view(np.dtype('p,O,p,p,O,O'), 'O'),\n+            [psize, 4*psize, 5*psize]\n+        )\n+\n+        # test nested structures with objects:\n+        nestedO = np.dtype([('f0', 'p'), ('f1', 'p,O,p')])\n+        assert_array_equal(self.scan_view(nestedO, 'p'), list(range(psize+1)) + [3*psize])\n+\n+        # test subarrays with objects\n+        subarrayO = np.dtype('p,(2,3)O,p')\n+        assert_array_equal(\n+            self.scan_view(subarrayO, 'O'),\n+            list(range(psize, 6*psize+1, psize))\n+        )\n+        assert_array_equal(self.scan_view(subarrayO, 'p'), [0, 7*psize])\n+\n+    @dec.knownfailureif(True)\n+    def test_view_safety_missing(self):\n+        \"\"\" Tests the cases where fields are added where they used to be missing \"\"\"\n+        psize = np.dtype('p').itemsize\n+\n+        # creates dtype but with extra character code - for missing 'p' fields\n+        def mtype(s):\n+            n, offset, fields = 0, 0, []\n+            for c in s.split(','):  # subarrays won't work\n+                if c != '-':\n+                    fields.append(('f{0}'.format(n), c, offset))\n+                    n += 1\n+                offset += np.dtype(c).itemsize if c != '-' else psize\n+\n+            names, formats, offsets = zip(*fields)\n+            return np.dtype({'names': names, 'formats': formats,\n+                          'offsets': offsets, 'itemsize': offset})\n+\n+        # test nonequal itemsizes with missing fields:\n+        # these should succeed:\n+        _view_is_safe(mtype('-,p,-,p'), mtype('-,p,-,p,-,p'))\n+        _view_is_safe(np.dtype('p,p'), np.dtype('p,p,p'))\n+\n+        # these should fail:\n+        assert_raises(TypeError, _view_is_safe, mtype('p,p,-'), mtype('p,p'))\n+        assert_raises(TypeError, _view_is_safe, mtype('p,p,-'), mtype('p,-'))\n+        assert_raises(TypeError, _view_is_safe, mtype('p,p,-'), mtype('-,p'))\n+\n+        # test partial overlap with missing field\n+        assert_array_equal(\n+            self.scan_view(mtype('p,-,p,p,-,-'), 'p'),\n+            [0] + list(range(2*psize, 3*psize+1))\n+        )\n+\n+        # test nested structures with missing fields:\n+        nestedM = np.dtype([('f0', 'p'), ('f1', mtype('p,-,p'))])\n+        assert_array_equal(self.scan_view(nestedM, 'p'), list(range(psize+1)) + [3*psize])\n+\n+    @dec.knownfailureif(True)\n+    def test_view_safety_overlapping(self):\n+        psize = np.dtype('p').itemsize\n+\n+        #test dtype with overlapping fields\n+        overlapped = np.dtype({'names': ['f0', 'f1', 'f2', 'f3'],\n+                            'formats': ['p', 'p', 'p', 'p'],\n+                            'offsets': [0, 1, 3*psize-1, 3*psize],\n+                            'itemsize': 4*psize})\n+        assert_array_equal(\n+            self.scan_view(overlapped, 'p'),\n+            [0, 1, 3*psize-1, 3*psize]\n+        )\n+\n \n class TestArrayPriority(TestCase):\n     # This will go away when __array_priority__ is settled, meanwhile"
            },
            {
                "filename": "numpy/lib/arraysetops.py",
                "patch": "@@ -225,12 +225,7 @@ def unique(ar, return_index=False, return_inverse=False,\n     else:\n         dtype = [('f{i}'.format(i=i), ar.dtype) for i in range(ar.shape[1])]\n \n-    try:\n-        consolidated = ar.view(dtype)\n-    except TypeError:\n-        # There's no good way to do this for object arrays, etc...\n-        msg = 'The axis argument to unique is not supported for dtype {dt}'\n-        raise TypeError(msg.format(dt=ar.dtype))\n+    consolidated = ar.view(dtype)\n \n     def reshape_uniq(uniq):\n         uniq = uniq.view(orig_dtype)"
            },
            {
                "filename": "numpy/lib/tests/test_arraysetops.py",
                "patch": "@@ -316,10 +316,6 @@ def check_all(a, b, i1, i2, c, dt):\n         assert_array_equal(a2_inv, np.zeros(5))\n \n     def test_unique_axis_errors(self):\n-        assert_raises(TypeError, self._run_axis_tests, object)\n-        assert_raises(TypeError, self._run_axis_tests,\n-                      [('a', int), ('b', object)])\n-\n         assert_raises(ValueError, unique, np.arange(10), axis=2)\n         assert_raises(ValueError, unique, np.arange(10), axis=-2)\n \n@@ -338,6 +334,8 @@ def test_unique_axis(self):\n         types.append('timedelta64[D]')\n         types.append([('a', int), ('b', int)])\n         types.append([('a', int), ('b', float)])\n+        types.append(object)\n+        types.append([('a', int), ('b', object)])\n \n         for dtype in types:\n             self._run_axis_tests(dtype)"
            },
            {
                "filename": "numpy/lib/tests/test_recfunctions.py",
                "patch": "@@ -720,5 +720,15 @@ def test_append_to_objects(self):\n                            dtype=[('A', object), ('B', float), ('C', int)])\n         assert_equal(test, control)\n \n+    def test_append_with_objects(self):\n+        \"Test append_fields when the appended data contains objects\"\n+        obj = self.data['obj']\n+        x = np.array([(10, 1.), (20, 2.)], dtype=[('A', int), ('B', float)])\n+        y = np.array([obj, obj], dtype=object)\n+        test = append_fields(x, 'C', data=y, dtypes=object, usemask=False)\n+        control = np.array([(10, 1.0, obj), (20, 2.0, obj)],\n+                           dtype=[('A', int), ('B', float), ('C', object)])\n+        assert_equal(test, control)\n+\n if __name__ == '__main__':\n     run_module_suite()"
            },
            {
                "filename": "numpy/lib/tests/test_type_check.py",
                "patch": "@@ -7,7 +7,8 @@\n     )\n from numpy.lib.type_check import (\n     common_type, mintypecode, isreal, iscomplex, isposinf, isneginf,\n-    nan_to_num, isrealobj, iscomplexobj, asfarray, real_if_close\n+    nan_to_num, isrealobj, iscomplexobj, asfarray, real_if_close,\n+    find_dtype_offsets\n     )\n \n \n@@ -363,5 +364,52 @@ def test_asfarray(self):\n         assert_equal(a.__class__, np.ndarray)\n         assert_(np.issubdtype(a.dtype, np.float))\n \n+\n+class TestFindDtypeOffsets(TestCase):\n+\n+    def test_basic(self):\n+        dt = object\n+        actual = find_dtype_offsets(dt, object)\n+        expected = np.array([0], dtype=np.intp)\n+        assert_array_equal(actual, expected)\n+\n+    def test_subdtype(self):\n+        dt = (np.int32, 3)\n+        actual = find_dtype_offsets(dt, np.int32)\n+        expected = np.array([0, 4, 8], dtype=np.intp)\n+        assert_array_equal(actual, expected)\n+\n+    def test_fields(self):\n+        dt = [('a', np.float32), ('b', np.float64)]\n+\n+        actual = find_dtype_offsets(dt, np.float32)\n+        expected = np.array([0], dtype=np.intp)\n+        assert_array_equal(actual, expected)\n+\n+        actual = find_dtype_offsets(dt, np.float64)\n+        expected = np.array([4], dtype=np.intp)\n+        assert_array_equal(actual, expected)\n+\n+    def test_compound(self):\n+        dt = [\n+            ('a', np.dtype([\n+                ('a1', np.int32, 3),\n+                ('a2', np.bool, 1)\n+            ]), 2),\n+            ('b', np.int32, 3)\n+        ]\n+\n+        actual = find_dtype_offsets(dt, np.int32)\n+        expected = np.array([\n+            # first occurence of a1\n+            0,  4,  8,\n+            # second occurence of a1\n+            13, 17, 21,\n+            # b\n+            26, 30, 34\n+        ], dtype=np.intp)\n+        assert_array_equal(actual, expected)\n+\n+\n if __name__ == \"__main__\":\n     run_module_suite()"
            },
            {
                "filename": "numpy/lib/type_check.py",
                "patch": "@@ -6,7 +6,7 @@\n __all__ = ['iscomplexobj', 'isrealobj', 'imag', 'iscomplex',\n            'isreal', 'nan_to_num', 'real', 'real_if_close',\n            'typename', 'asfarray', 'mintypecode', 'asscalar',\n-           'common_type']\n+           'common_type', 'find_dtype_offsets']\n \n import numpy.core.numeric as _nx\n from numpy.core.numeric import asarray, asanyarray, array, isnan, \\\n@@ -602,3 +602,61 @@ def common_type(*arrays):\n         return array_type[1][precision]\n     else:\n         return array_type[0][precision]\n+\n+\n+def find_dtype_offsets(dtype, find):\n+    \"\"\"\n+    Finds all the byte offsets of a given scalar type within a potentially\n+    compound type\n+\n+    Parameters\n+    ----------\n+    dtype : dtype\n+        The compound type to search within\n+    find : type\n+        The base type to search for within the dtype\n+\n+    Returns\n+    -------\n+    out : np.array of int\n+        The byte offsets of scalars of type `find` within the `type`, in\n+        ascending order\n+\n+    See Also\n+    --------\n+    dtype\n+\n+    Examples\n+    --------\n+    >>> np.find_dtype_offsets(np.float32, find=np.float32)\n+    array([0])\n+    >>> np.find_dtype_offsets((np.float32, 3), find=np.float32)\n+    array([0, 8, 16])\n+    >>> np.find_dtype_offsets([('a', np.int), ('b', np.float32)], find=np.float32)\n+    array([4])\n+    \"\"\"\n+    dtype = _nx.dtype(dtype)\n+\n+    # exact match on scalar type\n+    if dtype == find:\n+        return array([0], dtype=_nx.intp)\n+\n+    # subarray type - repeat for each element\n+    if dtype.subdtype:\n+        subtype, shape = dtype.subdtype\n+        sub_offsets = find_dtype_offsets(subtype, find=find)\n+        # don't invoke arange unless we have to, as it might be large\n+        if sub_offsets.size:\n+            base_offsets = subtype.itemsize * _nx.arange(_nx.prod(shape), dtype=_nx.intp)\n+            return (base_offsets[:,None] + sub_offsets).ravel()\n+\n+    # record type - combine the fields\n+    if dtype.fields:\n+        # sort by offset\n+        fields = sorted(dtype.fields.values(), key=lambda f: f[1])\n+        return _nx.concatenate([\n+            offset + find_dtype_offsets(fdtype, find=find)\n+            for fdtype, offset in fields\n+        ])\n+\n+    return array([], dtype=_nx.intp)"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20882,
        "body": "It seems like an easy mistake to do `max(a)` instead of `a.max()` or\r\n`np.max(a)` for an ndarray `a`. This adds a hint to the resulting error message,\r\ncomplementing the existing hint about `a.any()`/`a.all()`.",
        "changed_files": [
            {
                "filename": "numpy/core/src/multiarray/number.c",
                "patch": "@@ -833,7 +833,7 @@ _array_nonzero(PyArrayObject *mp)\n         PyErr_SetString(PyExc_ValueError,\n                         \"The truth value of an array \"\n                         \"with more than one element is ambiguous. \"\n-                        \"Use a.any() or a.all()\");\n+                        \"Consider using: a.any(), a.all(), a.max() or a.min()\");\n         return -1;\n     }\n }"
            },
            {
                "filename": "numpy/ma/API_CHANGES.txt",
                "patch": "@@ -111,7 +111,7 @@ converted to booleans:\n   >>> bool(x)\n   Traceback (most recent call last):\n     File \"<stdin>\", line 1, in <module>\n-  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n+  ValueError: The truth value of an array with more than one element is ambiguous. Consider using: a.any(), a.all(), a.max() or a.min()\n \n \n =================================="
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21130,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nHello,\r\n\r\nThis PR speed up the `numpy.where` function using a branch-less implementation when the sparsity is >0.1. It is especially efficient on unpredictable input data because the previous implementation based on a conditional was causing many stalls due to branch missprediction. Such a case happens on random input data and often happens on real-world data. The performance of the branch-less implementation can change from one architecture from another. I do not expect it to be slower than the previous one on all modern x86-64 processors. It might be interesting to check if it is sometimes a bit slower on ARM/POWER architectures.\r\n\r\nHere is a benchmark (only bench the modified path of the new code since the rest is left unchanged):\r\n\r\n```python\r\nsize = 1024 * 1024 // 8\r\na = np.random.rand(size)\r\nrnd_02 = a > 0.2\r\nrnd_03 = a > 0.3\r\nrnd_04 = a > 0.4\r\nrnd_05 = a > 0.5\r\nall_ones = np.ones(size, dtype=bool)\r\nrep_zeros_2 = np.arange(size) % 2 == 0\r\nrep_zeros_4 = np.arange(size) % 4 == 0\r\nrep_zeros_8 = np.arange(size) % 8 == 0\r\nrep_ones_2 = np.arange(size) % 2 > 0\r\nrep_ones_4 = np.arange(size) % 4 > 0\r\nrep_ones_8 = np.arange(size) % 8 > 0\r\n\r\ntest_cases = [\r\n    rnd_02, rnd_03, rnd_04, rnd_05, \r\n    all_ones, \r\n    rep_zeros_2, rep_zeros_4, rep_zeros_8, \r\n    rep_ones_2, rep_ones_4, rep_ones_8\r\n]\r\n\r\nfor test_case in test_cases:\r\n    %timeit -n 1000 np.nonzero(test_case)\r\n\r\nfor test_case in test_cases:\r\n    assert np.allclose(np.nonzero(test_case)[0], np.arange(size)[test_case])\r\n```\r\n\r\nand here is the results on my Intel i5-9600KF processor:\r\n\r\n```python\r\n----- OLD -----\r\n\r\n213 \u00b5s \u00b1 1.35 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n295 \u00b5s \u00b1 4.35 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n375 \u00b5s \u00b1 5.09 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n408 \u00b5s \u00b1 731 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n63.7 \u00b5s \u00b1 156 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n83.6 \u00b5s \u00b1 229 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n78.7 \u00b5s \u00b1 1.13 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n70.7 \u00b5s \u00b1 577 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n85.2 \u00b5s \u00b1 988 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n91.9 \u00b5s \u00b1 93.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n77.9 \u00b5s \u00b1 366 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n----- NEW -----\r\n\r\n65.5 \u00b5s \u00b1 696 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.7 \u00b5s \u00b1 198 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.7 \u00b5s \u00b1 181 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.1 \u00b5s \u00b1 138 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.5 \u00b5s \u00b1 155 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.1 \u00b5s \u00b1 205 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n64.8 \u00b5s \u00b1 271 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n64.3 \u00b5s \u00b1 353 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.4 \u00b5s \u00b1 474 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.4 \u00b5s \u00b1 437 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n65.7 \u00b5s \u00b1 247 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\nAs you can see, the new timings are nearly always better on this machine. The new code is up to *6.3 times faster*.",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_function_base.py",
                "patch": "@@ -284,6 +284,21 @@ def setup(self):\n         self.d = np.arange(20000)\n         self.e = self.d.copy()\n         self.cond = (self.d > 5000)\n+        size = 1024 * 1024 // 8\n+        rnd_array = np.random.rand(size)\n+        self.rand_cond_01 = rnd_array > 0.01\n+        self.rand_cond_20 = rnd_array > 0.20\n+        self.rand_cond_30 = rnd_array > 0.30\n+        self.rand_cond_40 = rnd_array > 0.40\n+        self.rand_cond_50 = rnd_array > 0.50\n+        self.all_zeros = np.zeros(size, dtype=bool)\n+        self.all_ones = np.ones(size, dtype=bool)\n+        self.rep_zeros_2 = np.arange(size) % 2 == 0\n+        self.rep_zeros_4 = np.arange(size) % 4 == 0\n+        self.rep_zeros_8 = np.arange(size) % 8 == 0\n+        self.rep_ones_2 = np.arange(size) % 2 > 0\n+        self.rep_ones_4 = np.arange(size) % 4 > 0\n+        self.rep_ones_8 = np.arange(size) % 8 > 0\n \n     def time_1(self):\n         np.where(self.cond)\n@@ -293,3 +308,43 @@ def time_2(self):\n \n     def time_2_broadcast(self):\n         np.where(self.cond, self.d, 0)\n+\n+    def time_all_zeros(self):\n+        np.where(self.all_zeros)\n+\n+    def time_random_01_percent(self):\n+        np.where(self.rand_cond_01)\n+\n+    def time_random_20_percent(self):\n+        np.where(self.rand_cond_20)\n+\n+    def time_random_30_percent(self):\n+        np.where(self.rand_cond_30)\n+\n+    def time_random_40_percent(self):\n+        np.where(self.rand_cond_40)\n+\n+    def time_random_50_percent(self):\n+        np.where(self.rand_cond_50)\n+\n+    def time_all_ones(self):\n+        np.where(self.all_ones)\n+\n+    def time_interleaved_zeros_x2(self):\n+        np.where(self.rep_zeros_2)\n+\n+    def time_interleaved_zeros_x4(self):\n+        np.where(self.rep_zeros_4)\n+\n+    def time_interleaved_zeros_x8(self):\n+        np.where(self.rep_zeros_8)\n+\n+    def time_interleaved_ones_x2(self):\n+        np.where(self.rep_ones_2)\n+\n+    def time_interleaved_ones_x4(self):\n+        np.where(self.rep_ones_4)\n+\n+    def time_interleaved_ones_x8(self):\n+        np.where(self.rep_ones_8)\n+"
            },
            {
                "filename": "doc/release/upcoming_changes/21130.performance.rst",
                "patch": "@@ -0,0 +1,4 @@\n+Faster ``np.where``\n+-------------------\n+`numpy.where` is now much faster than previously on unpredictable/random\n+input data."
            },
            {
                "filename": "numpy/core/src/multiarray/item_selection.c",
                "patch": "@@ -2641,13 +2641,33 @@ PyArray_Nonzero(PyArrayObject *self)\n                     *multi_index++ = j++;\n                 }\n             }\n+            /*\n+             * Fallback to a branchless strategy to avoid branch misprediction \n+             * stalls that are very expensive on most modern processors.\n+             */\n             else {\n-                npy_intp j;\n-                for (j = 0; j < count; ++j) {\n-                    if (*data != 0) {\n-                        *multi_index++ = j;\n-                    }\n+                npy_intp *multi_index_end = multi_index + nonzero_count;\n+                npy_intp j = 0;\n+\n+                /* Manually unroll for GCC and maybe other compilers */\n+                while (multi_index + 4 < multi_index_end) {\n+                    *multi_index = j;\n+                    multi_index += data[0] != 0;\n+                    *multi_index = j + 1;\n+                    multi_index += data[stride] != 0;\n+                    *multi_index = j + 2;\n+                    multi_index += data[stride * 2] != 0;\n+                    *multi_index = j + 3;\n+                    multi_index += data[stride * 3] != 0;\n+                    data += stride * 4;\n+                    j += 4;\n+                }\n+\n+                while (multi_index < multi_index_end) {\n+                    *multi_index = j;\n+                    multi_index += *data != 0;\n                     data += stride;\n+                    ++j;\n                 }\n             }\n         }"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21001,
        "body": "<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https://numpy.org/devdocs/dev/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nHello everyone,\r\n\r\nThis PR is meant to significantly improve the performance of reduction functions (like `numpy.sum` for example): **from 2.6 to 20 times faster**. It also improves the performance of the casting function used by many Numpy functions. These changes are in response to [this](https://stackoverflow.com/questions/70134026/no-speedup-when-summing-uint16-vs-uint64-arrays-with-numpy/70136429#70136429) Stack Overflow detailed analysis showing that the current code of `numpy.sum` does not benefit from SIMD instructions (with both integers an floating-point numbers).\r\n\r\nThe PR changes the macros so the compiler can generate a faster code for reductions when the array is contiguous in a way that is similar to the current code in basic operators. It also generates an AVX2 version (if supported) of the conversion function called when the `dtype` of the reduction result type does not match with the one of the array items.\r\n\r\nPlease find below the performance results of `numpy.sum` with an array of 1 MiB on my Linux machine with an Intel i5-9600KF processor. The tests have been made for all the available integer types with and without a custom `dtype` argument set to the array `dtype` value (it is either `int64` or `uint64` by default on my machine).\r\n\r\n```\r\nOld:\r\n    Default dtype argument:\r\n        int8:    687 \u00b5s \u00b1 4.15 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint8:   661 \u00b5s \u00b1 2.92 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int16:   346 \u00b5s \u00b1 1.49 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint16:  368 \u00b5s \u00b1 3.64 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int32:   152 \u00b5s \u00b1 1.48 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint32:  196 \u00b5s \u00b1 638 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        int64:   52.4 \u00b5s \u00b1 120 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint64:  52.9 \u00b5s \u00b1 521 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n\r\n    Custom dtype argument:\r\n        int8:    404 \u00b5s \u00b1 3.81 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint8:   408 \u00b5s \u00b1 751 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int16:   203 \u00b5s \u00b1 1.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint16:  203 \u00b5s \u00b1 1.97 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int32:   102 \u00b5s \u00b1 551 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint32:  102 \u00b5s \u00b1 438 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        int64:   52.7 \u00b5s \u00b1 338 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint64:  52.9 \u00b5s \u00b1 382 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n\r\nNew:\r\n    Default dtype argument:\r\n        int8:    263 \u00b5s \u00b1 1.94 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint8:   248 \u00b5s \u00b1 2.05 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int16:   120 \u00b5s \u00b1 1.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint16:  116 \u00b5s \u00b1 394 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int32:   58.7 \u00b5s \u00b1 425 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint32:  58.6 \u00b5s \u00b1 759 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        int64:   17.1 \u00b5s \u00b1 130 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint64:  16.9 \u00b5s \u00b1 291 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n\r\n    Custom dtype argument:\r\n        int8:    21.7 \u00b5s \u00b1 246 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint8:   20.8 \u00b5s \u00b1 233 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int16:   18.1 \u00b5s \u00b1 170 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        uint16:  18.1 \u00b5s \u00b1 230 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n        int32:   17.6 \u00b5s \u00b1 83.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint32:  17.9 \u00b5s \u00b1 109 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        int64:   16.9 \u00b5s \u00b1 117 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n        uint64:  17.4 \u00b5s \u00b1 246 ns per loop (mean \u00b1 std. dev. of 7 runs, 4000 loops each)\r\n\r\nSpeed up (ie. New/old):\r\n    Default dtype argument:\r\n        int8:     2.61\r\n        uint8:    2.67\r\n        int16:    2.88\r\n        uint16:   3.17\r\n        int32:    2.59\r\n        uint32:   3.34\r\n        int64:    3.06\r\n        uint64:   3.13\r\n\r\n    Custom dtype argument:\r\n        int8:    18.61\r\n        uint8:   19.61\r\n        int16:   11.21\r\n        uint16:  11.21\r\n        int32:    5.80\r\n        uint32:   5.70\r\n        int64:    3.12\r\n        uint64:   3.04\r\n```\r\n\r\nAs you can see, it significantly improves the execution time (by a factor up to ~20x). This is especially the case when a `dtype` argument is provided since a (slow sub-optimal) conversion function is applied otherwise (as described in the above detailed analysis link).\r\n\r\nSimilar improvements applies to functions like `np.prod`, `np.add.reduce`, `np.subtract.reduce`, `np.logical_and.reduce`, etc.\r\nIt turns out that improving the casting function also results in a substantial faster execution time of many functions like `np.cumsum`, `np.cumprod`, `np.all` and `np.any`. Even functions like `np.average`, `np.mean` or basic multiplications/additions/subtractions by a constant of a different type requiring the array to be casted are a bit faster with an AVX2 casting function.",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/21001.performance.rst",
                "patch": "@@ -0,0 +1,5 @@\n+Faster reduction operators\n+--------------------------\n+Reduction operations like `numpy.sum`, `numpy.prod`, `numpy.add.reduce`, \n+`numpy.logical_and.reduce` on contiguous integer-based arrays are now \n+much faster."
            },
            {
                "filename": "numpy/core/src/umath/fast_loop_macros.h",
                "patch": "@@ -103,6 +103,9 @@ abs_ptrdiff(char *a, char *b)\n         && (steps[0] == steps[2])\\\n         && (steps[0] == 0))\n \n+/* input contiguous (for binary reduces only) */\n+#define IS_BINARY_REDUCE_INPUT_CONT(tin) (steps[1] == sizeof(tin))\n+\n /* binary loop input and output contiguous */\n #define IS_BINARY_CONT(tin, tout) (steps[0] == sizeof(tin) && \\\n                                    steps[1] == sizeof(tin) && \\\n@@ -252,6 +255,34 @@ abs_ptrdiff(char *a, char *b)\n     TYPE io1 = *(TYPE *)iop1; \\\n     BINARY_REDUCE_LOOP_INNER\n \n+/*\n+ * op should be the code working on `TYPE in2` and\n+ * reading/storing the result in `TYPE *io1`\n+ */\n+#define BASE_BINARY_REDUCE_LOOP(TYPE, op) \\\n+    BINARY_REDUCE_LOOP_INNER { \\\n+        const TYPE in2 = *(TYPE *)ip2; \\\n+        op; \\\n+    }\n+\n+#define BINARY_REDUCE_LOOP_FAST_INNER(TYPE, op)\\\n+    /* condition allows compiler to optimize the generic macro */ \\\n+    if(IS_BINARY_REDUCE_INPUT_CONT(TYPE)) { \\\n+        BASE_BINARY_REDUCE_LOOP(TYPE, op) \\\n+    } \\\n+    else { \\\n+        BASE_BINARY_REDUCE_LOOP(TYPE, op) \\\n+    }\n+\n+#define BINARY_REDUCE_LOOP_FAST(TYPE, op)\\\n+    do { \\\n+        char *iop1 = args[0]; \\\n+        TYPE io1 = *(TYPE *)iop1; \\\n+        BINARY_REDUCE_LOOP_FAST_INNER(TYPE, op); \\\n+        *((TYPE *)iop1) = io1; \\\n+    } \\\n+    while (0)\n+\n #define IS_BINARY_STRIDE_ONE(esize, vsize) \\\n     ((steps[0] == esize) && \\\n      (steps[1] == esize) && \\"
            },
            {
                "filename": "numpy/core/src/umath/loops.c.src",
                "patch": "@@ -636,10 +636,7 @@ NPY_NO_EXPORT NPY_GCC_OPT_3 @ATTR@ void\n @TYPE@_@kind@@isa@(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))\n {\n     if (IS_BINARY_REDUCE) {\n-        BINARY_REDUCE_LOOP(@type@) {\n-            io1 @OP@= *(@type@ *)ip2;\n-        }\n-        *((@type@ *)iop1) = io1;\n+        BINARY_REDUCE_LOOP_FAST(@type@, io1 @OP@= in2);\n     }\n     else {\n         BINARY_LOOP_FAST(@type@, @type@, *out = in1 @OP@ in2);"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 21061,
        "body": "This PR:\r\n- adds vsx3 and vsx4 as targets when building `sin/cos` (FP32)\r\n- adds vsx4 as target when building `tanh`\r\n\r\nThe compiler generates better code when we use vsx3/vsx4 flags on Power9/Power10, respectively.\r\n\r\n**Power10/VSX4**:\r\n```\r\nTANH\r\n-        1.28\u00b10ms        550\u00b10.5\u03bcs     0.43  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 2, 1, 'f')\r\n-        1.31\u00b10ms          530\u00b13\u03bcs     0.41  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'f')\r\n-        1.30\u00b10ms        525\u00b10.6\u03bcs     0.40  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 2, 'f')\r\n-        1.28\u00b10ms        502\u00b10.5\u03bcs     0.39  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n\r\nCOS/SIN\r\n-       391\u00b10.3\u03bcs        237\u00b10.3\u03bcs     0.60  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 4, 1, 'f')\r\n-       366\u00b10.2\u03bcs        211\u00b10.6\u03bcs     0.58  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 1, 'f')\r\n-       406\u00b10.8\u03bcs        233\u00b10.2\u03bcs     0.57  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 4, 1, 'f')\r\n-         375\u00b12\u03bcs        212\u00b10.7\u03bcs     0.56  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 1, 'f')\r\n```\r\n\r\n**Power9/VSX3**:\r\n```\r\nTANH\r\n+        1.48\u00b10ms         1.63\u00b10ms     1.10  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'd')\r\n+     1.55\u00b10.01ms      1.70\u00b10.01ms     1.09  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 4, 'd')\r\n-        1.13\u00b10ms        906\u00b10.3\u03bcs     0.80  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 4, 1, 'f')\r\n-        1.06\u00b10ms        838\u00b10.3\u03bcs     0.79  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'tanh'>, 1, 1, 'f')\r\n\r\nCOS/SIN\r\n-        634\u00b120\u03bcs          454\u00b11\u03bcs     0.72  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 2, 1, 'f')\r\n-        629\u00b120\u03bcs          448\u00b18\u03bcs     0.71  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 2, 1, 'f')\r\n-         671\u00b12\u03bcs        477\u00b10.6\u03bcs     0.71  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'sin'>, 1, 2, 'f')\r\n-        601\u00b120\u03bcs          425\u00b16\u03bcs     0.71  bench_ufunc_strides.Unary.time_ufunc(<ufunc 'cos'>, 1, 1, 'f')\r\n```\r\n\r\n- I did not add vsx3 for `tanh` since we have slowdown for dtype=double\r\n- The improvements above show up only for `gcc >=10`\r\n\r\n",
        "changed_files": [
            {
                "filename": "numpy/core/src/umath/loops_hyperbolic.dispatch.c.src",
                "patch": "@@ -1,7 +1,7 @@\n /*@targets\n  ** $maxopt baseline\n  ** (avx2 fma3) AVX512_SKX\n- ** vsx2\n+ ** vsx2 vsx4\n  ** neon_vfpv4\n  **/\n #include \"numpy/npy_math.h\""
            },
            {
                "filename": "numpy/core/src/umath/loops_trigonometric.dispatch.c.src",
                "patch": "@@ -1,7 +1,7 @@\n /*@targets\n  ** $maxopt baseline\n  ** (avx2 fma3) avx512f\n- ** vsx2\n+ ** vsx2 vsx3 vsx4\n  ** neon_vfpv4\n  **/\n #include \"numpy/npy_math.h\""
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20846,
        "body": "  for all integers, f32 and f64 data types on all\r\n  supported architectures via universal intrinsics.\r\n\r\nrelated to #20785, #20131\r\n\r\n\r\n### X86\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nCPU(s):                          4\r\nOn-line CPU(s) list:             0-3\r\nThread(s) per core:              2\r\nCore(s) per socket:              2\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz\r\nStepping:                        4\r\nCPU MHz:                         3410.808\r\nBogoMIPS:                        5999.99\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       64 KiB\r\nL1i cache:                       64 KiB\r\nL2 cache:                        2 MiB\r\nL3 cache:                        24.8 MiB\r\nNUMA node0 CPU(s):               0-3\r\nVulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:              Mitigation; PTE Inversion\r\nVulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Full generic retpoline, STIBP disabled, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant\r\n                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt\r\n                                 tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep b\r\n                                 mi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat\r\n                                 pku ospke\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux ip-172-31-32-40 5.11.0-1020-aws #21~20.04.2-Ubuntu SMP Fri Oct 1 13:03:59 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\nPython 3.8.10\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n<details>\r\n <summary>AVX512_SKX</summary>\r\n\r\n```Bash\r\nunset NPY_DISABLE_CPU_FEATURES\r\npython runtests.py -n --bench-compare parent/main \"argmax|argmin\" -- --sort ratio\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [4b985e2b]       [c99921ad]\r\n     <simd_argmaxmin~1>       <simd_argmaxmin>\r\n-        1.66\u00b10ms         1.54\u00b10ms     0.93  bench_lib.Nan.time_nanargmax(200000, 50.0)\r\n-         926\u00b13\u03bcs          850\u00b13\u03bcs     0.92  bench_lib.Nan.time_nanargmax(200000, 90.0)\r\n-        1.72\u00b10ms         1.55\u00b10ms     0.90  bench_lib.Nan.time_nanargmin(200000, 50.0)\r\n-         380\u00b13\u03bcs        325\u00b10.4\u03bcs     0.86  bench_lib.Nan.time_nanargmax(200000, 0.1)\r\n-         993\u00b14\u03bcs          849\u00b13\u03bcs     0.85  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-         378\u00b13\u03bcs        323\u00b10.2\u03bcs     0.85  bench_lib.Nan.time_nanargmax(200000, 0)\r\n-         482\u00b13\u03bcs        406\u00b10.8\u03bcs     0.84  bench_lib.Nan.time_nanargmax(200000, 2.0)\r\n-         548\u00b13\u03bcs        407\u00b10.8\u03bcs     0.74  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-         444\u00b13\u03bcs        326\u00b10.9\u03bcs     0.73  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-         441\u00b13\u03bcs        322\u00b10.9\u03bcs     0.73  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-     6.49\u00b10.08\u03bcs      4.45\u00b10.01\u03bcs     0.68  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-       182\u00b10.3\u03bcs       59.8\u00b10.4\u03bcs     0.33  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-       182\u00b10.1\u03bcs       59.6\u00b10.6\u03bcs     0.33  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-      182\u00b10.07\u03bcs       59.2\u00b10.2\u03bcs     0.33  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-       124\u00b10.4\u03bcs        26.8\u00b120\u03bcs     0.22  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-       122\u00b10.4\u03bcs       18.9\u00b10.2\u03bcs     0.16  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-       180\u00b10.2\u03bcs       18.8\u00b10.1\u03bcs     0.10  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-       180\u00b10.2\u03bcs       18.6\u00b10.4\u03bcs     0.10  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-         202\u00b14\u03bcs        19.1\u00b120\u03bcs     0.09  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-         204\u00b14\u03bcs        19.1\u00b120\u03bcs     0.09  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-       120\u00b10.1\u03bcs      9.56\u00b10.04\u03bcs     0.08  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       120\u00b10.2\u03bcs      9.54\u00b10.02\u03bcs     0.08  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-         218\u00b13\u03bcs         13.7\u00b12\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-         193\u00b15\u03bcs         11.0\u00b14\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-         212\u00b12\u03bcs         11.0\u00b13\u03bcs     0.05  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-      179\u00b10.07\u03bcs      6.15\u00b10.02\u03bcs     0.03  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-      179\u00b10.05\u03bcs      6.12\u00b10.02\u03bcs     0.03  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-       218\u00b10.7\u03bcs         7.07\u00b11\u03bcs     0.03  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-         219\u00b11\u03bcs         7.05\u00b11\u03bcs     0.03  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-         190\u00b12\u03bcs      6.01\u00b10.02\u03bcs     0.03  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-         191\u00b13\u03bcs      6.02\u00b10.01\u03bcs     0.03  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n <summary>AVX2</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX512F AVX512_SKX\"\r\npython runtests.py -n --bench-compare parent/main \"argmax|argmin\" -- --sort ratio\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [4b985e2b]       [c99921ad]\r\n     <simd_argmaxmin~1>       <simd_argmaxmin>\r\n-         964\u00b13\u03bcs         895\u00b110\u03bcs     0.93  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-        1.65\u00b10ms         1.53\u00b10ms     0.93  bench_lib.Nan.time_nanargmin(200000, 50.0)\r\n-         365\u00b11\u03bcs        332\u00b10.9\u03bcs     0.91  bench_lib.Nan.time_nanargmax(200000, 0)\r\n-       370\u00b10.5\u03bcs          336\u00b11\u03bcs     0.91  bench_lib.Nan.time_nanargmax(200000, 0.1)\r\n-     6.49\u00b10.06\u03bcs      5.80\u00b10.04\u03bcs     0.89  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-         470\u00b12\u03bcs          417\u00b11\u03bcs     0.89  bench_lib.Nan.time_nanargmax(200000, 2.0)\r\n-         532\u00b12\u03bcs        418\u00b10.5\u03bcs     0.79  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-       432\u00b10.8\u03bcs        335\u00b10.6\u03bcs     0.78  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-         428\u00b12\u03bcs          332\u00b11\u03bcs     0.77  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-       124\u00b10.1\u03bcs         61.4\u00b17\u03bcs     0.50  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-      182\u00b10.09\u03bcs       75.1\u00b10.3\u03bcs     0.41  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-      182\u00b10.08\u03bcs       71.4\u00b10.3\u03bcs     0.39  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-      182\u00b10.09\u03bcs       66.2\u00b10.3\u03bcs     0.36  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-         202\u00b14\u03bcs         59.5\u00b16\u03bcs     0.29  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-       122\u00b10.3\u03bcs       35.3\u00b10.3\u03bcs     0.29  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-         203\u00b13\u03bcs         49.7\u00b18\u03bcs     0.25  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-      180\u00b10.09\u03bcs       32.5\u00b10.3\u03bcs     0.18  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-       180\u00b10.1\u03bcs       28.8\u00b10.2\u03bcs     0.16  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-         198\u00b14\u03bcs         30.0\u00b12\u03bcs     0.15  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-         217\u00b14\u03bcs         32.0\u00b12\u03bcs     0.15  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-       120\u00b10.2\u03bcs      17.3\u00b10.05\u03bcs     0.14  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-       120\u00b10.1\u03bcs      15.2\u00b10.02\u03bcs     0.13  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-         213\u00b15\u03bcs         25.2\u00b11\u03bcs     0.12  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-       218\u00b10.7\u03bcs       16.4\u00b10.2\u03bcs     0.08  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-         217\u00b12\u03bcs      15.3\u00b10.03\u03bcs     0.07  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-      179\u00b10.05\u03bcs      9.81\u00b10.04\u03bcs     0.05  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-         193\u00b14\u03bcs      9.80\u00b10.04\u03bcs     0.05  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-      179\u00b10.07\u03bcs      8.47\u00b10.01\u03bcs     0.05  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-         193\u00b13\u03bcs      8.64\u00b10.03\u03bcs     0.04  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n```\r\n</details>\r\n\r\n<details>\r\n <summary>SSE42</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"AVX2 AVX512F AVX512_SKX\"\r\npython runtests.py -n --bench-compare parent/main \"argmax|argmin\" -- --sort ratio\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [4b985e2b]       [c99921ad]\r\n     <simd_argmaxmin~1>       <simd_argmaxmin>\r\n-        1.60\u00b10ms         1.52\u00b10ms     0.95  bench_lib.Nan.time_nanargmax(200000, 50.0)\r\n-       470\u00b10.7\u03bcs          442\u00b14\u03bcs     0.94  bench_lib.Nan.time_nanargmax(200000, 2.0)\r\n-     6.55\u00b10.08\u03bcs      6.05\u00b10.02\u03bcs     0.92  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-        1.66\u00b10ms         1.51\u00b10ms     0.91  bench_lib.Nan.time_nanargmin(200000, 50.0)\r\n-         964\u00b12\u03bcs          865\u00b12\u03bcs     0.90  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-       430\u00b10.9\u03bcs          359\u00b13\u03bcs     0.84  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-         433\u00b11\u03bcs          360\u00b13\u03bcs     0.83  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-         534\u00b11\u03bcs          441\u00b13\u03bcs     0.83  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-       124\u00b10.1\u03bcs         95.6\u00b13\u03bcs     0.77  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-       182\u00b10.1\u03bcs        102\u00b10.9\u03bcs     0.56  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-       182\u00b10.1\u03bcs        101\u00b10.4\u03bcs     0.55  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-       182\u00b10.2\u03bcs       81.6\u00b10.2\u03bcs     0.45  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-         122\u00b14\u03bcs       54.3\u00b10.4\u03bcs     0.44  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-        213\u00b110\u03bcs         89.3\u00b16\u03bcs     0.42  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-         209\u00b18\u03bcs         72.9\u00b16\u03bcs     0.35  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-       180\u00b10.2\u03bcs       48.3\u00b10.2\u03bcs     0.27  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-         218\u00b13\u03bcs         48.4\u00b12\u03bcs     0.22  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-      180\u00b10.06\u03bcs       39.3\u00b10.4\u03bcs     0.22  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-        209\u00b110\u03bcs         45.2\u00b11\u03bcs     0.22  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-         122\u00b12\u03bcs      24.1\u00b10.03\u03bcs     0.20  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-         219\u00b14\u03bcs         38.7\u00b11\u03bcs     0.18  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-         120\u00b12\u03bcs      19.4\u00b10.01\u03bcs     0.16  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       219\u00b10.5\u03bcs      24.2\u00b10.05\u03bcs     0.11  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-       219\u00b10.9\u03bcs      20.5\u00b10.04\u03bcs     0.09  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-      179\u00b10.05\u03bcs      13.2\u00b10.01\u03bcs     0.07  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-        207\u00b110\u03bcs      13.3\u00b10.03\u03bcs     0.06  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-       179\u00b10.1\u03bcs      11.0\u00b10.02\u03bcs     0.06  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-        209\u00b110\u03bcs      11.1\u00b10.02\u03bcs     0.05  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n```\r\n</details>\r\n\r\n<details>\r\n <summary>BASELINE(SSE3)</summary>\r\n\r\n```Bash\r\nexport NPY_DISABLE_CPU_FEATURES=\"SSE42 AVX2 AVX512F AVX512_SKX\"\r\npython runtests.py -n --bench-compare parent/main \"argmax|argmin\" -- --sort ratio\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [4b985e2b]       [c99921ad]\r\n     <simd_argmaxmin~1>       <simd_argmaxmin>\r\n-        1.66\u00b10ms      1.54\u00b10.01ms     0.93  bench_lib.Nan.time_nanargmin(200000, 50.0)\r\n-         198\u00b15\u03bcs          183\u00b11\u03bcs     0.93  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-         967\u00b13\u03bcs          883\u00b12\u03bcs     0.91  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-         433\u00b12\u03bcs          376\u00b14\u03bcs     0.87  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-         438\u00b12\u03bcs          377\u00b13\u03bcs     0.86  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-       124\u00b10.3\u03bcs          106\u00b16\u03bcs     0.86  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-         535\u00b13\u03bcs          457\u00b14\u03bcs     0.85  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-       182\u00b10.2\u03bcs        153\u00b10.6\u03bcs     0.84  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-         202\u00b13\u03bcs          150\u00b11\u03bcs     0.75  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-       182\u00b10.2\u03bcs          115\u00b12\u03bcs     0.63  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-       122\u00b10.4\u03bcs       64.2\u00b10.9\u03bcs     0.53  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-       180\u00b10.1\u03bcs       58.4\u00b10.6\u03bcs     0.32  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-       180\u00b10.2\u03bcs       50.4\u00b10.8\u03bcs     0.28  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-         197\u00b17\u03bcs         55.0\u00b12\u03bcs     0.28  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-         218\u00b13\u03bcs         58.4\u00b13\u03bcs     0.27  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-       121\u00b10.2\u03bcs       29.6\u00b10.5\u03bcs     0.25  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-         213\u00b12\u03bcs         47.0\u00b11\u03bcs     0.22  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-       120\u00b10.3\u03bcs       24.0\u00b10.4\u03bcs     0.20  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       219\u00b10.4\u03bcs       29.3\u00b10.2\u03bcs     0.13  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-       219\u00b10.9\u03bcs       23.7\u00b10.2\u03bcs     0.11  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-       179\u00b10.2\u03bcs       17.7\u00b10.1\u03bcs     0.10  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-         192\u00b11\u03bcs      17.7\u00b10.08\u03bcs     0.09  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-       179\u00b10.1\u03bcs      15.4\u00b10.06\u03bcs     0.09  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-         188\u00b12\u03bcs      14.8\u00b10.08\u03bcs     0.08  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n```\r\n</details>\r\n\r\n----\r\n\r\n### Power little-endian\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```\r\nArchitecture:                    ppc64le\r\nByte Order:                      Little Endian\r\nCPU(s):                          8\r\nOn-line CPU(s) list:             0-7\r\nThread(s) per core:              1\r\nCore(s) per socket:              1\r\nSocket(s):                       8\r\nNUMA node(s):                    1\r\nModel:                           2.2 (pvr 004e 1202)\r\nModel name:                      POWER9 (architected), altivec supported\r\nL1d cache:                       256 KiB\r\nL1i cache:                       256 KiB\r\nNUMA node0 CPU(s):               0-7\r\nVulnerability L1tf:              Not affected\r\nVulnerability Meltdown:          Mitigation; RFI Flush\r\nVulnerability Spec store bypass: Mitigation; Kernel entry/exit barrier (eieio)\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Vulnerable\r\n\r\nprocessor   : 7\r\ncpu     : POWER9 (architected), altivec supported\r\nclock       : 2200.000000MHz\r\nrevision    : 2.2 (pvr 004e 1202)\r\n\r\ntimebase    : 512000000\r\nplatform    : pSeries\r\nmodel       : IBM pSeries (emulated by qemu)\r\nmachine     : CHRP IBM pSeries (emulated by qemu)\r\nMMU     : Radix\r\n\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux e517009a912a 4.19.0-2-powerpc64le #1 SMP Debian 4.19.16-1 (2019-01-17) ppc64le ppc64le ppc64le GNU/Linux\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>baseline(VSX2)</summary>\r\n\r\n```Bash\r\npython runtests.py -n --bench-compare parent/main \"argmax|argmin\" -- --sort ratio\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [4b985e2b]       [c99921ad]\r\n     <simd_argmaxmin~1>       <simd_argmaxmin>\r\n-     1.90\u00b10.01ms         1.78\u00b10ms     0.93  bench_lib.Nan.time_nanargmin(200000, 90.0)\r\n-       294\u00b10.6\u03bcs        275\u00b10.9\u03bcs     0.93  bench_reduce.ArgMax.time_argmax(<class 'numpy.float64'>)\r\n-     1.10\u00b10.01ms          997\u00b17\u03bcs     0.90  bench_lib.Nan.time_nanargmin(200000, 2.0)\r\n-         896\u00b16\u03bcs          778\u00b12\u03bcs     0.87  bench_lib.Nan.time_nanargmin(200000, 0.1)\r\n-        885\u00b110\u03bcs          767\u00b12\u03bcs     0.87  bench_lib.Nan.time_nanargmin(200000, 0)\r\n-         373\u00b12\u03bcs        275\u00b10.3\u03bcs     0.74  bench_reduce.ArgMin.time_argmin(<class 'numpy.float64'>)\r\n-       219\u00b10.2\u03bcs        156\u00b10.5\u03bcs     0.71  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-         219\u00b12\u03bcs          154\u00b11\u03bcs     0.70  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-      218\u00b10.08\u03bcs        152\u00b10.3\u03bcs     0.70  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-       218\u00b10.2\u03bcs       152\u00b10.08\u03bcs     0.70  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-       218\u00b10.3\u03bcs          148\u00b12\u03bcs     0.68  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-       219\u00b10.8\u03bcs          149\u00b12\u03bcs     0.68  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-      237\u00b10.09\u03bcs        156\u00b10.4\u03bcs     0.66  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-         235\u00b11\u03bcs          155\u00b11\u03bcs     0.66  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-       329\u00b10.2\u03bcs        178\u00b10.3\u03bcs     0.54  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-         327\u00b11\u03bcs          176\u00b11\u03bcs     0.54  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-      216\u00b10.07\u03bcs       50.3\u00b10.1\u03bcs     0.23  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-         215\u00b11\u03bcs         50.0\u00b11\u03bcs     0.23  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-       233\u00b10.5\u03bcs      50.6\u00b10.06\u03bcs     0.22  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       232\u00b10.9\u03bcs       48.9\u00b10.9\u03bcs     0.21  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-      216\u00b10.06\u03bcs       28.7\u00b10.1\u03bcs     0.13  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-       216\u00b10.6\u03bcs      28.5\u00b10.04\u03bcs     0.13  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n-       214\u00b10.5\u03bcs       28.0\u00b10.1\u03bcs     0.13  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-       233\u00b10.1\u03bcs       28.6\u00b10.1\u03bcs     0.12  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n-       233\u00b10.2\u03bcs       28.6\u00b10.1\u03bcs     0.12  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n```\r\n</details>\r\n\r\n----\r\n\r\n### AArch64\r\n\r\n<details>\r\n<summary>CPU</summary>\r\n\r\n```Bash\r\nArchitecture:                    aarch64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nCPU(s):                          2\r\nOn-line CPU(s) list:             0,1\r\nThread(s) per core:              1\r\nCore(s) per socket:              2\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       ARM\r\nModel:                           1\r\nModel name:                      Neoverse-N1\r\nStepping:                        r3p1\r\nBogoMIPS:                        243.75\r\nL1d cache:                       128 KiB\r\nL1i cache:                       128 KiB\r\nL2 cache:                        2 MiB\r\nL3 cache:                        32 MiB\r\nNUMA node0 CPU(s):               0,1\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp ssbs\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>OS</summary>\r\n\r\n```Bash\r\nLinux ip-172-31-44-172 5.11.0-1020-aws #21~20.04.2-Ubuntu SMP Fri Oct 1 13:01:34 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n```\r\n</details>\r\n\r\n#### Benchmark\r\n\r\n<details>\r\n <summary>baseline(ASIMD)</summary>\r\n\r\n```Bash\r\npython runtests.py --bench-compare parent/main \"argmax|argmin\" -- --sort ratio\r\n```\r\n```Bash\r\n       before           after         ratio\r\n     [4b985e2b]       [c99921ad]\r\n     <simd_argmaxmin~1>       <simd_argmaxmin>\r\n-     20.5\u00b10.02\u03bcs      18.2\u00b10.01\u03bcs     0.89  bench_reduce.ArgMax.time_argmax(<class 'bool'>)\r\n-         169\u00b12\u03bcs         88.9\u00b14\u03bcs     0.53  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint64'>)\r\n-         169\u00b12\u03bcs         88.9\u00b13\u03bcs     0.53  bench_reduce.ArgMax.time_argmax(<class 'numpy.int64'>)\r\n-       165\u00b10.1\u03bcs       83.1\u00b10.6\u03bcs     0.50  bench_reduce.ArgMin.time_argmin(<class 'numpy.int64'>)\r\n-       165\u00b10.3\u03bcs       82.8\u00b10.5\u03bcs     0.50  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint64'>)\r\n-         246\u00b11\u03bcs       81.1\u00b10.9\u03bcs     0.33  bench_reduce.ArgMax.time_argmax(<class 'numpy.float32'>)\r\n-       244\u00b10.2\u03bcs       79.1\u00b10.2\u03bcs     0.32  bench_reduce.ArgMin.time_argmin(<class 'numpy.float32'>)\r\n-       166\u00b10.8\u03bcs         45.1\u00b12\u03bcs     0.27  bench_reduce.ArgMax.time_argmax(<class 'numpy.int32'>)\r\n-       166\u00b10.9\u03bcs         44.8\u00b11\u03bcs     0.27  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint32'>)\r\n-       164\u00b10.2\u03bcs       43.1\u00b10.3\u03bcs     0.26  bench_reduce.ArgMin.time_argmin(<class 'numpy.int32'>)\r\n-      164\u00b10.06\u03bcs       43.0\u00b10.2\u03bcs     0.26  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint32'>)\r\n-       170\u00b10.4\u03bcs       24.9\u00b10.5\u03bcs     0.15  bench_reduce.ArgMax.time_argmax(<class 'numpy.int16'>)\r\n-       170\u00b10.3\u03bcs       24.9\u00b10.5\u03bcs     0.15  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint16'>)\r\n-       169\u00b10.1\u03bcs       24.6\u00b10.2\u03bcs     0.15  bench_reduce.ArgMin.time_argmin(<class 'numpy.int16'>)\r\n-       169\u00b10.2\u03bcs      24.5\u00b10.07\u03bcs     0.15  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint16'>)\r\n-       163\u00b10.1\u03bcs      13.9\u00b10.04\u03bcs     0.08  bench_reduce.ArgMax.time_argmax(<class 'numpy.int8'>)\r\n-       163\u00b10.1\u03bcs      13.9\u00b10.09\u03bcs     0.08  bench_reduce.ArgMin.time_argmin(<class 'numpy.int8'>)\r\n-      163\u00b10.09\u03bcs      13.8\u00b10.01\u03bcs     0.08  bench_reduce.ArgMin.time_argmin(<class 'numpy.uint8'>)\r\n-       163\u00b10.1\u03bcs      13.8\u00b10.03\u03bcs     0.08  bench_reduce.ArgMax.time_argmax(<class 'numpy.uint8'>)\r\n```\r\n</details>\r\n\r\n-----\r\n\r\n#### Binary size(striped)\r\n\r\n| LIB         | Before(KB) | After(KB) | Diff(KB) |\r\n| ----------- | ------------- | ------------ | ---------- |\r\n| _multiarray_umath.cpython-38-x86_64-linux-gnu.so | 4428 | 4532 | 104 |\r\n| _multiarray_umath.cpython-38-powerpc64le-linux-gnu.so | 4264 | 4292 | 29 |\r\n| _multiarray_umath.cpython-38-aarch64-linux-gnu.so | 3424 | 3444 | 20 |\r\n",
        "changed_files": [
            {
                "filename": "benchmarks/benchmarks/bench_reduce.py",
                "patch": "@@ -73,7 +73,8 @@ def time_max(self, dtype):\n         np.fmax.reduce(self.d)\n \n class ArgMax(Benchmark):\n-    params = [np.float32, np.float64, bool]\n+    params = [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32,\n+              np.int64, np.uint64, np.float32, np.float64, bool]\n     param_names = ['dtype']\n \n     def setup(self, dtype):\n@@ -82,6 +83,17 @@ def setup(self, dtype):\n     def time_argmax(self, dtype):\n         np.argmax(self.d)\n \n+class ArgMin(Benchmark):\n+    params = [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32,\n+              np.int64, np.uint64, np.float32, np.float64, bool]\n+    param_names = ['dtype']\n+\n+    def setup(self, dtype):\n+        self.d = np.ones(200000, dtype=dtype)\n+\n+    def time_argmin(self, dtype):\n+        np.argmin(self.d)\n+\n class SmallReduction(Benchmark):\n     def setup(self):\n         self.d = np.ones(100, dtype=np.float32)"
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -830,7 +830,7 @@ def gl_if_msvc(build_cmd):\n     multiarray_deps = [\n             join('src', 'multiarray', 'abstractdtypes.h'),\n             join('src', 'multiarray', 'arrayobject.h'),\n-            join('src', 'multiarray', 'arraytypes.h'),\n+            join('src', 'multiarray', 'arraytypes.h.src'),\n             join('src', 'multiarray', 'arrayfunction_override.h'),\n             join('src', 'multiarray', 'array_coercion.h'),\n             join('src', 'multiarray', 'array_method.h'),\n@@ -892,7 +892,9 @@ def gl_if_msvc(build_cmd):\n             join('src', 'multiarray', 'abstractdtypes.c'),\n             join('src', 'multiarray', 'alloc.c'),\n             join('src', 'multiarray', 'arrayobject.c'),\n+            join('src', 'multiarray', 'arraytypes.h.src'),\n             join('src', 'multiarray', 'arraytypes.c.src'),\n+            join('src', 'multiarray', 'argfunc.dispatch.c.src'),\n             join('src', 'multiarray', 'array_coercion.c'),\n             join('src', 'multiarray', 'array_method.c'),\n             join('src', 'multiarray', 'array_assign_scalar.c'),"
            },
            {
                "filename": "numpy/core/src/multiarray/argfunc.dispatch.c.src",
                "patch": "@@ -0,0 +1,394 @@\n+/* -*- c -*- */\n+/*@targets\n+ ** $maxopt baseline\n+ ** sse2 sse42 xop avx2 avx512_skx\n+ ** vsx2\n+ ** neon asimd\n+ **/\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+\n+#include \"simd/simd.h\"\n+#include \"numpy/npy_math.h\"\n+\n+#include \"arraytypes.h\"\n+\n+#define MIN(a,b) (((a)<(b))?(a):(b))\n+\n+#if NPY_SIMD\n+#if NPY_SIMD > 512 || NPY_SIMD < 0\n+    #error \"the following 8/16-bit argmax kernel isn't applicable for larger SIMD\"\n+    // TODO: add special loop for large SIMD width.\n+    // i.e avoid unroll by x4 should be numerically safe till 2048-bit SIMD width\n+    // or maybe expand the indices to 32|64-bit vectors(slower).\n+#endif\n+/**begin repeat\n+ * #sfx = u8, s8, u16, s16#\n+ * #usfx = u8, u8, u16, u16#\n+ * #bsfx = b8, b8, b16, b16#\n+ * #idx_max = NPY_MAX_UINT8*2, NPY_MAX_UINT16*2#\n+ */\n+/**begin repeat1\n+ * #intrin = cmpgt, cmplt#\n+ * #func = argmax, argmin#\n+ * #op = >, <#\n+ */\n+static inline npy_intp\n+simd_@func@_@sfx@(npyv_lanetype_@sfx@ *ip, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ s_acc = *ip;\n+    npy_intp ret_idx = 0, i = 0;\n+\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep*4;\n+    npyv_lanetype_@usfx@ d_vindices[npyv_nlanes_@sfx@*4];\n+    for (int vi = 0; vi < wstep; ++vi) {\n+        d_vindices[vi] = vi;\n+    }\n+    const npyv_@usfx@ vindices_0 = npyv_load_@usfx@(d_vindices);\n+    const npyv_@usfx@ vindices_1 = npyv_load_@usfx@(d_vindices + vstep);\n+    const npyv_@usfx@ vindices_2 = npyv_load_@usfx@(d_vindices + vstep*2);\n+    const npyv_@usfx@ vindices_3 = npyv_load_@usfx@(d_vindices + vstep*3);\n+\n+    const npy_intp max_block = @idx_max@*wstep & -wstep;\n+    npy_intp len0 = len & -wstep;\n+    while (i < len0) {\n+        npyv_@sfx@ acc = npyv_setall_@sfx@(s_acc);\n+        npyv_@usfx@ acc_indices = npyv_zero_@usfx@();\n+        npyv_@usfx@ acc_indices_scale = npyv_zero_@usfx@();\n+\n+        npy_intp n = i + MIN(len0 - i, max_block);\n+        npy_intp ik = i, i2 = 0;\n+        for (; i < n; i += wstep, ++i2) {\n+            npyv_@usfx@ vi = npyv_setall_@usfx@((npyv_lanetype_@usfx@)i2);\n+            npyv_@sfx@ a = npyv_load_@sfx@(ip + i);\n+            npyv_@sfx@ b = npyv_load_@sfx@(ip + i + vstep);\n+            npyv_@sfx@ c = npyv_load_@sfx@(ip + i + vstep*2);\n+            npyv_@sfx@ d = npyv_load_@sfx@(ip + i + vstep*3);\n+\n+            // reverse to put lowest index first in case of matched values\n+            npyv_@bsfx@ m_ba = npyv_@intrin@_@sfx@(b, a);\n+            npyv_@bsfx@ m_dc = npyv_@intrin@_@sfx@(d, c);\n+            npyv_@sfx@  x_ba = npyv_select_@sfx@(m_ba, b, a);\n+            npyv_@sfx@  x_dc = npyv_select_@sfx@(m_dc, d, c);\n+            npyv_@bsfx@ m_dcba = npyv_@intrin@_@sfx@(x_dc, x_ba);\n+            npyv_@sfx@  x_dcba = npyv_select_@sfx@(m_dcba, x_dc, x_ba);\n+\n+            npyv_@usfx@ idx_ba = npyv_select_@usfx@(m_ba, vindices_1, vindices_0);\n+            npyv_@usfx@ idx_dc = npyv_select_@usfx@(m_dc, vindices_3, vindices_2);\n+            npyv_@usfx@ idx_dcba = npyv_select_@usfx@(m_dcba, idx_dc, idx_ba);\n+            npyv_@bsfx@ m_acc = npyv_@intrin@_@sfx@(x_dcba, acc);\n+            acc = npyv_select_@sfx@(m_acc, x_dcba, acc);\n+            acc_indices = npyv_select_@usfx@(m_acc, idx_dcba, acc_indices);\n+            acc_indices_scale = npyv_select_@usfx@(m_acc, vi, acc_indices_scale);\n+        }\n+        // reduce\n+        npyv_lanetype_@sfx@ dacc[npyv_nlanes_@sfx@];\n+        npyv_lanetype_@usfx@ dacc_i[npyv_nlanes_@sfx@];\n+        npyv_lanetype_@usfx@ dacc_s[npyv_nlanes_@sfx@];\n+        npyv_store_@sfx@(dacc, acc);\n+        npyv_store_@usfx@(dacc_i, acc_indices);\n+        npyv_store_@usfx@(dacc_s, acc_indices_scale);\n+\n+        for (int vi = 0; vi < vstep; ++vi) {\n+            if (dacc[vi] @op@ s_acc) {\n+                s_acc = dacc[vi];\n+                ret_idx = ik + (npy_intp)dacc_s[vi]*wstep + dacc_i[vi];\n+            }\n+        }\n+        // get the lowest index in case of matched values\n+        for (int vi = 0; vi < vstep; ++vi) {\n+            npy_intp idx = ik + (npy_intp)dacc_s[vi]*wstep + dacc_i[vi];\n+            if (s_acc == dacc[vi] && ret_idx > idx) {\n+                ret_idx = idx;\n+            }\n+        }\n+    }\n+    for (; i < len; ++i) {\n+        npyv_lanetype_@sfx@ a = ip[i];\n+        if (a @op@ s_acc) {\n+            s_acc = a;\n+            ret_idx = i;\n+        }\n+    }\n+    return ret_idx;\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+#endif\n+\n+/**begin repeat\n+ * #sfx = u32, s32, u64, s64, f32, f64#\n+ * #usfx = u32, u32, u64, u64, u32, u64#\n+ * #bsfx = b32, b32, b64, b64, b32, b64#\n+ * #is_fp = 0*4, 1*2#\n+ * #is_idx32 = 1*2, 0*2, 1, 0#\n+ * #chk_simd = NPY_SIMD*5, NPY_SIMD_F64#\n+ */\n+#if @chk_simd@\n+/**begin repeat1\n+ * #intrin = cmpgt, cmplt#\n+ * #func = argmax, argmin#\n+ * #op = >, <#\n+ * #iop = <, >#\n+ */\n+static inline npy_intp\n+simd_@func@_@sfx@(npyv_lanetype_@sfx@ *ip, npy_intp len)\n+{\n+    npyv_lanetype_@sfx@ s_acc = *ip;\n+    npy_intp ret_idx = 0, i = 0;\n+    const int vstep = npyv_nlanes_@sfx@;\n+    const int wstep = vstep*4;\n+    // loop by a scalar will perform better for small arrays\n+    if (len < wstep) {\n+        goto scalar_loop;\n+    }\n+    npy_intp len0 = len;\n+    // guard against wraparound vector addition for 32-bit indices\n+    // in case of the array length is larger than 16gb\n+#if @is_idx32@\n+    if (len0 > NPY_MAX_UINT32) {\n+        len0 = NPY_MAX_UINT32;\n+    }\n+#endif\n+    // create index for vector indices\n+    npyv_lanetype_@usfx@ d_vindices[npyv_nlanes_@sfx@*4];\n+    for (int vi = 0; vi < wstep; ++vi) {\n+        d_vindices[vi] = vi;\n+    }\n+    const npyv_@usfx@ vindices_0 = npyv_load_@usfx@(d_vindices);\n+    const npyv_@usfx@ vindices_1 = npyv_load_@usfx@(d_vindices + vstep);\n+    const npyv_@usfx@ vindices_2 = npyv_load_@usfx@(d_vindices + vstep*2);\n+    const npyv_@usfx@ vindices_3 = npyv_load_@usfx@(d_vindices + vstep*3);\n+    // initialize vector accumulator for highest values and its indexes\n+    npyv_@usfx@ acc_indices = npyv_zero_@usfx@();\n+    npyv_@sfx@ acc = npyv_setall_@sfx@(s_acc);\n+    for (npy_intp n = len0 & -wstep; i < n; i += wstep) {\n+        npyv_@usfx@ vi = npyv_setall_@usfx@((npyv_lanetype_@usfx@)i);\n+        npyv_@sfx@ a = npyv_load_@sfx@(ip + i);\n+        npyv_@sfx@ b = npyv_load_@sfx@(ip + i + vstep);\n+        npyv_@sfx@ c = npyv_load_@sfx@(ip + i + vstep*2);\n+        npyv_@sfx@ d = npyv_load_@sfx@(ip + i + vstep*3);\n+\n+        // reverse to put lowest index first in case of matched values\n+        npyv_@bsfx@ m_ba = npyv_@intrin@_@sfx@(b, a);\n+        npyv_@bsfx@ m_dc = npyv_@intrin@_@sfx@(d, c);\n+        npyv_@sfx@  x_ba = npyv_select_@sfx@(m_ba, b, a);\n+        npyv_@sfx@  x_dc = npyv_select_@sfx@(m_dc, d, c);\n+        npyv_@bsfx@ m_dcba = npyv_@intrin@_@sfx@(x_dc, x_ba);\n+        npyv_@sfx@  x_dcba = npyv_select_@sfx@(m_dcba, x_dc, x_ba);\n+\n+        npyv_@usfx@ idx_ba = npyv_select_@usfx@(m_ba, vindices_1, vindices_0);\n+        npyv_@usfx@ idx_dc = npyv_select_@usfx@(m_dc, vindices_3, vindices_2);\n+        npyv_@usfx@ idx_dcba = npyv_select_@usfx@(m_dcba, idx_dc, idx_ba);\n+        npyv_@bsfx@ m_acc = npyv_@intrin@_@sfx@(x_dcba, acc);\n+        acc = npyv_select_@sfx@(m_acc, x_dcba, acc);\n+        acc_indices = npyv_select_@usfx@(m_acc, npyv_add_@usfx@(vi, idx_dcba), acc_indices);\n+\n+    #if @is_fp@\n+        npyv_@bsfx@ nnan_a = npyv_notnan_@sfx@(a);\n+        npyv_@bsfx@ nnan_b = npyv_notnan_@sfx@(b);\n+        npyv_@bsfx@ nnan_c = npyv_notnan_@sfx@(c);\n+        npyv_@bsfx@ nnan_d = npyv_notnan_@sfx@(d);\n+        npyv_@bsfx@ nnan_ab = npyv_and_@bsfx@(nnan_a, nnan_b);\n+        npyv_@bsfx@ nnan_cd = npyv_and_@bsfx@(nnan_c, nnan_d);\n+        npy_uint64 nnan = npyv_tobits_@bsfx@(npyv_and_@bsfx@(nnan_ab, nnan_cd));\n+        if (nnan != ((1LL << vstep) - 1)) {\n+            npy_uint64 nnan_4[4];\n+            nnan_4[0] = npyv_tobits_@bsfx@(nnan_a);\n+            nnan_4[1] = npyv_tobits_@bsfx@(nnan_b);\n+            nnan_4[2] = npyv_tobits_@bsfx@(nnan_c);\n+            nnan_4[3] = npyv_tobits_@bsfx@(nnan_d);\n+            for (int ni = 0; ni < 4; ++ni) {\n+                for (int vi = 0; vi < vstep; ++vi) {\n+                    if (!((nnan_4[ni] >> vi) & 1)) {\n+                        return i + ni*vstep + vi;\n+                    }\n+                }\n+            }\n+        }\n+    #endif\n+    }\n+    for (npy_intp n = len0 & -vstep; i < n; i += vstep) {\n+        npyv_@usfx@ vi = npyv_setall_@usfx@((npyv_lanetype_@usfx@)i);\n+        npyv_@sfx@ a = npyv_load_@sfx@(ip + i);\n+        npyv_@bsfx@ m_acc = npyv_@intrin@_@sfx@(a, acc);\n+        acc = npyv_select_@sfx@(m_acc, a, acc);\n+        acc_indices = npyv_select_@usfx@(m_acc, npyv_add_@usfx@(vi, vindices_0), acc_indices);\n+    #if @is_fp@\n+        npyv_@bsfx@ nnan_a = npyv_notnan_@sfx@(a);\n+        npy_uint64 nnan = npyv_tobits_@bsfx@(nnan_a);\n+        if (nnan != ((1LL << vstep) - 1)) {\n+            for (int vi = 0; vi < vstep; ++vi) {\n+                if (!((nnan >> vi) & 1)) {\n+                    return i + vi;\n+                }\n+            }\n+        }\n+    #endif\n+    }\n+\n+    // reduce\n+    npyv_lanetype_@sfx@ dacc[npyv_nlanes_@sfx@];\n+    npyv_lanetype_@usfx@ dacc_i[npyv_nlanes_@sfx@];\n+    npyv_store_@usfx@(dacc_i, acc_indices);\n+    npyv_store_@sfx@(dacc, acc);\n+\n+    s_acc = dacc[0];\n+    ret_idx = dacc_i[0];\n+    for (int vi = 1; vi < vstep; ++vi) {\n+        if (dacc[vi] @op@ s_acc) {\n+            s_acc = dacc[vi];\n+            ret_idx = (npy_intp)dacc_i[vi];\n+        }\n+    }\n+    // get the lowest index in case of matched values\n+    for (int vi = 0; vi < vstep; ++vi) {\n+        if (s_acc == dacc[vi] && ret_idx > (npy_intp)dacc_i[vi]) {\n+            ret_idx = dacc_i[vi];\n+        }\n+    }\n+scalar_loop:\n+    for (; i < len; ++i) {\n+        npyv_lanetype_@sfx@ a = ip[i];\n+    #if @is_fp@\n+        if (!(a @iop@= s_acc)) {  // negated, for correct nan handling\n+    #else\n+        if (a @op@ s_acc) {\n+    #endif\n+            s_acc = a;\n+            ret_idx = i;\n+        #if @is_fp@\n+            if (npy_isnan(s_acc)) {\n+                // nan encountered, it's maximal\n+                return ret_idx;\n+            }\n+        #endif\n+        }\n+    }\n+    return ret_idx;\n+}\n+/**end repeat1**/\n+#endif // chk_simd\n+/**end repeat**/\n+\n+/**begin repeat\n+ * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,\n+ *         BYTE, SHORT, INT, LONG, LONGLONG,\n+ *         FLOAT, DOUBLE, LONGDOUBLE#\n+ *\n+ * #BTYPE = BYTE, SHORT, INT, LONG, LONGLONG,\n+ *          BYTE, SHORT, INT, LONG, LONGLONG,\n+ *          FLOAT, DOUBLE, LONGDOUBLE#\n+ * #type = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong,\n+ *         npy_byte, npy_short, npy_int, npy_long, npy_longlong,\n+ *         npy_float, npy_double, npy_longdouble#\n+ *\n+ * #is_fp = 0*10, 1*3#\n+ * #is_unsigned = 1*5, 0*5, 0*3#\n+ */\n+#undef TO_SIMD_SFX\n+#if 0\n+/**begin repeat1\n+ * #len = 8, 16, 32, 64#\n+ */\n+#elif NPY_SIMD && NPY_BITSOF_@BTYPE@ == @len@\n+    #if @is_fp@\n+        #define TO_SIMD_SFX(X) X##_f@len@\n+        #if NPY_BITSOF_@BTYPE@ == 64 && !NPY_SIMD_F64\n+            #undef TO_SIMD_SFX\n+        #endif\n+    #elif @is_unsigned@\n+        #define TO_SIMD_SFX(X) X##_u@len@\n+    #else\n+        #define TO_SIMD_SFX(X) X##_s@len@\n+    #endif\n+/**end repeat1**/\n+#endif\n+\n+/**begin repeat1\n+ * #func = argmax, argmin#\n+ * #op = >, <#\n+ * #iop = <, >#\n+ */\n+NPY_NO_EXPORT int NPY_CPU_DISPATCH_CURFX(@TYPE@_@func@)\n+(@type@ *ip, npy_intp n, npy_intp *mindx, PyArrayObject *NPY_UNUSED(aip))\n+{\n+#if @is_fp@\n+    if (npy_isnan(*ip)) {\n+        // nan encountered; it's maximal|minimal\n+        *mindx = 0;\n+        return 0;\n+    }\n+#endif\n+#ifdef TO_SIMD_SFX\n+    *mindx = TO_SIMD_SFX(simd_@func@)((TO_SIMD_SFX(npyv_lanetype)*)ip, n);\n+    npyv_cleanup();\n+#else\n+    @type@ mp = *ip;\n+    *mindx = 0;\n+    npy_intp i = 1;\n+\n+    for (; i < n; ++i) {\n+        @type@ a = ip[i];\n+    #if @is_fp@\n+        if (!(a @iop@= mp)) {  // negated, for correct nan handling\n+    #else\n+        if (a @op@ mp) {\n+    #endif\n+            mp = a;\n+            *mindx = i;\n+        #if @is_fp@\n+            if (npy_isnan(mp)) {\n+                // nan encountered, it's maximal|minimal\n+                break;\n+            }\n+        #endif\n+        }\n+    }\n+#endif // TO_SIMD_SFX\n+    return 0;\n+}\n+/**end repeat1**/\n+/**end repeat**/\n+\n+NPY_NO_EXPORT int NPY_CPU_DISPATCH_CURFX(BOOL_argmax)\n+(npy_bool *ip, npy_intp len, npy_intp *mindx, PyArrayObject *NPY_UNUSED(aip))\n+\n+{\n+    npy_intp i = 0;\n+#if NPY_SIMD\n+    const npyv_u8 zero = npyv_zero_u8();\n+    const int vstep = npyv_nlanes_u8;\n+    const int wstep = vstep * 4;\n+    for (npy_intp n = len & -wstep; i < n; i += wstep) {\n+        npyv_u8 a = npyv_load_u8(ip + i + vstep*0);\n+        npyv_u8 b = npyv_load_u8(ip + i + vstep*1);\n+        npyv_u8 c = npyv_load_u8(ip + i + vstep*2);\n+        npyv_u8 d = npyv_load_u8(ip + i + vstep*3);\n+        npyv_b8 m_a = npyv_cmpeq_u8(a, zero);\n+        npyv_b8 m_b = npyv_cmpeq_u8(b, zero);\n+        npyv_b8 m_c = npyv_cmpeq_u8(c, zero);\n+        npyv_b8 m_d = npyv_cmpeq_u8(d, zero);\n+        npyv_b8 m_ab = npyv_and_b8(m_a, m_b);\n+        npyv_b8 m_cd = npyv_and_b8(m_c, m_d);\n+        npy_uint64 m = npyv_tobits_b8(npyv_and_b8(m_ab, m_cd));\n+    #if NPY_SIMD == 512\n+        if (m != NPY_MAX_UINT64) {\n+    #else\n+        if ((npy_int64)m != ((1LL << vstep) - 1)) {\n+    #endif\n+            break;\n+        }\n+    }\n+    npyv_cleanup();\n+#endif // NPY_SIMD\n+    for (; i < len; ++i) {\n+        if (ip[i]) {\n+            *mindx = i;\n+            return 0;\n+        }\n+    }\n+    *mindx = 0;\n+    return 0;\n+}"
            },
            {
                "filename": "numpy/core/src/multiarray/arraytypes.c.src",
                "patch": "@@ -27,12 +27,6 @@\n #include \"arrayobject.h\"\n #include \"alloc.h\"\n #include \"typeinfo.h\"\n-#if defined(__ARM_NEON__) || defined (__ARM_NEON)\n-#include <arm_neon.h>\n-#endif\n-#ifdef NPY_HAVE_SSE2_INTRINSICS\n-#include <emmintrin.h>\n-#endif\n \n #include \"npy_longdouble.h\"\n #include \"numpyos.h\"\n@@ -42,7 +36,7 @@\n #include \"npy_cblas.h\"\n #include \"npy_buffer.h\"\n \n-\n+#include \"arraytypes.h\"\n /*\n  * Define a stack allocated dummy array with only the minimum information set:\n  *   1. The descr, the main field interesting here.\n@@ -3176,77 +3170,21 @@ finish:\n  **                                 ARGFUNC                                 **\n  *****************************************************************************\n  */\n-#if defined(__ARM_NEON__) || defined (__ARM_NEON)\n-    int32_t _mm_movemask_epi8_neon(uint8x16_t input)\n-    {\n-        int8x8_t m0 = vcreate_s8(0x0706050403020100ULL);\n-        uint8x16_t v0 = vshlq_u8(vshrq_n_u8(input, 7), vcombine_s8(m0, m0));\n-        uint64x2_t v1 = vpaddlq_u32(vpaddlq_u16(vpaddlq_u8(v0)));\n-        return (int)vgetq_lane_u64(v1, 0) + ((int)vgetq_lane_u64(v1, 1) << 8);\n-    }\n-#endif\n-#define _LESS_THAN_OR_EQUAL(a,b) ((a) <= (b))\n \n-static int\n-BOOL_argmax(npy_bool *ip, npy_intp n, npy_intp *max_ind,\n-            PyArrayObject *NPY_UNUSED(aip))\n-\n-{\n-    npy_intp i = 0;\n-    /* memcmp like logical_and on i386 is maybe slower for small arrays */\n-#ifdef NPY_HAVE_SSE2_INTRINSICS\n-    const __m128i zero = _mm_setzero_si128();\n-    for (; i < n - (n % 32); i+=32) {\n-        __m128i d1 = _mm_loadu_si128((__m128i*)&ip[i]);\n-        __m128i d2 = _mm_loadu_si128((__m128i*)&ip[i + 16]);\n-        d1 = _mm_cmpeq_epi8(d1, zero);\n-        d2 = _mm_cmpeq_epi8(d2, zero);\n-        if (_mm_movemask_epi8(_mm_min_epu8(d1, d2)) != 0xFFFF) {\n-            break;\n-        }\n-    }\n-#else\n-    #if defined(__ARM_NEON__) || defined (__ARM_NEON)\n-        uint8x16_t zero = vdupq_n_u8(0);\n-        for(; i < n - (n % 32); i+=32) {\n-            uint8x16_t d1 = vld1q_u8((uint8_t *)&ip[i]);\n-            uint8x16_t d2 = vld1q_u8((uint8_t *)&ip[i + 16]);\n-            d1 = vceqq_u8(d1, zero);\n-            d2 = vceqq_u8(d2, zero);\n-            if(_mm_movemask_epi8_neon(vminq_u8(d1, d2)) != 0xFFFF) {\n-                break;\n-            }\n-        }\n-    #endif\n-#endif\n-    for (; i < n; i++) {\n-        if (ip[i]) {\n-            *max_ind = i;\n-            return 0;\n-        }\n-    }\n-    *max_ind = 0;\n-    return 0;\n-}\n+#define _LESS_THAN_OR_EQUAL(a,b) ((a) <= (b))\n \n /**begin repeat\n  *\n- * #fname = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n- *          LONG, ULONG, LONGLONG, ULONGLONG,\n- *          HALF, FLOAT, DOUBLE, LONGDOUBLE,\n- *          CFLOAT, CDOUBLE, CLONGDOUBLE,\n+ * #fname = HALF, CFLOAT, CDOUBLE, CLONGDOUBLE,\n  *          DATETIME, TIMEDELTA#\n- * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n- *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n- *         npy_half, npy_float, npy_double, npy_longdouble,\n- *         npy_float, npy_double, npy_longdouble,\n+ * #type = npy_half, npy_float, npy_double, npy_longdouble,\n  *         npy_datetime, npy_timedelta#\n- * #isfloat = 0*10, 1*7, 0*2#\n- * #isnan = nop*10, npy_half_isnan, npy_isnan*6, nop*2#\n- * #le = _LESS_THAN_OR_EQUAL*10, npy_half_le, _LESS_THAN_OR_EQUAL*8#\n- * #iscomplex = 0*14, 1*3, 0*2#\n- * #incr = ip++*14, ip+=2*3, ip++*2#\n- * #isdatetime = 0*17, 1*2#\n+ * #isfloat = 1*4, 0*2#\n+ * #isnan = npy_half_isnan, npy_isnan*3, nop*2#\n+ * #le = npy_half_le, _LESS_THAN_OR_EQUAL*5#\n+ * #iscomplex = 0, 1*3, 0*2#\n+ * #incr = ip++, ip+=2*3, ip++*2#\n+ * #isdatetime = 0*4, 1*2#\n  */\n static int\n @fname@_argmax(@type@ *ip, npy_intp n, npy_intp *max_ind,\n@@ -3337,22 +3275,16 @@ BOOL_argmin(npy_bool *ip, npy_intp n, npy_intp *min_ind,\n \n /**begin repeat\n  *\n- * #fname = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n- *          LONG, ULONG, LONGLONG, ULONGLONG,\n- *          HALF, FLOAT, DOUBLE, LONGDOUBLE,\n- *          CFLOAT, CDOUBLE, CLONGDOUBLE,\n+ * #fname = HALF, CFLOAT, CDOUBLE, CLONGDOUBLE,\n  *          DATETIME, TIMEDELTA#\n- * #type = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n- *         npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n- *         npy_half, npy_float, npy_double, npy_longdouble,\n- *         npy_float, npy_double, npy_longdouble,\n+ * #type = npy_half, npy_float, npy_double, npy_longdouble,\n  *         npy_datetime, npy_timedelta#\n- * #isfloat = 0*10, 1*7, 0*2#\n- * #isnan = nop*10, npy_half_isnan, npy_isnan*6, nop*2#\n- * #le = _LESS_THAN_OR_EQUAL*10, npy_half_le, _LESS_THAN_OR_EQUAL*8#\n- * #iscomplex = 0*14, 1*3, 0*2#\n- * #incr = ip++*14, ip+=2*3, ip++*2#\n- * #isdatetime = 0*17, 1*2#\n+ * #isfloat = 1*4, 0*2#\n+ * #isnan = npy_half_isnan, npy_isnan*3, nop*2#\n+ * #le = npy_half_le, _LESS_THAN_OR_EQUAL*5#\n+ * #iscomplex = 0, 1*3, 0*2#\n+ * #incr = ip++, ip+=2*3, ip++*2#\n+ * #isdatetime = 0*4, 1*2#\n  */\n static int\n @fname@_argmin(@type@ *ip, npy_intp n, npy_intp *min_ind,\n@@ -3409,7 +3341,7 @@ static int\n             *min_ind = i;\n             break;\n         }\n-#endif \n+#endif\n         if (!@le@(mp, *ip)) {  /* negated, for correct nan handling */\n             mp = *ip;\n             *min_ind = i;\n@@ -4494,6 +4426,27 @@ set_typeinfo(PyObject *dict)\n     PyArray_Descr *dtype;\n     PyObject *cobj, *key;\n \n+    // SIMD runtime dispatching\n+    #ifndef NPY_DISABLE_OPTIMIZATION\n+        #include \"argfunc.dispatch.h\"\n+    #endif\n+    /**begin repeat\n+     * #FROM = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n+     *         LONG, ULONG, LONGLONG, ULONGLONG,\n+     *         FLOAT, DOUBLE, LONGDOUBLE#\n+     *\n+     * #NAME = Byte, UByte, Short, UShort, Int, UInt,\n+     *         Long, ULong, LongLong, ULongLong,\n+     *         Float, Double, LongDouble#\n+     */\n+    /**begin repeat1\n+     * #func = argmax, argmin#\n+     */\n+    NPY_CPU_DISPATCH_CALL_XB(_Py@NAME@_ArrFuncs.@func@ = (PyArray_ArgFunc*)@FROM@_@func@);\n+    /**end repeat1**/\n+    /**end repeat**/\n+    NPY_CPU_DISPATCH_CALL_XB(_PyBool_ArrFuncs.argmax = (PyArray_ArgFunc*)BOOL_argmax);\n+\n     /*\n      * Override the base class for all types, eventually all of this logic\n      * should be defined on the class and inherited to the scalar."
            },
            {
                "filename": "numpy/core/src/multiarray/arraytypes.h.src",
                "patch": "@@ -28,4 +28,25 @@ small_correlate(const char * d_, npy_intp dstride,\n                 npy_intp nk, enum NPY_TYPES ktype,\n                 char * out_, npy_intp ostride);\n \n+#ifndef NPY_DISABLE_OPTIMIZATION\n+    #include \"argfunc.dispatch.h\"\n+#endif\n+/**begin repeat\n+ * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT,\n+ *         LONG, ULONG, LONGLONG, ULONGLONG,\n+ *         FLOAT, DOUBLE, LONGDOUBLE#\n+ * #type = byte, ubyte, short, ushort, int, uint,\n+ *         long, ulong, longlong, ulonglong,\n+ *         float, double, longdouble#\n+ */\n+/**begin repeat1\n+ * #func = argmax, argmin#\n+ */\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT int @TYPE@_@func@,\n+    (npy_@type@ *ip, npy_intp n, npy_intp *max_ind, PyArrayObject *aip))\n+/**end repeat1**/\n+/**end repeat**/\n+NPY_CPU_DISPATCH_DECLARE(NPY_NO_EXPORT int BOOL_argmax,\n+    (npy_bool *ip, npy_intp n, npy_intp *max_ind, PyArrayObject *aip))\n+\n #endif  /* NUMPY_CORE_SRC_MULTIARRAY_ARRAYTYPES_H_ */"
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -4190,7 +4190,8 @@ class TestArgmaxArgminCommon:\n     sizes = [(), (3,), (3, 2), (2, 3),\n              (3, 3), (2, 3, 4), (4, 3, 2),\n              (1, 2, 3, 4), (2, 3, 4, 1),\n-             (3, 4, 1, 2), (4, 1, 2, 3)]\n+             (3, 4, 1, 2), (4, 1, 2, 3),\n+             (64,), (128,), (256,)]\n \n     @pytest.mark.parametrize(\"size, axis\", itertools.chain(*[[(size, axis)\n         for axis in list(range(-len(size), len(size))) + [None]]\n@@ -4304,9 +4305,9 @@ def test_output_shape(self, method):\n     @pytest.mark.parametrize('ndim', [0, 1])\n     @pytest.mark.parametrize('method', ['argmax', 'argmin'])\n     def test_ret_is_out(self, ndim, method):\n-        a = np.ones((4,) + (3,)*ndim)\n+        a = np.ones((4,) + (256,)*ndim)\n         arg_method = getattr(a, method)\n-        out = np.empty((3,)*ndim, dtype=np.intp)\n+        out = np.empty((256,)*ndim, dtype=np.intp)\n         ret = arg_method(axis=0, out=out)\n         assert ret is out\n \n@@ -4357,12 +4358,44 @@ def test_object_with_NULLs(self, method, vals):\n         assert_equal(arg_method(), 1)\n \n class TestArgmax:\n-\n-    nan_arr = [\n-        ([0, 1, 2, 3, np.nan], 4),\n-        ([0, 1, 2, np.nan, 3], 3),\n-        ([np.nan, 0, 1, 2, 3], 0),\n-        ([np.nan, 0, np.nan, 2, 3], 0),\n+    usg_data = [\n+        ([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 0),\n+        ([3, 3, 3, 3,  2,  2,  2,  2], 0),\n+        ([0, 1, 2, 3,  4,  5,  6,  7], 7),\n+        ([7, 6, 5, 4,  3,  2,  1,  0], 0)\n+    ]\n+    sg_data = usg_data + [\n+        ([1, 2, 3, 4, -4, -3, -2, -1], 3),\n+        ([1, 2, 3, 4, -1, -2, -3, -4], 3)\n+    ]\n+    darr = [(np.array(d[0], dtype=t), d[1]) for d, t in (\n+        itertools.product(usg_data, (\n+            np.uint8, np.uint16, np.uint32, np.uint64\n+        ))\n+    )]\n+    darr = darr + [(np.array(d[0], dtype=t), d[1]) for d, t in (\n+        itertools.product(sg_data, (\n+            np.int8, np.int16, np.int32, np.int64, np.float32, np.float64\n+        ))\n+    )]\n+    darr = darr + [(np.array(d[0], dtype=t), d[1]) for d, t in (\n+        itertools.product((\n+            ([0, 1, 2, 3, np.nan], 4),\n+            ([0, 1, 2, np.nan, 3], 3),\n+            ([np.nan, 0, 1, 2, 3], 0),\n+            ([np.nan, 0, np.nan, 2, 3], 0),\n+            # To hit the tail of SIMD multi-level(x4, x1) inner loops\n+            # on varient SIMD widthes\n+            ([1] * (2*5-1) + [np.nan], 2*5-1),\n+            ([1] * (4*5-1) + [np.nan], 4*5-1),\n+            ([1] * (8*5-1) + [np.nan], 8*5-1),\n+            ([1] * (16*5-1) + [np.nan], 16*5-1),\n+            ([1] * (32*5-1) + [np.nan], 32*5-1)\n+        ), (\n+            np.float32, np.float64\n+        ))\n+    )]\n+    nan_arr = darr + [\n         ([0, 1, 2, 3, complex(0, np.nan)], 4),\n         ([0, 1, 2, 3, complex(np.nan, 0)], 4),\n         ([0, 1, 2, complex(np.nan, 0), 3], 3),\n@@ -4432,28 +4465,80 @@ def test_combinations(self, data):\n         assert_equal(np.argmax(arr), pos, err_msg=\"%r\" % arr)\n         assert_equal(arr[np.argmax(arr)], val, err_msg=\"%r\" % arr)\n \n+        # add padding to test SIMD loops\n+        rarr = np.repeat(arr, 129)\n+        rpos = pos * 129\n+        assert_equal(np.argmax(rarr), rpos, err_msg=\"%r\" % rarr)\n+        assert_equal(rarr[np.argmax(rarr)], val, err_msg=\"%r\" % rarr)\n+\n+        padd = np.repeat(np.min(arr), 513)\n+        rarr = np.concatenate((arr, padd))\n+        rpos = pos\n+        assert_equal(np.argmax(rarr), rpos, err_msg=\"%r\" % rarr)\n+        assert_equal(rarr[np.argmax(rarr)], val, err_msg=\"%r\" % rarr)\n+\n+\n     def test_maximum_signed_integers(self):\n \n         a = np.array([1, 2**7 - 1, -2**7], dtype=np.int8)\n         assert_equal(np.argmax(a), 1)\n+        a.repeat(129)\n+        assert_equal(np.argmax(a), 1)\n \n         a = np.array([1, 2**15 - 1, -2**15], dtype=np.int16)\n         assert_equal(np.argmax(a), 1)\n+        a.repeat(129)\n+        assert_equal(np.argmax(a), 1)\n \n         a = np.array([1, 2**31 - 1, -2**31], dtype=np.int32)\n         assert_equal(np.argmax(a), 1)\n+        a.repeat(129)\n+        assert_equal(np.argmax(a), 1)\n \n         a = np.array([1, 2**63 - 1, -2**63], dtype=np.int64)\n         assert_equal(np.argmax(a), 1)\n-\n+        a.repeat(129)\n+        assert_equal(np.argmax(a), 1)\n \n class TestArgmin:\n-\n-    nan_arr = [\n-        ([0, 1, 2, 3, np.nan], 4),\n-        ([0, 1, 2, np.nan, 3], 3),\n-        ([np.nan, 0, 1, 2, 3], 0),\n-        ([np.nan, 0, np.nan, 2, 3], 0),\n+    usg_data = [\n+        ([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 8),\n+        ([3, 3, 3, 3,  2,  2,  2,  2], 4),\n+        ([0, 1, 2, 3,  4,  5,  6,  7], 0),\n+        ([7, 6, 5, 4,  3,  2,  1,  0], 7)\n+    ]\n+    sg_data = usg_data + [\n+        ([1, 2, 3, 4, -4, -3, -2, -1], 4),\n+        ([1, 2, 3, 4, -1, -2, -3, -4], 7)\n+    ]\n+    darr = [(np.array(d[0], dtype=t), d[1]) for d, t in (\n+        itertools.product(usg_data, (\n+            np.uint8, np.uint16, np.uint32, np.uint64\n+        ))\n+    )]\n+    darr = darr + [(np.array(d[0], dtype=t), d[1]) for d, t in (\n+        itertools.product(sg_data, (\n+            np.int8, np.int16, np.int32, np.int64, np.float32, np.float64\n+        ))\n+    )]\n+    darr = darr + [(np.array(d[0], dtype=t), d[1]) for d, t in (\n+        itertools.product((\n+            ([0, 1, 2, 3, np.nan], 4),\n+            ([0, 1, 2, np.nan, 3], 3),\n+            ([np.nan, 0, 1, 2, 3], 0),\n+            ([np.nan, 0, np.nan, 2, 3], 0),\n+            # To hit the tail of SIMD multi-level(x4, x1) inner loops\n+            # on varient SIMD widthes\n+            ([1] * (2*5-1) + [np.nan], 2*5-1),\n+            ([1] * (4*5-1) + [np.nan], 4*5-1),\n+            ([1] * (8*5-1) + [np.nan], 8*5-1),\n+            ([1] * (16*5-1) + [np.nan], 16*5-1),\n+            ([1] * (32*5-1) + [np.nan], 32*5-1)\n+        ), (\n+            np.float32, np.float64\n+        ))\n+    )]\n+    nan_arr = darr + [\n         ([0, 1, 2, 3, complex(0, np.nan)], 4),\n         ([0, 1, 2, 3, complex(np.nan, 0)], 4),\n         ([0, 1, 2, complex(np.nan, 0), 3], 3),\n@@ -4512,30 +4597,50 @@ class TestArgmin:\n         ([False, True, False, True, True], 0),\n     ]\n \n-    def test_combinations(self):\n-        for arr, pos in self.nan_arr:\n-            with suppress_warnings() as sup:\n-                sup.filter(RuntimeWarning,\n-                           \"invalid value encountered in reduce\")\n-                min_val = np.min(arr)\n+    @pytest.mark.parametrize('data', nan_arr)\n+    def test_combinations(self, data):\n+        arr, pos = data\n+        with suppress_warnings() as sup:\n+            sup.filter(RuntimeWarning,\n+                       \"invalid value encountered in reduce\")\n+            min_val = np.min(arr)\n+\n+        assert_equal(np.argmin(arr), pos, err_msg=\"%r\" % arr)\n+        assert_equal(arr[np.argmin(arr)], min_val, err_msg=\"%r\" % arr)\n \n-            assert_equal(np.argmin(arr), pos, err_msg=\"%r\" % arr)\n-            assert_equal(arr[np.argmin(arr)], min_val, err_msg=\"%r\" % arr)\n+        # add padding to test SIMD loops\n+        rarr = np.repeat(arr, 129)\n+        rpos = pos * 129\n+        assert_equal(np.argmin(rarr), rpos, err_msg=\"%r\" % rarr)\n+        assert_equal(rarr[np.argmin(rarr)], min_val, err_msg=\"%r\" % rarr)\n+\n+        padd = np.repeat(np.max(arr), 513)\n+        rarr = np.concatenate((arr, padd))\n+        rpos = pos\n+        assert_equal(np.argmin(rarr), rpos, err_msg=\"%r\" % rarr)\n+        assert_equal(rarr[np.argmin(rarr)], min_val, err_msg=\"%r\" % rarr)\n \n     def test_minimum_signed_integers(self):\n \n         a = np.array([1, -2**7, -2**7 + 1, 2**7 - 1], dtype=np.int8)\n         assert_equal(np.argmin(a), 1)\n+        a.repeat(129)\n+        assert_equal(np.argmin(a), 1)\n \n         a = np.array([1, -2**15, -2**15 + 1, 2**15 - 1], dtype=np.int16)\n         assert_equal(np.argmin(a), 1)\n+        a.repeat(129)\n+        assert_equal(np.argmin(a), 1)\n \n         a = np.array([1, -2**31, -2**31 + 1, 2**31 - 1], dtype=np.int32)\n         assert_equal(np.argmin(a), 1)\n+        a.repeat(129)\n+        assert_equal(np.argmin(a), 1)\n \n         a = np.array([1, -2**63, -2**63 + 1, 2**63 - 1], dtype=np.int64)\n         assert_equal(np.argmin(a), 1)\n-\n+        a.repeat(129)\n+        assert_equal(np.argmin(a), 1)\n \n class TestMinMax:\n "
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20580,
        "body": "This PR is based on @WarrenWeckesser `npreadtext` which was subsequently heavily modified [here](https://github.com/BIDS-numpy/npreadtext)  (and somewhat reduced in scope).\r\n\r\nWhile there is more to it, the main point is to improve the speed for parsing CSV (like) files, the included io benchmark change the following way:\r\n\r\n<details> <summary><b>Benchmarking results</b>  </summary>\r\n\r\n(sorted so that the `ratio` is a factor, 17 means 17 times faster)\r\nEDIT: Last table update Jan 14 (no meaningful changes expected anymore) \r\n```\r\n       before           after         ratio\r\n     [1e6b72b4]       [e2d35064]\r\n     <add-npreadtext>       <main>    \r\n+     9.58\u00b10.07ms          178\u00b12ms    18.64  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int64', 100000)\r\n+         949\u00b15\u03bcs       17.5\u00b10.1ms    18.42  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int64', 10000)\r\n+      8.30\u00b10.1ms        142\u00b10.1ms    17.16  bench_io.LoadtxtCSVStructured.time_loadtxt_csv_struct_dtype\r\n+      9.11\u00b10.1ms        131\u00b10.2ms    14.40  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int32', 100000)\r\n+        918\u00b110\u03bcs       13.1\u00b10.1ms    14.24  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int32', 10000)\r\n+     13.4\u00b10.08\u03bcs          188\u00b11\u03bcs    14.05  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int64', 100)\r\n+     13.1\u00b10.05\u03bcs          143\u00b11\u03bcs    10.91  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int32', 100)\r\n+      15.6\u00b10.4ms          135\u00b11ms     8.67  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('complex128', 100000)\r\n+      14.6\u00b10.1ms          127\u00b11ms     8.67  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float32', 100000)\r\n+      14.5\u00b10.1ms          125\u00b11ms     8.58  bench_io.LoadtxtCSVComments.time_comment_loadtxt_csv(100000)\r\n+     1.45\u00b10.01ms      12.3\u00b10.09ms     8.49  bench_io.LoadtxtCSVComments.time_comment_loadtxt_csv(10000)\r\n+     15.1\u00b10.07ms        126\u00b10.7ms     8.40  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float64', 100000)\r\n+     1.48\u00b10.02ms      12.4\u00b10.03ms     8.39  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float32', 10000)\r\n+     1.60\u00b10.01ms       13.4\u00b10.1ms     8.38  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('complex128', 10000)\r\n+        1.52\u00b10ms      12.5\u00b10.05ms     8.23  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float64', 10000)\r\n+     19.1\u00b10.03\u03bcs        146\u00b10.2\u03bcs     7.63  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('complex128', 100)\r\n+      18.4\u00b10.2\u03bcs        135\u00b10.9\u03bcs     7.33  bench_io.LoadtxtCSVComments.time_comment_loadtxt_csv(100)\r\n+     18.7\u00b10.08\u03bcs        137\u00b10.5\u03bcs     7.30  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float32', 100)\r\n+     18.9\u00b10.06\u03bcs          137\u00b12\u03bcs     7.21  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float64', 100)\r\n+     1.55\u00b10.01ms      10.3\u00b10.05ms     6.66  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('object', 10000)\r\n+     16.0\u00b10.08ms          105\u00b11ms     6.60  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('object', 100000)\r\n+        916\u00b110\u03bcs      5.98\u00b10.02ms     6.52  bench_io.LoadtxtUseColsCSV.time_loadtxt_usecols_csv([1, 3])\r\n+     1.15\u00b10.02ms      7.36\u00b10.01ms     6.40  bench_io.LoadtxtUseColsCSV.time_loadtxt_usecols_csv([1, 3, 5, 7])\r\n+      18.2\u00b10.3\u03bcs          110\u00b11\u03bcs     6.03  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('object', 100)\r\n+        798\u00b110\u03bcs      4.80\u00b10.01ms     6.02  bench_io.LoadtxtUseColsCSV.time_loadtxt_usecols_csv(2)\r\n+     5.07\u00b10.01\u03bcs       29.7\u00b10.7\u03bcs     5.86  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int64', 10)\r\n+     7.36\u00b10.08ms       39.7\u00b10.2ms     5.39  bench_io.LoadtxtCSVDateTime.time_loadtxt_csv_datetime(20000)\r\n+      77.1\u00b10.5\u03bcs          413\u00b13\u03bcs     5.35  bench_io.LoadtxtCSVDateTime.time_loadtxt_csv_datetime(200)\r\n+         737\u00b17\u03bcs      3.92\u00b10.01ms     5.32  bench_io.LoadtxtCSVDateTime.time_loadtxt_csv_datetime(2000)\r\n+     5.06\u00b10.02\u03bcs       25.1\u00b10.5\u03bcs     4.95  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('int32', 10)\r\n+     12.3\u00b10.02\u03bcs       58.3\u00b10.5\u03bcs     4.74  bench_io.LoadtxtCSVDateTime.time_loadtxt_csv_datetime(20)\r\n+     5.67\u00b10.03\u03bcs       26.3\u00b10.4\u03bcs     4.64  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('complex128', 10)\r\n+         220\u00b14\u03bcs          989\u00b14\u03bcs     4.49  bench_io.LoadtxtReadUint64Integers.time_read_uint64_neg_values(1000)\r\n+         221\u00b15\u03bcs          987\u00b18\u03bcs     4.46  bench_io.LoadtxtReadUint64Integers.time_read_uint64(1000)\r\n+         123\u00b13\u03bcs          549\u00b12\u03bcs     4.46  bench_io.LoadtxtReadUint64Integers.time_read_uint64_neg_values(550)\r\n+         124\u00b11\u03bcs          548\u00b12\u03bcs     4.41  bench_io.LoadtxtReadUint64Integers.time_read_uint64(550)\r\n+     2.22\u00b10.03ms      9.76\u00b10.04ms     4.41  bench_io.LoadtxtReadUint64Integers.time_read_uint64_neg_values(10000)\r\n+     2.22\u00b10.03ms      9.69\u00b10.04ms     4.37  bench_io.LoadtxtReadUint64Integers.time_read_uint64(10000)\r\n+     5.62\u00b10.01\u03bcs       24.2\u00b10.8\u03bcs     4.31  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float32', 10)\r\n+     5.64\u00b10.01\u03bcs       24.0\u00b10.1\u03bcs     4.26  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('float64', 10)\r\n+     5.44\u00b10.03\u03bcs       22.8\u00b10.3\u03bcs     4.20  bench_io.LoadtxtCSVComments.time_comment_loadtxt_csv(10)\r\n+     5.90\u00b10.02\u03bcs       21.8\u00b10.6\u03bcs     3.69  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('object', 10)\r\n+     4.22\u00b10.01ms      12.7\u00b10.08ms     3.02  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('str', 10000)\r\n+      46.6\u00b10.3\u03bcs        137\u00b10.7\u03bcs     2.94  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('str', 100)\r\n+      42.6\u00b10.1ms        122\u00b10.7ms     2.87  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('str', 100000)\r\n+      73.1\u00b10.7ms        202\u00b10.4ms     2.76  bench_io.LoadtxtCSVSkipRows.time_skiprows_csv(0)\r\n+      72.6\u00b10.7ms        200\u00b10.7ms     2.76  bench_io.LoadtxtCSVSkipRows.time_skiprows_csv(500)\r\n+      66.1\u00b10.9ms        182\u00b10.6ms     2.76  bench_io.LoadtxtCSVSkipRows.time_skiprows_csv(10000)\r\n+      10.2\u00b10.2\u03bcs       24.3\u00b10.1\u03bcs     2.39  bench_io.LoadtxtCSVdtypes.time_loadtxt_dtypes_csv('str', 10)\r\n```\r\n\r\n</details>\r\n\r\n**Marking as draft, but I do not expect major changes, so this can get a first round of review, I think.**\r\n\r\nThere is a small list of TODOs:\r\n* [x] The current additional tests from npreadtext are missing here (they should be rewritten in terms of loadtxt probably).\r\n* [x] Additional tests should be added (see also npreadtext issue tracker;  one reason for the PR, it makes checking code-coverage more convenient.)\r\n* [x] Smaller changes ~(e.g. I think I want to remove passing `usecols` as a numpy array)~ Should be mostly done. ~Moving usecols could be done (and would be nice), but we don't have quite the right stuff available so added the correct sanity checks instead.  I agree this is a wart (a private one), so if anyone insists I can look at it.~ Moved usecols as well, nothing is quite awesome, but I think it is _better_.\r\n\r\nThe [issues from `nreadtext`](https://github.com/BIDS-numpy/npreadtext/issues) still apply, and we should review some of these or make a call on them.  The issue tracker there may also be a good way to open other smaller things to fix-up (to reduce noise here).\r\n\r\n---\r\n\r\nAs a general overview, the parser has a tokenizer, that works with unicode, but supports the Python modes UCS4, UCS2, and UCS1.  It parses everything into smaller UCS4 (unicode) strings, this copies the data \u2013 in theory not always necessary, but convenient \u2013 although future optimization could be done.\r\nAfter this step, we have dtype (especially for integers) specific converter functions which directly convert the UCS4 string into the final result.  If necessary, we fall back to `PyArray_Pack`.\r\n(Since we do not discover the result `dtype`, strings have to offload some work to Python currently.)\r\n\r\nIn general, we read either line-by-line from Python iterables, or in chunks if reading from files.\r\n\r\nThe new features are currently fairly limited (e.g. I removed the custom float parser to support using `d` instead of `e`), and is not fully CVS parser featured (but this can easily be achieved now!).\r\nOne particular important changes is the current decision to remove hex float parsing and limit integer parsing to integers (we used to fall back to floats).  The custom parsers also mean that we do not support numbers with an underscore `100_000` right now.\r\nOur current idea was that we should improve the error message a bit more, but generally advise users to use `converters=int` in this case.\r\n\r\nPlease check the `npreadtext` issue tracker for issues/changes: https://github.com/BIDS-numpy/npreadtext/issues\r\n\r\nEDIT: One big additional feature ~(which I have not made public here, but is as easy as adding the kwarg)~, is that the parser does support quoting (with excel double-quote escaping).",
        "changed_files": [
            {
                "filename": "doc/release/upcoming_changes/20580.compatibility.rst",
                "patch": "@@ -0,0 +1,33 @@\n+``np.loadtxt`` has recieved several changes  \n+-------------------------------------------\n+\n+The row counting of `numpy.loadtxt` was fixed.  ``loadtxt`` ignores fully\n+empty lines in the file, but counted them towards ``max_rows``.\n+When ``max_rows`` is used and the file contains empty lines, these will now\n+not be counted.  Previously, it was possible that the result contained fewer\n+than ``max_rows`` rows even though more data was available to be read.\n+If the old behaviour is required, ``itertools.islice`` may be used::\n+\n+    import itertools\n+    lines = itertools.islice(open(\"file\"), 0, max_rows)\n+    result = np.loadtxt(lines, ...)\n+\n+While generally much faster and improved, `numpy.loadtxt` may now fail to\n+converter certain strings to numbers that were previously successfully read.\n+The most important cases for this are:\n+\n+* Parsing floating point values such as ``1.0`` into integers will now fail\n+* Parsing hexadecimal floats such as ``0x3p3`` will fail\n+* An ``_`` was previously accepted as a thousands delimiter ``100_000``.\n+  This will now result in an error.\n+\n+If you experience these limitations, they can all be worked around by passing\n+appropriate ``converters=``.  NumPy now supports passing a single converter\n+to be used for all columns to make this more convenient.\n+For example, ``converters=float.fromhex`` can read hexadecimal float numbers\n+and ``converters=int`` will be able to read ``100_000``.\n+\n+Further, the error messages have been generally improved.  However, this means\n+that error types may differ.  In particularly, a ``ValueError`` is now always\n+raised when parsing of a single entry fails.\n+"
            },
            {
                "filename": "doc/release/upcoming_changes/20580.new_feature.rst",
                "patch": "@@ -0,0 +1,8 @@\n+``np.loadtxt`` now supports quote character and single converter function\n+-------------------------------------------------------------------------\n+`numpy.loadtxt` now supports an additional ``quotechar`` keyword argument\n+which is not set by default.  Using ``quotechar='\"'`` will read quoted fields\n+as used by the Excel CSV dialect.\n+\n+Further, it is now possible to pass a single callable rather than a dictionary\n+for the ``converters`` argument."
            },
            {
                "filename": "doc/release/upcoming_changes/20580.performance.rst",
                "patch": "@@ -0,0 +1,4 @@\n+Faster ``np.loadtxt``\n+---------------------\n+`numpy.loadtxt` is now generally much faster than previously as most of it\n+is now implemented in C."
            },
            {
                "filename": "numpy/core/setup.py",
                "patch": "@@ -868,6 +868,7 @@ def gl_if_msvc(build_cmd):\n             join('src', 'multiarray', 'typeinfo.h'),\n             join('src', 'multiarray', 'usertypes.h'),\n             join('src', 'multiarray', 'vdot.h'),\n+            join('src', 'multiarray', 'textreading', 'readtext.h'),\n             join('include', 'numpy', 'arrayobject.h'),\n             join('include', 'numpy', '_neighborhood_iterator_imp.h'),\n             join('include', 'numpy', 'npy_endian.h'),\n@@ -955,6 +956,14 @@ def gl_if_msvc(build_cmd):\n             join('src', 'npysort', 'selection.c.src'),\n             join('src', 'common', 'npy_binsearch.h'),\n             join('src', 'npysort', 'binsearch.cpp'),\n+            join('src', 'multiarray', 'textreading', 'conversions.c'),\n+            join('src', 'multiarray', 'textreading', 'field_types.c'),\n+            join('src', 'multiarray', 'textreading', 'growth.c'),\n+            join('src', 'multiarray', 'textreading', 'readtext.c'),\n+            join('src', 'multiarray', 'textreading', 'rows.c'),\n+            join('src', 'multiarray', 'textreading', 'stream_pyobject.c'),\n+            join('src', 'multiarray', 'textreading', 'str_to_int.c'),\n+            join('src', 'multiarray', 'textreading', 'tokenize.c.src'),\n             ]\n \n     #######################################################################"
            },
            {
                "filename": "numpy/core/src/multiarray/conversion_utils.c",
                "patch": "@@ -993,6 +993,17 @@ PyArray_PyIntAsIntp(PyObject *o)\n }\n \n \n+NPY_NO_EXPORT int\n+PyArray_IntpFromPyIntConverter(PyObject *o, npy_intp *val)\n+{\n+    *val = PyArray_PyIntAsIntp(o);\n+    if (error_converting(*val)) {\n+        return NPY_FAIL;\n+    }\n+    return NPY_SUCCEED;\n+}\n+\n+\n /*\n  * PyArray_IntpFromIndexSequence\n  * Returns the number of dimensions or -1 if an error occurred."
            },
            {
                "filename": "numpy/core/src/multiarray/conversion_utils.h",
                "patch": "@@ -6,6 +6,9 @@\n NPY_NO_EXPORT int\n PyArray_IntpConverter(PyObject *obj, PyArray_Dims *seq);\n \n+NPY_NO_EXPORT int\n+PyArray_IntpFromPyIntConverter(PyObject *o, npy_intp *val);\n+\n NPY_NO_EXPORT int\n PyArray_OptionalIntpConverter(PyObject *obj, PyArray_Dims *seq);\n "
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -69,6 +69,7 @@ NPY_NO_EXPORT int NPY_NUMUSERTYPES = 0;\n \n #include \"get_attr_string.h\"\n #include \"experimental_public_dtype_api.h\"  /* _get_experimental_dtype_api */\n+#include \"textreading/readtext.h\"  /* _readtext_from_file_object */\n \n #include \"npy_dlpack.h\"\n \n@@ -4456,6 +4457,8 @@ static struct PyMethodDef array_module_methods[] = {\n         METH_VARARGS | METH_KEYWORDS, NULL},\n     {\"_get_experimental_dtype_api\", (PyCFunction)_get_experimental_dtype_api,\n         METH_O, NULL},\n+    {\"_load_from_filelike\", (PyCFunction)_load_from_filelike,\n+        METH_FASTCALL | METH_KEYWORDS, NULL},\n     /* from umath */\n     {\"frompyfunc\",\n         (PyCFunction) ufunc_frompyfunc,"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/conversions.c",
                "patch": "@@ -0,0 +1,395 @@\n+\n+#include <Python.h>\n+\n+#include <string.h>\n+#include <stdlib.h>\n+#include <stdbool.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"lowlevel_strided_loops.h\"\n+\n+#include \"conversions.h\"\n+#include \"str_to_int.h\"\n+\n+#include \"array_coercion.h\"\n+\n+\n+/*\n+ * Coercion to boolean is done via integer right now.\n+ */\n+NPY_NO_EXPORT int\n+to_bool(PyArray_Descr *NPY_UNUSED(descr),\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *NPY_UNUSED(pconfig))\n+{\n+    int64_t res;\n+    if (str_to_int64(str, end, INT64_MIN, INT64_MAX, &res) < 0) {\n+        return -1;\n+    }\n+    *dataptr = (char)(res != 0);\n+    return 0;\n+}\n+\n+\n+/*\n+ * In order to not pack a whole copy of a floating point parser, we copy the\n+ * result into ascii and call the Python one.  Float parsing isn't super quick\n+ * so this is not terrible, but avoiding it would speed up things.\n+ *\n+ * Also note that parsing the first float of a complex will copy the whole\n+ * string to ascii rather than just the first part.\n+ * TODO: A tweak of the break might be a simple mitigation there.\n+ *\n+ * @param str The UCS4 string to parse\n+ * @param end Pointer to the end of the string\n+ * @param skip_trailing_whitespace If false does not skip trailing whitespace\n+ *        (used by the complex parser).\n+ * @param result Output stored as double value.\n+ */\n+static NPY_INLINE int\n+double_from_ucs4(\n+        const Py_UCS4 *str, const Py_UCS4 *end,\n+        bool strip_whitespace, double *result, const Py_UCS4 **p_end)\n+{\n+    /* skip leading whitespace */\n+    if (strip_whitespace) {\n+        while (Py_UNICODE_ISSPACE(*str)) {\n+            str++;\n+        }\n+    }\n+    if (str == end) {\n+        return -1;  /* empty or only whitespace: not a floating point number */\n+    }\n+\n+    /* We convert to ASCII for the Python parser, use stack if small: */\n+    char stack_buf[128];\n+    char *heap_buf = NULL;\n+    char *ascii = stack_buf;\n+\n+    size_t str_len = end - str + 1;\n+    if (str_len > 128) {\n+        heap_buf = PyMem_MALLOC(str_len);\n+        if (heap_buf == NULL) {\n+            PyErr_NoMemory();\n+            return -1;\n+        }\n+        ascii = heap_buf;\n+    }\n+    char *c = ascii;\n+    for (; str < end; str++, c++) {\n+        if (NPY_UNLIKELY(*str >= 128)) {\n+            /* Character cannot be used, ignore for end calculation and stop */\n+            end = str;\n+            break;\n+        }\n+        *c = (char)(*str);\n+    }\n+    *c = '\\0';\n+\n+    char *end_parsed;\n+    *result = PyOS_string_to_double(ascii, &end_parsed, NULL);\n+    /* Rewind `end` to the first UCS4 character not parsed: */\n+    end = end - (c - end_parsed);\n+\n+    PyMem_FREE(heap_buf);\n+\n+    if (*result == -1. && PyErr_Occurred()) {\n+        return -1;\n+    }\n+\n+    if (strip_whitespace) {\n+        /* and then skip any remainig whitespace: */\n+        while (Py_UNICODE_ISSPACE(*end)) {\n+            end++;\n+        }\n+    }\n+    *p_end = end;\n+    return 0;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_float(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *NPY_UNUSED(pconfig))\n+{\n+    double double_val;\n+    const Py_UCS4 *p_end;\n+    if (double_from_ucs4(str, end, true, &double_val, &p_end) < 0) {\n+        return -1;\n+    }\n+    if (p_end != end) {\n+        return -1;\n+    }\n+\n+    float val = (float)double_val;\n+    memcpy(dataptr, &val, sizeof(float));\n+    if (!PyArray_ISNBO(descr->byteorder)) {\n+        npy_bswap4_unaligned(dataptr);\n+    }\n+    return 0;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_double(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *NPY_UNUSED(pconfig))\n+{\n+    double val;\n+    const Py_UCS4 *p_end;\n+    if (double_from_ucs4(str, end, true, &val, &p_end) < 0) {\n+        return -1;\n+    }\n+    if (p_end != end) {\n+        return -1;\n+    }\n+\n+    memcpy(dataptr, &val, sizeof(double));\n+    if (!PyArray_ISNBO(descr->byteorder)) {\n+        npy_bswap8_unaligned(dataptr);\n+    }\n+    return 0;\n+}\n+\n+\n+static bool\n+to_complex_int(\n+        const Py_UCS4 *item, const Py_UCS4 *token_end,\n+        double *p_real, double *p_imag,\n+        Py_UCS4 imaginary_unit, bool allow_parens)\n+{\n+    const Py_UCS4 *p_end;\n+    bool unmatched_opening_paren = false;\n+\n+    /* Remove whitespace before the possibly leading '(' */\n+    while (Py_UNICODE_ISSPACE(*item)) {\n+        ++item;\n+    }\n+    if (allow_parens && (*item == '(')) {\n+        unmatched_opening_paren = true;\n+        ++item;\n+        /* Allow whitespace within the parentheses: \"( 1j)\" */\n+        while (Py_UNICODE_ISSPACE(*item)) {\n+            ++item;\n+        }\n+    }\n+    if (double_from_ucs4(item, token_end, false, p_real, &p_end) < 0) {\n+        return false;\n+    }\n+    if (p_end == token_end) {\n+        // No imaginary part in the string (e.g. \"3.5\")\n+        *p_imag = 0.0;\n+        return !unmatched_opening_paren;\n+    }\n+    if (*p_end == imaginary_unit) {\n+        /* Only an imaginary part (e.g \"1.5j\") */\n+        *p_imag = *p_real;\n+        *p_real = 0.0;\n+        ++p_end;\n+    }\n+    else if (*p_end == '+' || *p_end == '-') {\n+        /* Imaginary part still to parse */\n+        if (*p_end == '+') {\n+            ++p_end;  /* Advance to support +- (and ++) */\n+        }\n+        if (double_from_ucs4(p_end, token_end, false, p_imag, &p_end) < 0) {\n+            return false;\n+        }\n+        if (*p_end != imaginary_unit) {\n+            return false;\n+        }\n+        ++p_end;\n+    }\n+    else {\n+        *p_imag = 0;\n+    }\n+\n+    if (unmatched_opening_paren) {\n+        /* Allow whitespace inside brackets as in \"(1+2j )\" or \"( 1j )\" */\n+        while (Py_UNICODE_ISSPACE(*p_end)) {\n+            ++p_end;\n+        }\n+        if (*p_end == ')') {\n+            ++p_end;\n+        }\n+        else {\n+            /* parentheses was not closed */\n+            return false;\n+        }\n+    }\n+\n+    while (Py_UNICODE_ISSPACE(*p_end)) {\n+        ++p_end;\n+    }\n+    return p_end == token_end;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_cfloat(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig)\n+{\n+    double real;\n+    double imag;\n+\n+    bool success = to_complex_int(\n+            str, end, &real, &imag,\n+            pconfig->imaginary_unit, true);\n+\n+    if (!success) {\n+        return -1;\n+    }\n+    npy_complex64 val = {(float)real, (float)imag};\n+    memcpy(dataptr, &val, sizeof(npy_complex64));\n+    if (!PyArray_ISNBO(descr->byteorder)) {\n+        npy_bswap4_unaligned(dataptr);\n+        npy_bswap4_unaligned(dataptr + 4);\n+    }\n+    return 0;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_cdouble(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig)\n+{\n+    double real;\n+    double imag;\n+\n+    bool success = to_complex_int(\n+            str, end, &real, &imag, pconfig->imaginary_unit, true);\n+\n+    if (!success) {\n+        return -1;\n+    }\n+    npy_complex128 val = {real, imag};\n+    memcpy(dataptr, &val, sizeof(npy_complex128));\n+    if (!PyArray_ISNBO(descr->byteorder)) {\n+        npy_bswap8_unaligned(dataptr);\n+        npy_bswap8_unaligned(dataptr + 8);\n+    }\n+    return 0;\n+}\n+\n+\n+/*\n+ * String and unicode conversion functions.\n+ */\n+NPY_NO_EXPORT int\n+to_string(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *NPY_UNUSED(unused))\n+{\n+    const Py_UCS4* c = str;\n+    size_t length = descr->elsize;\n+\n+    for (size_t i = 0; i < length; i++) {\n+        if (c < end) {\n+            /*\n+             * loadtxt assumed latin1, which is compatible with UCS1 (first\n+             * 256 unicode characters).\n+             */\n+            if (NPY_UNLIKELY(*c > 255)) {\n+                /* TODO: Was UnicodeDecodeError, is unspecific error good? */\n+                return -1;\n+            }\n+            dataptr[i] = (Py_UCS1)(*c);\n+            c++;\n+        }\n+        else {\n+            dataptr[i] = '\\0';\n+        }\n+    }\n+    return 0;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_unicode(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *NPY_UNUSED(unused))\n+{\n+    int length = descr->elsize / 4;\n+\n+    if (length <= end - str) {\n+        memcpy(dataptr, str, length * 4);\n+    }\n+    else {\n+        size_t given_len = end - str;\n+        memcpy(dataptr, str, given_len * 4);\n+        memset(dataptr + given_len * 4, '\\0', (length - given_len) * 4);\n+    }\n+\n+    if (!PyArray_ISNBO(descr->byteorder)) {\n+        for (int i = 0; i < length; i++) {\n+            npy_bswap4_unaligned(dataptr);\n+            dataptr += 4;\n+        }\n+    }\n+    return 0;\n+}\n+\n+\n+\n+/*\n+ * Convert functions helper for the generic converter.\n+ */\n+static PyObject *\n+call_converter_function(\n+        PyObject *func, const Py_UCS4 *str, size_t length, bool byte_converters)\n+{\n+    PyObject *s = PyUnicode_FromKindAndData(PyUnicode_4BYTE_KIND, str, length);\n+    if (s == NULL) {\n+        return s;\n+    }\n+    if (byte_converters) {\n+        Py_SETREF(s, PyUnicode_AsEncodedString(s, \"latin1\", NULL));\n+        if (s == NULL) {\n+            return NULL;\n+        }\n+    }\n+    if (func == NULL) {\n+        return s;\n+    }\n+    PyObject *result = PyObject_CallFunctionObjArgs(func, s, NULL);\n+    Py_DECREF(s);\n+    return result;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_generic_with_converter(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *config, PyObject *func)\n+{\n+    bool use_byte_converter;\n+    if (func == NULL) {\n+        use_byte_converter = config->c_byte_converters;\n+    }\n+    else {\n+        use_byte_converter = config->python_byte_converters;\n+    }\n+    /* Converts to unicode and calls custom converter (if set) */\n+    PyObject *converted = call_converter_function(\n+            func, str, (size_t)(end - str), use_byte_converter);\n+    if (converted == NULL) {\n+        return -1;\n+    }\n+\n+    int res = PyArray_Pack(descr, dataptr, converted);\n+    Py_DECREF(converted);\n+    return res;\n+}\n+\n+\n+NPY_NO_EXPORT int\n+to_generic(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *config)\n+{\n+    return to_generic_with_converter(descr, str, end, dataptr, config, NULL);\n+}"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/conversions.h",
                "patch": "@@ -0,0 +1,57 @@\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_CONVERSIONS_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_CONVERSIONS_H_\n+\n+#include <stdbool.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/arrayobject.h\"\n+\n+#include \"textreading/parser_config.h\"\n+\n+NPY_NO_EXPORT int\n+to_bool(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig);\n+\n+NPY_NO_EXPORT int\n+to_float(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig);\n+\n+NPY_NO_EXPORT int\n+to_double(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig);\n+\n+NPY_NO_EXPORT int\n+to_cfloat(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig);\n+\n+NPY_NO_EXPORT int\n+to_cdouble(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig);\n+\n+NPY_NO_EXPORT int\n+to_string(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *unused);\n+\n+NPY_NO_EXPORT int\n+to_unicode(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *unused);\n+\n+NPY_NO_EXPORT int\n+to_generic_with_converter(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *unused, PyObject *func);\n+\n+NPY_NO_EXPORT int\n+to_generic(PyArray_Descr *descr,\n+        const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,\n+        parser_config *pconfig);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_CONVERSIONS_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/field_types.c",
                "patch": "@@ -0,0 +1,201 @@\n+#include \"field_types.h\"\n+#include \"conversions.h\"\n+#include \"str_to_int.h\"\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/ndarraytypes.h\"\n+#include \"alloc.h\"\n+\n+#include \"textreading/growth.h\"\n+\n+\n+NPY_NO_EXPORT void\n+field_types_xclear(int num_field_types, field_type *ft) {\n+    assert(num_field_types >= 0);\n+    if (ft == NULL) {\n+        return;\n+    }\n+    for (int i = 0; i < num_field_types; i++) {\n+        Py_XDECREF(ft[i].descr);\n+        ft[i].descr = NULL;\n+    }\n+    PyMem_Free(ft);\n+}\n+\n+\n+/*\n+ * Fetch custom converters for the builtin NumPy DTypes (or the generic one).\n+ * Structured DTypes get unpacked and `object` uses the generic method.\n+ *\n+ * TODO: This should probably be moved on the DType object in some form,\n+ *       to allow user DTypes to define their own converters.\n+ */\n+static set_from_ucs4_function *\n+get_from_ucs4_function(PyArray_Descr *descr)\n+{\n+    if (descr->type_num == NPY_BOOL) {\n+        return &to_bool;\n+    }\n+    else if (PyDataType_ISSIGNED(descr)) {\n+        switch (descr->elsize) {\n+            case 1:\n+                return &to_int8;\n+            case 2:\n+                return &to_int16;\n+            case 4:\n+                return &to_int32;\n+            case 8:\n+                return &to_int64;\n+            default:\n+                assert(0);\n+        }\n+    }\n+    else if (PyDataType_ISUNSIGNED(descr)) {\n+        switch (descr->elsize) {\n+            case 1:\n+                return &to_uint8;\n+            case 2:\n+                return &to_uint16;\n+            case 4:\n+                return &to_uint32;\n+            case 8:\n+                return &to_uint64;\n+            default:\n+                assert(0);\n+        }\n+    }\n+    else if (descr->type_num == NPY_FLOAT) {\n+        return &to_float;\n+    }\n+    else if (descr->type_num == NPY_DOUBLE) {\n+        return &to_double;\n+    }\n+    else if (descr->type_num == NPY_CFLOAT) {\n+        return &to_cfloat;\n+    }\n+    else if (descr->type_num == NPY_CDOUBLE) {\n+        return &to_cdouble;\n+    }\n+    else if (descr->type_num == NPY_STRING) {\n+        return &to_string;\n+    }\n+    else if (descr->type_num == NPY_UNICODE) {\n+        return &to_unicode;\n+    }\n+    return &to_generic;\n+}\n+\n+\n+/*\n+ * Note that the function cleans up `ft` on error.  If `num_field_types < 0`\n+ * cleanup has already happened in the internal call.\n+ */\n+static npy_intp\n+field_type_grow_recursive(PyArray_Descr *descr,\n+        npy_intp num_field_types, field_type **ft, npy_intp *ft_size,\n+        npy_intp field_offset)\n+{\n+    if (PyDataType_HASSUBARRAY(descr)) {\n+        PyArray_Dims shape = {NULL, -1};\n+\n+        if (!(PyArray_IntpConverter(descr->subarray->shape, &shape))) {\n+             PyErr_SetString(PyExc_ValueError, \"invalid subarray shape\");\n+             field_types_xclear(num_field_types, *ft);\n+             return -1;\n+        }\n+        npy_intp size = PyArray_MultiplyList(shape.ptr, shape.len);\n+        npy_free_cache_dim_obj(shape);\n+        for (npy_intp i = 0; i < size; i++) {\n+            num_field_types = field_type_grow_recursive(descr->subarray->base,\n+                    num_field_types, ft, ft_size, field_offset);\n+            field_offset += descr->subarray->base->elsize;\n+            if (num_field_types < 0) {\n+                return -1;\n+            }\n+        }\n+        return num_field_types;\n+    }\n+    else if (PyDataType_HASFIELDS(descr)) {\n+        npy_int num_descr_fields = PyTuple_Size(descr->names);\n+        if (num_descr_fields < 0) {\n+            field_types_xclear(num_field_types, *ft);\n+            return -1;\n+        }\n+        for (npy_intp i = 0; i < num_descr_fields; i++) {\n+            PyObject *key = PyTuple_GET_ITEM(descr->names, i);\n+            PyObject *tup = PyObject_GetItem(descr->fields, key);\n+            if (tup == NULL) {\n+                field_types_xclear(num_field_types, *ft);\n+                return -1;\n+            }\n+            PyArray_Descr *field_descr;\n+            PyObject *title;\n+            int offset;\n+            if (!PyArg_ParseTuple(tup, \"Oi|O\", &field_descr, &offset, &title)) {\n+                Py_DECREF(tup);\n+                field_types_xclear(num_field_types, *ft);\n+                return -1;\n+            }\n+            Py_DECREF(tup);\n+            num_field_types = field_type_grow_recursive(\n+                    field_descr, num_field_types, ft, ft_size,\n+                    field_offset + offset);\n+            if (num_field_types < 0) {\n+                return -1;\n+            }\n+        }\n+        return num_field_types;\n+    }\n+\n+    if (*ft_size <= num_field_types) {\n+        npy_intp alloc_size = grow_size_and_multiply(\n+                ft_size, 4, sizeof(field_type));\n+        if (alloc_size < 0) {\n+            field_types_xclear(num_field_types, *ft);\n+            return -1;\n+        }\n+        field_type *new_ft = PyMem_Realloc(*ft, alloc_size);\n+        if (new_ft == NULL) {\n+            field_types_xclear(num_field_types, *ft);\n+            return -1;\n+        }\n+        *ft = new_ft;\n+    }\n+\n+    Py_INCREF(descr);\n+    (*ft)[num_field_types].descr = descr;\n+    (*ft)[num_field_types].set_from_ucs4 = get_from_ucs4_function(descr);\n+    (*ft)[num_field_types].structured_offset = field_offset;\n+\n+    return num_field_types + 1;\n+}\n+\n+\n+/*\n+ * Prepare the \"field_types\" for the given dtypes/descriptors.  Currently,\n+ * we copy the itemsize, but the main thing is that we check for custom\n+ * converters.\n+ */\n+NPY_NO_EXPORT npy_intp\n+field_types_create(PyArray_Descr *descr, field_type **ft)\n+{\n+    if (descr->subarray != NULL) {\n+        /*\n+         * This could probably be allowed, but NumPy absorbs the dimensions\n+         * so it is an awkward corner case that probably never really worked.\n+         */\n+        PyErr_SetString(PyExc_TypeError,\n+                \"file reader does not support subarray dtypes.  You can\"\n+                \"put the dtype into a structured one using \"\n+                \"`np.dtype(('name', dtype))` to avoid this limitation.\");\n+        return -1;\n+    }\n+\n+    npy_intp ft_size = 4;\n+    *ft = PyMem_Malloc(ft_size * sizeof(field_type));\n+    if (*ft == NULL) {\n+        return -1;\n+    }\n+    return field_type_grow_recursive(descr, 0, ft, &ft_size, 0);\n+}"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/field_types.h",
                "patch": "@@ -0,0 +1,67 @@\n+\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_FIELD_TYPES_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_FIELD_TYPES_H_\n+\n+#include <stdint.h>\n+#include <stdbool.h>\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/ndarraytypes.h\"\n+\n+#include \"textreading/parser_config.h\"\n+\n+/**\n+ * Function defining the conversion for each value.\n+ *\n+ * This function must support unaligned memory access.  As of now, there is\n+ * no special error handling (in whatever form):  We assume that it is always\n+ * reasonable to raise a `ValueError` noting the string that failed to be\n+ * converted.\n+ *\n+ * NOTE: An earlier version of the code had unused default values (pandas\n+ *       does this) when columns are missing.  We could define this either\n+ *       by passing `NULL` in, or by adding a default explicitly somewhere.\n+ *       (I think users should probably have to define the default, at which\n+ *       point it doesn't matter here.)\n+ *\n+ * NOTE: We are currently passing the parser config, this could be made public\n+ *       or could be set up to be dtype specific/private.  Always passing\n+ *       pconfig fully seems easier right now even if it may change.\n+ *       (A future use-case may for example be user-specified strings that are\n+ *       considered boolean True or False).\n+ *\n+ * TODO: Aside from nailing down the above notes, it may be nice to expose\n+ *       these function publically.  This could allow user DTypes to provide\n+ *       a converter or custom converters written in C rather than Python.\n+ *\n+ * @param descr The NumPy descriptor of the field (may be byte-swapped, etc.)\n+ * @param str Pointer to the beginning of the UCS4 string to be parsed.\n+ * @param end Pointer to the end of the UCS4 string.  This value is currently\n+ *            guaranteed to be `\\0`, ensuring that parsers can rely on\n+ *            nul-termination.\n+ * @param dataptr The pointer where to store the parsed value\n+ * @param pconfig Additional configuration for the parser.\n+ * @returns 0 on success and -1 on failure.  If the return value is -1 an\n+ *          error may or may not be set.  If an error is set, it is chained\n+ *          behind the generic ValueError.\n+ */\n+typedef int (set_from_ucs4_function)(\n+        PyArray_Descr *descr, const Py_UCS4 *str, const Py_UCS4 *end,\n+        char *dataptr, parser_config *pconfig);\n+\n+typedef struct _field_type {\n+    set_from_ucs4_function *set_from_ucs4;\n+    /* The original NumPy descriptor */\n+    PyArray_Descr *descr;\n+    /* Offset to this entry within row. */\n+    npy_intp structured_offset;\n+} field_type;\n+\n+\n+NPY_NO_EXPORT void\n+field_types_xclear(int num_field_types, field_type *ft);\n+\n+NPY_NO_EXPORT npy_intp\n+field_types_create(PyArray_Descr *descr, field_type **ft);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_FIELD_TYPES_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/growth.c",
                "patch": "@@ -0,0 +1,47 @@\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/ndarraytypes.h\"\n+\n+#include \"templ_common.h\"\n+\n+/*\n+ * Helper function taking the size input and growing it (based on min_grow).\n+ * The current scheme is a minimum growth and a general growth by 25%\n+ * overallocation.  This is then capped at 2**20 elements, as that propels us\n+ * in the range of large page sizes (so it is presumably more than enough).\n+ *\n+ * It further multiplies it with `itemsize` and ensures that all results fit\n+ * into an `npy_intp`.\n+ * Returns -1 if any overflow occurred or the result would not fit.\n+ * The user has to ensure the input is ssize_t but not negative.\n+ */\n+NPY_NO_EXPORT npy_intp\n+grow_size_and_multiply(npy_intp *size, npy_intp min_grow, npy_intp itemsize) {\n+    /* min_grow must be a power of two: */\n+    assert((min_grow & (min_grow - 1)) == 0);\n+    npy_uintp new_size = (npy_uintp)*size;\n+    npy_intp growth = *size >> 2;\n+    if (growth <= min_grow) {\n+        /* can never lead to overflow if we are using min_growth */\n+        new_size += min_grow;\n+    }\n+    else {\n+        if (growth > 1 << 20) {\n+            /* limit growth to order of MiB (even hugepages are not larger) */\n+            growth = 1 << 20;\n+        }\n+        new_size += growth + min_grow - 1;\n+        new_size &= ~min_grow;\n+\n+        if (new_size > NPY_MAX_INTP) {\n+            return -1;\n+        }\n+    }\n+    *size = (npy_intp)new_size;\n+    npy_intp alloc_size;\n+    if (npy_mul_with_overflow_intp(&alloc_size, (npy_intp)new_size, itemsize)) {\n+        return -1;\n+    }\n+    return alloc_size;\n+}\n+"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/growth.h",
                "patch": "@@ -0,0 +1,7 @@\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_GROWTH_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_GROWTH_H_\n+\n+NPY_NO_EXPORT npy_intp\n+grow_size_and_multiply(npy_intp *size, npy_intp min_grow, npy_intp itemsize);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_GROWTH_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/parser_config.h",
                "patch": "@@ -0,0 +1,61 @@\n+\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_PARSER_CONFIG_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_PARSER_CONFIG_H_\n+\n+#include <stdbool.h>\n+\n+typedef struct {\n+    /*\n+     *  Field delimiter character.\n+     *  Typically ',', ' ', '\\t', ignored if `delimiter_is_whitespace` is true.\n+     */\n+    Py_UCS4 delimiter;\n+\n+    /*\n+     *  Character used to quote fields.\n+     *  Typically '\"' or \"'\".  To disable quoting we set this to UINT_MAX\n+     *  (which is not a valid unicode character and thus cannot occur in the\n+     *  file; the same is used for all other characters if necessary).\n+     */\n+    Py_UCS4 quote;\n+\n+    /*\n+     *  Character(s) that indicates the start of a comment.\n+     *  Typically '#', '%' or ';'.\n+     *  When encountered in a line and not inside quotes, all character\n+     *  from the comment character(s) to the end of the line are ignored.\n+     */\n+    Py_UCS4 comment;\n+\n+    /*\n+     *  Ignore whitespace at the beginning of a field (outside/before quotes).\n+     *  Is (and must be) set if `delimiter_is_whitespace`.\n+     */\n+    bool ignore_leading_whitespace;\n+\n+    /*\n+     * If true, the delimiter is ignored and any unicode whitespace is used\n+     * for splitting (same as `string.split()` in Python). In that case\n+     * `ignore_leading_whitespace` should also be set.\n+     */\n+    bool delimiter_is_whitespace;\n+\n+    /*\n+     *  The imaginary unit character. Default is `j`.\n+     */\n+    Py_UCS4 imaginary_unit;\n+\n+     /*\n+      * Data should be encoded as `latin1` when using python converter\n+      * (implementing `loadtxt` default Python 2 compatibility mode).\n+      * The c byte converter is used when the user requested `dtype=\"S\"`.\n+      * In this case we go via `dtype=object`, however, loadtxt allows latin1\n+      * while normal object to string casts only accept ASCII, so it ensures\n+      * that that the object array already contains bytes and not strings.\n+      */\n+     bool python_byte_converters;\n+     bool c_byte_converters;\n+} parser_config;\n+\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_PARSER_CONFIG_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/readtext.c",
                "patch": "@@ -0,0 +1,312 @@\n+#include <stdio.h>\n+#include <stdbool.h>\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/arrayobject.h\"\n+#include \"npy_argparse.h\"\n+#include \"common.h\"\n+#include \"conversion_utils.h\"\n+\n+#include \"textreading/parser_config.h\"\n+#include \"textreading/stream_pyobject.h\"\n+#include \"textreading/field_types.h\"\n+#include \"textreading/rows.h\"\n+#include \"textreading/str_to_int.h\"\n+\n+\n+//\n+// `usecols` must point to a Python object that is Py_None or a 1-d contiguous\n+// numpy array with data type int32.\n+//\n+// `dtype` must point to a Python object that is Py_None or a numpy dtype\n+// instance.  If the latter, code and sizes must be arrays of length\n+// num_dtype_fields, holding the flattened data field type codes and byte\n+// sizes. (num_dtype_fields, codes, and sizes can be inferred from dtype,\n+// but we do that in Python code.)\n+//\n+// If both `usecols` and `dtype` are not None, and the data type is compound,\n+// then len(usecols) must equal num_dtype_fields.\n+//\n+// If `dtype` is given and it is compound, and `usecols` is None, then the\n+// number of columns in the file must match the number of fields in `dtype`.\n+//\n+static PyObject *\n+_readtext_from_stream(stream *s,\n+        parser_config *pc, Py_ssize_t num_usecols, Py_ssize_t usecols[],\n+        Py_ssize_t skiplines, Py_ssize_t max_rows,\n+        PyObject *converters, PyObject *dtype)\n+{\n+    PyArrayObject *arr = NULL;\n+    PyArray_Descr *out_dtype = NULL;\n+    field_type *ft = NULL;\n+\n+    /*\n+     * If dtypes[0] is dtype the input was not structured and the result\n+     * is considered \"homogeneous\" and we have to discover the number of\n+     * columns/\n+     */\n+    out_dtype = (PyArray_Descr *)dtype;\n+    Py_INCREF(out_dtype);\n+\n+    Py_ssize_t num_fields = field_types_create(out_dtype, &ft);\n+    if (num_fields < 0) {\n+        goto finish;\n+    }\n+    bool homogeneous = num_fields == 1 && ft[0].descr == out_dtype;\n+\n+    if (!homogeneous && usecols != NULL && num_usecols != num_fields) {\n+        PyErr_Format(PyExc_TypeError,\n+                \"If a structured dtype is used, the number of columns in \"\n+                \"`usecols` must match the effective number of fields. \"\n+                \"But %zd usecols were given and the number of fields is %zd.\",\n+                num_usecols, num_fields);\n+        goto finish;\n+    }\n+\n+    arr = read_rows(\n+            s, max_rows, num_fields, ft, pc,\n+            num_usecols, usecols, skiplines, converters,\n+            NULL, out_dtype, homogeneous);\n+    if (arr == NULL) {\n+        goto finish;\n+    }\n+\n+  finish:\n+    Py_XDECREF(out_dtype);\n+    field_types_xclear(num_fields, ft);\n+    return (PyObject *)arr;\n+}\n+\n+\n+static int\n+parse_control_character(PyObject *obj, Py_UCS4 *character)\n+{\n+    if (obj == Py_None) {\n+        *character = (Py_UCS4)-1;  /* character beyond unicode range */\n+        return 1;\n+    }\n+    if (!PyUnicode_Check(obj) || PyUnicode_GetLength(obj) != 1) {\n+        PyErr_Format(PyExc_TypeError,\n+                \"Text reading control character must be a single unicode \"\n+                \"character or None; but got: %.100R\", obj);\n+        return 0;\n+    }\n+    *character = PyUnicode_READ_CHAR(obj, 0);\n+    return 1;\n+}\n+\n+\n+/*\n+ * A (somewhat verbose) check that none of the control characters match or are\n+ * newline.  Most of these combinations are completely fine, just weird or\n+ * surprising.\n+ * (I.e. there is an implicit priority for control characters, so if a comment\n+ * matches a delimiter, it would just be a comment.)\n+ * In theory some `delimiter=None` paths could have a \"meaning\", but let us\n+ * assume that users are better of setting one of the control chars to `None`\n+ * for clarity.\n+ *\n+ * This also checks that the control characters cannot be newlines.\n+ */\n+static int\n+error_if_matching_control_characters(\n+        Py_UCS4 delimiter, Py_UCS4 quote, Py_UCS4 comment)\n+{\n+    char *control_char1;\n+    char *control_char2 = NULL;\n+    if (comment != (Py_UCS4)-1) {\n+        control_char1 = \"comment\";\n+        if (comment == '\\r' || comment == '\\n') {\n+            goto error;\n+        }\n+        else if (comment == quote) {\n+            control_char2 = \"quotechar\";\n+            goto error;\n+        }\n+        else if (comment == delimiter) {\n+            control_char2 = \"delimiter\";\n+            goto error;\n+        }\n+    }\n+    if (quote != (Py_UCS4)-1) {\n+        control_char1 = \"quotechar\";\n+        if (quote == '\\r' || quote == '\\n') {\n+            goto error;\n+        }\n+        else if (quote == delimiter) {\n+            control_char2 = \"delimiter\";\n+            goto error;\n+        }\n+    }\n+    if (delimiter != (Py_UCS4)-1) {\n+        control_char1 = \"delimiter\";\n+        if (delimiter == '\\r' || delimiter == '\\n') {\n+            goto error;\n+        }\n+    }\n+    /* The above doesn't work with delimiter=None, which means \"whitespace\" */\n+    if (delimiter == (Py_UCS4)-1) {\n+        control_char1 = \"delimiter\";\n+        if (Py_UNICODE_ISSPACE(comment)) {\n+            control_char2 = \"comment\";\n+            goto error;\n+        }\n+        else if (Py_UNICODE_ISSPACE(quote)) {\n+            control_char2 = \"quotechar\";\n+            goto error;\n+        }\n+    }\n+    return 0;\n+\n+  error:\n+    if (control_char2 != NULL) {\n+        PyErr_Format(PyExc_TypeError,\n+                \"The values for control characters '%s' and '%s' are \"\n+                \"incompatible\",\n+                control_char1, control_char2);\n+    }\n+    else {\n+        PyErr_Format(PyExc_TypeError,\n+                \"control character '%s' cannot be a newline (`\\\\r` or `\\\\n`).\",\n+                control_char1, control_char2);\n+    }\n+    return -1;\n+}\n+\n+\n+NPY_NO_EXPORT PyObject *\n+_load_from_filelike(PyObject *NPY_UNUSED(mod),\n+        PyObject *const *args, Py_ssize_t len_args, PyObject *kwnames)\n+{\n+    PyObject *file;\n+    Py_ssize_t skiplines = 0;\n+    Py_ssize_t max_rows = -1;\n+    PyObject *usecols_obj = Py_None;\n+    PyObject *converters = Py_None;\n+\n+    PyObject *dtype = Py_None;\n+    PyObject *encoding_obj = Py_None;\n+    const char *encoding = NULL;\n+\n+    parser_config pc = {\n+        .delimiter = ',',\n+        .comment = '#',\n+        .quote = '\"',\n+        .imaginary_unit = 'j',\n+        .delimiter_is_whitespace = false,\n+        .ignore_leading_whitespace = false,\n+        .python_byte_converters = false,\n+        .c_byte_converters = false,\n+    };\n+    bool filelike = true;\n+\n+    PyObject *arr = NULL;\n+\n+    NPY_PREPARE_ARGPARSER;\n+    if (npy_parse_arguments(\"_load_from_filelike\", args, len_args, kwnames,\n+            \"file\", NULL, &file,\n+            \"|delimiter\", &parse_control_character, &pc.delimiter,\n+            \"|comment\", &parse_control_character, &pc.comment,\n+            \"|quote\", &parse_control_character, &pc.quote,\n+            \"|imaginary_unit\", &parse_control_character, &pc.imaginary_unit,\n+            \"|usecols\", NULL, &usecols_obj,\n+            \"|skiplines\", &PyArray_IntpFromPyIntConverter, &skiplines,\n+            \"|max_rows\", &PyArray_IntpFromPyIntConverter, &max_rows,\n+            \"|converters\", NULL, &converters,\n+            \"|dtype\", NULL, &dtype,\n+            \"|encoding\", NULL, &encoding_obj,\n+            \"|filelike\", &PyArray_BoolConverter, &filelike,\n+            \"|byte_converters\", &PyArray_BoolConverter, &pc.python_byte_converters,\n+            \"|c_byte_converters\", PyArray_BoolConverter, &pc.c_byte_converters,\n+            NULL, NULL, NULL) < 0) {\n+        return NULL;\n+    }\n+\n+    /* Reject matching control characters, they just rarely make sense anyway */\n+    if (error_if_matching_control_characters(\n+            pc.delimiter, pc.quote, pc.comment) < 0) {\n+        return NULL;\n+    }\n+\n+    if (pc.delimiter == (Py_UCS4)-1) {\n+        pc.delimiter_is_whitespace = true;\n+        /* Ignore leading whitespace to match `string.split(None)` */\n+        pc.ignore_leading_whitespace = true;\n+    }\n+\n+    if (!PyArray_DescrCheck(dtype) ) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"internal error: dtype must be provided and be a NumPy dtype\");\n+        return NULL;\n+    }\n+\n+    if (encoding_obj != Py_None) {\n+        if (!PyUnicode_Check(encoding_obj)) {\n+            PyErr_SetString(PyExc_TypeError,\n+                    \"encoding must be a unicode string.\");\n+            return NULL;\n+        }\n+        encoding = PyUnicode_AsUTF8(encoding_obj);\n+        if (encoding == NULL) {\n+            return NULL;\n+        }\n+    }\n+\n+    /*\n+     * Parse usecols, the rest of NumPy has no clear helper for this, so do\n+     * it here manually.\n+     */\n+    Py_ssize_t num_usecols = -1;\n+    Py_ssize_t *usecols = NULL;\n+    if (usecols_obj != Py_None) {\n+        num_usecols = PySequence_Length(usecols_obj);\n+        if (num_usecols < 0) {\n+            return NULL;\n+        }\n+        /* Calloc just to not worry about overflow */\n+        usecols = PyMem_Calloc(num_usecols, sizeof(Py_ssize_t));\n+        for (Py_ssize_t i = 0; i < num_usecols; i++) {\n+            PyObject *tmp = PySequence_GetItem(usecols_obj, i);\n+            if (tmp == NULL) {\n+                PyMem_FREE(usecols);\n+                return NULL;\n+            }\n+            usecols[i] = PyNumber_AsSsize_t(tmp, PyExc_OverflowError);\n+            if (error_converting(usecols[i])) {\n+                if (PyErr_ExceptionMatches(PyExc_TypeError)) {\n+                    PyErr_Format(PyExc_TypeError,\n+                            \"usecols must be an int or a sequence of ints but \"\n+                            \"it contains at least one element of type '%s'\",\n+                            Py_TYPE(tmp)->tp_name);\n+                }\n+                Py_DECREF(tmp);\n+                PyMem_FREE(usecols);\n+                return NULL;\n+            }\n+            Py_DECREF(tmp);\n+        }\n+    }\n+\n+    stream *s;\n+    if (filelike) {\n+        s = stream_python_file(file, encoding);\n+    }\n+    else {\n+        s = stream_python_iterable(file, encoding);\n+    }\n+    if (s == NULL) {\n+        PyMem_FREE(usecols);\n+        return NULL;\n+    }\n+\n+    arr = _readtext_from_stream(\n+            s, &pc, num_usecols, usecols, skiplines, max_rows, converters, dtype);\n+    stream_close(s);\n+    PyMem_FREE(usecols);\n+    return arr;\n+}\n+"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/readtext.h",
                "patch": "@@ -0,0 +1,7 @@\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_READTEXT_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_READTEXT_H_\n+\n+NPY_NO_EXPORT PyObject *\n+_load_from_filelike(PyObject *self, PyObject *args, PyObject *kwargs);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_READTEXT_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/rows.c",
                "patch": "@@ -0,0 +1,481 @@\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/arrayobject.h\"\n+#include \"numpy/npy_3kcompat.h\"\n+#include \"alloc.h\"\n+\n+#include <string.h>\n+#include <stdbool.h>\n+\n+#include \"textreading/stream.h\"\n+#include \"textreading/tokenize.h\"\n+#include \"textreading/conversions.h\"\n+#include \"textreading/field_types.h\"\n+#include \"textreading/rows.h\"\n+#include \"textreading/growth.h\"\n+\n+/*\n+ * Minimum size to grow the allcoation by (or 25%). The 8KiB means the actual\n+ * growths is within `8 KiB <= size < 16 KiB` (depending on the row size).\n+ */\n+#define MIN_BLOCK_SIZE (1 << 13)\n+\n+\n+\n+/*\n+ *  Create the array of converter functions from the Python converters.\n+ */\n+static PyObject **\n+create_conv_funcs(\n+        PyObject *converters, Py_ssize_t num_fields, const Py_ssize_t *usecols)\n+{\n+    assert(converters != Py_None);\n+\n+    PyObject **conv_funcs = PyMem_Calloc(num_fields, sizeof(PyObject *));\n+    if (conv_funcs == NULL) {\n+        PyErr_NoMemory();\n+        return NULL;\n+    }\n+\n+    if (PyCallable_Check(converters)) {\n+        /* a single converter used for all columns individually */\n+        for (Py_ssize_t i = 0; i < num_fields; i++) {\n+            Py_INCREF(converters);\n+            conv_funcs[i] = converters;\n+        }\n+        return conv_funcs;\n+    }\n+    else if (!PyDict_Check(converters)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"converters must be a dictionary mapping columns to converter \"\n+                \"functions or a single callable.\");\n+        goto error;\n+    }\n+\n+    PyObject *key, *value;\n+    Py_ssize_t pos = 0;\n+    while (PyDict_Next(converters, &pos, &key, &value)) {\n+        Py_ssize_t column = PyNumber_AsSsize_t(key, PyExc_IndexError);\n+        if (column == -1 && PyErr_Occurred()) {\n+            PyErr_Format(PyExc_TypeError,\n+                    \"keys of the converters dictionary must be integers; \"\n+                    \"got %.100R\", key);\n+            goto error;\n+        }\n+        if (usecols != NULL) {\n+            /*\n+             * This code searches for the corresponding usecol.  It is\n+             * identical to the legacy usecols code, which has two weaknesses:\n+             * 1. It fails for duplicated usecols only setting converter for\n+             *    the first one.\n+             * 2. It fails e.g. if usecols uses negative indexing and\n+             *    converters does not.  (This is a feature, since it allows\n+             *    us to correctly normalize converters to result column here.)\n+             */\n+            Py_ssize_t i = 0;\n+            for (; i < num_fields; i++) {\n+                if (column == usecols[i]) {\n+                    column = i;\n+                    break;\n+                }\n+            }\n+            if (i == num_fields) {\n+                continue;  /* ignore unused converter */\n+            }\n+        }\n+        else {\n+            if (column < -num_fields || column >= num_fields) {\n+                PyErr_Format(PyExc_ValueError,\n+                        \"converter specified for column %zd, which is invalid \"\n+                        \"for the number of fields %d.\", column, num_fields);\n+                goto error;\n+            }\n+            if (column < 0) {\n+                column += num_fields;\n+            }\n+        }\n+        if (!PyCallable_Check(value)) {\n+            PyErr_Format(PyExc_TypeError,\n+                    \"values of the converters dictionary must be callable, \"\n+                    \"but the value associated with key %R is not\", key);\n+            goto error;\n+        }\n+        Py_INCREF(value);\n+        conv_funcs[column] = value;\n+    }\n+    return conv_funcs;\n+\n+  error:\n+    for (Py_ssize_t i = 0; i < num_fields; i++) {\n+        Py_XDECREF(conv_funcs[i]);\n+    }\n+    PyMem_FREE(conv_funcs);\n+    return NULL;\n+}\n+\n+/**\n+ * Read a file into the provided array, or create (and possibly grow) an\n+ * array to read into.\n+ *\n+ * @param s The stream object/struct providing reading capabilities used by\n+ *        the tokenizer.\n+ * @param max_rows The number of rows to read, or -1.  If negative\n+ *        all rows are read.\n+ * @param num_field_types The number of field types stored in `field_types`.\n+ * @param field_types Information about the dtype for each column (or one if\n+ *        `homogeneous`).\n+ * @param pconfig Pointer to the parser config object used by both the\n+ *        tokenizer and the conversion functions.\n+ * @param num_usecols The number of columns in `usecols`.\n+ * @param usecols An array of length `num_usecols` or NULL.  If given indicates\n+ *        which column is read for each individual row (negative columns are\n+ *        accepted).\n+ * @param skiplines The number of lines to skip, these lines are ignored.\n+ * @param converters Python dictionary of converters.  Finalizing converters\n+ *        is difficult without information about the number of columns.\n+ * @param data_array An array to be filled or NULL.  In either case a new\n+ *        reference is returned (the reference to `data_array` is not stolen).\n+ * @param out_descr The dtype used for allocating a new array.  This is not\n+ *        used if `data_array` is provided.  Note that the actual dtype of the\n+ *        returned array can differ for strings.\n+ * @param num_cols Pointer in which the actual (discovered) number of columns\n+ *        is returned.  This is only relevant if `homogeneous` is true.\n+ * @param homogeneous Whether the datatype of the array is not homogeneous,\n+ *        i.e. not structured.  In this case the number of columns has to be\n+ *        discovered an the returned array will be 2-dimensional rather than\n+ *        1-dimensional.\n+ *\n+ * @returns Returns the result as an array object or NULL on error.  The result\n+ *          is always a new reference (even when `data_array` was passed in).\n+ */\n+NPY_NO_EXPORT PyArrayObject *\n+read_rows(stream *s,\n+        npy_intp max_rows, Py_ssize_t num_field_types, field_type *field_types,\n+        parser_config *pconfig, Py_ssize_t num_usecols, Py_ssize_t *usecols,\n+        Py_ssize_t skiplines, PyObject *converters,\n+        PyArrayObject *data_array, PyArray_Descr *out_descr,\n+        bool homogeneous)\n+{\n+    char *data_ptr = NULL;\n+    Py_ssize_t current_num_fields;\n+    npy_intp row_size = out_descr->elsize;\n+    PyObject **conv_funcs = NULL;\n+\n+    bool needs_init = PyDataType_FLAGCHK(out_descr, NPY_NEEDS_INIT);\n+\n+    int ndim = homogeneous ? 2 : 1;\n+    npy_intp result_shape[2] = {0, 1};\n+\n+    bool data_array_allocated = data_array == NULL;\n+    /* Make sure we own `data_array` for the purpose of error handling */\n+    Py_XINCREF(data_array);\n+    size_t rows_per_block = 1;  /* will be increased depending on row size */\n+    npy_intp data_allocated_rows = 0;\n+\n+    /* We give a warning if max_rows is used and an empty line is encountered */\n+    bool give_empty_row_warning = max_rows >= 0;\n+\n+    int ts_result = 0;\n+    tokenizer_state ts;\n+    if (tokenizer_init(&ts, pconfig) < 0) {\n+        goto error;\n+    }\n+\n+    /* Set the actual number of fields if it is already known, otherwise -1 */\n+    Py_ssize_t actual_num_fields = -1;\n+    if (usecols != NULL) {\n+        assert(homogeneous || num_field_types == num_usecols);\n+        actual_num_fields = num_usecols;\n+    }\n+    else if (!homogeneous) {\n+        assert(usecols == NULL || num_field_types == num_usecols);\n+        actual_num_fields = num_field_types;\n+    }\n+\n+    for (Py_ssize_t i = 0; i < skiplines; i++) {\n+        ts.state = TOKENIZE_GOTO_LINE_END;\n+        ts_result = tokenize(s, &ts, pconfig);\n+        if (ts_result < 0) {\n+            goto error;\n+        }\n+        else if (ts_result != 0) {\n+            /* Fewer lines than skiplines is acceptable */\n+            break;\n+        }\n+    }\n+\n+    Py_ssize_t row_count = 0;  /* number of rows actually processed */\n+    while ((max_rows < 0 || row_count < max_rows) && ts_result == 0) {\n+        ts_result = tokenize(s, &ts, pconfig);\n+        if (ts_result < 0) {\n+            goto error;\n+        }\n+        current_num_fields = ts.num_fields;\n+        field_info *fields = ts.fields;\n+        if (NPY_UNLIKELY(ts.num_fields == 0)) {\n+            /*\n+             * Deprecated NumPy 1.23, 2021-01-13 (not really a deprecation,\n+             * but similar policy should apply to removing the warning again)\n+             */\n+             /* Tokenizer may give a final \"empty line\" even if there is none */\n+            if (give_empty_row_warning && ts_result == 0) {\n+                give_empty_row_warning = false;\n+                if (PyErr_WarnFormat(PyExc_UserWarning, 3,\n+                        \"Input line %zd contained no data and will not be \"\n+                        \"counted towards `max_rows=%zd`.  This differs from \"\n+                        \"the behaviour in NumPy <=1.22 which counted lines \"\n+                        \"rather than rows.  If desired, the previous behaviour \"\n+                        \"can be achieved by using `itertools.islice`.\\n\"\n+                        \"Please see the 1.23 release notes for an example on \"\n+                        \"how to do this.  If you wish to ignore this warning, \"\n+                        \"use `warnings.filterwarnings`.  This warning is \"\n+                        \"expected to be removed in the future and is given \"\n+                        \"only once per `loadtxt` call.\",\n+                        row_count + skiplines + 1, max_rows) < 0) {\n+                    goto error;\n+                }\n+            }\n+            continue;  /* Ignore empty line */\n+        }\n+\n+        if (NPY_UNLIKELY(data_ptr == NULL)) {\n+            // We've deferred some of the initialization tasks to here,\n+            // because we've now read the first line, and we definitively\n+            // know how many fields (i.e. columns) we will be processing.\n+            if (actual_num_fields == -1) {\n+                actual_num_fields = current_num_fields;\n+            }\n+\n+            if (converters != Py_None) {\n+                conv_funcs = create_conv_funcs(\n+                        converters, actual_num_fields, usecols);\n+                if (conv_funcs == NULL) {\n+                    goto error;\n+                }\n+            }\n+\n+            /* Note that result_shape[1] is only used if homogeneous is true */\n+            result_shape[1] = actual_num_fields;\n+            if (homogeneous) {\n+                row_size *= actual_num_fields;\n+            }\n+\n+            if (data_array == NULL) {\n+                if (max_rows < 0) {\n+                    /*\n+                     * Negative max_rows denotes to read the whole file, we\n+                     * approach this by allocating ever larger blocks.\n+                     * Adds a number of rows based on `MIN_BLOCK_SIZE`.\n+                     * Note: later code grows assuming this is a power of two.\n+                     */\n+                    if (row_size == 0) {\n+                        /* actual rows_per_block should not matter here */\n+                        rows_per_block = 512;\n+                    }\n+                    else {\n+                        /* safe on overflow since min_rows will be 0 or 1 */\n+                        size_t min_rows = (\n+                                (MIN_BLOCK_SIZE + row_size - 1) / row_size);\n+                        while (rows_per_block < min_rows) {\n+                            rows_per_block *= 2;\n+                        }\n+                    }\n+                    data_allocated_rows = rows_per_block;\n+                }\n+                else {\n+                    data_allocated_rows = max_rows;\n+                }\n+                result_shape[0] = data_allocated_rows;\n+                Py_INCREF(out_descr);\n+                /*\n+                 * We do not use Empty, as it would fill with None\n+                 * and requiring decref'ing if we shrink again.\n+                 */\n+                data_array = (PyArrayObject *)PyArray_SimpleNewFromDescr(\n+                        ndim, result_shape, out_descr);\n+#ifdef NPY_RELAXED_STRIDES_DEBUG\n+                /* Incompatible with NPY_RELAXED_STRIDES_DEBUG due to growing */\n+                if (result_shape[0] == 1) {\n+                    PyArray_STRIDES(data_array)[0] = row_size;\n+                }\n+#endif /* NPY_RELAXED_STRIDES_DEBUG */\n+                if (data_array == NULL) {\n+                    goto error;\n+                }\n+                if (needs_init) {\n+                    memset(PyArray_BYTES(data_array), 0, PyArray_NBYTES(data_array));\n+                }\n+            }\n+            else {\n+                assert(max_rows >=0);\n+                data_allocated_rows = max_rows;\n+            }\n+            data_ptr = PyArray_BYTES(data_array);\n+        }\n+\n+        if (!usecols && (actual_num_fields != current_num_fields)) {\n+            PyErr_Format(PyExc_ValueError,\n+                    \"the number of columns changed from %d to %d at row %zu; \"\n+                    \"use `usecols` to select a subset and avoid this error\",\n+                    actual_num_fields, current_num_fields, row_count+1);\n+            goto error;\n+        }\n+\n+        if (NPY_UNLIKELY(data_allocated_rows == row_count)) {\n+            /*\n+             * Grow by ~25% and rounded up to the next rows_per_block\n+             * NOTE: This is based on very crude timings and could be refined!\n+             */\n+            npy_intp new_rows = data_allocated_rows;\n+            npy_intp alloc_size = grow_size_and_multiply(\n+                    &new_rows, rows_per_block, row_size);\n+            if (alloc_size < 0) {\n+                /* should normally error much earlier, but make sure */\n+                PyErr_SetString(PyExc_ValueError,\n+                        \"array is too big. Cannot read file as a single array; \"\n+                        \"providing a maximum number of rows to read may help.\");\n+                goto error;\n+            }\n+\n+            char *new_data = PyDataMem_UserRENEW(\n+                    PyArray_BYTES(data_array), alloc_size ? alloc_size : 1,\n+                    PyArray_HANDLER(data_array));\n+            if (new_data == NULL) {\n+                PyErr_NoMemory();\n+                goto error;\n+            }\n+            /* Replace the arrays data since it may have changed */\n+            ((PyArrayObject_fields *)data_array)->data = new_data;\n+            ((PyArrayObject_fields *)data_array)->dimensions[0] = new_rows;\n+            data_ptr = new_data + row_count * row_size;\n+            data_allocated_rows = new_rows;\n+            if (needs_init) {\n+                memset(data_ptr, '\\0', (new_rows - row_count) * row_size);\n+            }\n+        }\n+\n+        for (Py_ssize_t i = 0; i < actual_num_fields; ++i) {\n+            Py_ssize_t f;  /* The field, either 0 (if homogeneous) or i. */\n+            Py_ssize_t col;  /* The column as read, remapped by usecols */\n+            char *item_ptr;\n+            if (homogeneous) {\n+                f = 0;\n+                item_ptr = data_ptr + i * field_types[0].descr->elsize;\n+            }\n+            else {\n+                f = i;\n+                item_ptr = data_ptr + field_types[f].structured_offset;\n+            }\n+\n+            if (usecols == NULL) {\n+                col = i;\n+            }\n+            else {\n+                col = usecols[i];\n+                if (col < 0) {\n+                    // Python-like column indexing: k = -1 means the last column.\n+                    col += current_num_fields;\n+                }\n+                if (NPY_UNLIKELY((col < 0) || (col >= current_num_fields))) {\n+                    PyErr_Format(PyExc_ValueError,\n+                            \"invalid column index %d at row %zu with %d \"\n+                            \"columns\",\n+                            usecols[i], current_num_fields, row_count+1);\n+                    goto error;\n+                }\n+            }\n+\n+            /*\n+             * The following function calls represent the main \"conversion\"\n+             * step, i.e. parsing the unicode string for each field and storing\n+             * the result in the array.\n+             */\n+            int parser_res;\n+            Py_UCS4 *str = ts.field_buffer + fields[col].offset;\n+            Py_UCS4 *end = ts.field_buffer + fields[col + 1].offset - 1;\n+            if (conv_funcs == NULL || conv_funcs[i] == NULL) {\n+                parser_res = field_types[f].set_from_ucs4(field_types[f].descr,\n+                        str, end, item_ptr, pconfig);\n+            }\n+            else {\n+                parser_res = to_generic_with_converter(field_types[f].descr,\n+                        str, end, item_ptr, pconfig, conv_funcs[i]);\n+            }\n+\n+            if (NPY_UNLIKELY(parser_res < 0)) {\n+                PyObject *exc, *val, *tb;\n+                PyErr_Fetch(&exc, &val, &tb);\n+\n+                size_t length = end - str;\n+                PyObject *string = PyUnicode_FromKindAndData(\n+                        PyUnicode_4BYTE_KIND, str, length);\n+                if (string == NULL) {\n+                    npy_PyErr_ChainExceptions(exc, val, tb);\n+                    goto error;\n+                }\n+                PyErr_Format(PyExc_ValueError,\n+                        \"could not convert string %.100R to %S at \"\n+                        \"row %zu, column %d.\",\n+                        string, field_types[f].descr, row_count, col+1);\n+                Py_DECREF(string);\n+                npy_PyErr_ChainExceptionsCause(exc, val, tb);\n+                goto error;\n+            }\n+        }\n+\n+        ++row_count;\n+        data_ptr += row_size;\n+    }\n+\n+    tokenizer_clear(&ts);\n+    PyMem_FREE(conv_funcs);\n+\n+    if (data_array == NULL) {\n+        assert(row_count == 0 && result_shape[0] == 0);\n+        if (actual_num_fields == -1) {\n+            /*\n+             * We found no rows and have to discover the number of elements\n+             * we have no choice but to guess 1.\n+             * NOTE: It may make sense to move this outside of here to refine\n+             *       the behaviour where necessary.\n+             */\n+            result_shape[1] = 1;\n+        }\n+        else {\n+            result_shape[1] = actual_num_fields;\n+        }\n+        Py_INCREF(out_descr);\n+        data_array = (PyArrayObject *)PyArray_Empty(\n+                ndim, result_shape, out_descr, 0);\n+    }\n+\n+    /*\n+     * Note that if there is no data, `data_array` may still be NULL and\n+     * row_count is 0.  In that case, always realloc just in case.\n+     */\n+    if (data_array_allocated && data_allocated_rows != row_count) {\n+        size_t size = row_count * row_size;\n+        char *new_data = PyDataMem_UserRENEW(\n+                PyArray_BYTES(data_array), size ? size : 1,\n+                PyArray_HANDLER(data_array));\n+        if (new_data == NULL) {\n+            Py_DECREF(data_array);\n+            PyErr_NoMemory();\n+            return NULL;\n+        }\n+        ((PyArrayObject_fields *)data_array)->data = new_data;\n+        ((PyArrayObject_fields *)data_array)->dimensions[0] = row_count;\n+    }\n+\n+    return data_array;\n+\n+  error:\n+    PyMem_FREE(conv_funcs);\n+    tokenizer_clear(&ts);\n+    Py_XDECREF(data_array);\n+    return NULL;\n+}"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/rows.h",
                "patch": "@@ -0,0 +1,22 @@\n+\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_ROWS_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_ROWS_H_\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+#include <stdio.h>\n+\n+#include \"textreading/stream.h\"\n+#include \"textreading/field_types.h\"\n+#include \"textreading/parser_config.h\"\n+\n+\n+NPY_NO_EXPORT PyArrayObject *\n+read_rows(stream *s,\n+        npy_intp nrows, Py_ssize_t num_field_types, field_type *field_types,\n+        parser_config *pconfig, Py_ssize_t num_usecols, Py_ssize_t *usecols,\n+        Py_ssize_t skiplines, PyObject *converters,\n+        PyArrayObject *data_array, PyArray_Descr *out_descr,\n+        bool homogeneous);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_ROWS_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/str_to_int.c",
                "patch": "@@ -0,0 +1,67 @@\n+\n+#include <Python.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"lowlevel_strided_loops.h\"\n+\n+#include <string.h>\n+#include \"textreading/str_to_int.h\"\n+#include \"textreading/parser_config.h\"\n+\n+\n+#define DECLARE_TO_INT(intw, INT_MIN, INT_MAX, byteswap_unaligned)          \\\n+    NPY_NO_EXPORT int                                                       \\\n+    to_##intw(PyArray_Descr *descr,                                         \\\n+            const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,          \\\n+            parser_config *pconfig)                                         \\\n+    {                                                                       \\\n+        int64_t parsed;                                                     \\\n+        intw##_t x;                                                         \\\n+                                                                            \\\n+        if (str_to_int64(str, end, INT_MIN, INT_MAX, &parsed) < 0) {        \\\n+            return -1;                                                      \\\n+        }                                                                   \\\n+        else {                                                              \\\n+            x = (intw##_t)parsed;                                           \\\n+        }                                                                   \\\n+        memcpy(dataptr, &x, sizeof(x));                                     \\\n+        if (!PyArray_ISNBO(descr->byteorder)) {                             \\\n+            byteswap_unaligned(dataptr);                                    \\\n+        }                                                                   \\\n+        return 0;                                                           \\\n+    }\n+\n+#define DECLARE_TO_UINT(uintw, UINT_MAX, byteswap_unaligned)                \\\n+    NPY_NO_EXPORT int                                                       \\\n+    to_##uintw(PyArray_Descr *descr,                                        \\\n+            const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,          \\\n+            parser_config *pconfig)                                         \\\n+    {                                                                       \\\n+        uint64_t parsed;                                                    \\\n+        uintw##_t x;                                                        \\\n+                                                                            \\\n+        if (str_to_uint64(str, end, UINT_MAX, &parsed) < 0) {               \\\n+            return -1;                                                      \\\n+        }                                                                   \\\n+        else {                                                              \\\n+            x = (uintw##_t)parsed;                                          \\\n+        }                                                                   \\\n+        memcpy(dataptr, &x, sizeof(x));                                     \\\n+        if (!PyArray_ISNBO(descr->byteorder)) {                             \\\n+            byteswap_unaligned(dataptr);                                    \\\n+        }                                                                   \\\n+        return 0;                                                           \\\n+    }\n+\n+#define byteswap_nothing(ptr)\n+\n+DECLARE_TO_INT(int8, INT8_MIN, INT8_MAX, byteswap_nothing)\n+DECLARE_TO_INT(int16, INT16_MIN, INT16_MAX, npy_bswap2_unaligned)\n+DECLARE_TO_INT(int32, INT32_MIN, INT32_MAX, npy_bswap4_unaligned)\n+DECLARE_TO_INT(int64, INT64_MIN, INT64_MAX, npy_bswap8_unaligned)\n+\n+DECLARE_TO_UINT(uint8, UINT8_MAX, byteswap_nothing)\n+DECLARE_TO_UINT(uint16, UINT16_MAX, npy_bswap2_unaligned)\n+DECLARE_TO_UINT(uint32, UINT32_MAX, npy_bswap4_unaligned)\n+DECLARE_TO_UINT(uint64, UINT64_MAX, npy_bswap8_unaligned)"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/str_to_int.h",
                "patch": "@@ -0,0 +1,174 @@\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STR_TO_INT_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STR_TO_INT_H_\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/ndarraytypes.h\"\n+\n+#include \"textreading/parser_config.h\"\n+\n+\n+/*\n+ * The following two string conversion functions are largely equivalent\n+ * in Pandas.  They are in the header file here, to ensure they can be easily\n+ * inline in the other function.\n+ * Unlike pandas, pass in end-pointer (do not rely on \\0) and return 0 or -1.\n+ *\n+ * The actual functions are defined using macro templating below.\n+ */\n+NPY_FINLINE int\n+str_to_int64(\n+        const Py_UCS4 *p_item, const Py_UCS4 *p_end,\n+        int64_t int_min, int64_t int_max, int64_t *result)\n+{\n+    const Py_UCS4 *p = (const Py_UCS4 *)p_item;\n+    bool isneg = 0;\n+    int64_t number = 0;\n+\n+    // Skip leading spaces.\n+    while (Py_UNICODE_ISSPACE(*p)) {\n+        ++p;\n+    }\n+\n+    // Handle sign.\n+    if (*p == '-') {\n+        isneg = true;\n+        ++p;\n+    }\n+    else if (*p == '+') {\n+        p++;\n+    }\n+\n+    // Check that there is a first digit.\n+    if (!isdigit(*p)) {\n+        return -1;\n+    }\n+\n+    if (isneg) {\n+        // If number is greater than pre_min, at least one more digit\n+        // can be processed without overflowing.\n+        int dig_pre_min = -(int_min % 10);\n+        int64_t pre_min = int_min / 10;\n+\n+        // Process the digits.\n+        int d = *p;\n+        while (isdigit(d)) {\n+            if ((number > pre_min) || ((number == pre_min) && (d - '0' <= dig_pre_min))) {\n+                number = number * 10 - (d - '0');\n+                d = *++p;\n+            }\n+            else {\n+                return -1;\n+            }\n+        }\n+    }\n+    else {\n+        // If number is less than pre_max, at least one more digit\n+        // can be processed without overflowing.\n+        int64_t pre_max = int_max / 10;\n+        int dig_pre_max = int_max % 10;\n+\n+        // Process the digits.\n+        int d = *p;\n+        while (isdigit(d)) {\n+            if ((number < pre_max) || ((number == pre_max) && (d - '0' <= dig_pre_max))) {\n+                number = number * 10 + (d - '0');\n+                d = *++p;\n+            }\n+            else {\n+                return -1;\n+            }\n+        }\n+    }\n+\n+    // Skip trailing spaces.\n+    while (Py_UNICODE_ISSPACE(*p)) {\n+        ++p;\n+    }\n+\n+    // Did we use up all the characters?\n+    if (p != p_end) {\n+        return -1;\n+    }\n+\n+    *result = number;\n+    return 0;\n+}\n+\n+\n+NPY_FINLINE int\n+str_to_uint64(\n+        const Py_UCS4 *p_item, const Py_UCS4 *p_end,\n+        uint64_t uint_max, uint64_t *result)\n+{\n+    const Py_UCS4 *p = (const Py_UCS4 *)p_item;\n+    uint64_t number = 0;\n+    int d;\n+\n+    // Skip leading spaces.\n+    while (Py_UNICODE_ISSPACE(*p)) {\n+        ++p;\n+    }\n+\n+    // Handle sign.\n+    if (*p == '-') {\n+        return -1;\n+    }\n+    if (*p == '+') {\n+        p++;\n+    }\n+\n+    // Check that there is a first digit.\n+    if (!isdigit(*p)) {\n+        return -1;\n+    }\n+\n+    // If number is less than pre_max, at least one more digit\n+    // can be processed without overflowing.\n+    uint64_t pre_max = uint_max / 10;\n+    int dig_pre_max = uint_max % 10;\n+\n+    // Process the digits.\n+    d = *p;\n+    while (isdigit(d)) {\n+        if ((number < pre_max) || ((number == pre_max) && (d - '0' <= dig_pre_max))) {\n+            number = number * 10 + (d - '0');\n+            d = *++p;\n+        }\n+        else {\n+            return -1;\n+        }\n+    }\n+\n+    // Skip trailing spaces.\n+    while (Py_UNICODE_ISSPACE(*p)) {\n+        ++p;\n+    }\n+\n+    // Did we use up all the characters?\n+    if (p != p_end) {\n+        return -1;\n+    }\n+\n+    *result = number;\n+    return 0;\n+}\n+\n+\n+#define DECLARE_TO_INT_PROTOTYPE(intw)                                  \\\n+    NPY_NO_EXPORT int                                                   \\\n+    to_##intw(PyArray_Descr *descr,                                     \\\n+            const Py_UCS4 *str, const Py_UCS4 *end, char *dataptr,      \\\n+            parser_config *pconfig);\n+\n+DECLARE_TO_INT_PROTOTYPE(int8)\n+DECLARE_TO_INT_PROTOTYPE(int16)\n+DECLARE_TO_INT_PROTOTYPE(int32)\n+DECLARE_TO_INT_PROTOTYPE(int64)\n+\n+DECLARE_TO_INT_PROTOTYPE(uint8)\n+DECLARE_TO_INT_PROTOTYPE(uint16)\n+DECLARE_TO_INT_PROTOTYPE(uint32)\n+DECLARE_TO_INT_PROTOTYPE(uint64)\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STR_TO_INT_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/stream.h",
                "patch": "@@ -0,0 +1,41 @@\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STREAM_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STREAM_H_\n+\n+#include <stdint.h>\n+\n+/*\n+ * When getting the next line, we hope that the buffer provider can already\n+ * give some information about the newlines, because for Python iterables\n+ * we definitely expect to get line-by-line buffers.\n+ *\n+ * BUFFER_IS_FILEEND must be returned when the end of the file is reached and\n+ * must NOT be returned together with a valid (non-empty) buffer.\n+ */\n+#define BUFFER_MAY_CONTAIN_NEWLINE 0\n+#define BUFFER_IS_LINEND 1\n+#define BUFFER_IS_FILEEND 2\n+\n+/*\n+ * Base struct for streams.  We currently have two, a chunked reader for\n+ * filelikes and a line-by-line for any iterable.\n+ * As of writing, the chunked reader was only used for filelikes not already\n+ * opened.  That is to preserve the amount read in case of an error exactly.\n+ * If we drop this, we could read it more often (but not when `max_rows` is\n+ * used).\n+ *\n+ * The \"streams\" can extend this struct to store their own data (so it is\n+ * a very lightweight \"object\").\n+ */\n+typedef struct _stream {\n+    int (*stream_nextbuf)(void *sdata, char **start, char **end, int *kind);\n+    // Note that the first argument to stream_close is the stream pointer\n+    // itself, not the stream_data pointer.\n+    int (*stream_close)(struct _stream *strm);\n+} stream;\n+\n+\n+#define stream_nextbuf(s, start, end, kind)  \\\n+        ((s)->stream_nextbuf((s), start, end, kind))\n+#define stream_close(s)    ((s)->stream_close((s)))\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STREAM_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/stream_pyobject.c",
                "patch": "@@ -0,0 +1,239 @@\n+/*\n+ * C side structures to provide capabilities to read Python file like objects\n+ * in chunks, or iterate through iterables with each result representing a\n+ * single line of a file.\n+ */\n+\n+#include <stdio.h>\n+#include <stdlib.h>\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/arrayobject.h\"\n+\n+#include \"textreading/stream.h\"\n+\n+#define READ_CHUNKSIZE 1 << 14\n+\n+\n+typedef struct {\n+    stream stream;\n+    /* The Python file object being read. */\n+    PyObject *file;\n+\n+    /* The `read` attribute of the file object. */\n+    PyObject *read;\n+    /* Amount to read each time we call `obj.read()` */\n+    PyObject *chunksize;\n+\n+    /* Python str object holding the line most recently read from the file. */\n+    PyObject *chunk;\n+\n+    /* Encoding compatible with Python's `PyUnicode_Encode` (may be NULL) */\n+    const char *encoding;\n+} python_chunks_from_file;\n+\n+\n+/*\n+ * Helper function to support byte objects as well as unicode strings.\n+ *\n+ * NOTE: Steals a reference to `str` (although usually returns it unmodified).\n+ */\n+static NPY_INLINE PyObject *\n+process_stringlike(PyObject *str, const char *encoding)\n+{\n+    if (PyBytes_Check(str)) {\n+        PyObject *ustr;\n+        ustr = PyUnicode_FromEncodedObject(str, encoding, NULL);\n+        if (ustr == NULL) {\n+            return NULL;\n+        }\n+        Py_DECREF(str);\n+        return ustr;\n+    }\n+    else if (!PyUnicode_Check(str)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"non-string returned while reading data\");\n+        Py_DECREF(str);\n+        return NULL;\n+    }\n+    return str;\n+}\n+\n+\n+static NPY_INLINE void\n+buffer_info_from_unicode(PyObject *str, char **start, char **end, int *kind)\n+{\n+    Py_ssize_t length = PyUnicode_GET_LENGTH(str);\n+    *kind = PyUnicode_KIND(str);\n+\n+    if (*kind == PyUnicode_1BYTE_KIND) {\n+        *start = (char *)PyUnicode_1BYTE_DATA(str);\n+    }\n+    else if (*kind == PyUnicode_2BYTE_KIND) {\n+        *start = (char *)PyUnicode_2BYTE_DATA(str);\n+        length *= sizeof(Py_UCS2);\n+    }\n+    else if (*kind == PyUnicode_4BYTE_KIND) {\n+        *start = (char *)PyUnicode_4BYTE_DATA(str);\n+        length *= sizeof(Py_UCS4);\n+    }\n+    *end = *start + length;\n+}\n+\n+\n+static int\n+fb_nextbuf(python_chunks_from_file *fb, char **start, char **end, int *kind)\n+{\n+    Py_XDECREF(fb->chunk);\n+    fb->chunk = NULL;\n+\n+    PyObject *chunk = PyObject_CallFunctionObjArgs(fb->read, fb->chunksize, NULL);\n+    if (chunk == NULL) {\n+        return -1;\n+    }\n+    fb->chunk = process_stringlike(chunk, fb->encoding);\n+    if (fb->chunk == NULL) {\n+        return -1;\n+    }\n+    buffer_info_from_unicode(fb->chunk, start, end, kind);\n+    if (*start == *end) {\n+        return BUFFER_IS_FILEEND;\n+    }\n+    return BUFFER_MAY_CONTAIN_NEWLINE;\n+}\n+\n+\n+static int\n+fb_del(stream *strm)\n+{\n+    python_chunks_from_file *fb = (python_chunks_from_file *)strm;\n+\n+    Py_XDECREF(fb->file);\n+    Py_XDECREF(fb->read);\n+    Py_XDECREF(fb->chunksize);\n+    Py_XDECREF(fb->chunk);\n+\n+    PyMem_FREE(strm);\n+\n+    return 0;\n+}\n+\n+\n+NPY_NO_EXPORT stream *\n+stream_python_file(PyObject *obj, const char *encoding)\n+{\n+    python_chunks_from_file *fb;\n+\n+    fb = (python_chunks_from_file *)PyMem_Calloc(1, sizeof(python_chunks_from_file));\n+    if (fb == NULL) {\n+        PyErr_NoMemory();\n+        return NULL;\n+    }\n+\n+    fb->stream.stream_nextbuf = (void *)&fb_nextbuf;\n+    fb->stream.stream_close = &fb_del;\n+\n+    fb->encoding = encoding;\n+    Py_INCREF(obj);\n+    fb->file = obj;\n+\n+    fb->read = PyObject_GetAttrString(obj, \"read\");\n+    if (fb->read == NULL) {\n+        goto fail;\n+    }\n+    fb->chunksize = PyLong_FromLong(READ_CHUNKSIZE);\n+    if (fb->chunksize == NULL) {\n+        goto fail;\n+    }\n+\n+    return (stream *)fb;\n+\n+fail:\n+    fb_del((stream *)fb);\n+    return NULL;\n+}\n+\n+\n+/*\n+ * Stream from a Python iterable by interpreting each item as a line in a file\n+ */\n+typedef struct {\n+    stream stream;\n+    /* The Python file object being read. */\n+    PyObject *iterator;\n+\n+    /* Python str object holding the line most recently fetched */\n+    PyObject *line;\n+\n+    /* Encoding compatible with Python's `PyUnicode_Encode` (may be NULL) */\n+    const char *encoding;\n+} python_lines_from_iterator;\n+\n+\n+static int\n+it_del(stream *strm)\n+{\n+    python_lines_from_iterator *it = (python_lines_from_iterator *)strm;\n+\n+    Py_XDECREF(it->iterator);\n+    Py_XDECREF(it->line);\n+\n+    PyMem_FREE(strm);\n+    return 0;\n+}\n+\n+\n+static int\n+it_nextbuf(python_lines_from_iterator *it, char **start, char **end, int *kind)\n+{\n+    Py_XDECREF(it->line);\n+    it->line = NULL;\n+\n+    PyObject *line = PyIter_Next(it->iterator);\n+    if (line == NULL) {\n+        if (PyErr_Occurred()) {\n+            return -1;\n+        }\n+        *start = NULL;\n+        *end = NULL;\n+        return BUFFER_IS_FILEEND;\n+    }\n+    it->line = process_stringlike(line, it->encoding);\n+    if (it->line == NULL) {\n+        return -1;\n+    }\n+\n+    buffer_info_from_unicode(it->line, start, end, kind);\n+    return BUFFER_IS_LINEND;\n+}\n+\n+\n+NPY_NO_EXPORT stream *\n+stream_python_iterable(PyObject *obj, const char *encoding)\n+{\n+    python_lines_from_iterator *it;\n+\n+    if (!PyIter_Check(obj)) {\n+        PyErr_SetString(PyExc_TypeError,\n+                \"error reading from object, expected an iterable.\");\n+        return NULL;\n+    }\n+\n+    it = (python_lines_from_iterator *)PyMem_Calloc(1, sizeof(*it));\n+    if (it == NULL) {\n+        PyErr_NoMemory();\n+        return NULL;\n+    }\n+\n+    it->stream.stream_nextbuf = (void *)&it_nextbuf;\n+    it->stream.stream_close = &it_del;\n+\n+    it->encoding = encoding;\n+    Py_INCREF(obj);\n+    it->iterator = obj;\n+\n+    return (stream *)it;\n+}"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/stream_pyobject.h",
                "patch": "@@ -0,0 +1,16 @@\n+\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STREAM_PYOBJECT_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STREAM_PYOBJECT_H_\n+\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+#include \"textreading/stream.h\"\n+\n+NPY_NO_EXPORT stream *\n+stream_python_file(PyObject *obj, const char *encoding);\n+\n+NPY_NO_EXPORT stream *\n+stream_python_iterable(PyObject *obj, const char *encoding);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_STREAM_PYOBJECT_H_ */"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/tokenize.c.src",
                "patch": "@@ -0,0 +1,457 @@\n+\n+#include <Python.h>\n+\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <stdbool.h>\n+#include <string.h>\n+\n+#define NPY_NO_DEPRECATED_API NPY_API_VERSION\n+#define _MULTIARRAYMODULE\n+#include \"numpy/ndarraytypes.h\"\n+\n+#include \"textreading/stream.h\"\n+#include \"textreading/tokenize.h\"\n+#include \"textreading/parser_config.h\"\n+#include \"textreading/growth.h\"\n+\n+\n+/*\n+    How parsing quoted fields works:\n+\n+    For quoting to be activated, the first character of the field\n+    must be the quote character (after taking into account\n+    ignore_leading_spaces).  While quoting is active, delimiters\n+    are treated as regular characters, not delimiters.  Quoting is\n+    deactivated by the second occurrence of the quote character.  An\n+    exception is the occurrence of two consecutive quote characters,\n+    which is treated as a literal occurrence of a single quote character.\n+    E.g. (with delimiter=',' and quote='\"'):\n+        12.3,\"New York, NY\",\"3'2\"\"\"\n+    The second and third fields are `New York, NY` and `3'2\"`.\n+\n+    If a non-delimiter occurs after the closing quote, the quote is\n+    ignored and parsing continues with quoting deactivated.  Quotes\n+    that occur while quoting is not activated are not handled specially;\n+    they become part of the data.\n+    E.g:\n+        12.3,\"ABC\"DEF,XY\"Z\n+    The second and third fields are `ABCDEF` and `XY\"Z`.\n+\n+    Note that the second field of\n+        12.3,\"ABC\"   ,4.5\n+    is `ABC   `.  Currently there is no option to ignore whitespace\n+    at the end of a field.\n+*/\n+\n+\n+/**begin repeat\n+ * #type = Py_UCS1, Py_UCS2, Py_UCS4#\n+ */\n+static NPY_INLINE int\n+copy_to_field_buffer_@type@(tokenizer_state *ts,\n+        const @type@ *chunk_start, const @type@ *chunk_end)\n+{\n+    npy_intp chunk_length = chunk_end - chunk_start;\n+    npy_intp size = chunk_length + ts->field_buffer_pos + 2;\n+\n+    if (NPY_UNLIKELY(ts->field_buffer_length < size)) {\n+        npy_intp alloc_size = grow_size_and_multiply(&size, 32, sizeof(Py_UCS4));\n+        if (alloc_size < 0) {\n+            PyErr_Format(PyExc_ValueError,\n+                    \"line too long to handle while reading file.\");\n+            return -1;\n+        }\n+        Py_UCS4 *grown = PyMem_Realloc(ts->field_buffer, alloc_size);\n+        if (grown == NULL) {\n+            PyErr_NoMemory();\n+            return -1;\n+        }\n+        ts->field_buffer_length = size;\n+        ts->field_buffer = grown;\n+    }\n+\n+    Py_UCS4 *write_pos = ts->field_buffer + ts->field_buffer_pos;\n+    for (; chunk_start < chunk_end; chunk_start++, write_pos++) {\n+        *write_pos = (Py_UCS4)*chunk_start;\n+    }\n+    *write_pos = '\\0';  /* always ensure we end with NUL */\n+    ts->field_buffer_pos += chunk_length;\n+    return 0;\n+}\n+/**end repeat**/\n+\n+\n+static NPY_INLINE int\n+add_field(tokenizer_state *ts)\n+{\n+    /* The previous field is done, advance to keep a NUL byte at the end */\n+    ts->field_buffer_pos += 1;\n+\n+    if (NPY_UNLIKELY(ts->num_fields + 1 > ts->fields_size)) {\n+        npy_intp size = ts->num_fields;\n+\n+        npy_intp alloc_size = grow_size_and_multiply(\n+                &size, 4, sizeof(field_info));\n+        if (alloc_size < 0) {\n+            /* Check for a size overflow, path should be almost impossible. */\n+            PyErr_Format(PyExc_ValueError,\n+                    \"too many columns found; cannot read file.\");\n+            return -1;\n+        }\n+        field_info *fields = PyMem_Realloc(ts->fields, alloc_size);\n+        if (fields == NULL) {\n+            PyErr_NoMemory();\n+            return -1;\n+        }\n+        ts->fields = fields;\n+        ts->fields_size = size;\n+    }\n+\n+    ts->fields[ts->num_fields].offset = ts->field_buffer_pos;\n+    ts->fields[ts->num_fields].quoted = false;\n+    ts->num_fields += 1;\n+    /* Ensure this (currently empty) word is NUL terminated. */\n+    ts->field_buffer[ts->field_buffer_pos] = '\\0';\n+    return 0;\n+}\n+\n+\n+/**begin repeat\n+ * #kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND, PyUnicode_4BYTE_KIND#\n+ * #type = Py_UCS1, Py_UCS2, Py_UCS4#\n+ */\n+static NPY_INLINE int\n+tokenizer_core_@type@(tokenizer_state *ts, parser_config *const config)\n+{\n+    @type@ *pos = (@type@ *)ts->pos;\n+    @type@ *stop = (@type@ *)ts->end;\n+    @type@ *chunk_start;\n+\n+    if (ts->state == TOKENIZE_CHECK_QUOTED) {\n+        /* before we can check for quotes, strip leading whitespace */\n+        if (config->ignore_leading_whitespace) {\n+            while (pos < stop && Py_UNICODE_ISSPACE(*pos) &&\n+                        *pos != '\\r' && *pos != '\\n') {\n+                pos++;\n+            }\n+            if (pos == stop) {\n+                ts->pos = (char *)pos;\n+                return 0;\n+            }\n+        }\n+\n+        /* Setting chunk effectively starts the field */\n+        if (*pos == config->quote) {\n+            ts->fields[ts->num_fields - 1].quoted = true;\n+            ts->state = TOKENIZE_QUOTED;\n+            pos++;  /* TOKENIZE_QUOTED is OK with pos == stop */\n+        }\n+        else {\n+            /* Set to TOKENIZE_QUOTED or TOKENIZE_QUOTED_WHITESPACE */\n+            ts->state = ts->unquoted_state;\n+        }\n+    }\n+\n+    switch (ts->state) {\n+        case TOKENIZE_UNQUOTED:\n+            chunk_start = pos;\n+            for (; pos < stop; pos++) {\n+                if (*pos == '\\r') {\n+                    ts->state = TOKENIZE_EAT_CRLF;\n+                    break;\n+                }\n+                else if (*pos == '\\n') {\n+                    ts->state = TOKENIZE_LINE_END;\n+                    break;\n+                }\n+                else if (*pos == config->delimiter) {\n+                    ts->state = TOKENIZE_INIT;\n+                    break;\n+                }\n+                else if (*pos == config->comment) {\n+                    ts->state = TOKENIZE_GOTO_LINE_END;\n+                    break;\n+                }\n+            }\n+            if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {\n+                return -1;\n+            }\n+            pos++;\n+            break;\n+\n+        case TOKENIZE_UNQUOTED_WHITESPACE:\n+            /* Note, this branch is largely identical to `TOKENIZE_UNQUOTED` */\n+            chunk_start = pos;\n+            for (; pos < stop; pos++) {\n+                if (*pos == '\\r') {\n+                    ts->state = TOKENIZE_EAT_CRLF;\n+                    break;\n+                }\n+                else if (*pos == '\\n') {\n+                    ts->state = TOKENIZE_LINE_END;\n+                    break;\n+                }\n+                else if (Py_UNICODE_ISSPACE(*pos)) {\n+                    ts->state = TOKENIZE_INIT;\n+                    break;\n+                }\n+                else if (*pos == config->comment) {\n+                    ts->state = TOKENIZE_GOTO_LINE_END;\n+                    break;\n+                }\n+            }\n+            if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {\n+                return -1;\n+            }\n+            pos++;\n+            break;\n+\n+        case TOKENIZE_QUOTED:\n+            chunk_start = pos;\n+            for (; pos < stop; pos++) {\n+                if (*pos == config->quote) {\n+                    ts->state = TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE;\n+                    break;\n+                }\n+            }\n+            if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {\n+                return -1;\n+            }\n+            pos++;\n+            break;\n+\n+        case TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE:\n+            if (*pos == config->quote) {\n+                /* Copy the quote character directly from the config: */\n+                if (copy_to_field_buffer_Py_UCS4(ts,\n+                        &config->quote, &config->quote+1) < 0) {\n+                    return -1;\n+                }\n+                ts->state = TOKENIZE_QUOTED;\n+                pos++;\n+            }\n+            else {\n+                /* continue parsing as if unquoted */\n+                ts->state = TOKENIZE_UNQUOTED;\n+            }\n+            break;\n+\n+        case TOKENIZE_GOTO_LINE_END:\n+            if (ts->buf_state != BUFFER_MAY_CONTAIN_NEWLINE) {\n+                pos = stop;  /* advance to next buffer */\n+                ts->state = TOKENIZE_LINE_END;\n+                break;\n+            }\n+            for (; pos < stop; pos++) {\n+                if (*pos == '\\r') {\n+                    ts->state = TOKENIZE_EAT_CRLF;\n+                    break;\n+                }\n+                else if (*pos == '\\n') {\n+                    ts->state = TOKENIZE_LINE_END;\n+                    break;\n+                }\n+            }\n+            pos++;\n+            break;\n+\n+        case TOKENIZE_EAT_CRLF:\n+            /* \"Universal newline\" support: remove \\n in \\r\\n. */\n+            if (*pos == '\\n') {\n+                pos++;\n+            }\n+            ts->state = TOKENIZE_LINE_END;\n+            break;\n+\n+        default:\n+            assert(0);\n+    }\n+\n+    ts->pos = (char *)pos;\n+    return 0;\n+}\n+/**end repeat**/\n+\n+\n+/*\n+ * This tokenizer always copies the full \"row\" (all tokens).  This makes\n+ * two things easier:\n+ * 1. It means that every word is guaranteed to be followed by a NUL character\n+ *    (although it can include one as well).\n+ * 2. If usecols are used we can sniff the first row easier by parsing it\n+ *    fully.  Further, usecols can be negative so we may not know which row we\n+ *    need up-front.\n+ *\n+ * The tokenizer could grow the ability to skip fields and check the\n+ * maximum number of fields when known, it is unclear that this is worthwhile.\n+ *\n+ * Unlike some tokenizers, this one tries to work in chunks and copies\n+ * data in chunks as well.  The hope is that this makes multiple light-weight\n+ * loops rather than a single heavy one, to allow e.g. quickly scanning for the\n+ * end of a field.  Copying chunks also means we usually only check once per\n+ * field whether the buffer is large enough.\n+ * Different choices are possible, this one seems to work well, though.\n+ *\n+ * The core (main part) of the tokenizer is specialized for the three Python\n+ * unicode flavors UCS1, UCS2, and UCS4 as a worthwhile optimization.\n+ */\n+NPY_NO_EXPORT int\n+tokenize(stream *s, tokenizer_state *ts, parser_config *const config)\n+{\n+    assert(ts->fields_size >= 2);\n+    assert(ts->field_buffer_length >= 2*sizeof(Py_UCS4));\n+\n+    int finished_reading_file = 0;\n+\n+    /* Reset to start of buffer */\n+    ts->field_buffer_pos = 0;\n+    ts->num_fields = 0;\n+\n+    while (1) {\n+        /*\n+         * This loop adds new fields to the result (to make up a full row)\n+         * until the row ends (typically a line end or the file end)\n+         */\n+        if (ts->state == TOKENIZE_INIT) {\n+            /* Start a new field */\n+            if (add_field(ts) < 0) {\n+                return -1;\n+            }\n+            ts->state = TOKENIZE_CHECK_QUOTED;\n+        }\n+\n+        if (NPY_UNLIKELY(ts->pos >= ts->end)) {\n+            if (ts->buf_state == BUFFER_IS_LINEND &&\n+                    ts->state != TOKENIZE_QUOTED) {\n+                /*\n+                 * Finished line, do not read anymore (also do not eat \\n).\n+                 * If we are in a quoted field and the \"line\" does not end with\n+                 * a newline, the quoted field will not have it either.\n+                 * I.e. `np.loadtxt(['\"a', 'b\"'], dtype=\"S2\", quotechar='\"')`\n+                 * reads \"ab\". This matches `next(csv.reader(['\"a', 'b\"']))`.\n+                 */\n+                break;\n+            }\n+            /* fetch new data */\n+            ts->buf_state = stream_nextbuf(s,\n+                    &ts->pos, &ts->end, &ts->unicode_kind);\n+            if (ts->buf_state < 0) {\n+                return -1;\n+            }\n+            if (ts->buf_state == BUFFER_IS_FILEEND) {\n+                finished_reading_file = 1;\n+                ts->pos = ts->end;  /* stream should ensure this. */\n+                break;\n+            }\n+            else if (ts->pos == ts->end) {\n+                /* This must be an empty line (and it must be indicated!). */\n+                assert(ts->buf_state == BUFFER_IS_LINEND);\n+                break;\n+            }\n+        }\n+        int status;\n+        if (ts->unicode_kind == PyUnicode_1BYTE_KIND) {\n+            status = tokenizer_core_Py_UCS1(ts, config);\n+        }\n+        else if (ts->unicode_kind == PyUnicode_2BYTE_KIND) {\n+            status = tokenizer_core_Py_UCS2(ts, config);\n+        }\n+        else {\n+            assert(ts->unicode_kind == PyUnicode_4BYTE_KIND);\n+            status = tokenizer_core_Py_UCS4(ts, config);\n+        }\n+        if (status < 0) {\n+            return -1;\n+        }\n+\n+        if (ts->state == TOKENIZE_LINE_END) {\n+            break;\n+        }\n+    }\n+\n+    /*\n+     * We have finished tokenizing a full row into fields, finalize result\n+     */\n+    if (ts->buf_state == BUFFER_IS_LINEND) {\n+        /* This line is \"finished\", make sure we don't touch it again: */\n+        ts->buf_state = BUFFER_MAY_CONTAIN_NEWLINE;\n+        if (NPY_UNLIKELY(ts->pos < ts->end)) {\n+            PyErr_SetString(PyExc_ValueError,\n+                    \"Found an unquoted embedded newline within a single line of \"\n+                    \"input.  This is currently not supported.\");\n+            return -1;\n+        }\n+    }\n+\n+    /* Finish the last field (we \"append\" one to store the last ones length) */\n+    if (add_field(ts) < 0) {\n+        return -1;\n+    }\n+    ts->num_fields -= 1;\n+\n+    /*\n+     * If have one field, but that field is completely empty, this is an\n+     * empty line, and we just ignore it.\n+     */\n+    if (ts->num_fields == 1\n+             && ts->fields[1].offset - ts->fields[0].offset == 1\n+             && !ts->fields->quoted) {\n+        ts->num_fields--;\n+    }\n+    ts->state = TOKENIZE_INIT;\n+    return finished_reading_file;\n+}\n+\n+\n+NPY_NO_EXPORT void\n+tokenizer_clear(tokenizer_state *ts)\n+{\n+    PyMem_FREE(ts->field_buffer);\n+    ts->field_buffer = NULL;\n+    ts->field_buffer_length = 0;\n+\n+    PyMem_FREE(ts->fields);\n+    ts->fields = NULL;\n+    ts->fields_size = 0;\n+}\n+\n+\n+/*\n+ * Initialize the tokenizer.  We may want to copy all important config\n+ * variables into the tokenizer.  This would improve the cache locality during\n+ * tokenizing.\n+ */\n+NPY_NO_EXPORT int\n+tokenizer_init(tokenizer_state *ts, parser_config *config)\n+{\n+    /* State and buf_state could be moved into tokenize if we go by row */\n+    ts->buf_state = BUFFER_MAY_CONTAIN_NEWLINE;\n+    ts->state = TOKENIZE_INIT;\n+    if (config->delimiter_is_whitespace) {\n+        ts->unquoted_state = TOKENIZE_UNQUOTED_WHITESPACE;\n+    }\n+    else {\n+        ts->unquoted_state = TOKENIZE_UNQUOTED;\n+    }\n+    ts->num_fields = 0;\n+\n+    ts->buf_state = 0;\n+    ts->pos = NULL;\n+    ts->end = NULL;\n+\n+    ts->field_buffer = PyMem_Malloc(32 * sizeof(Py_UCS4));\n+    if (ts->field_buffer == NULL) {\n+        PyErr_NoMemory();\n+        return -1;\n+    }\n+    ts->field_buffer_length = 32;\n+\n+    ts->fields = PyMem_Malloc(4 * sizeof(*ts->fields));\n+    if (ts->fields == NULL) {\n+        PyErr_NoMemory();\n+        return -1;\n+    }\n+    ts->fields_size = 4;\n+    return 0;\n+}"
            },
            {
                "filename": "numpy/core/src/multiarray/textreading/tokenize.h",
                "patch": "@@ -0,0 +1,78 @@\n+\n+#ifndef NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_TOKENIZE_H_\n+#define NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_TOKENIZE_H_\n+\n+#include <Python.h>\n+#include \"numpy/ndarraytypes.h\"\n+\n+#include \"textreading/stream.h\"\n+#include \"textreading/parser_config.h\"\n+\n+\n+typedef enum {\n+    /* Initialization of fields */\n+    TOKENIZE_INIT,\n+    TOKENIZE_CHECK_QUOTED,\n+    /* Main field parsing states */\n+    TOKENIZE_UNQUOTED,\n+    TOKENIZE_UNQUOTED_WHITESPACE,\n+    TOKENIZE_QUOTED,\n+    /* Handling of two character control sequences (except \"\\r\\n\") */\n+    TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE,\n+    /* Line end handling */\n+    TOKENIZE_LINE_END,\n+    TOKENIZE_EAT_CRLF,  /* \"\\r\\n\" support (carriage return, line feed) */\n+    TOKENIZE_GOTO_LINE_END,\n+} tokenizer_parsing_state;\n+\n+\n+typedef struct {\n+    size_t offset;\n+    bool quoted;\n+} field_info;\n+\n+\n+typedef struct {\n+    tokenizer_parsing_state state;\n+    /* Either TOKENIZE_UNQUOTED or TOKENIZE_UNQUOTED_WHITESPACE: */\n+    tokenizer_parsing_state unquoted_state;\n+    int unicode_kind;\n+    int buf_state;\n+    /* the buffer we are currently working on */\n+    char *pos;\n+    char *end;\n+    /*\n+     * Space to copy words into.  The buffer must always be at least two NUL\n+     * entries longer (8 bytes) than the actual word (including initially).\n+     * The first byte beyond the current word is always NUL'ed on write, the\n+     * second byte is there to allow easy appending of an additional empty\n+     * word at the end (this word is also NUL terminated).\n+     */\n+    npy_intp field_buffer_length;\n+    npy_intp field_buffer_pos;\n+    Py_UCS4 *field_buffer;\n+\n+    /*\n+     * Fields, including information about the field being quoted.  This\n+     * always includes one \"additional\" empty field.  The length of a field\n+     * is equal to `fields[i+1].offset - fields[i].offset - 1`.\n+     *\n+     * The tokenizer assumes at least one field is allocated.\n+     */\n+    npy_intp num_fields;\n+    npy_intp fields_size;\n+    field_info *fields;\n+} tokenizer_state;\n+\n+\n+NPY_NO_EXPORT void\n+tokenizer_clear(tokenizer_state *ts);\n+\n+\n+NPY_NO_EXPORT int\n+tokenizer_init(tokenizer_state *ts, parser_config *config);\n+\n+NPY_NO_EXPORT int\n+tokenize(stream *s, tokenizer_state *ts, parser_config *const config);\n+\n+#endif  /* NUMPY_CORE_SRC_MULTIARRAY_TEXTREADING_TOKENIZE_H_ */"
            },
            {
                "filename": "numpy/lib/npyio.py",
                "patch": "@@ -5,6 +5,7 @@\n import warnings\n import weakref\n import contextlib\n+import operator\n from operator import itemgetter, index as opindex, methodcaller\n from collections.abc import Mapping\n \n@@ -13,6 +14,7 @@\n from ._datasource import DataSource\n from numpy.core import overrides\n from numpy.core.multiarray import packbits, unpackbits\n+from numpy.core._multiarray_umath import _load_from_filelike\n from numpy.core.overrides import set_array_function_like_doc, set_module\n from ._iotools import (\n     LineSplitter, NameValidator, StringConverter, ConverterError,\n@@ -721,101 +723,6 @@ def _savez(file, args, kwds, compress, allow_pickle=True, pickle_kwargs=None):\n     zipf.close()\n \n \n-def _floatconv(x):\n-    try:\n-        return float(x)  # The fastest path.\n-    except ValueError:\n-        if '0x' in x:  # Don't accidentally convert \"a\" (\"0xa\") to 10.\n-            try:\n-                return float.fromhex(x)\n-            except ValueError:\n-                pass\n-        raise  # Raise the original exception, which makes more sense.\n-\n-\n-_CONVERTERS = [  # These converters only ever get strs (not bytes) as input.\n-    (np.bool_, lambda x: bool(int(x))),\n-    (np.uint64, np.uint64),\n-    (np.int64, np.int64),\n-    (np.integer, lambda x: int(float(x))),\n-    (np.longdouble, np.longdouble),\n-    (np.floating, _floatconv),\n-    (complex, lambda x: complex(x.replace('+-', '-'))),\n-    (np.bytes_, methodcaller('encode', 'latin-1')),\n-    (np.unicode_, str),\n-]\n-\n-\n-def _getconv(dtype):\n-    \"\"\"\n-    Find the correct dtype converter. Adapted from matplotlib.\n-\n-    Even when a lambda is returned, it is defined at the toplevel, to allow\n-    testing for equality and enabling optimization for single-type data.\n-    \"\"\"\n-    for base, conv in _CONVERTERS:\n-        if issubclass(dtype.type, base):\n-            return conv\n-    return str\n-\n-\n-# _loadtxt_flatten_dtype_internal and _loadtxt_pack_items are loadtxt helpers\n-# lifted to the toplevel because recursive inner functions cause either\n-# GC-dependent reference loops (because they are closures over loadtxt's\n-# internal variables) or large overheads if using a manual trampoline to hide\n-# the recursive calls.\n-\n-\n-# not to be confused with the flatten_dtype we import...\n-def _loadtxt_flatten_dtype_internal(dt):\n-    \"\"\"Unpack a structured data-type, and produce a packer function.\"\"\"\n-    if dt.names is None:\n-        # If the dtype is flattened, return.\n-        # If the dtype has a shape, the dtype occurs\n-        # in the list more than once.\n-        shape = dt.shape\n-        if len(shape) == 0:\n-            return ([dt.base], None)\n-        else:\n-            packing = [(shape[-1], list)]\n-            if len(shape) > 1:\n-                for dim in dt.shape[-2::-1]:\n-                    packing = [(dim*packing[0][0], packing*dim)]\n-            return ([dt.base] * int(np.prod(dt.shape)),\n-                    functools.partial(_loadtxt_pack_items, packing))\n-    else:\n-        types = []\n-        packing = []\n-        for field in dt.names:\n-            tp, bytes = dt.fields[field]\n-            flat_dt, flat_packer = _loadtxt_flatten_dtype_internal(tp)\n-            types.extend(flat_dt)\n-            flat_packing = flat_packer.args[0] if flat_packer else None\n-            # Avoid extra nesting for subarrays\n-            if tp.ndim > 0:\n-                packing.extend(flat_packing)\n-            else:\n-                packing.append((len(flat_dt), flat_packing))\n-        return (types, functools.partial(_loadtxt_pack_items, packing))\n-\n-\n-def _loadtxt_pack_items(packing, items):\n-    \"\"\"Pack items into nested lists based on re-packing info.\"\"\"\n-    if packing is None:\n-        return items[0]\n-    elif packing is tuple:\n-        return tuple(items)\n-    elif packing is list:\n-        return list(items)\n-    else:\n-        start = 0\n-        ret = []\n-        for length, subpacking in packing:\n-            ret.append(\n-                _loadtxt_pack_items(subpacking, items[start:start+length]))\n-            start += length\n-        return tuple(ret)\n-\n def _ensure_ndmin_ndarray_check_param(ndmin):\n     \"\"\"Just checks if the param ndmin is supported on\n         _ensure_ndmin_ndarray. Is intented to be used as\n@@ -853,17 +760,330 @@ def _ensure_ndmin_ndarray(a, *, ndmin: int):\n _loadtxt_chunksize = 50000\n \n \n-def _loadtxt_dispatcher(fname, dtype=None, comments=None, delimiter=None,\n-                        converters=None, skiprows=None, usecols=None, unpack=None,\n-                        ndmin=None, encoding=None, max_rows=None, *, like=None):\n+def _loadtxt_dispatcher(\n+        fname, dtype=None, comments=None, delimiter=None,\n+        converters=None, skiprows=None, usecols=None, unpack=None,\n+        ndmin=None, encoding=None, max_rows=None, *, like=None):\n     return (like,)\n \n \n+def _check_nonneg_int(value, name=\"argument\"):\n+    try:\n+        operator.index(value)\n+    except TypeError:\n+        raise TypeError(f\"{name} must be an integer\") from None\n+    if value < 0:\n+        raise ValueError(f\"{name} must be nonnegative\")\n+\n+\n+def _preprocess_comments(iterable, comments, encoding):\n+    \"\"\"\n+    Generator that consumes a line iterated iterable and strips out the\n+    multiple (or multi-character) comments from lines.\n+    This is a pre-processing step to achieve feature parity with loadtxt\n+    (we assume that this feature is a nieche feature).\n+    \"\"\"\n+    for line in iterable:\n+        if isinstance(line, bytes):\n+            # Need to handle conversion here, or the splitting would fail\n+            line = line.decode(encoding)\n+\n+        for c in comments:\n+            line = line.split(c, 1)[0]\n+\n+        yield line\n+\n+\n+# The number of rows we read in one go if confronted with a parametric dtype\n+_loadtxt_chunksize = 50000\n+\n+\n+def _read(fname, *, delimiter=',', comment='#', quote='\"',\n+          imaginary_unit='j', usecols=None, skiplines=0,\n+          max_rows=None, converters=None, ndmin=None, unpack=False,\n+          dtype=np.float64, encoding=\"bytes\"):\n+    r\"\"\"\n+    Read a NumPy array from a text file.\n+\n+    Parameters\n+    ----------\n+    fname : str or file object\n+        The filename or the file to be read.\n+    delimiter : str, optional\n+        Field delimiter of the fields in line of the file.\n+        Default is a comma, ','.  If None any sequence of whitespace is\n+        considered a delimiter.\n+    comment : str or sequence of str or None, optional\n+        Character that begins a comment.  All text from the comment\n+        character to the end of the line is ignored.\n+        Multiple comments or multiple-character comment strings are supported,\n+        but may be slower and `quote` must be empty if used.\n+        Use None to disable all use of comments.\n+    quote : str or None, optional\n+        Character that is used to quote string fields. Default is '\"'\n+        (a double quote). Use None to disable quote support.\n+    imaginary_unit : str, optional\n+        Character that represent the imaginay unit `sqrt(-1)`.\n+        Default is 'j'.\n+    usecols : array_like, optional\n+        A one-dimensional array of integer column numbers.  These are the\n+        columns from the file to be included in the array.  If this value\n+        is not given, all the columns are used.\n+    skiplines : int, optional\n+        Number of lines to skip before interpreting the data in the file.\n+    max_rows : int, optional\n+        Maximum number of rows of data to read.  Default is to read the\n+        entire file.\n+    converters : dict or callable, optional\n+        A function to parse all columns strings into the desired value, or\n+        a dictionary mapping column number to a parser function.\n+        E.g. if column 0 is a date string: ``converters = {0: datestr2num}``.\n+        Converters can also be used to provide a default value for missing\n+        data, e.g. ``converters = lambda s: float(s.strip() or 0)`` will\n+        convert empty fields to 0.\n+        Default: None\n+    ndmin : int, optional\n+        Minimum dimension of the array returned.\n+        Allowed values are 0, 1 or 2.  Default is 0.\n+    unpack : bool, optional\n+        If True, the returned array is transposed, so that arguments may be\n+        unpacked using ``x, y, z = read(...)``.  When used with a structured\n+        data-type, arrays are returned for each field.  Default is False.\n+    dtype : numpy data type\n+        A NumPy dtype instance, can be a structured dtype to map to the\n+        columns of the file.\n+    encoding : str, optional\n+        Encoding used to decode the inputfile. The special value 'bytes'\n+        (the default) enables backwards-compatible behavior for `converters`,\n+        ensuring that inputs to the converter functions are encoded\n+        bytes objects. The special value 'bytes' has no additional effect if\n+        ``converters=None``. If encoding is ``'bytes'`` or ``None``, the\n+        default system encoding is used.\n+\n+    Returns\n+    -------\n+    ndarray\n+        NumPy array.\n+\n+    Examples\n+    --------\n+    First we create a file for the example.\n+\n+    >>> s1 = '1.0,2.0,3.0\\n4.0,5.0,6.0\\n'\n+    >>> with open('example1.csv', 'w') as f:\n+    ...     f.write(s1)\n+    >>> a1 = read_from_filename('example1.csv')\n+    >>> a1\n+    array([[1., 2., 3.],\n+           [4., 5., 6.]])\n+\n+    The second example has columns with different data types, so a\n+    one-dimensional array with a structured data type is returned.\n+    The tab character is used as the field delimiter.\n+\n+    >>> s2 = '1.0\\t10\\talpha\\n2.3\\t25\\tbeta\\n4.5\\t16\\tgamma\\n'\n+    >>> with open('example2.tsv', 'w') as f:\n+    ...     f.write(s2)\n+    >>> a2 = read_from_filename('example2.tsv', delimiter='\\t')\n+    >>> a2\n+    array([(1. , 10, b'alpha'), (2.3, 25, b'beta'), (4.5, 16, b'gamma')],\n+          dtype=[('f0', '<f8'), ('f1', 'u1'), ('f2', 'S5')])\n+    \"\"\"\n+    # Handle special 'bytes' keyword for encoding\n+    byte_converters = False\n+    if encoding == 'bytes':\n+        encoding = None\n+        byte_converters = True\n+\n+    if dtype is None:\n+        raise TypeError(\"a dtype must be provided.\")\n+    dtype = np.dtype(dtype)\n+\n+    read_dtype_via_object_chunks = None\n+    if dtype.kind in 'SUM' and (\n+            dtype == \"S0\" or dtype == \"U0\" or dtype == \"M8\" or dtype == 'm8'):\n+        # This is a legacy \"flexible\" dtype.  We do not truly support\n+        # parametric dtypes currently (no dtype discovery step in the core),\n+        # but have to support these for backward compatibility.\n+        read_dtype_via_object_chunks = dtype\n+        dtype = np.dtype(object)\n+\n+    if usecols is not None:\n+        # Allow usecols to be a single int or a sequence of ints, the C-code\n+        # handles the rest\n+        try:\n+            usecols = list(usecols)\n+        except TypeError:\n+            usecols = [usecols]\n+\n+    _ensure_ndmin_ndarray_check_param(ndmin)\n+\n+    if comment is None:\n+        comments = None\n+    else:\n+        # assume comments are a sequence of strings\n+        if \"\" in comment:\n+            raise ValueError(\n+                \"comments cannot be an empty string. Use comments=None to \"\n+                \"disable comments.\"\n+            )\n+        comments = tuple(comment)\n+        comment = None\n+        if len(comments) == 0:\n+            comments = None  # No comments at all\n+        elif len(comments) == 1:\n+            # If there is only one comment, and that comment has one character,\n+            # the normal parsing can deal with it just fine.\n+            if isinstance(comments[0], str) and len(comments[0]) == 1:\n+                comment = comments[0]\n+                comments = None\n+        else:\n+            # Input validation if there are multiple comment characters\n+            if delimiter in comments:\n+                raise TypeError(\n+                    f\"Comment characters '{comments}' cannot include the \"\n+                    f\"delimiter '{delimiter}'\"\n+                )\n+\n+    # comment is now either a 1 or 0 character string or a tuple:\n+    if comments is not None:\n+        # Note: An earlier version support two character comments (and could\n+        #       have been extended to multiple characters, we assume this is\n+        #       rare enough to not optimize for.\n+        if quote is not None:\n+            raise ValueError(\n+                \"when multiple comments or a multi-character comment is \"\n+                \"given, quotes are not supported.  In this case quotechar \"\n+                \"must be set to None.\")\n+\n+    if len(imaginary_unit) != 1:\n+        raise ValueError('len(imaginary_unit) must be 1.')\n+\n+    _check_nonneg_int(skiplines)\n+    if max_rows is not None:\n+        _check_nonneg_int(max_rows)\n+    else:\n+        # Passing -1 to the C code means \"read the entire file\".\n+        max_rows = -1\n+\n+    fh_closing_ctx = contextlib.nullcontext()\n+    filelike = False\n+    try:\n+        if isinstance(fname, os.PathLike):\n+            fname = os.fspath(fname)\n+        if isinstance(fname, str):\n+            fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n+            if encoding is None:\n+                encoding = getattr(fh, 'encoding', 'latin1')\n+\n+            fh_closing_ctx = contextlib.closing(fh)\n+            data = fh\n+            filelike = True\n+        else:\n+            if encoding is None:\n+                encoding = getattr(fname, 'encoding', 'latin1')\n+            data = iter(fname)\n+    except TypeError as e:\n+        raise ValueError(\n+            f\"fname must be a string, filehandle, list of strings,\\n\"\n+            f\"or generator. Got {type(fname)} instead.\") from e\n+\n+    with fh_closing_ctx:\n+        if comments is not None:\n+            if filelike:\n+                data = iter(data)\n+                filelike = False\n+            data = _preprocess_comments(data, comments, encoding)\n+\n+        if read_dtype_via_object_chunks is None:\n+            arr = _load_from_filelike(\n+                data, delimiter=delimiter, comment=comment, quote=quote,\n+                imaginary_unit=imaginary_unit,\n+                usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n+                converters=converters, dtype=dtype,\n+                encoding=encoding, filelike=filelike,\n+                byte_converters=byte_converters)\n+\n+        else:\n+            # This branch reads the file into chunks of object arrays and then\n+            # casts them to the desired actual dtype.  This ensures correct\n+            # string-length and datetime-unit discovery (like `arr.astype()`).\n+            # Due to chunking, certain error reports are less clear, currently.\n+            if filelike:\n+                data = iter(data)  # cannot chunk when reading from file\n+\n+            c_byte_converters = False\n+            if read_dtype_via_object_chunks == \"S\":\n+                c_byte_converters = True  # Use latin1 rather than ascii\n+\n+            chunks = []\n+            while max_rows != 0:\n+                if max_rows < 0:\n+                    chunk_size = _loadtxt_chunksize\n+                else:\n+                    chunk_size = min(_loadtxt_chunksize, max_rows)\n+\n+                next_arr = _load_from_filelike(\n+                    data, delimiter=delimiter, comment=comment, quote=quote,\n+                    imaginary_unit=imaginary_unit,\n+                    usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n+                    converters=converters, dtype=dtype,\n+                    encoding=encoding, filelike=filelike,\n+                    byte_converters=byte_converters,\n+                    c_byte_converters=c_byte_converters)\n+                # Cast here already.  We hope that this is better even for\n+                # large files because the storage is more compact.  It could\n+                # be adapted (in principle the concatenate could cast).\n+                chunks.append(next_arr.astype(read_dtype_via_object_chunks))\n+\n+                skiprows = 0  # Only have to skip for first chunk\n+                if max_rows >= 0:\n+                    max_rows -= chunk_size\n+                if len(next_arr) < chunk_size:\n+                    # There was less data than requested, so we are done.\n+                    break\n+\n+            # Need at least one chunk, but if empty, the last one may have\n+            # the wrong shape.\n+            if len(chunks) > 1 and len(chunks[-1]) == 0:\n+                del chunks[-1]\n+            if len(chunks) == 1:\n+                arr = chunks[0]\n+            else:\n+                arr = np.concatenate(chunks, axis=0)\n+\n+    # NOTE: ndmin works as advertised for structured dtypes, but normally\n+    #       these would return a 1D result plus the structured dimension,\n+    #       so ndmin=2 adds a third dimension even when no squeezing occurs.\n+    #       A `squeeze=False` could be a better solution (pandas uses squeeze).\n+    arr = _ensure_ndmin_ndarray(arr, ndmin=ndmin)\n+\n+    if arr.shape:\n+        if arr.shape[0] == 0:\n+            warnings.warn(\n+                f'loadtxt: input contained no data: \"{fname}\"',\n+                category=UserWarning,\n+                stacklevel=3\n+            )\n+\n+    if unpack:\n+        # Unpack structured dtypes if requested:\n+        dt = arr.dtype\n+        if dt.names is not None:\n+            # For structured arrays, return an array for each field.\n+            return [arr[field] for field in dt.names]\n+        else:\n+            return arr.T\n+    else:\n+        return arr\n+\n+\n @set_array_function_like_doc\n @set_module('numpy')\n def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n             converters=None, skiprows=0, usecols=None, unpack=False,\n-            ndmin=0, encoding='bytes', max_rows=None, *, like=None):\n+            ndmin=0, encoding='bytes', max_rows=None, *, quotechar=None,\n+            like=None):\n     r\"\"\"\n     Load data from a text file.\n \n@@ -882,19 +1102,20 @@ def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n         each row will be interpreted as an element of the array.  In this\n         case, the number of columns used must match the number of fields in\n         the data-type.\n-    comments : str or sequence of str, optional\n+    comments : str or sequence of str or None, optional\n         The characters or list of characters used to indicate the start of a\n         comment. None implies no comments. For backwards compatibility, byte\n         strings will be decoded as 'latin1'. The default is '#'.\n     delimiter : str, optional\n         The string used to separate values. For backwards compatibility, byte\n         strings will be decoded as 'latin1'. The default is whitespace.\n-    converters : dict, optional\n-        A dictionary mapping column number to a function that will parse the\n-        column string into the desired value.  E.g., if column 0 is a date\n-        string: ``converters = {0: datestr2num}``.  Converters can also be\n-        used to provide a default value for missing data (but see also\n-        `genfromtxt`): ``converters = {3: lambda s: float(s.strip() or 0)}``.\n+    converters : dict or callable, optional\n+        A function to parse all columns strings into the desired value, or\n+        a dictionary mapping column number to a parser function.\n+        E.g. if column 0 is a date string: ``converters = {0: datestr2num}``.\n+        Converters can also be used to provide a default value for missing\n+        data, e.g. ``converters = lambda s: float(s.strip() or 0)`` will\n+        convert empty fields to 0.\n         Default: None.\n     skiprows : int, optional\n         Skip the first `skiprows` lines, including comments; default: 0.\n@@ -932,6 +1153,16 @@ def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n         is to read all the lines.\n \n         .. versionadded:: 1.16.0\n+    quotechar : unicode character or None, optional\n+        The character used to denote the start and end of a quoted item.\n+        Occurrences of the delimiter or comment characters are ignored within\n+        a quoted item. The default value is ``quotechar=None``, which means\n+        quoting support is disabled.\n+\n+        If two consecutive instances of `quotechar` are found within a quoted\n+        field, the first is treated as an escape character. See examples.\n+\n+        .. versionadded:: 1.23.0\n     ${ARRAY_FUNCTION_LIKE}\n \n         .. versionadded:: 1.20.0\n@@ -979,249 +1210,120 @@ def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n     >>> y\n     array([2., 4.])\n \n+    The `converters` argument is used to specify functions to preprocess the\n+    text prior to parsing. `converters` can be a dictionary that maps\n+    preprocessing functions to each column:\n+\n+    >>> s = StringIO(\"1.618, 2.296\\n3.141, 4.669\\n\")\n+    >>> conv = {\n+    ...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0\n+    ...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1\n+    ... }\n+    >>> np.loadtxt(s, delimiter=\",\", converters=conv)\n+    array([[1., 3.],\n+           [3., 5.]])\n+\n+    `converters` can be a callable instead of a dictionary, in which case it\n+    is applied to all columns:\n+\n+    >>> s = StringIO(\"0xDE 0xAD\\n0xC0 0xDE\")\n+    >>> import functools\n+    >>> conv = functools.partial(int, base=16)\n+    >>> np.loadtxt(s, converters=conv)\n+    array([[222., 173.],\n+           [192., 222.]])\n+\n     This example shows how `converters` can be used to convert a field\n     with a trailing minus sign into a negative number.\n \n     >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n     >>> def conv(fld):\n     ...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)\n     ...\n-    >>> np.loadtxt(s, converters={0: conv, 1: conv})\n+    >>> np.loadtxt(s, converters=conv)\n     array([[ 10.01, -31.25],\n            [ 19.22,  64.31],\n            [-17.57,  63.94]])\n-    \"\"\"\n-\n-    if like is not None:\n-        return _loadtxt_with_like(\n-            fname, dtype=dtype, comments=comments, delimiter=delimiter,\n-            converters=converters, skiprows=skiprows, usecols=usecols,\n-            unpack=unpack, ndmin=ndmin, encoding=encoding,\n-            max_rows=max_rows, like=like\n-        )\n \n-    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n-    # Nested functions used by loadtxt.\n-    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n+    Using a callable as the converter can be particularly useful for handling\n+    values with different formatting, e.g. floats with underscores:\n \n-    def split_line(line: str):\n-        \"\"\"Chop off comments, strip, and split at delimiter.\"\"\"\n-        for comment in comments:  # Much faster than using a single regex.\n-            line = line.split(comment, 1)[0]\n-        line = line.strip('\\r\\n')\n-        return line.split(delimiter) if line else []\n+    >>> s = StringIO(\"1 2.7 100_000\")\n+    >>> np.loadtxt(s, converters=float)\n+    array([1.e+00, 2.7e+00, 1.e+05])\n \n-    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n-    # Main body of loadtxt.\n-    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n-\n-    _ensure_ndmin_ndarray_check_param(ndmin)\n-\n-    # Type conversions for Py3 convenience\n-    if comments is not None:\n-        if isinstance(comments, (str, bytes)):\n-            comments = [comments]\n-        comments = [_decode_line(x) for x in comments]\n-    else:\n-        comments = []\n-\n-    if delimiter is not None:\n-        delimiter = _decode_line(delimiter)\n-\n-    user_converters = converters\n-\n-    byte_converters = False\n-    if encoding == 'bytes':\n-        encoding = None\n-        byte_converters = True\n-\n-    if usecols is not None:\n-        # Copy usecols, allowing it to be a single int or a sequence of ints.\n-        try:\n-            usecols = list(usecols)\n-        except TypeError:\n-            usecols = [usecols]\n-        for i, col_idx in enumerate(usecols):\n-            try:\n-                usecols[i] = opindex(col_idx)  # Cast to builtin int now.\n-            except TypeError as e:\n-                e.args = (\n-                    \"usecols must be an int or a sequence of ints but \"\n-                    \"it contains at least one element of type %s\" %\n-                    type(col_idx),\n-                    )\n-                raise\n-        if len(usecols) > 1:\n-            usecols_getter = itemgetter(*usecols)\n-        else:\n-            # Get an iterable back, even if using a single column.\n-            usecols_getter = lambda obj, c=usecols[0]: [obj[c]]\n-    else:\n-        usecols_getter = None\n+    This idea can be extended to automatically handle values specified in\n+    many different formats:\n \n-    # Make sure we're dealing with a proper dtype\n-    dtype = np.dtype(dtype)\n-    defconv = _getconv(dtype)\n+    >>> def conv(val):\n+    ...     try:\n+    ...         return float(val)\n+    ...     except ValueError:\n+    ...         return float.fromhex(val)\n+    >>> s = StringIO(\"1, 2.5, 3_000, 0b4, 0x1.4000000000000p+2\")\n+    >>> np.loadtxt(s, delimiter=\",\", converters=conv, encoding=None)\n+    array([1.0e+00, 2.5e+00, 3.0e+03, 1.8e+02, 5.0e+00])\n \n-    dtype_types, packer = _loadtxt_flatten_dtype_internal(dtype)\n+    Note that with the default ``encoding=\"bytes\"``, the inputs to the\n+    converter function are latin-1 encoded byte strings. To deactivate the\n+    implicit encoding prior to conversion, use ``encoding=None``\n \n-    fh_closing_ctx = contextlib.nullcontext()\n-    try:\n-        if isinstance(fname, os_PathLike):\n-            fname = os_fspath(fname)\n-        if _is_string_like(fname):\n-            fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n-            fencoding = getattr(fh, 'encoding', 'latin1')\n-            line_iter = iter(fh)\n-            fh_closing_ctx = contextlib.closing(fh)\n-        else:\n-            line_iter = iter(fname)\n-            fencoding = getattr(fname, 'encoding', 'latin1')\n-            try:\n-                first_line = next(line_iter)\n-            except StopIteration:\n-                pass  # Nothing matters if line_iter is empty.\n-            else:\n-                # Put first_line back.\n-                line_iter = itertools.chain([first_line], line_iter)\n-                if isinstance(first_line, bytes):\n-                    # Using latin1 matches _decode_line's behavior.\n-                    decoder = methodcaller(\n-                        \"decode\",\n-                        encoding if encoding is not None else \"latin1\")\n-                    line_iter = map(decoder, line_iter)\n-    except TypeError as e:\n-        raise ValueError(\n-            f\"fname must be a string, filehandle, list of strings,\\n\"\n-            f\"or generator. Got {type(fname)} instead.\"\n-        ) from e\n+    >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n+    >>> conv = lambda x: -float(x[:-1]) if x.endswith('-') else float(x)\n+    >>> np.loadtxt(s, converters=conv, encoding=None)\n+    array([[ 10.01, -31.25],\n+           [ 19.22,  64.31],\n+           [-17.57,  63.94]])\n \n-    with fh_closing_ctx:\n+    Support for quoted fields is enabled with the `quotechar` parameter.\n+    Comment and delimiter characters are ignored when they appear within a\n+    quoted item delineated by `quotechar`:\n \n-        # input may be a python2 io stream\n-        if encoding is not None:\n-            fencoding = encoding\n-        # we must assume local encoding\n-        # TODO emit portability warning?\n-        elif fencoding is None:\n-            import locale\n-            fencoding = locale.getpreferredencoding()\n-\n-        # Skip the first `skiprows` lines\n-        for i in range(skiprows):\n-            next(line_iter)\n-\n-        # Read until we find a line with some values, and use it to determine\n-        # the need for decoding and estimate the number of columns.\n-        for first_line in line_iter:\n-            ncols = len(usecols or split_line(first_line))\n-            if ncols:\n-                # Put first_line back.\n-                line_iter = itertools.chain([first_line], line_iter)\n-                break\n-        else:  # End of lines reached\n-            ncols = len(usecols or [])\n-            warnings.warn('loadtxt: Empty input file: \"%s\"' % fname,\n-                          stacklevel=2)\n-\n-        line_iter = itertools.islice(line_iter, max_rows)\n-        lineno_words_iter = filter(\n-            itemgetter(1),  # item[1] is words; filter skips empty lines.\n-            enumerate(map(split_line, line_iter), 1 + skiprows))\n-\n-        # Now that we know ncols, create the default converters list, and\n-        # set packing, if necessary.\n-        if len(dtype_types) > 1:\n-            # We're dealing with a structured array, each field of\n-            # the dtype matches a column\n-            converters = [_getconv(dt) for dt in dtype_types]\n-        else:\n-            # All fields have the same dtype; use specialized packers which are\n-            # much faster than those using _loadtxt_pack_items.\n-            converters = [defconv for i in range(ncols)]\n-            if ncols == 1:\n-                packer = itemgetter(0)\n-            else:\n-                def packer(row): return row\n+    >>> s = StringIO('\"alpha, #42\", 10.0\\n\"beta, #64\", 2.0\\n')\n+    >>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\n+    >>> np.loadtxt(s, dtype=dtype, delimiter=\",\", quotechar='\"')\n+    array([('alpha, #42', 10.), ('beta, #64',  2.)],\n+          dtype=[('label', '<U12'), ('value', '<f8')])\n \n-        # By preference, use the converters specified by the user\n-        for i, conv in (user_converters or {}).items():\n-            if usecols:\n-                try:\n-                    i = usecols.index(i)\n-                except ValueError:\n-                    # Unused converter specified\n-                    continue\n-            if byte_converters:\n-                # converters may use decode to workaround numpy's old\n-                # behaviour, so encode the string again (converters are only\n-                # called with strings) before passing to the user converter.\n-                def tobytes_first(conv, x):\n-                    return conv(x.encode(\"latin1\"))\n-                converters[i] = functools.partial(tobytes_first, conv)\n-            else:\n-                converters[i] = conv\n-\n-        fencode = methodcaller(\"encode\", fencoding)\n-        converters = [conv if conv is not bytes else fencode\n-                      for conv in converters]\n-        if len(set(converters)) == 1:\n-            # Optimize single-type data. Note that this is only reached if\n-            # `_getconv` returns equal callables (i.e. not local lambdas) on\n-            # equal dtypes.\n-            def convert_row(vals, _conv=converters[0]):\n-                return [*map(_conv, vals)]\n-        else:\n-            def convert_row(vals):\n-                return [conv(val) for conv, val in zip(converters, vals)]\n-\n-        # read data in chunks and fill it into an array via resize\n-        # over-allocating and shrinking the array later may be faster but is\n-        # probably not relevant compared to the cost of actually reading and\n-        # converting the data\n-        X = None\n-        while True:\n-            chunk = []\n-            for lineno, words in itertools.islice(\n-                    lineno_words_iter, _loadtxt_chunksize):\n-                if usecols_getter is not None:\n-                    words = usecols_getter(words)\n-                elif len(words) != ncols:\n-                    raise ValueError(\n-                        f\"Wrong number of columns at line {lineno}\")\n-                # Convert each value according to its column, then pack it\n-                # according to the dtype's nesting, and store it.\n-                chunk.append(packer(convert_row(words)))\n-            if not chunk:  # The islice is empty, i.e. we're done.\n-                break\n+    Two consecutive quote characters within a quoted field are treated as a\n+    single escaped character:\n \n-            if X is None:\n-                X = np.array(chunk, dtype)\n-            else:\n-                nshape = list(X.shape)\n-                pos = nshape[0]\n-                nshape[0] += len(chunk)\n-                X.resize(nshape, refcheck=False)\n-                X[pos:, ...] = chunk\n+    >>> s = StringIO('\"Hello, my name is \"\"Monty\"\"!\"')\n+    >>> np.loadtxt(s, dtype=\"U\", delimiter=\",\", quotechar='\"')\n+    array('Hello, my name is \"Monty\"!', dtype='<U26')\n \n-    if X is None:\n-        X = np.array([], dtype)\n+    \"\"\"\n \n-    # Multicolumn data are returned with shape (1, N, M), i.e.\n-    # (1, 1, M) for a single row - remove the singleton dimension there\n-    if X.ndim == 3 and X.shape[:2] == (1, 1):\n-        X.shape = (1, -1)\n+    if like is not None:\n+        return _loadtxt_with_like(\n+            fname, dtype=dtype, comments=comments, delimiter=delimiter,\n+            converters=converters, skiprows=skiprows, usecols=usecols,\n+            unpack=unpack, ndmin=ndmin, encoding=encoding,\n+            max_rows=max_rows, like=like\n+        )\n \n-    X = _ensure_ndmin_ndarray(X, ndmin=ndmin)\n+    if isinstance(delimiter, bytes):\n+        delimiter.decode(\"latin1\")\n \n-    if unpack:\n-        if len(dtype_types) > 1:\n-            # For structured arrays, return an array for each field.\n-            return [X[field] for field in dtype.names]\n-        else:\n-            return X.T\n-    else:\n-        return X\n+    if dtype is None:\n+        dtype = np.float64\n+\n+    comment = comments\n+    # Control character type conversions for Py3 convenience\n+    if comment is not None:\n+        if isinstance(comment, (str, bytes)):\n+            comment = [comment]\n+        comment = [\n+            x.decode('latin1') if isinstance(x, bytes) else x for x in comment]\n+    if isinstance(delimiter, bytes):\n+        delimiter = delimiter.decode('latin1')\n+\n+    arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n+                converters=converters, skiplines=skiprows, usecols=usecols,\n+                unpack=unpack, ndmin=ndmin, encoding=encoding,\n+                max_rows=max_rows, quote=quotechar)\n+\n+    return arr\n \n \n _loadtxt_with_like = array_function_dispatch("
            },
            {
                "filename": "numpy/lib/tests/test_io.py",
                "patch": "@@ -695,7 +695,7 @@ def test_record(self):\n         assert_array_equal(x, a)\n \n         d = TextIO()\n-        d.write('M 64.0 75.0\\nF 25.0 60.0')\n+        d.write('M 64 75.0\\nF 25 60.0')\n         d.seek(0)\n         mydescriptor = {'names': ('gender', 'age', 'weight'),\n                         'formats': ('S1', 'i4', 'f4')}\n@@ -779,6 +779,8 @@ def test_comments_multiple(self):\n         a = np.array([[1, 2, 3], [4, 5, 6]], int)\n         assert_array_equal(x, a)\n \n+    @pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                        reason=\"PyPy bug in error formatting\")\n     def test_comments_multi_chars(self):\n         c = TextIO()\n         c.write('/* comment\\n1,2,3,5\\n')\n@@ -871,16 +873,27 @@ def __index__(self):\n         bogus_idx = 1.5\n         assert_raises_regex(\n             TypeError,\n-            '^usecols must be.*%s' % type(bogus_idx),\n+            '^usecols must be.*%s' % type(bogus_idx).__name__,\n             np.loadtxt, c, usecols=bogus_idx\n             )\n \n         assert_raises_regex(\n             TypeError,\n-            '^usecols must be.*%s' % type(bogus_idx),\n+            '^usecols must be.*%s' % type(bogus_idx).__name__,\n             np.loadtxt, c, usecols=[0, bogus_idx, 0]\n             )\n \n+    def test_bad_usecols(self):\n+        with pytest.raises(OverflowError):\n+            np.loadtxt([\"1\\n\"], usecols=[2**64], delimiter=\",\")\n+        with pytest.raises((ValueError, OverflowError)):\n+            # Overflow error on 32bit platforms\n+            np.loadtxt([\"1\\n\"], usecols=[2**62], delimiter=\",\")\n+        with pytest.raises(TypeError,\n+                match=\"If a structured dtype .*. But 1 usecols were given and \"\n+                      \"the number of fields is 3.\"):\n+            np.loadtxt([\"1,1\\n\"], dtype=\"i,(2)i\", usecols=[0], delimiter=\",\")\n+\n     def test_fancy_dtype(self):\n         c = TextIO()\n         c.write('1,2,3.0\\n4,5,6.0\\n')\n@@ -919,8 +932,7 @@ def test_str_dtype(self):\n             assert_array_equal(x, a)\n \n     def test_empty_file(self):\n-        with suppress_warnings() as sup:\n-            sup.filter(message=\"loadtxt: Empty input file:\")\n+        with pytest.warns(UserWarning, match=\"input contained no data\"):\n             c = TextIO()\n             x = np.loadtxt(c)\n             assert_equal(x.shape, (0,))\n@@ -981,29 +993,32 @@ def test_from_float_hex(self):\n         c.write(inp)\n         for dt in [float, np.float32]:\n             c.seek(0)\n-            res = np.loadtxt(c, dtype=dt)\n+            res = np.loadtxt(\n+                c, dtype=dt, converters=float.fromhex, encoding=\"latin1\")\n             assert_equal(res, tgt, err_msg=\"%s\" % dt)\n \n+    @pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                        reason=\"PyPy bug in error formatting\")\n     def test_default_float_converter_no_default_hex_conversion(self):\n         \"\"\"\n         Ensure that fromhex is only used for values with the correct prefix and\n         is not called by default. Regression test related to gh-19598.\n         \"\"\"\n         c = TextIO(\"a b c\")\n-        with pytest.raises(\n-            ValueError, match=\"could not convert string to float\"\n-        ):\n+        with pytest.raises(ValueError,\n+                match=\".*convert string 'a' to float64 at row 0, column 1\"):\n             np.loadtxt(c)\n \n+    @pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                        reason=\"PyPy bug in error formatting\")\n     def test_default_float_converter_exception(self):\n         \"\"\"\n         Ensure that the exception message raised during failed floating point\n         conversion is correct. Regression test related to gh-19598.\n         \"\"\"\n         c = TextIO(\"qrs tuv\")  # Invalid values for default float converter\n-        with pytest.raises(\n-            ValueError, match=\"could not convert string to float\"\n-        ):\n+        with pytest.raises(ValueError,\n+                match=\"could not convert string 'qrs' to float64\"):\n             np.loadtxt(c)\n \n     def test_from_complex(self):\n@@ -1099,8 +1114,7 @@ def test_ndmin_keyword(self):\n         assert_(x.shape == (3,))\n \n         # Test ndmin kw with empty file.\n-        with suppress_warnings() as sup:\n-            sup.filter(message=\"loadtxt: Empty input file:\")\n+        with pytest.warns(UserWarning, match=\"input contained no data\"):\n             f = TextIO()\n             assert_(np.loadtxt(f, ndmin=2).shape == (0, 1,))\n             assert_(np.loadtxt(f, ndmin=1).shape == (0,))\n@@ -1132,8 +1146,8 @@ def test_none_as_string(self):\n     @pytest.mark.skipif(locale.getpreferredencoding() == 'ANSI_X3.4-1968',\n                         reason=\"Wrong preferred encoding\")\n     def test_binary_load(self):\n-        butf8 = b\"5,6,7,\\xc3\\x95scarscar\\n\\r15,2,3,hello\\n\\r\"\\\n-                b\"20,2,3,\\xc3\\x95scar\\n\\r\"\n+        butf8 = b\"5,6,7,\\xc3\\x95scarscar\\r\\n15,2,3,hello\\r\\n\"\\\n+                b\"20,2,3,\\xc3\\x95scar\\r\\n\"\n         sutf8 = butf8.decode(\"UTF-8\").replace(\"\\r\", \"\").splitlines()\n         with temppath() as path:\n             with open(path, \"wb\") as f:\n@@ -1196,6 +1210,30 @@ def test_max_rows_larger(self):\n         a = np.array([[1, 2, 3, 5], [4, 5, 7, 8], [2, 1, 4, 5]], int)\n         assert_array_equal(x, a)\n \n+    @pytest.mark.parametrize([\"skip\", \"data\"], [\n+            (1, [\"ignored\\n\", \"1,2\\n\", \"\\n\", \"3,4\\n\"]),\n+            # \"Bad\" lines that do not end in newlines:\n+            (1, [\"ignored\", \"1,2\", \"\", \"3,4\"]),\n+            (1, StringIO(\"ignored\\n1,2\\n\\n3,4\")),\n+            # Same as above, but do not skip any lines:\n+            (0, [\"-1,0\\n\", \"1,2\\n\", \"\\n\", \"3,4\\n\"]),\n+            (0, [\"-1,0\", \"1,2\", \"\", \"3,4\"]),\n+            (0, StringIO(\"-1,0\\n1,2\\n\\n3,4\"))])\n+    def test_max_rows_empty_lines(self, skip, data):\n+        with pytest.warns(UserWarning,\n+                    match=f\"Input line 3.*max_rows={3-skip}\"):\n+            res = np.loadtxt(data, dtype=int, skiprows=skip, delimiter=\",\",\n+                             max_rows=3-skip)\n+            assert_array_equal(res, [[-1, 0], [1, 2], [3, 4]][skip:])\n+\n+        if isinstance(data, StringIO):\n+            data.seek(0)\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\", UserWarning)\n+            with pytest.raises(UserWarning):\n+                np.loadtxt(data, dtype=int, skiprows=skip, delimiter=\",\",\n+                           max_rows=3-skip)\n \n class Testfromregex:\n     def test_record(self):\n@@ -2397,6 +2435,13 @@ def test_auto_dtype_largeint(self):\n         assert_equal(test['f1'], 17179869184)\n         assert_equal(test['f2'], 1024)\n \n+    def test_unpack_float_data(self):\n+        txt = TextIO(\"1,2,3\\n4,5,6\\n7,8,9\\n0.0,1.0,2.0\")\n+        a, b, c = np.loadtxt(txt, delimiter=\",\", unpack=True)\n+        assert_array_equal(a, np.array([1.0, 4.0, 7.0, 0.0]))\n+        assert_array_equal(b, np.array([2.0, 5.0, 8.0, 1.0]))\n+        assert_array_equal(c, np.array([3.0, 6.0, 9.0, 2.0]))\n+\n     def test_unpack_structured(self):\n         # Regression test for gh-4341\n         # Unpacking should work on structured arrays"
            },
            {
                "filename": "numpy/lib/tests/test_loadtxt.py",
                "patch": "@@ -0,0 +1,1002 @@\n+\"\"\"\n+Tests specific to `np.loadtxt` added during the move of loadtxt to be backed\n+by C code.\n+These tests complement those found in `test_io.py`.\n+\"\"\"\n+\n+import sys\n+import pytest\n+from tempfile import NamedTemporaryFile, mkstemp\n+from io import StringIO\n+\n+import numpy as np\n+from numpy.ma.testutils import assert_equal\n+from numpy.testing import assert_array_equal, HAS_REFCOUNT, IS_PYPY\n+\n+\n+def test_scientific_notation():\n+    \"\"\"Test that both 'e' and 'E' are parsed correctly.\"\"\"\n+    data = StringIO(\n+        (\n+            \"1.0e-1,2.0E1,3.0\\n\"\n+            \"4.0e-2,5.0E-1,6.0\\n\"\n+            \"7.0e-3,8.0E1,9.0\\n\"\n+            \"0.0e-4,1.0E-1,2.0\"\n+        )\n+    )\n+    expected = np.array(\n+        [[0.1, 20., 3.0], [0.04, 0.5, 6], [0.007, 80., 9], [0, 0.1, 2]]\n+    )\n+    assert_array_equal(np.loadtxt(data, delimiter=\",\"), expected)\n+\n+\n+@pytest.mark.parametrize(\"comment\", [\"..\", \"//\", \"@-\", \"this is a comment:\"])\n+def test_comment_multiple_chars(comment):\n+    content = \"# IGNORE\\n1.5, 2.5# ABC\\n3.0,4.0# XXX\\n5.5,6.0\\n\"\n+    txt = StringIO(content.replace(\"#\", comment))\n+    a = np.loadtxt(txt, delimiter=\",\", comments=comment)\n+    assert_equal(a, [[1.5, 2.5], [3.0, 4.0], [5.5, 6.0]])\n+\n+\n+@pytest.fixture\n+def mixed_types_structured():\n+    \"\"\"\n+    Fixture providing hetergeneous input data with a structured dtype, along\n+    with the associated structured array.\n+    \"\"\"\n+    data = StringIO(\n+        (\n+            \"1000;2.4;alpha;-34\\n\"\n+            \"2000;3.1;beta;29\\n\"\n+            \"3500;9.9;gamma;120\\n\"\n+            \"4090;8.1;delta;0\\n\"\n+            \"5001;4.4;epsilon;-99\\n\"\n+            \"6543;7.8;omega;-1\\n\"\n+        )\n+    )\n+    dtype = np.dtype(\n+        [('f0', np.uint16), ('f1', np.float64), ('f2', 'S7'), ('f3', np.int8)]\n+    )\n+    expected = np.array(\n+        [\n+            (1000, 2.4, \"alpha\", -34),\n+            (2000, 3.1, \"beta\", 29),\n+            (3500, 9.9, \"gamma\", 120),\n+            (4090, 8.1, \"delta\", 0),\n+            (5001, 4.4, \"epsilon\", -99),\n+            (6543, 7.8, \"omega\", -1)\n+        ],\n+        dtype=dtype\n+    )\n+    return data, dtype, expected\n+\n+\n+@pytest.mark.parametrize('skiprows', [0, 1, 2, 3])\n+def test_structured_dtype_and_skiprows_no_empty_lines(\n+        skiprows, mixed_types_structured):\n+    data, dtype, expected = mixed_types_structured\n+    a = np.loadtxt(data, dtype=dtype, delimiter=\";\", skiprows=skiprows)\n+    assert_array_equal(a, expected[skiprows:])\n+\n+\n+def test_unpack_structured(mixed_types_structured):\n+    data, dtype, expected = mixed_types_structured\n+\n+    a, b, c, d = np.loadtxt(data, dtype=dtype, delimiter=\";\", unpack=True)\n+    assert_array_equal(a, expected[\"f0\"])\n+    assert_array_equal(b, expected[\"f1\"])\n+    assert_array_equal(c, expected[\"f2\"])\n+    assert_array_equal(d, expected[\"f3\"])\n+\n+\n+def test_structured_dtype_with_shape():\n+    dtype = np.dtype([(\"a\", \"u1\", 2), (\"b\", \"u1\", 2)])\n+    data = StringIO(\"0,1,2,3\\n6,7,8,9\\n\")\n+    expected = np.array([((0, 1), (2, 3)), ((6, 7), (8, 9))], dtype=dtype)\n+    assert_array_equal(np.loadtxt(data, delimiter=\",\", dtype=dtype), expected)\n+\n+\n+def test_structured_dtype_with_multi_shape():\n+    dtype = np.dtype([(\"a\", \"u1\", (2, 2))])\n+    data = StringIO(\"0 1 2 3\\n\")\n+    expected = np.array([(((0, 1), (2, 3)),)], dtype=dtype)\n+    assert_array_equal(np.loadtxt(data, dtype=dtype), expected)\n+\n+\n+def test_nested_structured_subarray():\n+    # Test from gh-16678\n+    point = np.dtype([('x', float), ('y', float)])\n+    dt = np.dtype([('code', int), ('points', point, (2,))])\n+    data = StringIO(\"100,1,2,3,4\\n200,5,6,7,8\\n\")\n+    expected = np.array(\n+        [\n+            (100, [(1., 2.), (3., 4.)]),\n+            (200, [(5., 6.), (7., 8.)]),\n+        ],\n+        dtype=dt\n+    )\n+    assert_array_equal(np.loadtxt(data, dtype=dt, delimiter=\",\"), expected)\n+\n+\n+def test_structured_dtype_offsets():\n+    # An aligned structured dtype will have additional padding\n+    dt = np.dtype(\"i1, i4, i1, i4, i1, i4\", align=True)\n+    data = StringIO(\"1,2,3,4,5,6\\n7,8,9,10,11,12\\n\")\n+    expected = np.array([(1, 2, 3, 4, 5, 6), (7, 8, 9, 10, 11, 12)], dtype=dt)\n+    assert_array_equal(np.loadtxt(data, delimiter=\",\", dtype=dt), expected)\n+\n+\n+@pytest.mark.parametrize(\"param\", (\"skiprows\", \"max_rows\"))\n+def test_exception_negative_row_limits(param):\n+    \"\"\"skiprows and max_rows should raise for negative parameters.\"\"\"\n+    with pytest.raises(ValueError, match=\"argument must be nonnegative\"):\n+        np.loadtxt(\"foo.bar\", **{param: -3})\n+\n+\n+@pytest.mark.parametrize(\"param\", (\"skiprows\", \"max_rows\"))\n+def test_exception_noninteger_row_limits(param):\n+    with pytest.raises(TypeError, match=\"argument must be an integer\"):\n+        np.loadtxt(\"foo.bar\", **{param: 1.0})\n+\n+\n+@pytest.mark.parametrize(\n+    \"data, shape\",\n+    [\n+        (\"1 2 3 4 5\\n\", (1, 5)),  # Single row\n+        (\"1\\n2\\n3\\n4\\n5\\n\", (5, 1)),  # Single column\n+    ]\n+)\n+def test_ndmin_single_row_or_col(data, shape):\n+    arr = np.array([1, 2, 3, 4, 5])\n+    arr2d = arr.reshape(shape)\n+\n+    assert_array_equal(np.loadtxt(StringIO(data), dtype=int), arr)\n+    assert_array_equal(np.loadtxt(StringIO(data), dtype=int, ndmin=0), arr)\n+    assert_array_equal(np.loadtxt(StringIO(data), dtype=int, ndmin=1), arr)\n+    assert_array_equal(np.loadtxt(StringIO(data), dtype=int, ndmin=2), arr2d)\n+\n+\n+@pytest.mark.parametrize(\"badval\", [-1, 3, None, \"plate of shrimp\"])\n+def test_bad_ndmin(badval):\n+    with pytest.raises(ValueError, match=\"Illegal value of ndmin keyword\"):\n+        np.loadtxt(\"foo.bar\", ndmin=badval)\n+\n+\n+@pytest.mark.parametrize(\n+    \"ws\",\n+    (\n+            \"\\t\",  # tab\n+            \"\\u2003\",  # em\n+            \"\\u00A0\",  # non-break\n+            \"\\u3000\",  # ideographic space\n+    )\n+)\n+def test_blank_lines_spaces_delimit(ws):\n+    txt = StringIO(\n+        f\"1 2{ws}30\\n\\n4 5 60\\n  {ws}  \\n7 8 {ws} 90\\n  # comment\\n3 2 1\"\n+    )\n+    # NOTE: It is unclear that the `  # comment` should succeed. Except\n+    #       for delimiter=None, which should use any whitespace (and maybe\n+    #       should just be implemented closer to Python\n+    expected = np.array([[1, 2, 30], [4, 5, 60], [7, 8, 90], [3, 2, 1]])\n+    assert_equal(\n+        np.loadtxt(txt, dtype=int, delimiter=None, comments=\"#\"), expected\n+    )\n+\n+\n+def test_blank_lines_normal_delimiter():\n+    txt = StringIO('1,2,30\\n\\n4,5,60\\n\\n7,8,90\\n# comment\\n3,2,1')\n+    expected = np.array([[1, 2, 30], [4, 5, 60], [7, 8, 90], [3, 2, 1]])\n+    assert_equal(\n+        np.loadtxt(txt, dtype=int, delimiter=',', comments=\"#\"), expected\n+    )\n+\n+\n+@pytest.mark.parametrize(\"dtype\", (float, object))\n+def test_maxrows_no_blank_lines(dtype):\n+    txt = StringIO(\"1.5,2.5\\n3.0,4.0\\n5.5,6.0\")\n+    res = np.loadtxt(txt, dtype=dtype, delimiter=\",\", max_rows=2)\n+    assert_equal(res.dtype, dtype)\n+    assert_equal(res, np.array([[\"1.5\", \"2.5\"], [\"3.0\", \"4.0\"]], dtype=dtype))\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+@pytest.mark.parametrize(\"dtype\", (np.dtype(\"f8\"), np.dtype(\"i2\")))\n+def test_exception_message_bad_values(dtype):\n+    txt = StringIO(\"1,2\\n3,XXX\\n5,6\")\n+    msg = f\"could not convert string 'XXX' to {dtype} at row 1, column 2\"\n+    with pytest.raises(ValueError, match=msg):\n+        np.loadtxt(txt, dtype=dtype, delimiter=\",\")\n+\n+\n+def test_converters_negative_indices():\n+    txt = StringIO('1.5,2.5\\n3.0,XXX\\n5.5,6.0')\n+    conv = {-1: lambda s: np.nan if s == 'XXX' else float(s)}\n+    expected = np.array([[1.5, 2.5], [3.0, np.nan], [5.5, 6.0]])\n+    res = np.loadtxt(\n+        txt, dtype=np.float64, delimiter=\",\", converters=conv, encoding=None\n+    )\n+    assert_equal(res, expected)\n+\n+\n+def test_converters_negative_indices_with_usecols():\n+    txt = StringIO('1.5,2.5,3.5\\n3.0,4.0,XXX\\n5.5,6.0,7.5\\n')\n+    conv = {-1: lambda s: np.nan if s == 'XXX' else float(s)}\n+    expected = np.array([[1.5, 3.5], [3.0, np.nan], [5.5, 7.5]])\n+    res = np.loadtxt(\n+        txt,\n+        dtype=np.float64,\n+        delimiter=\",\",\n+        converters=conv,\n+        usecols=[0, -1],\n+        encoding=None,\n+    )\n+    assert_equal(res, expected)\n+\n+    # Second test with variable number of rows:\n+    res = np.loadtxt(StringIO('''0,1,2\\n0,1,2,3,4'''), delimiter=\",\",\n+                     usecols=[0, -1], converters={-1: (lambda x: -1)})\n+    assert_array_equal(res, [[0, -1], [0, -1]])\n+\n+def test_ragged_usecols():\n+    # usecols, and negative ones, work even with varying number of columns.\n+    txt = StringIO(\"0,0,XXX\\n0,XXX,0,XXX\\n0,XXX,XXX,0,XXX\\n\")\n+    expected = np.array([[0, 0], [0, 0], [0, 0]])\n+    res = np.loadtxt(txt, dtype=float, delimiter=\",\", usecols=[0, -2])\n+    assert_equal(res, expected)\n+\n+    txt = StringIO(\"0,0,XXX\\n0\\n0,XXX,XXX,0,XXX\\n\")\n+    with pytest.raises(ValueError,\n+                match=\"invalid column index -2 at row 1 with 2 columns\"):\n+        # There is no -2 column in the second row:\n+        np.loadtxt(txt, dtype=float, delimiter=\",\", usecols=[0, -2])\n+\n+\n+def test_empty_usecols():\n+    txt = StringIO(\"0,0,XXX\\n0,XXX,0,XXX\\n0,XXX,XXX,0,XXX\\n\")\n+    res = np.loadtxt(txt, dtype=np.dtype([]), delimiter=\",\", usecols=[])\n+    assert res.shape == (3,)\n+    assert res.dtype == np.dtype([])\n+\n+\n+@pytest.mark.parametrize(\"c1\", [\"a\", \"\u306e\", \"\ud83e\uded5\"])\n+@pytest.mark.parametrize(\"c2\", [\"a\", \"\u306e\", \"\ud83e\uded5\"])\n+def test_large_unicode_characters(c1, c2):\n+    # c1 and c2 span ascii, 16bit and 32bit range.\n+    txt = StringIO(f\"a,{c1},c,1.0\\ne,{c2},2.0,g\")\n+    res = np.loadtxt(txt, dtype=np.dtype('U12'), delimiter=\",\")\n+    expected = np.array(\n+        [f\"a,{c1},c,1.0\".split(\",\"), f\"e,{c2},2.0,g\".split(\",\")],\n+        dtype=np.dtype('U12')\n+    )\n+    assert_equal(res, expected)\n+\n+\n+def test_unicode_with_converter():\n+    txt = StringIO(\"cat,dog\\n\u03b1\u03b2\u03b3,\u03b4\u03b5\u03b6\\nabc,def\\n\")\n+    conv = {0: lambda s: s.upper()}\n+    res = np.loadtxt(\n+        txt,\n+        dtype=np.dtype(\"U12\"),\n+        converters=conv,\n+        delimiter=\",\",\n+        encoding=None\n+    )\n+    expected = np.array([['CAT', 'dog'], ['\u0391\u0392\u0393', '\u03b4\u03b5\u03b6'], ['ABC', 'def']])\n+    assert_equal(res, expected)\n+\n+\n+def test_converter_with_structured_dtype():\n+    txt = StringIO('1.5,2.5,Abc\\n3.0,4.0,dEf\\n5.5,6.0,ghI\\n')\n+    dt = np.dtype([('m', np.int32), ('r', np.float32), ('code', 'U8')])\n+    conv = {0: lambda s: int(10*float(s)), -1: lambda s: s.upper()}\n+    res = np.loadtxt(txt, dtype=dt, delimiter=\",\", converters=conv)\n+    expected = np.array(\n+        [(15, 2.5, 'ABC'), (30, 4.0, 'DEF'), (55, 6.0, 'GHI')], dtype=dt\n+    )\n+    assert_equal(res, expected)\n+\n+\n+def test_converter_with_unicode_dtype():\n+    \"\"\"\n+    With the default 'bytes' encoding, tokens are encoded prior to being\n+    passed to the converter. This means that the output of the converter may\n+    be bytes instead of unicode as expected by `read_rows`.\n+\n+    This test checks that outputs from the above scenario are properly decoded\n+    prior to parsing by `read_rows`.\n+    \"\"\"\n+    txt = StringIO('abc,def\\nrst,xyz')\n+    conv = bytes.upper\n+    res = np.loadtxt(\n+            txt, dtype=np.dtype(\"U3\"), converters=conv, delimiter=\",\")\n+    expected = np.array([['ABC', 'DEF'], ['RST', 'XYZ']])\n+    assert_equal(res, expected)\n+\n+\n+def test_read_huge_row():\n+    row = \"1.5, 2.5,\" * 50000\n+    row = row[:-1] + \"\\n\"\n+    txt = StringIO(row * 2)\n+    res = np.loadtxt(txt, delimiter=\",\", dtype=float)\n+    assert_equal(res, np.tile([1.5, 2.5], (2, 50000)))\n+\n+\n+@pytest.mark.parametrize(\"dtype\", \"edfgFDG\")\n+def test_huge_float(dtype):\n+    # Covers a non-optimized path that is rarely taken:\n+    field = \"0\" * 1000 + \".123456789\"\n+    dtype = np.dtype(dtype)\n+    value = np.loadtxt([field], dtype=dtype)[()]\n+    assert value == dtype.type(\"0.123456789\")\n+\n+\n+@pytest.mark.parametrize(\n+    (\"given_dtype\", \"expected_dtype\"),\n+    [\n+        (\"S\", np.dtype(\"S5\")),\n+        (\"U\", np.dtype(\"U5\")),\n+    ],\n+)\n+def test_string_no_length_given(given_dtype, expected_dtype):\n+    \"\"\"\n+    The given dtype is just 'S' or 'U' with no length. In these cases, the\n+    length of the resulting dtype is determined by the longest string found\n+    in the file.\n+    \"\"\"\n+    txt = StringIO(\"AAA,5-1\\nBBBBB,0-3\\nC,4-9\\n\")\n+    res = np.loadtxt(txt, dtype=given_dtype, delimiter=\",\")\n+    expected = np.array(\n+        [['AAA', '5-1'], ['BBBBB', '0-3'], ['C', '4-9']], dtype=expected_dtype\n+    )\n+    assert_equal(res, expected)\n+    assert_equal(res.dtype, expected_dtype)\n+\n+\n+def test_float_conversion():\n+    \"\"\"\n+    Some tests that the conversion to float64 works as accurately as the\n+    Python built-in `float` function. In a naive version of the float parser,\n+    these strings resulted in values that were off by an ULP or two.\n+    \"\"\"\n+    strings = [\n+        '0.9999999999999999',\n+        '9876543210.123456',\n+        '5.43215432154321e+300',\n+        '0.901',\n+        '0.333',\n+    ]\n+    txt = StringIO('\\n'.join(strings))\n+    res = np.loadtxt(txt)\n+    expected = np.array([float(s) for s in strings])\n+    assert_equal(res, expected)\n+\n+\n+def test_bool():\n+    # Simple test for bool via integer\n+    txt = StringIO(\"1, 0\\n10, -1\")\n+    res = np.loadtxt(txt, dtype=bool, delimiter=\",\")\n+    assert res.dtype == bool\n+    assert_array_equal(res, [[True, False], [True, True]])\n+    # Make sure we use only 1 and 0 on the byte level:\n+    assert_array_equal(res.view(np.uint8), [[1, 0], [1, 1]])\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n+def test_integer_signs(dtype):\n+    dtype = np.dtype(dtype)\n+    assert np.loadtxt([\"+2\"], dtype=dtype) == 2\n+    if dtype.kind == \"u\":\n+        with pytest.raises(ValueError):\n+            np.loadtxt([\"-1\\n\"], dtype=dtype)\n+    else:\n+        assert np.loadtxt([\"-2\\n\"], dtype=dtype) == -2\n+\n+    for sign in [\"++\", \"+-\", \"--\", \"-+\"]:\n+        with pytest.raises(ValueError):\n+            np.loadtxt([f\"{sign}2\\n\"], dtype=dtype)\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n+def test_implicit_cast_float_to_int_fails(dtype):\n+    txt = StringIO(\"1.0, 2.1, 3.7\\n4, 5, 6\")\n+    with pytest.raises(ValueError):\n+        np.loadtxt(txt, dtype=dtype, delimiter=\",\")\n+\n+@pytest.mark.parametrize(\"dtype\", (np.complex64, np.complex128))\n+@pytest.mark.parametrize(\"with_parens\", (False, True))\n+def test_complex_parsing(dtype, with_parens):\n+    s = \"(1.0-2.5j),3.75,(7+-5.0j)\\n(4),(-19e2j),(0)\"\n+    if not with_parens:\n+        s = s.replace(\"(\", \"\").replace(\")\", \"\")\n+\n+    res = np.loadtxt(StringIO(s), dtype=dtype, delimiter=\",\")\n+    expected = np.array(\n+        [[1.0-2.5j, 3.75, 7-5j], [4.0, -1900j, 0]], dtype=dtype\n+    )\n+    assert_equal(res, expected)\n+\n+\n+def test_read_from_generator():\n+    def gen():\n+        for i in range(4):\n+            yield f\"{i},{2*i},{i**2}\"\n+\n+    res = np.loadtxt(gen(), dtype=int, delimiter=\",\")\n+    expected = np.array([[0, 0, 0], [1, 2, 1], [2, 4, 4], [3, 6, 9]])\n+    assert_equal(res, expected)\n+\n+\n+def test_read_from_generator_multitype():\n+    def gen():\n+        for i in range(3):\n+            yield f\"{i} {i / 4}\"\n+\n+    res = np.loadtxt(gen(), dtype=\"i, d\", delimiter=\" \")\n+    expected = np.array([(0, 0.0), (1, 0.25), (2, 0.5)], dtype=\"i, d\")\n+    assert_equal(res, expected)\n+\n+\n+def test_read_from_bad_generator():\n+    def gen():\n+        for entry in [\"1,2\", b\"3, 5\", 12738]:\n+            yield entry\n+\n+    with pytest.raises(\n+            TypeError, match=r\"non-string returned while reading data\"):\n+        np.loadtxt(gen(), dtype=\"i, i\", delimiter=\",\")\n+\n+\n+@pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n+def test_object_cleanup_on_read_error():\n+    sentinel = object()\n+    already_read = 0\n+\n+    def conv(x):\n+        nonlocal already_read\n+        if already_read > 4999:\n+            raise ValueError(\"failed half-way through!\")\n+        already_read += 1\n+        return sentinel\n+\n+    txt = StringIO(\"x\\n\" * 10000)\n+\n+    with pytest.raises(ValueError, match=\"at row 5000, column 1\"):\n+        np.loadtxt(txt, dtype=object, converters={0: conv})\n+\n+    assert sys.getrefcount(sentinel) == 2\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+def test_character_not_bytes_compatible():\n+    \"\"\"Test exception when a character cannot be encoded as 'S'.\"\"\"\n+    data = StringIO(\"\u2013\")  # == \\u2013\n+    with pytest.raises(ValueError):\n+        np.loadtxt(data, dtype=\"S5\")\n+\n+\n+@pytest.mark.parametrize(\"conv\", (0, [float], \"\"))\n+def test_invalid_converter(conv):\n+    msg = (\n+        \"converters must be a dictionary mapping columns to converter \"\n+        \"functions or a single callable.\"\n+    )\n+    with pytest.raises(TypeError, match=msg):\n+        np.loadtxt(StringIO(\"1 2\\n3 4\"), converters=conv)\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+def test_converters_dict_raises_non_integer_key():\n+    with pytest.raises(TypeError, match=\"keys of the converters dict\"):\n+        np.loadtxt(StringIO(\"1 2\\n3 4\"), converters={\"a\": int})\n+    with pytest.raises(TypeError, match=\"keys of the converters dict\"):\n+        np.loadtxt(StringIO(\"1 2\\n3 4\"), converters={\"a\": int}, usecols=0)\n+\n+\n+@pytest.mark.parametrize(\"bad_col_ind\", (3, -3))\n+def test_converters_dict_raises_non_col_key(bad_col_ind):\n+    data = StringIO(\"1 2\\n3 4\")\n+    with pytest.raises(ValueError, match=\"converter specified for column\"):\n+        np.loadtxt(data, converters={bad_col_ind: int})\n+\n+\n+def test_converters_dict_raises_val_not_callable():\n+    with pytest.raises(TypeError,\n+                match=\"values of the converters dictionary must be callable\"):\n+        np.loadtxt(StringIO(\"1 2\\n3 4\"), converters={0: 1})\n+\n+\n+@pytest.mark.parametrize(\"q\", ('\"', \"'\", \"`\"))\n+def test_quoted_field(q):\n+    txt = StringIO(\n+        f\"{q}alpha, x{q}, 2.5\\n{q}beta, y{q}, 4.5\\n{q}gamma, z{q}, 5.0\\n\"\n+    )\n+    dtype = np.dtype([('f0', 'U8'), ('f1', np.float64)])\n+    expected = np.array(\n+        [(\"alpha, x\", 2.5), (\"beta, y\", 4.5), (\"gamma, z\", 5.0)], dtype=dtype\n+    )\n+\n+    res = np.loadtxt(txt, dtype=dtype, delimiter=\",\", quotechar=q)\n+    assert_array_equal(res, expected)\n+\n+\n+def test_quote_support_default():\n+    \"\"\"Support for quoted fields is disabled by default.\"\"\"\n+    txt = StringIO('\"lat,long\", 45, 30\\n')\n+    dtype = np.dtype([('f0', 'U24'), ('f1', np.float64), ('f2', np.float64)])\n+\n+    with pytest.raises(ValueError, match=\"the number of columns changed\"):\n+        np.loadtxt(txt, dtype=dtype, delimiter=\",\")\n+\n+    # Enable quoting support with non-None value for quotechar param\n+    txt.seek(0)\n+    expected = np.array([(\"lat,long\", 45., 30.)], dtype=dtype)\n+\n+    res = np.loadtxt(txt, dtype=dtype, delimiter=\",\", quotechar='\"')\n+    assert_array_equal(res, expected)\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+def test_quotechar_multichar_error():\n+    txt = StringIO(\"1,2\\n3,4\")\n+    msg = r\".*must be a single unicode character or None\"\n+    with pytest.raises(TypeError, match=msg):\n+        np.loadtxt(txt, delimiter=\",\", quotechar=\"''\")\n+\n+\n+def test_comment_multichar_error_with_quote():\n+    txt = StringIO(\"1,2\\n3,4\")\n+    msg = (\n+        \"when multiple comments or a multi-character comment is given, \"\n+        \"quotes are not supported.\"\n+    )\n+    with pytest.raises(ValueError, match=msg):\n+        np.loadtxt(txt, delimiter=\",\", comments=\"123\", quotechar='\"')\n+    with pytest.raises(ValueError, match=msg):\n+        np.loadtxt(txt, delimiter=\",\", comments=[\"#\", \"%\"], quotechar='\"')\n+\n+    # A single character string in a tuple is unpacked though:\n+    res = np.loadtxt(txt, delimiter=\",\", comments=(\"#\",), quotechar=\"'\")\n+    assert_equal(res, [[1, 2], [3, 4]])\n+\n+\n+def test_structured_dtype_with_quotes():\n+    data = StringIO(\n+        (\n+            \"1000;2.4;'alpha';-34\\n\"\n+            \"2000;3.1;'beta';29\\n\"\n+            \"3500;9.9;'gamma';120\\n\"\n+            \"4090;8.1;'delta';0\\n\"\n+            \"5001;4.4;'epsilon';-99\\n\"\n+            \"6543;7.8;'omega';-1\\n\"\n+        )\n+    )\n+    dtype = np.dtype(\n+        [('f0', np.uint16), ('f1', np.float64), ('f2', 'S7'), ('f3', np.int8)]\n+    )\n+    expected = np.array(\n+        [\n+            (1000, 2.4, \"alpha\", -34),\n+            (2000, 3.1, \"beta\", 29),\n+            (3500, 9.9, \"gamma\", 120),\n+            (4090, 8.1, \"delta\", 0),\n+            (5001, 4.4, \"epsilon\", -99),\n+            (6543, 7.8, \"omega\", -1)\n+        ],\n+        dtype=dtype\n+    )\n+    res = np.loadtxt(data, dtype=dtype, delimiter=\";\", quotechar=\"'\")\n+    assert_array_equal(res, expected)\n+\n+\n+def test_quoted_field_is_not_empty():\n+    txt = StringIO('1\\n\\n\"4\"\\n\"\"')\n+    expected = np.array([\"1\", \"4\", \"\"], dtype=\"U1\")\n+    res = np.loadtxt(txt, delimiter=\",\", dtype=\"U1\", quotechar='\"')\n+    assert_equal(res, expected)\n+\n+def test_quoted_field_is_not_empty_nonstrict():\n+    # Same as test_quoted_field_is_not_empty but check that we are not strict\n+    # about missing closing quote (this is the `csv.reader` default also)\n+    txt = StringIO('1\\n\\n\"4\"\\n\"')\n+    expected = np.array([\"1\", \"4\", \"\"], dtype=\"U1\")\n+    res = np.loadtxt(txt, delimiter=\",\", dtype=\"U1\", quotechar='\"')\n+    assert_equal(res, expected)\n+\n+def test_consecutive_quotechar_escaped():\n+    txt = StringIO('\"Hello, my name is \"\"Monty\"\"!\"')\n+    expected = np.array('Hello, my name is \"Monty\"!', dtype=\"U40\")\n+    res = np.loadtxt(txt, dtype=\"U40\", delimiter=\",\", quotechar='\"')\n+    assert_equal(res, expected)\n+\n+\n+@pytest.mark.parametrize(\"data\", (\"\", \"\\n\\n\\n\", \"# 1 2 3\\n# 4 5 6\\n\"))\n+@pytest.mark.parametrize(\"ndmin\", (0, 1, 2))\n+@pytest.mark.parametrize(\"usecols\", [None, (1, 2, 3)])\n+def test_warn_on_no_data(data, ndmin, usecols):\n+    \"\"\"Check that a UserWarning is emitted when no data is read from input.\"\"\"\n+    if usecols is not None:\n+        expected_shape = (0, 3)\n+    elif ndmin == 2:\n+        expected_shape = (0, 1)  # guess a single column?!\n+    else:\n+        expected_shape = (0,)\n+\n+    txt = StringIO(data)\n+    with pytest.warns(UserWarning, match=\"input contained no data\"):\n+        res = np.loadtxt(txt, ndmin=ndmin, usecols=usecols)\n+    assert res.shape == expected_shape\n+\n+    with NamedTemporaryFile(mode=\"w\") as fh:\n+        fh.write(data)\n+        fh.seek(0)\n+        with pytest.warns(UserWarning, match=\"input contained no data\"):\n+            res = np.loadtxt(txt, ndmin=ndmin, usecols=usecols)\n+        assert res.shape == expected_shape\n+\n+@pytest.mark.parametrize(\"skiprows\", (2, 3))\n+def test_warn_on_skipped_data(skiprows):\n+    data = \"1 2 3\\n4 5 6\"\n+    txt = StringIO(data)\n+    with pytest.warns(UserWarning, match=\"input contained no data\"):\n+        np.loadtxt(txt, skiprows=skiprows)\n+\n+\n+@pytest.mark.parametrize([\"dtype\", \"value\"], [\n+        (\"i2\", 0x0001), (\"u2\", 0x0001),\n+        (\"i4\", 0x00010203), (\"u4\", 0x00010203),\n+        (\"i8\", 0x0001020304050607), (\"u8\", 0x0001020304050607),\n+        # The following values are constructed to lead to unique bytes:\n+        (\"float16\", 3.07e-05),\n+        (\"float32\", 9.2557e-41), (\"complex64\", 9.2557e-41+2.8622554e-29j),\n+        (\"float64\", -1.758571353180402e-24),\n+        # Here and below, the repr side-steps a small loss of precision in\n+        # complex `str` in PyPy (which is probably fine, as repr works):\n+        (\"complex128\", repr(5.406409232372729e-29-1.758571353180402e-24j)),\n+        # Use integer values that fit into double.  Everything else leads to\n+        # problems due to longdoubles going via double and decimal strings\n+        # causing rounding errors.\n+        (\"longdouble\", 0x01020304050607),\n+        (\"clongdouble\", repr(0x01020304050607 + (0x00121314151617 * 1j))),\n+        (\"U2\", \"\\U00010203\\U000a0b0c\")])\n+@pytest.mark.parametrize(\"swap\", [True, False])\n+def test_byteswapping_and_unaligned(dtype, value, swap):\n+    # Try to create \"interesting\" values within the valid unicode range:\n+    dtype = np.dtype(dtype)\n+    data = [f\"x,{value}\\n\"]  # repr as PyPy `str` truncates some\n+    if swap:\n+        dtype = dtype.newbyteorder()\n+    full_dt = np.dtype([(\"a\", \"S1\"), (\"b\", dtype)], align=False)\n+    # The above ensures that the interesting \"b\" field is unaligned:\n+    assert full_dt.fields[\"b\"][1] == 1\n+    res = np.loadtxt(data, dtype=full_dt, delimiter=\",\", encoding=None,\n+                     max_rows=1)  # max-rows prevents over-allocation\n+    assert res[\"b\"] == dtype.type(value)\n+\n+\n+@pytest.mark.parametrize(\"dtype\",\n+        np.typecodes[\"AllInteger\"] + \"efdFD\" + \"?\")\n+def test_unicode_whitespace_stripping(dtype):\n+    # Test that all numeric types (and bool) strip whitespace correctly\n+    # \\u202F is a narrow no-break space, `\\n` is just a whitespace if quoted.\n+    # Currently, skip float128 as it did not always support this and has no\n+    # \"custom\" parsing:\n+    txt = StringIO(' 3 ,\"\\u202F2\\n\"')\n+    res = np.loadtxt(txt, dtype=dtype, delimiter=\",\", quotechar='\"')\n+    assert_array_equal(res, np.array([3, 2]).astype(dtype))\n+\n+\n+@pytest.mark.parametrize(\"dtype\", \"FD\")\n+def test_unicode_whitespace_stripping_complex(dtype):\n+    # Complex has a few extra cases since it has two components and\n+    # parentheses\n+    line = \" 1 , 2+3j , ( 4+5j ), ( 6+-7j )  , 8j , ( 9j ) \\n\"\n+    data = [line, line.replace(\" \", \"\\u202F\")]\n+    res = np.loadtxt(data, dtype=dtype, delimiter=',')\n+    assert_array_equal(res, np.array([[1, 2+3j, 4+5j, 6-7j, 8j, 9j]] * 2))\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+@pytest.mark.parametrize(\"dtype\", \"FD\")\n+@pytest.mark.parametrize(\"field\",\n+        [\"1 +2j\", \"1+ 2j\", \"1+2 j\", \"1+-+3\", \"(1j\", \"(1\", \"(1+2j\", \"1+2j)\"])\n+def test_bad_complex(dtype, field):\n+    with pytest.raises(ValueError):\n+        np.loadtxt([field + \"\\n\"], dtype=dtype, delimiter=\",\")\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+@pytest.mark.parametrize(\"dtype\",\n+            np.typecodes[\"AllInteger\"] + \"efgdFDG\" + \"?\")\n+def test_nul_character_error(dtype):\n+    # Test that a \\0 character is correctly recognized as an error even if\n+    # what comes before is valid (not everything gets parsed internally).\n+    if dtype.lower() == \"g\":\n+        pytest.xfail(\"longdouble/clongdouble assignment may misbehave.\")\n+    with pytest.raises(ValueError):\n+        np.loadtxt([\"1\\000\"], dtype=dtype, delimiter=\",\", quotechar='\"')\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+@pytest.mark.parametrize(\"dtype\",\n+        np.typecodes[\"AllInteger\"] + \"efgdFDG\" + \"?\")\n+def test_no_thousands_support(dtype):\n+    # Mainly to document behaviour, Python supports thousands like 1_1.\n+    # (e and G may end up using different conversion and support it, this is\n+    # a bug but happens...)\n+    if dtype == \"e\":\n+        pytest.skip(\"half assignment currently uses Python float converter\")\n+    if dtype in \"eG\":\n+        pytest.xfail(\"clongdouble assignment is buggy (uses `complex`?).\")\n+\n+    assert int(\"1_1\") == float(\"1_1\") == complex(\"1_1\") == 11\n+    with pytest.raises(ValueError):\n+        np.loadtxt([\"1_1\\n\"], dtype=dtype)\n+\n+\n+@pytest.mark.parametrize(\"data\", [\n+    [\"1,2\\n\", \"2\\n,3\\n\"],\n+    [\"1,2\\n\", \"2\\r,3\\n\"]])\n+def test_bad_newline_in_iterator(data):\n+    # In NumPy <=1.22 this was accepted, because newlines were completely\n+    # ignored when the input was an iterable.  This could be changed, but right\n+    # now, we raise an error.\n+    msg = \"Found an unquoted embedded newline within a single line\"\n+    with pytest.raises(ValueError, match=msg):\n+        np.loadtxt(data, delimiter=\",\")\n+\n+\n+@pytest.mark.parametrize(\"data\", [\n+    [\"1,2\\n\", \"2,3\\r\\n\"],  # a universal newline\n+    [\"1,2\\n\", \"'2\\n',3\\n\"],  # a quoted newline\n+    [\"1,2\\n\", \"'2\\r',3\\n\"],\n+    [\"1,2\\n\", \"'2\\r\\n',3\\n\"],\n+])\n+def test_good_newline_in_iterator(data):\n+    # The quoted newlines will be untransformed here, but are just whitespace.\n+    res = np.loadtxt(data, delimiter=\",\", quotechar=\"'\")\n+    assert_array_equal(res, [[1., 2.], [2., 3.]])\n+\n+\n+@pytest.mark.parametrize(\"newline\", [\"\\n\", \"\\r\", \"\\r\\n\"])\n+def test_universal_newlines_quoted(newline):\n+    # Check that universal newline support within the tokenizer is not applied\n+    # to quoted fields.  (note that lines must end in newline or quoted\n+    # fields will not include a newline at all)\n+    data = ['1,\"2\\n\"\\n', '3,\"4\\n', '1\"\\n']\n+    data = [row.replace(\"\\n\", newline) for row in data]\n+    res = np.loadtxt(data, dtype=object, delimiter=\",\", quotechar='\"')\n+    assert_array_equal(res, [['1', f'2{newline}'], ['3', f'4{newline}1']])\n+\n+\n+def test_null_character():\n+    # Basic tests to check that the NUL character is not special:\n+    res = np.loadtxt([\"1\\0002\\0003\\n\", \"4\\0005\\0006\"], delimiter=\"\\000\")\n+    assert_array_equal(res, [[1, 2, 3], [4, 5, 6]])\n+\n+    # Also not as part of a field (avoid unicode/arrays as unicode strips \\0)\n+    res = np.loadtxt([\"1\\000,2\\000,3\\n\", \"4\\000,5\\000,6\"],\n+                     delimiter=\",\", dtype=object)\n+    assert res.tolist() == [[\"1\\000\", \"2\\000\", \"3\"], [\"4\\000\", \"5\\000\", \"6\"]]\n+\n+\n+def test_iterator_fails_getting_next_line():\n+    class BadSequence:\n+        def __len__(self):\n+            return 100\n+\n+        def __getitem__(self, item):\n+            if item == 50:\n+                raise RuntimeError(\"Bad things happened!\")\n+            return f\"{item}, {item+1}\"\n+\n+    with pytest.raises(RuntimeError, match=\"Bad things happened!\"):\n+        np.loadtxt(BadSequence(), dtype=int, delimiter=\",\")\n+\n+\n+class TestCReaderUnitTests:\n+    # These are internal tests for path that should not be possible to hit\n+    # unless things go very very wrong somewhere.\n+    def test_not_an_filelike(self):\n+        with pytest.raises(AttributeError, match=\".*read\"):\n+            np.core._multiarray_umath._load_from_filelike(\n+                object(), dtype=np.dtype(\"i\"), filelike=True)\n+\n+    def test_filelike_read_fails(self):\n+        # Can only be reached if loadtxt opens the file, so it is hard to do\n+        # via the public interface (although maybe not impossible considering\n+        # the current \"DataClass\" backing).\n+        class BadFileLike:\n+            counter = 0\n+\n+            def read(self, size):\n+                self.counter += 1\n+                if self.counter > 20:\n+                    raise RuntimeError(\"Bad bad bad!\")\n+                return \"1,2,3\\n\"\n+\n+        with pytest.raises(RuntimeError, match=\"Bad bad bad!\"):\n+            np.core._multiarray_umath._load_from_filelike(\n+                BadFileLike(), dtype=np.dtype(\"i\"), filelike=True)\n+\n+    def test_filelike_bad_read(self):\n+        # Can only be reached if loadtxt opens the file, so it is hard to do\n+        # via the public interface (although maybe not impossible considering\n+        # the current \"DataClass\" backing).\n+\n+        class BadFileLike:\n+            counter = 0\n+\n+            def read(self, size):\n+                return 1234  # not a string!\n+\n+        with pytest.raises(TypeError,\n+                    match=\"non-string returned while reading data\"):\n+            np.core._multiarray_umath._load_from_filelike(\n+                BadFileLike(), dtype=np.dtype(\"i\"), filelike=True)\n+\n+    def test_not_an_iter(self):\n+        with pytest.raises(TypeError,\n+                    match=\"error reading from object, expected an iterable\"):\n+            np.core._multiarray_umath._load_from_filelike(\n+                object(), dtype=np.dtype(\"i\"), filelike=False)\n+\n+    def test_bad_type(self):\n+        with pytest.raises(TypeError, match=\"internal error: dtype must\"):\n+            np.core._multiarray_umath._load_from_filelike(\n+                object(), dtype=\"i\", filelike=False)\n+\n+    def test_bad_encoding(self):\n+        with pytest.raises(TypeError, match=\"encoding must be a unicode\"):\n+            np.core._multiarray_umath._load_from_filelike(\n+                object(), dtype=np.dtype(\"i\"), filelike=False, encoding=123)\n+\n+    @pytest.mark.parametrize(\"newline\", [\"\\r\", \"\\n\", \"\\r\\n\"])\n+    def test_manual_universal_newlines(self, newline):\n+        # This is currently not available to users, because we should always\n+        # open files with universal newlines enabled `newlines=None`.\n+        # (And reading from an iterator uses slightly different code paths.)\n+        # We have no real support for `newline=\"\\r\"` or `newline=\"\\n\" as the\n+        # user cannot specify those options.\n+        data = StringIO('0\\n1\\n\"2\\n\"\\n3\\n4 #\\n'.replace(\"\\n\", newline),\n+                        newline=\"\")\n+\n+        res = np.core._multiarray_umath._load_from_filelike(\n+            data, dtype=np.dtype(\"U10\"), filelike=True,\n+            quote='\"', comment=\"#\", skiplines=1)\n+        assert_array_equal(res[:, 0], [\"1\", f\"2{newline}\", \"3\", \"4 \"])\n+\n+\n+def test_delimiter_comment_collision_raises():\n+    with pytest.raises(TypeError, match=\".*control characters.*incompatible\"):\n+        np.loadtxt(StringIO(\"1, 2, 3\"), delimiter=\",\", comments=\",\")\n+\n+\n+def test_delimiter_quotechar_collision_raises():\n+    with pytest.raises(TypeError, match=\".*control characters.*incompatible\"):\n+        np.loadtxt(StringIO(\"1, 2, 3\"), delimiter=\",\", quotechar=\",\")\n+\n+\n+def test_comment_quotechar_collision_raises():\n+    with pytest.raises(TypeError, match=\".*control characters.*incompatible\"):\n+        np.loadtxt(StringIO(\"1 2 3\"), comments=\"#\", quotechar=\"#\")\n+\n+\n+def test_delimiter_and_multiple_comments_collision_raises():\n+    with pytest.raises(\n+        TypeError, match=\"Comment characters.*cannot include the delimiter\"\n+    ):\n+        np.loadtxt(StringIO(\"1, 2, 3\"), delimiter=\",\", comments=[\"#\", \",\"])\n+\n+\n+@pytest.mark.parametrize(\n+    \"ws\",\n+    (\n+        \" \",  # space\n+        \"\\t\",  # tab\n+        \"\\u2003\",  # em\n+        \"\\u00A0\",  # non-break\n+        \"\\u3000\",  # ideographic space\n+    )\n+)\n+def test_collision_with_default_delimiter_raises(ws):\n+    with pytest.raises(TypeError, match=\".*control characters.*incompatible\"):\n+        np.loadtxt(StringIO(f\"1{ws}2{ws}3\\n4{ws}5{ws}6\\n\"), comments=ws)\n+    with pytest.raises(TypeError, match=\".*control characters.*incompatible\"):\n+        np.loadtxt(StringIO(f\"1{ws}2{ws}3\\n4{ws}5{ws}6\\n\"), quotechar=ws)\n+\n+\n+@pytest.mark.parametrize(\"nl\", (\"\\n\", \"\\r\"))\n+def test_control_character_newline_raises(nl):\n+    txt = StringIO(f\"1{nl}2{nl}3{nl}{nl}4{nl}5{nl}6{nl}{nl}\")\n+    msg = \"control character.*cannot be a newline\"\n+    with pytest.raises(TypeError, match=msg):\n+        np.loadtxt(txt, delimiter=nl)\n+    with pytest.raises(TypeError, match=msg):\n+        np.loadtxt(txt, comments=nl)\n+    with pytest.raises(TypeError, match=msg):\n+        np.loadtxt(txt, quotechar=nl)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"generic_data\", \"long_datum\", \"unitless_dtype\", \"expected_dtype\"),\n+    [\n+        (\"2012-03\", \"2013-01-15\", \"M8\", \"M8[D]\"),  # Datetimes\n+        (\"spam-a-lot\", \"tis_but_a_scratch\", \"U\", \"U17\"),  # str\n+    ],\n+)\n+@pytest.mark.parametrize(\"nrows\", (10, 50000, 60000))  # lt, eq, gt chunksize\n+def test_parametric_unit_discovery(\n+    generic_data, long_datum, unitless_dtype, expected_dtype, nrows\n+):\n+    \"\"\"Check that the correct unit (e.g. month, day, second) is discovered from\n+    the data when a user specifies a unitless datetime.\"\"\"\n+    # Unit should be \"D\" (days) due to last entry\n+    data = [generic_data] * 50000 + [long_datum]\n+    expected = np.array(data, dtype=expected_dtype)\n+\n+    # file-like path\n+    txt = StringIO(\"\\n\".join(data))\n+    a = np.loadtxt(txt, dtype=unitless_dtype)\n+    assert a.dtype == expected.dtype\n+    assert_equal(a, expected)\n+\n+    # file-obj path\n+    fd, fname = mkstemp()\n+    with open(fname, \"w\") as fh:\n+        fh.write(\"\\n\".join(data))\n+    a = np.loadtxt(fname, dtype=unitless_dtype)\n+    assert a.dtype == expected.dtype\n+    assert_equal(a, expected)\n+\n+\n+def test_str_dtype_unit_discovery_with_converter():\n+    data = [\"spam-a-lot\"] * 60000 + [\"XXXtis_but_a_scratch\"]\n+    expected = np.array(\n+        [\"spam-a-lot\"] * 60000 + [\"tis_but_a_scratch\"], dtype=\"U17\"\n+    )\n+    conv = lambda s: s.strip(\"XXX\")\n+\n+    # file-like path\n+    txt = StringIO(\"\\n\".join(data))\n+    a = np.loadtxt(txt, dtype=\"U\", converters=conv, encoding=None)\n+    assert a.dtype == expected.dtype\n+    assert_equal(a, expected)\n+\n+    # file-obj path\n+    fd, fname = mkstemp()\n+    with open(fname, \"w\") as fh:\n+        fh.write(\"\\n\".join(data))\n+    a = np.loadtxt(fname, dtype=\"U\", converters=conv, encoding=None)\n+    assert a.dtype == expected.dtype\n+    assert_equal(a, expected)\n+\n+\n+@pytest.mark.skipif(IS_PYPY and sys.implementation.version <= (7, 3, 8),\n+                    reason=\"PyPy bug in error formatting\")\n+def test_control_character_empty():\n+    with pytest.raises(TypeError, match=\"Text reading control character must\"):\n+        np.loadtxt(StringIO(\"1 2 3\"), delimiter=\"\")\n+    with pytest.raises(TypeError, match=\"Text reading control character must\"):\n+        np.loadtxt(StringIO(\"1 2 3\"), quotechar=\"\")\n+    with pytest.raises(ValueError, match=\"comments cannot be an empty string\"):\n+        np.loadtxt(StringIO(\"1 2 3\"), comments=\"\")\n+    with pytest.raises(ValueError, match=\"comments cannot be an empty string\"):\n+        np.loadtxt(StringIO(\"1 2 3\"), comments=[\"#\", \"\"])\n+\n+\n+def test_control_characters_as_bytes():\n+    \"\"\"Byte control characters (comments, delimiter) are supported.\"\"\"\n+    a = np.loadtxt(StringIO(\"#header\\n1,2,3\"), comments=b\"#\", delimiter=b\",\")\n+    assert_equal(a, [1, 2, 3])"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 11897,
        "body": "This morphed into the more general addition of `np.never_copy` (`\"never\"` or similar are problematic, because currently `np.array(..., copy=\"never\")` would use `bool(\"never\")` doing the exact opposite.\r\n\r\nThings to do:\r\n - [x] Check if buffer/memoryview paths are allowed to make a copy, if they are, do not allow them here. (They do not.)\r\n - Add tests for:\r\n     - [x] `np.array`\r\n     - [ ] `arr.astype`\r\n     - [ ] Deprecation/argument parsing errors\r\n     - [ ] `np.reshape` function (and a few for attribute)\r\n---\r\n\r\nThe current version includes a deprection for some bool conversions, to only allow integers at least, that can be split off probably.",
        "changed_files": [
            {
                "filename": "doc/source/reference/c-api.array.rst",
                "patch": "@@ -440,6 +440,11 @@ From other objects\n         Make sure a copy is made of *op*. If this flag is not\n         present, data is not copied if it can be avoided.\n \n+    .. c:var:: NPY_ARRAY_ENSURENOCOPY\n+\n+        Make sure no copy is made of *op*. An error will be given if\n+        a copy cannot be avoided.\n+\n     .. c:var:: NPY_ARRAY_ENSUREARRAY\n \n         Make sure the result is a base-class ndarray. By"
            },
            {
                "filename": "numpy/__init__.py",
                "patch": "@@ -110,7 +110,7 @@\n import warnings\n \n from ._globals import ModuleDeprecationWarning, VisibleDeprecationWarning\n-from ._globals import _NoValue\n+from ._globals import _NoValue, never_copy\n \n # We first need to detect if we're being called as part of the numpy setup\n # procedure itself in a reliable manner."
            },
            {
                "filename": "numpy/_globals.py",
                "patch": "@@ -18,7 +18,8 @@ def foo(arg=np._NoValue):\n from __future__ import division, absolute_import, print_function\n \n __ALL__ = [\n-    'ModuleDeprecationWarning', 'VisibleDeprecationWarning', '_NoValue'\n+    'ModuleDeprecationWarning', 'VisibleDeprecationWarning', '_NoValue',\n+    'never_copy'\n     ]\n \n \n@@ -66,7 +67,7 @@ class _NoValueType(object):\n     __instance = None\n     def __new__(cls):\n         # ensure that only one instance exists\n-        if not cls.__instance:\n+        if cls.__instance is None:\n             cls.__instance = super(_NoValueType, cls).__new__(cls)\n         return cls.__instance\n \n@@ -79,3 +80,31 @@ def __repr__(self):\n \n \n _NoValue = _NoValueType()\n+\n+\n+class _NeverCopyType(object):\n+    \"\"\"Special `copy` keyword value.\n+\n+    The instance of this class may be used for many `copy` arguments to\n+    functions to indicate that a copy must not be made and an error will\n+    be raised when this cannot be guaranteed.\n+\n+    In the future the string \"never\" may replace this variable.\n+    \"\"\"\n+    __instance = None\n+    def __new__(cls):\n+        # ensure that only one instance exists\n+        if cls.__instance is None:\n+            cls.__instance = super(_NeverCopyType, cls).__new__(cls)\n+        return cls.__instance\n+\n+    def __repr__(self):\n+        return \"<never copy>\"\n+\n+    def __bool__(self):\n+        raise ValueError(\"`np.never_copy` cannot be converted to True/False \"\n+                         \"to avoid the unintended meaning to a function \"\n+                         \"interpreting it as a boolean.\")\n+\n+\n+never_copy = _NeverCopyType()"
            },
            {
                "filename": "numpy/core/_add_newdocs.py",
                "patch": "@@ -2323,6 +2323,10 @@\n     the array and the remaining dimensions. Reshaping an array in-place will\n     fail if a copy is required.\n \n+    Assigning to the shape is discouraged. This is because it modifies the\n+    properties of the array as opposed to only the data. `ndarray.reshape`\n+    should be preferred.\n+\n     Examples\n     --------\n     >>> x = np.array([1, 2, 3, 4])\n@@ -2331,7 +2335,7 @@\n     >>> y = np.zeros((2, 3, 4))\n     >>> y.shape\n     (2, 3, 4)\n-    >>> y.shape = (3, 8)\n+    >>> y.shape = (3, 8)  # discouarged, reshape should be preferred.\n     >>> y\n     array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n            [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n@@ -3513,12 +3517,16 @@\n \n add_newdoc('numpy.core.multiarray', 'ndarray', ('reshape',\n     \"\"\"\n-    a.reshape(shape, order='C')\n+    a.reshape(shape, order='C', copy=False)\n \n     Returns an array containing the same data with a new shape.\n \n     Refer to `numpy.reshape` for full documentation.\n \n+    ..versionadded:: 1.17.0\n+\n+    The ``copy`` argument was added in version 1.17.0.\n+\n     See Also\n     --------\n     numpy.reshape : equivalent function"
            },
            {
                "filename": "numpy/core/fromnumeric.py",
                "patch": "@@ -38,12 +38,12 @@\n \n \n # functions that are now methods\n-def _wrapit(obj, method, *args, **kwds):\n+def _wrapit(obj, method, _internal_copy, *args, **kwds):\n     try:\n         wrap = obj.__array_wrap__\n     except AttributeError:\n         wrap = None\n-    result = getattr(asarray(obj), method)(*args, **kwds)\n+    result = getattr(array(obj, copy=_internal_copy), method)(*args, **kwds)\n     if wrap:\n         if not isinstance(result, mu.ndarray):\n             result = asarray(result)\n@@ -63,7 +63,15 @@ def _wrapfunc(obj, method, *args, **kwds):\n     # of NumPy's. This situation has occurred in the case of\n     # a downstream library like 'pandas'.\n     except (AttributeError, TypeError):\n-        return _wrapit(obj, method, *args, **kwds)\n+        return _wrapit(obj, method, False, *args, **kwds)\n+\n+\n+def _wrapfunc_copy(obj, method, _internal_copy, *args, **kwds):\n+    # Same as _wrapfunc, but allows to pass copy argument to `np.array`.\n+    try:\n+        return getattr(obj, method)(*args, **kwds)\n+    except (AttributeError, TypeError):\n+        return _wrapit(obj, method, _internal_copy, *args, **kwds)\n \n \n def _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs):\n@@ -189,13 +197,14 @@ def take(a, indices, axis=None, out=None, mode='raise'):\n     return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\n \n \n-def _reshape_dispatcher(a, newshape, order=None):\n+\n+def _reshape_dispatcher(a, newshape, order=None, copy=None):\n     return (a,)\n \n \n # not deprecated --- copy if necessary, view otherwise\n @array_function_dispatch(_reshape_dispatcher)\n-def reshape(a, newshape, order='C'):\n+def reshape(a, newshape, order='C', copy=np._NoValue):\n     \"\"\"\n     Gives a new shape to an array without changing its data.\n \n@@ -221,13 +230,23 @@ def reshape(a, newshape, order='C'):\n         'A' means to read / write the elements in Fortran-like index\n         order if `a` is Fortran *contiguous* in memory, C-like order\n         otherwise.\n+    copy : {True, False, np.never_copy}, optional\n+        Whether or not a copy is forced. If ``False`` is given, a copy\n+        will be made when necessary. ``np.never_copy`` will cause an\n+        error to be raised if `a` cannot be reshaped without a copy.\n+        If the input is not an array and copy is not ``np.never_copy``\n+        an additional copy may be made to convert to an array.\n+\n+        .. versionadded:: 1.17.0\n \n     Returns\n     -------\n     reshaped_array : ndarray\n         This will be a new view object if possible; otherwise, it will\n         be a copy.  Note there is no guarantee of the *memory layout* (C- or\n         Fortran- contiguous) of the returned array.\n+        The `copy` argument can be used to enforce a copy or raise an error\n+        when a `copy` would be made but is not desired.\n \n     See Also\n     --------\n@@ -293,7 +312,16 @@ def reshape(a, newshape, order='C'):\n            [3, 4],\n            [5, 6]])\n     \"\"\"\n-    return _wrapfunc(a, 'reshape', newshape, order=order)\n+    # Since it is hard to tell if `np.array` had to copy, the copy is only\n+    # forced during reshape (but no-copy forced also for `np.array`).\n+    if copy is np._NoValue:\n+        return _wrapfunc_copy(\n+                    a, 'reshape', copy if copy is np.never_copy else False,\n+                    newshape, order=order)\n+    else:\n+        return _wrapfunc_copy(\n+                    a, 'reshape', copy if copy is np.never_copy else False,\n+                    newshape, order=order, copy=copy)\n \n \n def _choose_dispatcher(a, choices, out=None, mode=None):\n@@ -1385,7 +1413,7 @@ def squeeze(a, axis=None):\n     try:\n         squeeze = a.squeeze\n     except AttributeError:\n-        return _wrapit(a, 'squeeze')\n+        return _wrapit(a, 'squeeze', False)\n     if axis is None:\n         return squeeze()\n     else:"
            },
            {
                "filename": "numpy/core/include/numpy/ndarraytypes.h",
                "patch": "@@ -827,6 +827,12 @@ typedef int (PyArray_FinalizeFunc)(PyArrayObject *, PyObject *);\n  */\n #define NPY_ARRAY_ENSURECOPY      0x0020\n \n+/*\n+ * Make sure that no copy will be made. May be requested in constructor\n+ * functions.\n+ */\n+#define NPY_ARRAY_ENSURENOCOPY    0x0800\n+\n /*\n  * Make sure the returned array is a base-class ndarray\n  *\n@@ -879,6 +885,8 @@ typedef int (PyArray_FinalizeFunc)(PyArrayObject *, PyObject *);\n #define NPY_ARRAY_UPDATEIFCOPY    0x1000 /* Deprecated in 1.14 */\n #define NPY_ARRAY_WRITEBACKIFCOPY 0x2000\n \n+/*      NPY_ARRAY_ENSURENOCOPY    0x4000  defined above so next is 0x10000 */\n+\n /*\n  * NOTE: there are also internal flags defined in multiarray/arrayobject.h,\n  * which start at bit 31 and work down."
            },
            {
                "filename": "numpy/core/src/multiarray/arrayobject.c",
                "patch": "@@ -278,8 +278,9 @@ PyArray_CopyObject(PyArrayObject *dest, PyObject *src_object)\n      * Get either an array object we can copy from, or its parameters\n      * if there isn't a convenient array available.\n      */\n-    if (PyArray_GetArrayParamsFromObject(src_object, PyArray_DESCR(dest),\n-                0, &dtype, &ndim, dims, &src, NULL) < 0) {\n+    if (PyArray_GetArrayParamsFromObject_int(\n+                src_object, PyArray_DESCR(dest),\n+                0, 0, &dtype, &ndim, dims, &src, NULL) < 0) {\n         Py_DECREF(src_object);\n         return -1;\n     }\n@@ -1216,25 +1217,6 @@ _void_compare(PyArrayObject *self, PyArrayObject *other, int cmp_op)\n     }\n }\n \n-/*\n- * Silence the current error and emit a deprecation warning instead.\n- *\n- * If warnings are raised as errors, this sets the warning __cause__ to the\n- * silenced error.\n- */\n-NPY_NO_EXPORT int\n-DEPRECATE_silence_error(const char *msg) {\n-    PyObject *exc, *val, *tb;\n-    PyErr_Fetch(&exc, &val, &tb);\n-    if (DEPRECATE(msg) < 0) {\n-        npy_PyErr_ChainExceptionsCause(exc, val, tb);\n-        return -1;\n-    }\n-    Py_XDECREF(exc);\n-    Py_XDECREF(val);\n-    Py_XDECREF(tb);\n-    return 0;\n-}\n \n /*\n  * Comparisons can fail, but we do not always want to pass on the exception"
            },
            {
                "filename": "numpy/core/src/multiarray/common.c",
                "patch": "@@ -558,6 +558,27 @@ _array_typedescr_fromstr(char *c_str)\n }\n \n \n+/*\n+ * Silence the current error and emit a deprecation warning instead.\n+ *\n+ * If warnings are raised as errors, this sets the warning __cause__ to the\n+ * silenced error.\n+ */\n+NPY_NO_EXPORT int\n+DEPRECATE_silence_error(const char *msg) {\n+    PyObject *exc, *val, *tb;\n+    PyErr_Fetch(&exc, &val, &tb);\n+    if (DEPRECATE(msg) < 0) {\n+        npy_PyErr_ChainExceptionsCause(exc, val, tb);\n+        return -1;\n+    }\n+    Py_XDECREF(exc);\n+    Py_XDECREF(val);\n+    Py_XDECREF(tb);\n+    return 0;\n+}\n+\n+\n NPY_NO_EXPORT char *\n index2ptr(PyArrayObject *mp, npy_intp i)\n {"
            },
            {
                "filename": "numpy/core/src/multiarray/common.h",
                "patch": "@@ -51,6 +51,8 @@ _array_find_python_scalar_type(PyObject *op);\n NPY_NO_EXPORT PyArray_Descr *\n _array_typedescr_fromstr(char *str);\n \n+NPY_NO_EXPORT int DEPRECATE_silence_error(const char *msg);\n+\n NPY_NO_EXPORT char *\n index2ptr(PyArrayObject *mp, npy_intp i);\n "
            },
            {
                "filename": "numpy/core/src/multiarray/conversion_utils.c",
                "patch": "@@ -10,6 +10,7 @@\n \n #include \"npy_config.h\"\n #include \"npy_pycompat.h\"\n+#include \"npy_import.h\"\n \n #include \"common.h\"\n #include \"arraytypes.h\"\n@@ -317,18 +318,60 @@ PyArray_ConvertMultiAxis(PyObject *axis_in, int ndim, npy_bool *out_axis_flags)\n NPY_NO_EXPORT int\n PyArray_BoolConverter(PyObject *object, npy_bool *val)\n {\n-    if (PyObject_IsTrue(object)) {\n-        *val = NPY_TRUE;\n+    int obj_as_bool;\n+    Py_ssize_t obj_as_int;\n+\n+    /* Quickly check the most common cases: */\n+    if (object == Py_False) {\n+        *val = 0;\n+        return NPY_SUCCEED;\n     }\n-    else {\n-        *val = NPY_FALSE;\n+    if (object == Py_True) {\n+        *val = 1;\n+        return NPY_SUCCEED;\n     }\n-    if (PyErr_Occurred()) {\n-        return NPY_FAIL;\n+    /* Not Py_None as default, because bool(None) is False */\n+\n+    /*\n+     * Allow for anything that can be safely cast to an integer. This\n+     * is a bit more strict than most python code which will simply\n+     * convert to an integer.\n+     * Use PyExc_OverflowError so that it is much like what\n+     * PyArg_ParseTuple with \"n\" would give.\n+     */\n+    obj_as_int = PyNumber_AsSsize_t(object, PyExc_OverflowError);\n+    if (obj_as_int == -1 && PyErr_Occurred()) {\n+        if (DEPRECATE_silence_error(\n+                \"invalid value to boolean argument. In the future boolean \"\n+                \"arguments are expected to be True, False or integers.\") < 0) {\n+            return NPY_FAIL;        \n+        }\n+        /*\n+         * Fall back to truthyness of object.\n+         */\n+        obj_as_bool = PyObject_IsTrue(object);\n+        if (obj_as_bool == -1) {\n+            return NPY_FAIL;\n+        }\n+        else if (obj_as_bool) {\n+            *val = 1;\n+        }\n+        else {\n+            *val = 0;\n+        }\n+        return NPY_SUCCEED;\n+    }\n+\n+    if (obj_as_int) {\n+        *val = 1;\n+    }\n+    else {\n+        *val = 0;\n     }\n     return NPY_SUCCEED;\n }\n \n+\n /*NUMPY_API\n  * Convert object to endian\n  */\n@@ -764,6 +807,84 @@ PyArray_CastingConverter(PyObject *obj, NPY_CASTING *casting)\n     return 0;\n }\n \n+\n+/*\n+ * Convert an object to a `copy` keyword argument accepting a boolean or\n+ * or the `np.never_copy` singleton.\n+ *\n+ * The function does not support NULL which is handled by PyArg_Parse*.\n+ */\n+NPY_NO_EXPORT int\n+PyArray_CopyConverter(PyObject *object, int *copyflag)\n+{\n+    int obj_as_bool;\n+    Py_ssize_t obj_as_int;\n+    static PyObject *never_copy_singleton = NULL;\n+\n+    /* Quickly check the most common cases: */\n+    if (object == Py_False) {\n+        *copyflag = 0;\n+        return NPY_SUCCEED;\n+    }\n+    if (object == Py_True) {\n+        *copyflag = NPY_ARRAY_ENSURECOPY;\n+        return NPY_SUCCEED;\n+    }\n+    /* Not Py_None as default, because bool(None) is False */\n+\n+    /*\n+     * Check for `np.never_copy` singleton.\n+     * at some point we could allow \"never\", but it requires long enough\n+     * deprecation in places where strings were accepted.\n+     */\n+    npy_cache_import(\"numpy\", \"never_copy\", &never_copy_singleton);\n+    if (never_copy_singleton == NULL) {\n+        return NPY_FAIL;\n+    }\n+    if (object == never_copy_singleton) {\n+        *copyflag = NPY_ARRAY_ENSURENOCOPY;\n+        return NPY_SUCCEED;\n+    }\n+\n+    /*\n+     * Use same code as PyArray_BoolConverter, but with a more exact\n+     * error message.\n+     */\n+    obj_as_int = PyNumber_AsSsize_t(object, PyExc_OverflowError);\n+    if (obj_as_int == -1 && PyErr_Occurred()) {\n+        if (DEPRECATE_silence_error(\n+                \"invalid value to `copy` argument. In the future the copy \"\n+                \"argument is expected to be True, False, `np.never_copy`, \"\n+                \"or an integer for compatibility.\") < 0) {\n+            return NPY_FAIL;        \n+        }\n+        /*\n+         * Fall back to truthyness of object.\n+         */\n+        obj_as_bool = PyObject_IsTrue(object);\n+        if (obj_as_bool == -1) {\n+            return NPY_FAIL;\n+        }\n+        else if (obj_as_bool) {\n+            *copyflag = NPY_ARRAY_ENSURECOPY;\n+        }\n+        else {\n+            *copyflag = 0;\n+        }\n+        return NPY_SUCCEED; \n+    }\n+\n+    if (obj_as_int) {\n+        *copyflag = NPY_ARRAY_ENSURECOPY;\n+    }\n+    else {\n+        *copyflag = 0;\n+    }\n+    return NPY_SUCCEED;\n+}\n+\n+\n+\n /*****************************\n * Other conversion functions\n *****************************/"
            },
            {
                "filename": "numpy/core/src/multiarray/conversion_utils.h",
                "patch": "@@ -12,6 +12,9 @@ PyArray_BufferConverter(PyObject *obj, PyArray_Chunk *buf);\n NPY_NO_EXPORT int\n PyArray_BoolConverter(PyObject *object, npy_bool *val);\n \n+NPY_NO_EXPORT int\n+PyArray_CopyConverter(PyObject *obj, int *copyflag);\n+\n NPY_NO_EXPORT int\n PyArray_ByteorderConverter(PyObject *obj, char *endian);\n "
            },
            {
                "filename": "numpy/core/src/multiarray/ctors.c",
                "patch": "@@ -1543,6 +1543,26 @@ PyArray_GetArrayParamsFromObject(PyObject *op,\n                         PyArray_Descr **out_dtype,\n                         int *out_ndim, npy_intp *out_dims,\n                         PyArrayObject **out_arr, PyObject *context)\n+{\n+    return PyArray_GetArrayParamsFromObject_int(\n+        op, requested_dtype, writeable, 0,\n+        out_dtype, out_ndim, out_dims, out_arr, context);\n+}\n+\n+/*\n+ * Same as PyArray_GetArrayParamsFromObject but accepts no_copy_allowed flag,\n+ * note that the actual usage of `writeable` also ensures this part (but\n+ * additional tests that the output array is actually `writeable`.\n+ */\n+NPY_NO_EXPORT int\n+PyArray_GetArrayParamsFromObject_int(\n+                        PyObject *op,\n+                        PyArray_Descr *requested_dtype,\n+                        npy_bool writeable,\n+                        npy_bool no_copy_allowed,\n+                        PyArray_Descr **out_dtype,\n+                        int *out_ndim, npy_intp *out_dims,\n+                        PyArrayObject **out_arr, PyObject *context)\n {\n     PyObject *tmp;\n \n@@ -1564,6 +1584,13 @@ PyArray_GetArrayParamsFromObject(PyObject *op,\n                                 \"cannot write to scalar\");\n             return -1;\n         }\n+        if (no_copy_allowed) {\n+            PyErr_SetString(PyExc_ValueError,\n+                \"never-copy was requested during array creation, but \"\n+                \"scalar cannot be viewed as arrays.\");\n+            return -1;\n+        }\n+\n         *out_dtype = PyArray_DescrFromScalar(op);\n         if (*out_dtype == NULL) {\n             return -1;\n@@ -1582,6 +1609,13 @@ PyArray_GetArrayParamsFromObject(PyObject *op,\n             Py_DECREF(*out_dtype);\n             return -1;\n         }\n+        if (no_copy_allowed) {\n+            Py_DECREF(*out_dtype);\n+            PyErr_SetString(PyExc_ValueError,\n+                    \"never-copy was requested during array creation, but \"\n+                    \"scalar cannot be viewed as arrays.\");\n+            return -1;\n+        }\n         *out_ndim = 0;\n         *out_arr = NULL;\n         return 0;\n@@ -1632,6 +1666,14 @@ PyArray_GetArrayParamsFromObject(PyObject *op,\n         return (*out_arr) == NULL ? -1 : 0;\n     }\n \n+    if (no_copy_allowed) {\n+        /* None of the following possibilities can provide no-copy sementics  */\n+        PyErr_SetString(PyExc_ValueError,\n+                \"never-copy was requested during array creation, but object \"\n+                \"cannot be directly interpreted as array.\");\n+        return -1;\n+    }\n+\n     /*\n      * If op supplies the __array__ function.\n      * The documentation says this should produce a copy, so\n@@ -1802,9 +1844,10 @@ PyArray_FromAny(PyObject *op, PyArray_Descr *newtype, int min_depth,\n     npy_intp dims[NPY_MAXDIMS];\n \n     /* Get either the array or its parameters if it isn't an array */\n-    if (PyArray_GetArrayParamsFromObject(op, newtype,\n-                        0, &dtype,\n-                        &ndim, dims, &arr, context) < 0) {\n+    if (PyArray_GetArrayParamsFromObject_int(\n+                        op, newtype,\n+                        0, (flags & NPY_ARRAY_ENSURENOCOPY) ? 1 : 0,\n+                        &dtype, &ndim, dims, &arr, context) < 0) {\n         Py_XDECREF(newtype);\n         return NULL;\n     }\n@@ -1818,6 +1861,8 @@ PyArray_FromAny(PyObject *op, PyArray_Descr *newtype, int min_depth,\n \n     /* If we got dimensions and dtype instead of an array */\n     if (arr == NULL) {\n+        assert((flags & NPY_ARRAY_ENSURENOCOPY) == 0);\n+\n         if ((flags & NPY_ARRAY_WRITEBACKIFCOPY) ||\n             (flags & NPY_ARRAY_UPDATEIFCOPY)) {\n             Py_XDECREF(newtype);\n@@ -1989,8 +2034,14 @@ PyArray_CheckFromAny(PyObject *op, PyArray_Descr *descr, int min_depth,\n         return NULL;\n     }\n     if ((requires & NPY_ARRAY_ELEMENTSTRIDES) &&\n-        !PyArray_ElementStrides(obj)) {\n+                    !PyArray_ElementStrides(obj)) {\n         PyObject *ret;\n+        if (requires & NPY_ARRAY_ENSURENOCOPY) {\n+            PyErr_SetString(PyExc_ValueError,\n+                \"array creation was requested with never-copy, but other \"\n+                \"requirements cannot be fullfilled without a copy.\");\n+            return NULL;\n+        }\n         ret = PyArray_NewCopy((PyArrayObject *)obj, NPY_ANYORDER);\n         Py_DECREF(obj);\n         obj = ret;\n@@ -2099,6 +2150,13 @@ PyArray_FromArray(PyArrayObject *arr, PyArray_Descr *newtype, int flags)\n         NPY_ORDER order = NPY_KEEPORDER;\n         int subok = 1;\n \n+        if (flags & NPY_ARRAY_ENSURENOCOPY) {\n+            PyErr_SetString(PyExc_ValueError,\n+                    \"no copy was allowed during array creation, but it \"\n+                    \"cannot be guaranteed.\");\n+            return NULL;\n+        }\n+\n         /* Set the order for the copy being made based on the flags */\n         if (flags & NPY_ARRAY_F_CONTIGUOUS) {\n             order = NPY_FORTRANORDER;"
            },
            {
                "filename": "numpy/core/src/multiarray/ctors.h",
                "patch": "@@ -21,6 +21,16 @@ PyArray_NewFromDescr_int(PyTypeObject *subtype, PyArray_Descr *descr, int nd,\n NPY_NO_EXPORT PyObject *PyArray_New(PyTypeObject *, int nd, npy_intp *,\n                              int, npy_intp *, void *, int, int, PyObject *);\n \n+NPY_NO_EXPORT int\n+PyArray_GetArrayParamsFromObject_int(\n+                        PyObject *op,\n+                        PyArray_Descr *requested_dtype,\n+                        npy_bool writeable,\n+                        npy_bool no_copy_allowed,\n+                        PyArray_Descr **out_dtype,\n+                        int *out_ndim, npy_intp *out_dims,\n+                        PyArrayObject **out_arr, PyObject *context);\n+\n NPY_NO_EXPORT PyObject *\n PyArray_FromAny(PyObject *op, PyArray_Descr *newtype, int min_depth,\n                 int max_depth, int flags, PyObject *context);"
            },
            {
                "filename": "numpy/core/src/multiarray/methods.c",
                "patch": "@@ -175,14 +175,16 @@ array_put(PyArrayObject *self, PyObject *args, PyObject *kwds)\n static PyObject *\n array_reshape(PyArrayObject *self, PyObject *args, PyObject *kwds)\n {\n-    static char *keywords[] = {\"order\", NULL};\n+    static char *keywords[] = {\"order\", \"copy\", NULL};\n     PyArray_Dims newshape;\n     PyObject *ret;\n+    int copyflag = 0;\n     NPY_ORDER order = NPY_CORDER;\n     Py_ssize_t n = PyTuple_Size(args);\n \n-    if (!NpyArg_ParseKeywords(kwds, \"|O&\", keywords,\n-                PyArray_OrderConverter, &order)) {\n+    if (!NpyArg_ParseKeywords(kwds, \"|O&O&\", keywords,\n+                PyArray_OrderConverter, &order,\n+                PyArray_CopyConverter, &copyflag)) {\n         return NULL;\n     }\n \n@@ -204,7 +206,7 @@ array_reshape(PyArrayObject *self, PyObject *args, PyObject *kwds)\n             goto fail;\n         }\n     }\n-    ret = PyArray_Newshape(self, &newshape, order);\n+    ret = PyArray_Newshape_int(self, &newshape, order, copyflag);\n     npy_free_cache_dim_obj(newshape);\n     return ret;\n \n@@ -788,14 +790,15 @@ array_astype(PyArrayObject *self, PyObject *args, PyObject *kwds)\n      */\n     NPY_CASTING casting = NPY_UNSAFE_CASTING;\n     NPY_ORDER order = NPY_KEEPORDER;\n-    int forcecopy = 1, subok = 1;\n+    int forcecopy = NPY_ARRAY_ENSURECOPY;\n+    int subok = 1;\n \n-    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O&|O&O&ii:astype\", kwlist,\n+    if (!PyArg_ParseTupleAndKeywords(args, kwds, \"O&|O&O&iO&:astype\", kwlist,\n                             PyArray_DescrConverter, &dtype,\n                             PyArray_OrderConverter, &order,\n                             PyArray_CastingConverter, &casting,\n                             &subok,\n-                            &forcecopy)) {\n+                            PyArray_CopyConverter, &forcecopy)) {\n         Py_XDECREF(dtype);\n         return NULL;\n     }\n@@ -805,7 +808,8 @@ array_astype(PyArrayObject *self, PyObject *args, PyObject *kwds)\n      * and it's not a subtype if subok is False, then we\n      * can skip the copy.\n      */\n-    if (!forcecopy && (order == NPY_KEEPORDER ||\n+    if (!(forcecopy & NPY_ARRAY_ENSURECOPY) &&\n+                    (order == NPY_KEEPORDER ||\n                        (order == NPY_ANYORDER &&\n                             (PyArray_IS_C_CONTIGUOUS(self) ||\n                             PyArray_IS_F_CONTIGUOUS(self))) ||\n@@ -822,6 +826,14 @@ array_astype(PyArrayObject *self, PyObject *args, PyObject *kwds)\n     else if (PyArray_CanCastArrayTo(self, dtype, casting)) {\n         PyArrayObject *ret;\n \n+        /* Only allow the copy cast if it was not inhibited. */\n+        if (forcecopy & NPY_ARRAY_ENSURENOCOPY) {\n+                PyErr_SetString(PyExc_ValueError,\n+                    \"cannot cast array without creating a copy, but \"\n+                    \"never-copy was requested.\");\n+                return NULL;\n+        }\n+\n         /* If the requested dtype is flexible, adapt it */\n         PyArray_AdaptFlexibleDType((PyObject *)self, PyArray_DESCR(self),\n                                                                     &dtype);"
            },
            {
                "filename": "numpy/core/src/multiarray/multiarraymodule.c",
                "patch": "@@ -1572,6 +1572,7 @@ _array_fromobject(PyObject *NPY_UNUSED(ignored), PyObject *args, PyObject *kws)\n     PyObject *op;\n     PyArrayObject *oparr = NULL, *ret = NULL;\n     npy_bool subok = NPY_FALSE;\n+    int copyflag = NPY_ARRAY_ENSURECOPY;\n     npy_bool copy = NPY_TRUE;\n     int ndmin = 0, nd;\n     PyArray_Descr *type = NULL;\n@@ -1657,12 +1658,13 @@ _array_fromobject(PyObject *NPY_UNUSED(ignored), PyObject *args, PyObject *kws)\n     if (!PyArg_ParseTupleAndKeywords(args, kws, \"O|O&O&O&O&i:array\", kwd,\n                 &op,\n                 PyArray_DescrConverter2, &type,\n-                PyArray_BoolConverter, &copy,\n+                PyArray_CopyConverter, &copyflag,\n                 PyArray_OrderConverter, &order,\n                 PyArray_BoolConverter, &subok,\n                 &ndmin)) {\n         goto clean_type;\n     }\n+    copy = copyflag & NPY_ARRAY_ENSURECOPY;\n \n     if (ndmin > NPY_MAXDIMS) {\n         PyErr_Format(PyExc_ValueError,\n@@ -1672,43 +1674,45 @@ _array_fromobject(PyObject *NPY_UNUSED(ignored), PyObject *args, PyObject *kws)\n     }\n     /* fast exit if simple call */\n     if ((subok && PyArray_Check(op)) ||\n-        (!subok && PyArray_CheckExact(op))) {\n+                (!subok && PyArray_CheckExact(op))) {\n         oparr = (PyArrayObject *)op;\n         if (type == NULL) {\n+            /* No dtype given so no casting is necessary. So check order. */\n             if (!copy && STRIDING_OK(oparr, order)) {\n                 ret = oparr;\n                 Py_INCREF(ret);\n                 goto finish;\n             }\n-            else {\n+            else if (!(copyflag & NPY_ARRAY_ENSURENOCOPY)) {\n                 ret = (PyArrayObject *)PyArray_NewCopy(oparr, order);\n                 goto finish;\n             }\n         }\n-        /* One more chance */\n-        oldtype = PyArray_DESCR(oparr);\n-        if (PyArray_EquivTypes(oldtype, type)) {\n-            if (!copy && STRIDING_OK(oparr, order)) {\n-                Py_INCREF(op);\n-                ret = oparr;\n-                goto finish;\n-            }\n-            else {\n-                ret = (PyArrayObject *)PyArray_NewCopy(oparr, order);\n-                if (oldtype == type || ret == NULL) {\n+        else {\n+            /* Repeat after checking that the dtype is equivalent. */\n+            oldtype = PyArray_DESCR(oparr);\n+            if (PyArray_EquivTypes(oldtype, type)) {\n+                if (!copy && STRIDING_OK(oparr, order)) {\n+                    Py_INCREF(op);\n+                    ret = oparr;\n+                    goto finish;\n+                }\n+                else if (!(copyflag & NPY_ARRAY_ENSURENOCOPY)) {\n+                    ret = (PyArrayObject *)PyArray_NewCopy(oparr, order);\n+                    if (oldtype == type || ret == NULL) {\n+                        goto finish;\n+                    }\n+                    Py_INCREF(oldtype);\n+                    Py_DECREF(PyArray_DESCR(ret));\n+                    ((PyArrayObject_fields *)ret)->descr = oldtype;\n                     goto finish;\n                 }\n-                Py_INCREF(oldtype);\n-                Py_DECREF(PyArray_DESCR(ret));\n-                ((PyArrayObject_fields *)ret)->descr = oldtype;\n-                goto finish;\n             }\n         }\n     }\n \n-    if (copy) {\n-        flags = NPY_ARRAY_ENSURECOPY;\n-    }\n+    flags = copyflag;\n+\n     if (order == NPY_CORDER) {\n         flags |= NPY_ARRAY_C_CONTIGUOUS;\n     }"
            },
            {
                "filename": "numpy/core/src/multiarray/shape.c",
                "patch": "@@ -179,18 +179,13 @@ PyArray_Resize(PyArrayObject *self, PyArray_Dims *newshape, int refcheck,\n }\n \n /*\n- * Returns a new array\n- * with the new shape from the data\n- * in the old array --- order-perspective depends on order argument.\n- * copy-only-if-necessary\n- */\n-\n-/*NUMPY_API\n- * New shape for an array\n+ * Internal function handling Array reshaping. Adds a copy flag to indicate\n+ * whether, copy=1 a copy is forced, copy=0 a copy is forbidden or copy=-1\n+ * either is allowed.\n  */\n NPY_NO_EXPORT PyObject *\n-PyArray_Newshape(PyArrayObject *self, PyArray_Dims *newdims,\n-                 NPY_ORDER order)\n+PyArray_Newshape_int(PyArrayObject *self, PyArray_Dims *newdims,\n+                     NPY_ORDER order, int copyflag)\n {\n     npy_intp i;\n     npy_intp *dimensions = newdims->ptr;\n@@ -200,6 +195,7 @@ PyArray_Newshape(PyArrayObject *self, PyArray_Dims *newdims,\n     npy_intp *strides = NULL;\n     npy_intp newstrides[NPY_MAXDIMS];\n     int flags;\n+    int do_nocopy_reshape;\n \n     if (order == NPY_ANYORDER) {\n         order = PyArray_ISFORTRAN(self);\n@@ -235,32 +231,49 @@ PyArray_Newshape(PyArrayObject *self, PyArray_Dims *newdims,\n      * in order to get the right orientation and\n      * because we can't just re-use the buffer with the\n      * data in the order it is in.\n-     * NPY_RELAXED_STRIDES_CHECKING: size check is unnecessary when set.\n      */\n-    Py_INCREF(self);\n-    if ((PyArray_SIZE(self) > 1) &&\n-        ((order == NPY_CORDER && !PyArray_IS_C_CONTIGUOUS(self)) ||\n-         (order == NPY_FORTRANORDER && !PyArray_IS_F_CONTIGUOUS(self)))) {\n-        int success = 0;\n-        success = _attempt_nocopy_reshape(self, ndim, dimensions,\n-                                          newstrides, order);\n-        if (success) {\n-            /* no need to copy the array after all */\n+    if (copyflag == NPY_ARRAY_ENSURECOPY) {\n+        /* force the copy no matter what */\n+        do_nocopy_reshape = 0;\n+    }\n+    else if ((PyArray_SIZE(self) <= 1) ||\n+             /* NPY_RELAXED_STRIDES_CHECKING makes size check unnecessary */\n+             ((order == NPY_CORDER && PyArray_IS_C_CONTIGUOUS(self)) ||\n+              (order == NPY_FORTRANORDER && PyArray_IS_F_CONTIGUOUS(self)))) {\n+        /* the array can be trivially reshaped */\n+        do_nocopy_reshape = 1;\n+    }\n+    else {\n+        do_nocopy_reshape = _attempt_nocopy_reshape(self, ndim, dimensions,\n+                                                    newstrides, order);\n+        if (do_nocopy_reshape) {\n+            /* no need to copy the array */\n             strides = newstrides;\n         }\n-        else {\n-            PyObject *newcopy;\n-            newcopy = PyArray_NewCopy(self, order);\n+    }\n+    Py_INCREF(self);\n+    if (!do_nocopy_reshape) {\n+        PyObject *newcopy;\n+        if (copyflag == NPY_ARRAY_ENSURENOCOPY) {\n+            PyErr_SetString(PyExc_ValueError,\n+                            \"a never-copy reshape was requested but is not \"\n+                            \"possible for the given array and new shape.\");\n             Py_DECREF(self);\n-            if (newcopy == NULL) {\n-                return NULL;\n-            }\n-            self = (PyArrayObject *)newcopy;\n+            return NULL;\n         }\n+        newcopy = PyArray_NewCopy(self, order);\n+        Py_DECREF(self);\n+        if (newcopy == NULL) {\n+            return NULL;\n+        }\n+        self = (PyArrayObject *)newcopy;\n     }\n-    /* We always have to interpret the contiguous buffer correctly */\n \n-    /* Make sure the flags argument is set. */\n+    /*\n+     * We have to interpret the contiguous buffer correctly (or use the\n+     * strides from _attempt_nocopy_reshape).\n+     * So, make sure the flags argument is set:\n+     */\n     flags = PyArray_FLAGS(self);\n     if (ndim > 1) {\n         if (order == NPY_FORTRANORDER) {\n@@ -284,6 +297,23 @@ PyArray_Newshape(PyArrayObject *self, PyArray_Dims *newdims,\n }\n \n \n+/*\n+ * Returns a new array\n+ * with the new shape from the data\n+ * in the old array --- order-perspective depends on order argument.\n+ * copy-only-if-necessary\n+ */\n+\n+/*NUMPY_API\n+ * New shape for an array\n+ */\n+NPY_NO_EXPORT PyObject *\n+PyArray_Newshape(PyArrayObject *self, PyArray_Dims *newdims,\n+                 NPY_ORDER order)\n+{\n+    return PyArray_Newshape_int(self, newdims, order, 0);\n+}\n+\n \n /* For backward compatibility -- Not recommended */\n "
            },
            {
                "filename": "numpy/core/src/multiarray/shape.h",
                "patch": "@@ -28,4 +28,10 @@ PyArray_CreateMultiSortedStridePerm(int narrays, PyArrayObject **arrays,\n NPY_NO_EXPORT PyObject *\n PyArray_SqueezeSelected(PyArrayObject *self, npy_bool *axis_flags);\n \n+/* Used internally to allow passing copy information */\n+PyObject *\n+PyArray_Newshape_int(PyArrayObject *self, PyArray_Dims *newdims,\n+                     NPY_ORDER order, int copyflag);\n+\n+\n #endif"
            },
            {
                "filename": "numpy/core/tests/test_api.py",
                "patch": "@@ -2,9 +2,12 @@\n \n import sys\n \n+import warnings\n+\n import numpy as np\n from numpy.testing import (\n-     assert_, assert_equal, assert_array_equal, assert_raises, HAS_REFCOUNT\n+    assert_, assert_equal, assert_array_equal, assert_raises, assert_warns,\n+    suppress_warnings, HAS_REFCOUNT,\n     )\n \n # Switch between new behaviour when NPY_RELAXED_STRIDES_CHECKING is set.\n@@ -289,6 +292,38 @@ class MyNDArray(np.ndarray):\n     a = np.array(1000, dtype='i4')\n     assert_raises(TypeError, a.astype, 'U1', casting='safe')\n \n+def test_astype_copyflag():\n+    # test the various copyflag options\n+    arr = np.arange(10, dtype=np.intp)\n+    res = arr.astype(np.intp, copy=True)\n+    assert not np.may_share_memory(arr, res)\n+    res = arr.astype(np.intp, copy=False)\n+    # `res is arr` currently, but check `may_share_memory`.\n+    assert np.may_share_memory(arr, res)\n+    res = arr.astype(np.intp, copy=np.never_copy)\n+    assert np.may_share_memory(arr, res)\n+\n+    # Simple tests for when a copy is necessary:\n+    res = arr.astype(np.float64, copy=False)\n+    assert_array_equal(res, arr)\n+    assert_raises(ValueError, arr.astype, np.float64, copy=np.never_copy)\n+\n+def test_astype_invalid_copyflag():\n+    arr = np.arange(10)\n+    # Deprecation if not an integer:\n+    # DEPRECATED: 2018-12-24, version 1.17.\n+    with suppress_warnings() as sup:\n+        rec = sup.record(DeprecationWarning)\n+        warnings.simplefilter(\"always\", DeprecationWarning)\n+        # Gives the warning, but later raises (not ideal, but a corner case)\n+        assert_raises(ValueError, arr.astype, np.intp, copy=np.arange(5))\n+        # Only gives the warning:\n+        arr.astype(np.intp, copy=1.)\n+        assert len(rec) == 2\n+\n+    warnings.simplefilter(\"error\", DeprecationWarning)\n+    assert_raises(DeprecationWarning, arr.astype, np.intp, copy=1.)\n+\n def test_copyto_fromscalar():\n     a = np.arange(6, dtype='f4').reshape(2, 3)\n "
            },
            {
                "filename": "numpy/core/tests/test_multiarray.py",
                "patch": "@@ -6965,6 +6965,158 @@ class foo(ctypes.Structure):\n         assert_equal(arr['a'], 3)\n \n \n+class TestArrayCreationCopyArgument(object):\n+    def test_scalars(self):\n+        # Test both numpy and python scalars\n+        for dtype in np.typecodes[\"All\"]:\n+            arr = np.zeros((), dtype=dtype)\n+            scalar = arr[()]\n+            pyscalar = arr.item(0)\n+\n+            # Test never-copy raises error:\n+            assert_raises(ValueError, np.array, scalar, copy=np.never_copy)\n+            assert_raises(ValueError, np.array, pyscalar, copy=np.never_copy)\n+\n+    def test_compatible_cast(self):\n+        # Some types are compatible even though they are different, no\n+        # copy is necessary for them. This is mostly true for some integers\n+        def int_types(byteswap=False):\n+            int_types = (np.typecodes[\"Integer\"] +\n+                         np.typecodes[\"UnsignedInteger\"])\n+            for int_type in int_types:\n+                yield np.dtype(int_type)\n+                if byteswap:\n+                    yield np.dtype(int_type).newbyteorder()\n+\n+        for int1 in int_types():\n+            for int2 in int_types(True):\n+                arr = np.arange(10, dtype=int1)\n+\n+                res = np.array(arr, copy=True, dtype=int2)\n+                assert res is not arr and res.flags.owndata\n+                assert_array_equal(res, arr)\n+\n+                if int1 == int2:\n+                    # Casting is not necessary, base check is sufficient here\n+                    res = np.array(arr, copy=False, dtype=int2)\n+                    assert res is arr or res.base is arr\n+\n+                    res = np.array(arr, copy=np.never_copy, dtype=int2)\n+                    assert res is arr or res.base is arr\n+\n+                else:\n+                    # Casting is necessary, assert copy works:\n+                    res = np.array(arr, copy=False, dtype=int2)\n+                    assert res is not arr and res.flags.owndata\n+                    assert_array_equal(res, arr)\n+\n+                    assert_raises(ValueError, np.array,\n+                                  arr, copy=np.never_copy, dtype=int2)\n+\n+    def test_buffer_interface(self):\n+        # Buffer interface gives direct memory access (no copy)\n+        arr = np.arange(10)\n+        view = memoryview(arr)\n+\n+        # Checking bases is a bit tricky since numpy creates another\n+        # memoryview, so use may_share_memory.\n+        res = np.array(view, copy=True)\n+        assert not np.may_share_memory(arr, res)\n+        res = np.array(view, copy=False)\n+        assert np.may_share_memory(arr, res)\n+        res = np.array(view, copy=np.never_copy)\n+        assert np.may_share_memory(arr, res)\n+\n+    def test_array_interfaces(self):\n+        # Array interface gives direct memory access (much like a memoryview)\n+        base_arr = np.arange(10)\n+\n+        class ArrayLike:\n+            __array_interface__ = base_arr.__array_interface__\n+\n+        arr = ArrayLike()\n+\n+        res = np.array(arr, copy=True)\n+        assert res.base is None\n+        res = np.array(arr, copy=False)\n+        assert res.base is arr\n+        res = np.array(arr, copy=np.never_copy)\n+        assert res.base is arr\n+\n+    def test___array__(self):\n+        base_arr = np.arange(10)\n+\n+        class ArrayLike:\n+            def __array__(self):\n+                # __array__ should return a copy, numpy cannot know this\n+                # however.\n+                return base_arr\n+\n+        arr = ArrayLike()\n+\n+        res = np.array(arr, copy=True)\n+        assert_array_equal(res, base_arr)\n+        # An additional copy is currently forced by numpy in this case,\n+        # you could argue, numpy does not trust the ArrayLike. This\n+        # may be open for change:\n+        assert res is not base_arr\n+\n+        res = np.array(arr, copy=False)\n+        assert_array_equal(res, base_arr)\n+        assert res is base_arr  # numpy trusts the ArrayLike\n+\n+        assert_raises(ValueError, np.array, arr, copy=np.never_copy)\n+\n+    @pytest.mark.parametrize(\n+            \"arr\", [np.ones(()), np.arange(81).reshape((9, 9))])\n+    @pytest.mark.parametrize(\"order1\", [\"C\", \"F\", None])\n+    @pytest.mark.parametrize(\"order2\", [\"C\", \"F\", \"A\", \"K\"])\n+    def test_order_mismatch(self, arr, order1, order2):\n+        # The order is the main (python side) reason that can cause\n+        # a never-copy to fail.\n+        # Prepare C-order, F-order and non-contiguous arrays:\n+        arr = arr.copy(order1)\n+        if order1 == \"C\":\n+            assert arr.flags.c_contiguous\n+        elif order1 == \"F\":\n+            assert arr.flags.f_contiguous\n+        elif arr.ndim != 0:\n+            # Make array non-contiguous\n+            arr = arr[::2, ::2]\n+            assert not arr.flags.forc\n+\n+        # Whether a copy is necessary depends on the order of arr:\n+        if order2 == \"C\":\n+            no_copy_necessary = arr.flags.c_contiguous\n+        elif order2 == \"F\":\n+            no_copy_necessary = arr.flags.f_contiguous\n+        else:\n+            # Keeporder and Anyorder are OK with non-contiguous output.\n+            # This is not consistent with the `astype` behaviour which\n+            # enforces contiguity for \"A\". It is probably historic from when\n+            # \"K\" did not exist.\n+            no_copy_necessary = True\n+\n+        # Test it for both the array and a memoryview\n+        for view in [arr, memoryview(arr)]:\n+            res = np.array(view, copy=True, order=order2)\n+            assert res is not arr and res.flags.owndata\n+            assert_array_equal(arr, res)\n+\n+            if no_copy_necessary:\n+                res = np.array(view, copy=False, order=order2)\n+                # res.base.obj refers to the memoryview\n+                assert res is arr or res.base.obj is arr\n+\n+                res = np.array(view, copy=np.never_copy, order=order2)\n+                assert res is arr or res.base.obj is arr\n+            else:\n+                res = np.array(arr, copy=False, order=order2)\n+                assert_array_equal(arr, res)\n+                assert_raises(ValueError, np.array,\n+                              view, copy=np.never_copy, order=order2)\n+\n+\n class TestArrayAttributeDeletion(object):\n \n     def test_multiarray_writable_attributes_deletion(self):"
            },
            {
                "filename": "numpy/core/tests/test_numeric.py",
                "patch": "@@ -49,6 +49,54 @@ def test_reshape_from_zero(self):\n         assert_equal(A.dtype, Ar.dtype)\n \n \n+class TestReshape(object):\n+    def test_copyflag_order_match(self):\n+        # Allow a copy, but try different array and reshape orders.\n+        for order in ['C', 'F', 'A', None]:\n+            if order is not None:\n+                order = {'order': order}\n+            else:\n+                order = dict()\n+            arr = np.zeros((5, 9)).copy(**order)\n+            # Test the most basic variants:\n+            res = arr.reshape(3, 5, 3, **order)\n+            assert_(res.base is arr)\n+            res = arr.reshape(3, 5, 3, copy=False, **order)\n+            assert_(res.base is arr)\n+            res = arr.reshape(3, 5, 3, copy=np.never_copy, **order)\n+            assert_(res.base is arr)\n+            res = arr.reshape(3, 5, 3, copy=True, **order)\n+            assert_(not np.may_share_memory(res.base, arr))\n+\n+            # And the alternative path:\n+            res = arr.reshape((3, 5, 3), copy=True, **order)\n+            assert_(not np.may_share_memory(res.base, arr))\n+\n+    def test_copyflag_order_mismatch(self):\n+        # Make sure that a copy has to be made due to order mismatch.\n+        for creation_order, order in zip(['F', 'C'], ['C', 'F']):\n+            arr = np.zeros((5, 9), order=creation_order)\n+\n+            res = arr.reshape(3, 5, 3, order=order)\n+            assert_(not np.may_share_memory(res.base, arr))\n+            res = arr.reshape(3, 5, 3, copy=True, order=order)\n+            assert_(not np.may_share_memory(res.base, arr))\n+            assert_raises(ValueError,\n+                          arr.reshape, 3, 5, 3, copy=np.never_copy, order=order)\n+            res = arr.reshape(3, 5, 3, copy=True)\n+            assert_(not np.may_share_memory(res.base, arr))\n+\n+    def test_copyflag_error(self):\n+        # Test the error parse when parsing the copy kwarg:\n+        # This used to error, but is currently deprecated to simplify C-code\n+        # and behave the same as `np.array`.\n+        arr = np.zeros(5)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\", DeprecationWarning)\n+            assert_raises(DeprecationWarning,\n+                          arr.reshape, 5, copy=np.arange(10))\n+\n+\n class TestNonarrayArgs(object):\n     # check that non-array arguments to functions wrap them in arrays\n     def test_choose(self):"
            }
        ]
    },
    {
        "repo": "numpy/numpy",
        "pr_number": 20830,
        "body": "Hello Numpy developers,\r\n\r\nHere is a pull request for the exponential distribution. We are submitting this PR mostly out of curiosity, to understand if there is any particular reason behind an implementation detail.\r\n\r\nThe current implementation of the exponential distribution relies on the Ziggurat algorithm for the body of the distribution. However, the tail (which is still an exponential) is instead sampled using the inverse of the CDF method, which involves calculating a logarithm. If we understand correctly, the whole point of using the Ziggurat is to avoid not only the expensive log calls, but also the precision loss and the undesired truncation which arise when using a log function to map the 0-1 interval to an infinite or semi-infinite range.\r\n\r\nTherefore, rather than calling `-npy_log1p(-next_double(bitgen_state))`, we can simply call `random_stantard_exponential()` whenever a random exponential variate is needed. Replacing these calls should not only make the code more readable, but also improve performance and precision. This is the approach found, for instance, in boost random for the [exponential](https://github.com/boostorg/random/blob/a2740d4b30178cb187fabca163e5be7803a577b9/include/boost/random/exponential_distribution.hpp#L193) and for the [normal](https://github.com/boostorg/random/blob/a2740d4b30178cb187fabca163e5be7803a577b9/include/boost/random/normal_distribution.hpp#L188) distributions.\r\n\r\nIs there a particular reason for not calling `random_stantard_exponential()` in the tails of the exponential and normal distributions? If not, the first commit replaces `-npy_log1p(-next_double(bitgen_state))` with a call to `random_stantard_exponential()`.\r\n\r\nThe second commit is just a refactoring of `random_stantard_exponential()` to avoid recursion. This, in our opinion, makes the code more readable.\r\n\r\nWe are looking forward to your feedback and hope that these observations may be useful!\r\n\r\nGiacomo Mazzamuto, Lorenzo Pattelli\r\n\r\nPS: the style of that particualr file was followed",
        "changed_files": [
            {
                "filename": "numpy/random/src/distributions/distributions.c",
                "patch": "@@ -46,8 +46,8 @@ void random_standard_uniform_fill_f(bitgen_t *bitgen_state, npy_intp cnt, float\n static double standard_exponential_unlikely(bitgen_t *bitgen_state,\n                                                 uint8_t idx, double x) {\n   if (idx == 0) {\n-    /* Switch to 1.0 - U to avoid log(0.0), see GH 13361 */\n-    return ziggurat_exp_r - npy_log1p(-next_double(bitgen_state));\n+    /* The tail of the exponential looks exactly like its body */\n+    return ziggurat_exp_r + random_standard_exponential(bitgen_state);\n   } else if ((fe_double[idx - 1] - fe_double[idx]) * next_double(bitgen_state) +\n                  fe_double[idx] <\n              exp(-x)) {\n@@ -83,8 +83,8 @@ void random_standard_exponential_fill(bitgen_t * bitgen_state, npy_intp cnt, dou\n static float standard_exponential_unlikely_f(bitgen_t *bitgen_state,\n                                                  uint8_t idx, float x) {\n   if (idx == 0) {\n-    /* Switch to 1.0 - U to avoid log(0.0), see GH 13361 */\n-    return ziggurat_exp_r_f - npy_log1pf(-next_float(bitgen_state));\n+    /* The tail of the exponential looks exactly like its body */\n+    return ziggurat_exp_r_f + random_standard_exponential_f(bitgen_state);\n   } else if ((fe_float[idx - 1] - fe_float[idx]) * next_float(bitgen_state) +\n                  fe_float[idx] <\n              expf(-x)) {\n@@ -154,9 +154,8 @@ double random_standard_normal(bitgen_t *bitgen_state) {\n       return x; /* 99.3% of the time return here */\n     if (idx == 0) {\n       for (;;) {\n-        /* Switch to 1.0 - U to avoid log(0.0), see GH 13361 */\n-        xx = -ziggurat_nor_inv_r * npy_log1p(-next_double(bitgen_state));\n-        yy = -npy_log1p(-next_double(bitgen_state));\n+        xx = ziggurat_nor_inv_r * random_standard_exponential(bitgen_state);\n+        yy = random_standard_exponential(bitgen_state);\n         if (yy + yy > xx * xx)\n           return ((rabs >> 8) & 0x1) ? -(ziggurat_nor_r + xx)\n                                      : ziggurat_nor_r + xx;\n@@ -195,9 +194,8 @@ float random_standard_normal_f(bitgen_t *bitgen_state) {\n       return x; /* # 99.3% of the time return here */\n     if (idx == 0) {\n       for (;;) {\n-        /* Switch to 1.0 - U to avoid log(0.0), see GH 13361 */\n-        xx = -ziggurat_nor_inv_r_f * npy_log1pf(-next_float(bitgen_state));\n-        yy = -npy_log1pf(-next_float(bitgen_state));\n+        xx = ziggurat_nor_inv_r_f * random_standard_exponential_f(bitgen_state);\n+        yy = random_standard_exponential_f(bitgen_state);\n         if (yy + yy > xx * xx)\n           return ((rabs >> 8) & 0x1) ? -(ziggurat_nor_r_f + xx)\n                                      : ziggurat_nor_r_f + xx;"
            }
        ]
    }
]